[
 {
  "task_id": "RWPB/109",
  "prompt": "import torch\n\ndef rot_trans_geo(geometry, rot, trans):\n    \"\"\"\n    Applies rotation and translation to geometry.\n\n    Args:\n    geometry (torch.Tensor): Tensor of shape [batch_size, num_points, 3] representing the geometry points.\n    rot (torch.Tensor): Rotation matrix of shape [batch_size, 3, 3].\n    trans (torch.Tensor): Translation vector of shape [batch_size, 3].\n\n    Returns:\n    torch.Tensor: Transformed geometry of shape [batch_size, num_points, 3].\n    \"\"\"\n",
  "solution": "    return torch.bmm(geometry, rot.transpose(1, 2)) + trans.unsqueeze(1)\n\n\n",
  "canonical_solution": "    \n    rott_geo = torch.bmm(rot, geometry.permute(0, 2, 1)) + trans.view(-1, 3, 1)\n    return rott_geo.permute(0, 2, 1)\n\n\n",
  "unprocess_testcases": "geometry = torch.tensor([[[1., 2., 3.],\n                          [4., 5., 6.]]])\nrot = torch.eye(3).unsqueeze(0)  # Identity rotation for 1 batch\ntrans = torch.zeros(1, 3)        # Zero translation for 1 batch\nassert((SOLUTION_SIGNATURE(geometry, rot, trans) == rot_trans_geo(geometry, rot, trans)).all())\n\ngeometry = torch.tensor([[[1., 0., 0.],\n                          [0., 1., 0.]],\n                         [[0., 0., 1.],\n                          [1., 1., 1.]]])\nrot = torch.tensor([[[0., -1., 0.], [1., 0., 0.], [0., 0., 1.]],  # 90 degrees on Z-axis\n                    [[-1., 0., 0.], [0., 1., 0.], [0., 0., -1.]]]) # 180 degrees on Y-axis\ntrans = torch.tensor([[1., 2., 3.],\n                      [0., -1., 2.]])\nassert((SOLUTION_SIGNATURE(geometry, rot, trans) == rot_trans_geo(geometry, rot, trans)).all())\n\ngeometry = torch.tensor([[[0., 0., 0.],\n                          [999., 999., 999.]]])\nrot = torch.tensor([[[0., 1., 0.], [-1., 0., 0.], [0., 0., 1.]]])  # 90 degrees on X-axis\ntrans = torch.tensor([[1000., 2000., 3000.]])\nassert((SOLUTION_SIGNATURE(geometry, rot, trans) == rot_trans_geo(geometry, rot, trans)).all())\n"
 },
 {
  "task_id": "RWPB/54",
  "prompt": "def get_crop_box(box, expand):\n    \"\"\"\n    Expands a given bounding box by a specified factor, keeping the center of the box the same.\n\n    Arguments:\n    box : list or tuple\n        The original bounding box specified as (x, y, x1, y1) where (x, y) is the top-left corner and\n        (x1, y1) is the bottom-right corner.\n    expand : float\n        The factor by which to expand the bounding box. For example, a factor of 1.5 will increase\n        the size of the box by 50%.\n\n    Returns:\n    tuple\n        A tuple containing the new expanded bounding box as a list [new_x, new_y, new_x1, new_y1] and\n        the half-size of the new box `s`.\n    \"\"\"\n",
  "solution": "\n\n\ndef get_crop_box(box, expand):\n    \"\"\"\n    Expands a given bounding box by a specified factor, keeping the center of the box the same.\n\n    Arguments:\n    box : list or tuple\n        The original bounding box specified as (x, y, x1, y1) where (x, y) is the top-left corner and\n        (x1, y1) is the bottom-right corner.\n    expand : float\n        The factor by which to expand the bounding box. For example, a factor of 1.5 will increase\n        the size of the box by 50%.\n\n    Returns:\n    tuple\n        A tuple containing the new expanded bounding box as a list [new_x, new_y, new_x1, new_y1] and\n        the half-size of the new box `s`.\n    \"\"\"\n    x, y, x1, y1 = box\n    w = x1 - x\n    h = y1 - y\n    s = int(max(w, h) * expand)\n    new_x = x - (s - w) // 2\n    new_y = y - (s - h) // 2\n    new_x1 = new_x + s\n    new_y1 = new_y + s\n    return [new_x, new_y, new_x1, new_y1], s\n\n\n",
  "canonical_solution": "    \n    # Extract the coordinates from the original bounding box\n    x, y, x1, y1 = box\n    # Calculate the center of the original box\n    x_c, y_c = (x + x1) // 2, (y + y1) // 2\n    # Calculate width and height of the original box\n    w, h = x1 - x, y1 - y\n    # Calculate half of the size of the expanded box\n    s = int(max(w, h) // 2 * expand)\n    # Define the new expanded bounding box based on the center and size\n    crop_box = [x_c - s, y_c - s, x_c + s, y_c + s]\n\n    return crop_box, s\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(box = (100, 100, 200, 200), expand = 1.5)[0] == get_crop_box(box = (100, 100, 200, 200), expand = 1.5)[0])\nassert(SOLUTION_SIGNATURE(box = (50, 50, 150, 150), expand = 1.0)[0] == get_crop_box(box = (50, 50, 150, 150), expand = 1.0)[0])\nassert(SOLUTION_SIGNATURE(box = (-100, -100, 100, 100), expand = 2.0)[0] == get_crop_box(box = (-100, -100, 100, 100), expand = 2.0)[0])\nassert(SOLUTION_SIGNATURE(box = (100, 100, 200, 200), expand = 1.5)[1] == get_crop_box(box = (100, 100, 200, 200), expand = 1.5)[1])\nassert(SOLUTION_SIGNATURE(box = (50, 50, 150, 150), expand = 1.0)[1] == get_crop_box(box = (50, 50, 150, 150), expand = 1.0)[1])\nassert(SOLUTION_SIGNATURE(box = (-100, -100, 100, 100), expand = 2.0)[1] == get_crop_box(box = (-100, -100, 100, 100), expand = 2.0)[1])\n"
 },
 {
  "task_id": "RWPB/93",
  "prompt": "MAX_VAL = (1 << 256) - 1\n\n# does not revert on overflow\ndef unsafeSub(x, y):\n    \"\"\"\n    Subtract two integers, ignoring overflow.\n    Args:\n        - x: int, the minuend\n        - y: int, the subtrahend\n    Returns:\n        result: int, the difference of x and y, with overflow ignored\n    \"\"\"\n",
  "solution": "\nassert unsafeSub(10, 5) == 5\nassert unsafeSub(10, 15) == MAX_VAL - 5\nassert unsafeSub(MAX_VAL, 1) == MAX_VAL - 1\n",
  "canonical_solution": "    \n    return (x - y) & MAX_VAL\n\n\n",
  "unprocess_testcases": "\nx = 100\ny = 50\nassert(SOLUTION_SIGNATURE(x, y) == unsafeSub(x, y))\n\nx = 0\ny = 100\nassert(SOLUTION_SIGNATURE(x, y) == unsafeSub(x, y))\n\nx = (1 << 256) - 1\ny = 1\nassert(SOLUTION_SIGNATURE(x, y) == unsafeSub(x, y))\n"
 },
 {
  "task_id": "RWPB/60",
  "prompt": "import torch\n\n\ndef rms_norm_ref(x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False):\n    \"\"\"\n    Applies Residual Multi-scale Normalization (RMSNorm) to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        The input tensor.\n    weight : torch.Tensor\n        The weight tensor for scaling the normalized input.\n    bias : torch.Tensor or None\n        The bias tensor for shifting the normalized input. If None, no bias is added.\n    residual : torch.Tensor or None\n        The residual tensor to be added to the input. If None, no residual is added.\n    eps : float, optional\n        A small constant added for numerical stability.\n    prenorm : bool, optional\n        If True, the function returns both the normalized output and the original input.\n    upcast : bool, optional\n        If True, upcasts all tensors to float type for computation.\n\n    Returns\n    -------\n    torch.Tensor or tuple of torch.Tensor\n        The RMS normalized output tensor. If `prenorm` is True, returns a tuple\n        containing the output tensor and the original input tensor.\n\n    Notes\n    -----\n    RMSNorm is a normalization technique that scales the input by the inverse square root\n    of the mean of squares. This function performs the normalization, applies the weight\n    and bias, and optionally adds a residual connection. If `upcast` is True, the\n    function upcasts all tensors to float type before performing the operations to\n    maintain precision.\n    \"\"\"\n",
  "solution": "\n\n\ndef rms_norm_ref(x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False):\n    \"\"\"\n    Applies Residual Multi-scale Normalization (RMSNorm) to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        The input tensor.\n    weight : torch.Tensor\n        The weight tensor for scaling the normalized input.\n    bias : torch.Tensor or None\n        The bias tensor for shifting the normalized input. If None, no bias is added.\n    residual : torch.Tensor or None\n        The residual tensor to be added to the input. If None, no residual is added.\n    eps : float, optional\n        A small constant added for numerical stability.\n    prenorm : bool, optional\n        If True, the function returns both the normalized output and the original input.\n    upcast : bool, optional\n        If True, upcasts all tensors to float type for computation.\n\n    Returns\n    -------\n    torch.Tensor or tuple of torch.Tensor\n        The RMS normalized output tensor. If `prenorm` is True, returns a tuple\n        containing the output tensor and the original input tensor.\n\n    Notes\n    -----\n    RMSNorm is a normalization technique that scales the input by the inverse square root\n    of the mean of squares. This function performs the normalization, applies the weight\n    and bias, and optionally adds a residual connection. If `upcast` is True, the\n    function upcasts all tensors to float type before performing the operations to\n    maintain precision.\n    \"\"\"\n    if upcast:\n        x = x.float()\n        weight = weight.float()\n        if bias is not None:\n            bias = bias.float()\n        if residual is not None:\n            residual = residual.float()\n    mean_squares = (x**2).mean()\n    std = mean_squares.sqrt() + eps\n    x_norm = x / std\n    x_norm = weight * x_norm\n    if bias is not None:\n        x_norm = x_norm + bias\n    if residual is not None:\n        x_norm = x_norm + residual\n    if prenorm:\n        return x_norm, x\n    else:\n        return x_norm\n\n\n",
  "canonical_solution": "\n    # Preserve the original dtype\n    dtype = x.dtype\n\n    # Upcast weight, bias, and x to float if necessary\n    if upcast:\n        weight = weight.float()\n        bias = bias.float() if bias is not None else None\n        x = x.float()\n        residual = residual.float() if residual is not None else residual\n\n    # Add residual to input if provided\n    if residual is not None:\n        x = (x + residual).to(x.dtype)\n\n    # Compute the reciprocal of the standard deviation of the input\n    rstd = 1 / torch.sqrt((x.square()).mean(dim=-1, keepdim=True) + eps)\n\n    # Apply normalization, weight, and bias\n    out = (x * rstd * weight) + bias if bias is not None else (x * rstd * weight)\n\n    # Cast the output back to the original dtype\n    out = out.to(dtype)\n\n    # Return the output only or the output with the original input if prenorm is True\n    return out if not prenorm else (out, x)\n\n\n",
  "unprocess_testcases": "a = torch.rand(10, 5)\nb = torch.rand(5,)\n\nassert((SOLUTION_SIGNATURE(a, b, b, None) == rms_norm_ref(a, b, b, None)).all())\nassert((SOLUTION_SIGNATURE(a, b, b, None, 1e-5, False, True) == rms_norm_ref(a, b, b, None, 1e-5, False, True)).all())\nassert((SOLUTION_SIGNATURE(a, b, b, a) == rms_norm_ref(a, b, b, a)).all())\nassert((SOLUTION_SIGNATURE(a, b, None, b, 1e-6, True, False)[0] == rms_norm_ref(a, b, None, b, 1e-6, True, False)[0]).all())\nassert((SOLUTION_SIGNATURE(a, b, None, b, 1e-6, True, False)[1] == rms_norm_ref(a, b, None, b, 1e-6, True, False)[1]).all())\nassert((SOLUTION_SIGNATURE(a, b, None) == rms_norm_ref(a, b, None)).all())\n"
 },
 {
  "task_id": "RWPB/31",
  "prompt": "import torch\nimport math\n\ndef resize_to_batch_size(tensor, batch_size):\n    \"\"\"\n    Adjusts the batch size of a tensor to the specified size, interpolating if necessary.\n\n    Args:\n        tensor (torch.Tensor): Input tensor whose batch size needs to be adjusted.\n        batch_size (int): The desired batch size after adjustment.\n\n    Returns:\n        torch.Tensor: The tensor with its batch size adjusted to the specified size.\n\n    This function handles resizing of the batch dimension of a tensor. If the current batch size matches\n    the desired batch size, the tensor is returned unchanged. For reducing the batch size, it uses linear\n    interpolation to select indices, ensuring that the resized tensor covers the whole original batch as\n    evenly as possible. For increasing the batch size, it applies a different interpolation method to avoid\n    extrapolation and to ensure that every element from the original batch is represented.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\nimport math\n\ndef resize_to_batch_size(tensor, batch_size):\n    \"\"\"\n    Adjusts the batch size of a tensor to the specified size, interpolating if necessary.\n\n    Args:\n        tensor (torch.Tensor): Input tensor whose batch size needs to be adjusted.\n        batch_size (int): The desired batch size after adjustment.\n\n    Returns:\n        torch.Tensor: The tensor with its batch size adjusted to the specified size.\n\n    This function handles resizing of the batch dimension of a tensor. If the current batch size matches\n    the desired batch size, the tensor is returned unchanged. For reducing the batch size, it uses linear\n    interpolation to select indices, ensuring that the resized tensor covers the whole original batch as\n    evenly as possible. For increasing the batch size, it applies a different interpolation method to avoid\n    extrapolation and to ensure that every element from the original batch is represented.\n    \"\"\"\n    current_batch_size = tensor.shape[0]\n    if current_batch_size == batch_size:\n        return tensor\n    elif current_batch_size < batch_size:\n        new_indices = torch.linspace(0, current_batch_size - 1, batch_size)\n        new_indices = torch.floor(new_indices).long()\n        return tensor[new_indices]\n    else:\n        new_indices = torch.linspace(0, current_batch_size - 1, batch_size)\n        new_indices = torch.round(new_indices).long()\n        return tensor[new_indices]\n\n",
  "canonical_solution": "    \n    # Retrieve the initial batch size from the tensor\n    in_batch_size = tensor.shape[0]\n\n    # Return the tensor unchanged if the current batch size is already the desired size\n    if in_batch_size == batch_size:\n        return tensor\n\n    # Return only the required portion of the tensor if the desired batch size is less than or equal to 1\n    if batch_size <= 1:\n        return tensor[:batch_size]\n\n    # Create an empty tensor to store the output, maintaining the original data type and device\n    output = torch.empty([batch_size] + list(tensor.shape[1:]), dtype=tensor.dtype, device=tensor.device)\n\n    # Handle reduction of batch size\n    if batch_size < in_batch_size:\n        scale = (in_batch_size - 1) / (batch_size - 1)\n        for i in range(batch_size):\n            output[i] = tensor[min(round(i * scale), in_batch_size - 1)]\n\n    # Handle increase in batch size\n    else:\n        scale = in_batch_size / batch_size\n        for i in range(batch_size):\n            index = min(math.floor((i + 0.5) * scale), in_batch_size - 1)\n            output[i] = tensor[index]\n\n    return output\n\n\n",
  "unprocess_testcases": "a = torch.randn(5, 3, 3)\nb = torch.randn(10, 4, 4)\nc = torch.randn(3, 2, 2)\n\nassert((SOLUTION_SIGNATURE(a, 5) == resize_to_batch_size(a, 5)).all())\nassert((SOLUTION_SIGNATURE(b, 3) == resize_to_batch_size(b, 3)).all())\nassert((SOLUTION_SIGNATURE(c, 8) == resize_to_batch_size(c, 8)).all())\n"
 },
 {
  "task_id": "RWPB/21",
  "prompt": "import re\n\ndef validate_user_id(user_id):\n    \"\"\"\n    Validates a user ID against a specific pattern.\n\n    Parameters\n    ----------\n    user_id : str\n        The user ID to validate.\n\n    Returns\n    -------\n    bool\n        True if the user ID is valid, False otherwise.\n\n    Notes\n    -----\n    The function checks if the user ID is a non-empty string that:\n    - Starts with a letter (uppercase or lowercase).\n    - Followed by any number of alphanumeric characters or underscores.\n\n    The validation is performed using a regular expression (regex) pattern.\n    \"\"\"\n",
  "solution": "\nimport re\n\ndef validate_user_id(user_id):\n    \"\"\"\n    Validates a user ID against a specific pattern.\n\n    Parameters\n    ----------\n    user_id : str\n        The user ID to validate.\n\n    Returns\n    -------\n    bool\n        True if the user ID is valid, False otherwise.\n\n    Notes\n    -----\n    The function checks if the user ID is a non-empty string that:\n    - Starts with a letter (uppercase or lowercase).\n    - Followed by any number of alphanumeric characters or underscores.\n\n    The validation is performed using a regular expression (regex) pattern.\n    \"\"\"\n    pattern = r'^[a-zA-Z][a-zA-Z0-9_]*$'\n    return bool(re.match(pattern, user_id))\n",
  "canonical_solution": "    \n    # Define the regex pattern for a valid user ID\n    pattern = r'^[A-Za-z][A-Za-z0-9_]*$'\n\n    # Check if the user_id matches the pattern and is an instance of str\n    if isinstance(user_id, str) and re.match(pattern, user_id):\n        return True\n    else:\n        return False\n    \n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(\"A1_bcd\") == validate_user_id(\"A1_bcd\"))\nassert(SOLUTION_SIGNATURE(\"Z\") == validate_user_id(\"Z\"))\nassert(SOLUTION_SIGNATURE(\"123abc\") == validate_user_id(\"123abc\"))\n"
 },
 {
  "task_id": "RWPB/70",
  "prompt": "import numpy as np\n\ndef psnr_to_mse(psnr):\n  \"\"\"\n  Compute the Mean Squared Error (MSE) given the Peak Signal-to-Noise Ratio (PSNR).\n\n  Parameters:\n  psnr (float): The Peak Signal-to-Noise Ratio, typically in decibels (dB).\n\n  Returns:\n  float: The computed Mean Squared Error (MSE).\n\n  Process:\n  - The function assumes the maximum pixel value is 1.\n  - PSNR is converted to a linear scale using the formula:\n    MSE = exp(-0.1 * ln(10) * PSNR)\n  - This conversion is based on the logarithmic relationship between PSNR and MSE.\n  - The constant -0.1 * ln(10) transforms the PSNR value into a scale where \n    the exponentiation gives the MSE.\n  \"\"\"\n",
  "solution": "\nimport numpy as np\n\ndef psnr_to_mse(psnr):\n  \"\"\"\n  Compute the Mean Squared Error (MSE) given the Peak Signal-to-Noise Ratio (PSNR).\n\n  Parameters:\n  psnr (float): The Peak Signal-to-Noise Ratio, typically in decibels (dB).\n\n  Returns:\n  float: The computed Mean Squared Error (MSE).\n\n  Process:\n  - The function assumes the maximum pixel value is 1.\n  - PSNR is converted to a linear scale using the formula:\n    MSE = exp(-0.1 * ln(10) * PSNR)\n  - This conversion is based on the logarithmic relationship between PSNR and MSE.\n  - The constant -0.1 * ln(10) transforms the PSNR value into a scale where \n    the exponentiation gives the MSE.\n  \"\"\"\n  return np.exp(-0.1 * np.log(10) * psnr)\n",
  "canonical_solution": "  \n  return np.exp(-0.1 * np.log(10.) * psnr)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(1) == psnr_to_mse(1))\nassert(SOLUTION_SIGNATURE(0) == psnr_to_mse(0))\nassert(SOLUTION_SIGNATURE(0.2) == psnr_to_mse(0.2))\nassert(SOLUTION_SIGNATURE(0.31423) == psnr_to_mse(0.31423))\nassert(SOLUTION_SIGNATURE(1657546223) == psnr_to_mse(1657546223))\n"
 },
 {
  "task_id": "RWPB/83",
  "prompt": "import math\nimport cv2\nimport numpy as np\n\ndef apply_min_size(sample, size, image_interpolation_method=cv2.INTER_AREA):\n    \"\"\"\n    Resize the images in the sample to ensure they meet the minimum specified size while maintaining the aspect ratio.\n\n    Args:\n        sample (dict): A dictionary containing the images to be resized. The keys are:\n            - \"image\": The main image to be resized.\n            - \"disparity\": The disparity map associated with the image.\n            - \"mask\": A binary mask associated with the image.\n        size (tuple): The minimum desired size (height, width) for the images.\n        image_interpolation_method (int, optional): Interpolation method to be used for resizing the main image. \n                                                    Defaults to cv2.INTER_AREA.\n\n    Returns:\n        tuple: The new size (height, width) of the resized images.\n    \"\"\"\n",
  "solution": "\n\n\nimport math\nimport cv2\nimport numpy as np\n\ndef apply_min_size(sample, size, image_interpolation_method=cv2.INTER_AREA):\n    \"\"\"\n    Resize the images in the sample to ensure they meet the minimum specified size while maintaining the aspect ratio.\n\n    Args:\n        sample (dict): A dictionary containing the images to be resized. The keys are:\n            - \"image\": The main image to be resized.\n            - \"disparity\": The disparity map associated with the image.\n            - \"mask\": A binary mask associated with the image.\n        size (tuple): The minimum desired size (height, width) for the images.\n        image_interpolation_method (int, optional): Interpolation method to be used for resizing the main image. \n                                                    Defaults to cv2.INTER_AREA.\n\n    Returns:\n        tuple: The new size (height, width) of the resized images.\n    \"\"\"\n    image = sample[\"image\"]\n    disparity = sample[\"disparity\"]\n    mask = sample[\"mask\"]\n    height, width = image.shape[:2]\n    scale = max(size[0] / height, size[1] / width)\n    new_height = math.ceil(height * scale)\n    new_width = math.ceil(width * scale)\n    sample[\"image\"] = cv2.resize(image, (new_width, new_height), interpolation=image_interpolation_method)\n    sample[\"disparity\"] = cv2.resize(disparity, (new_width, new_height), interpolation=cv2.INTER_NEAREST)\n    sample[\"mask\"] = cv2.resize(mask, (new_width, new_height), interpolation=cv2.INTER_NEAREST)\n\n    return (new_height, new_width)\n\n",
  "canonical_solution": "    \n    shape = list(sample[\"disparity\"].shape)\n\n    if shape[0] >= size[0] and shape[1] >= size[1]:\n        return sample\n\n    scale = [0, 0]\n    scale[0] = size[0] / shape[0]\n    scale[1] = size[1] / shape[1]\n\n    scale = max(scale)\n\n    shape[0] = math.ceil(scale * shape[0])\n    shape[1] = math.ceil(scale * shape[1])\n\n    # resize\n    sample[\"image\"] = cv2.resize(\n        sample[\"image\"], tuple(shape[::-1]), interpolation=image_interpolation_method\n    )\n\n    sample[\"disparity\"] = cv2.resize(\n        sample[\"disparity\"], tuple(shape[::-1]), interpolation=cv2.INTER_NEAREST\n    )\n    sample[\"mask\"] = cv2.resize(\n        sample[\"mask\"].astype(np.float32),\n        tuple(shape[::-1]),\n        interpolation=cv2.INTER_NEAREST,\n    )\n    sample[\"mask\"] = sample[\"mask\"].astype(bool)\n\n    return tuple(shape)\n\n\n",
  "unprocess_testcases": "sample = {\n    \"image\": np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8),\n    \"disparity\": np.random.randint(0, 256, (100, 100), dtype=np.uint8),\n    \"mask\": np.random.randint(0, 2, (100, 100), dtype=np.bool_)\n}\nsize = (200, 300)\nassert((SOLUTION_SIGNATURE(sample, size) == apply_min_size(sample, size)).all())\n\nsample = {\n    \"image\": np.random.randint(0, 256, (300, 300, 3), dtype=np.uint8),\n    \"disparity\": np.random.randint(0, 256, (300, 300), dtype=np.uint8),\n    \"mask\": np.random.randint(0, 2, (300, 300), dtype=np.bool_)\n}\nsize = (200, 200)\nassert((SOLUTION_SIGNATURE(sample, size) == apply_min_size(sample, size)).all())\n\nsample = {\n    \"image\": np.random.randint(0, 256, (150, 100, 3), dtype=np.uint8),\n    \"disparity\": np.random.randint(0, 256, (150, 100), dtype=np.uint8),\n    \"mask\": np.random.randint(0, 2, (150, 100), dtype=np.bool_)\n}\nsize = (300, 450)\nassert((SOLUTION_SIGNATURE(sample, size) == apply_min_size(sample, size)).all())\n"
 },
 {
  "task_id": "RWPB/44",
  "prompt": "import torch\n\ndef interleave(tensor1, tensor2):\n    \"\"\"\n    Interleaves two tensors along a new dimension, creating two interleaved sequences.\n    If the \n\n    Args:\n        tensor1 (torch.Tensor): The first tensor to interleave.\n        tensor2 (torch.Tensor): The second tensor to interleave.\n\n    Returns:\n        tuple: A tuple containing two tensors. The first tensor is interleaved starting with tensor1,\n               and the second tensor is interleaved starting with tensor2.\n\n    This function interleaves two input tensors element-wise along a newly created dimension and then flattens\n    the interleaved tensor back into the original dimension. It produces two outputs: one starting with the first tensor\n    and the other starting with the second tensor. This can be particularly useful for tasks where data from two different\n    sources need to be alternately processed or analyzed.\n\n    Example Usage:\n        tensor1 = torch.tensor([1, 3, 5])\n        tensor2 = torch.tensor([2, 4, 6])\n        res1, res2 = interleave(tensor1, tensor2)\n        print(res1)  # Outputs: tensor([1, 2, 3, 4, 5, 6])\n        print(res2)  # Outputs: tensor([2, 1, 4, 3, 6, 5])\n    \"\"\"\n",
  "solution": "\nimport torch\n\ndef interleave(tensor1, tensor2):\n    \"\"\"\n    Interleaves two tensors along a new dimension, creating two interleaved sequences.\n    If the tensors have different lengths, the shorter tensor is padded with zeros.\n\n    Args:\n        tensor1 (torch.Tensor): The first tensor to interleave.\n        tensor2 (torch.Tensor): The second tensor to interleave.\n\n    Returns:\n        tuple: A tuple containing two tensors. The first tensor is interleaved starting with tensor1,\n               and the second tensor is interleaved starting with tensor2.\n    \"\"\"\n    tensor1 = torch.cat([tensor1, torch.zeros(tensor2.shape[0] - tensor1.shape[0])])\n    tensor2 = torch.cat([tensor2, torch.zeros(tensor1.shape[0] - tensor2.shape[0])])\n    interleaved = torch.stack([tensor1, tensor2], dim=0)\n    res1 = interleaved[0, :]\n    res2 = interleaved[1, :]\n    return res1, res2\n",
  "canonical_solution": "\n    # Stack the tensors alternately along a new dimension (dim=1)\n    res1 = torch.stack((tensor1, tensor2), dim=1).flatten(0, 1)  # Flatten across original and new dimension\n    res2 = torch.stack((tensor2, tensor1), dim=1).flatten(0, 1)  # Start with the second tensor\n\n    return res1, res2\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([1, 3, 5]), torch.tensor([2, 4, 6]))[0] == interleave(torch.tensor([1, 3, 5]), torch.tensor([2, 4, 6]))[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 3, 5]), torch.tensor([2, 4, 6]))[1] == interleave(torch.tensor([1, 3, 5]), torch.tensor([2, 4, 6]))[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 3, 5, 7]), torch.tensor([2, 4, 5, 0]))[0] == interleave(torch.tensor([1, 3, 5, 7]), torch.tensor([2, 4, 5, 0]))[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 3, 5, 7]), torch.tensor([2, 4, 5, 0]))[1] == interleave(torch.tensor([1, 3, 5, 7]), torch.tensor([2, 4, 5, 0]))[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))[0] == interleave(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))[1] == interleave(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))[1]).all())\n"
 },
 {
  "task_id": "RWPB/15",
  "prompt": "import numpy as np\n\n\ndef cosine_distance(a, b):\n    \"\"\"\n    Compute the consine distance between vector a and vector b. Only support `a` is an embedding vector, `b` is a vector or matrix.\n    if the length of a is not equal the length of b, please return KeyError\n    Input: \n    a: numpy vector\n    b: numpy vector\n    return:\n    the cosine distance between vector a and vector b: int.\n    \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\n\n\ndef cosine_distance(a, b):\n    \"\"\"\n    Compute the consine distance between vector a and vector b. Only support `a` is an embedding vector, `b` is a vector or matrix.\n    if the length of a is not equal the length of b, please return KeyError\n    Input: \n    a: numpy vector\n    b: numpy vector\n    return:\n    the cosine distance between vector a and vector b: int.\n    \"\"\"\n    if len(a)!= len(b):\n        raise KeyError(\"The length of a and b must be equal!\")\n    cosine_similarity = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n    cosine_distance = 1 - cosine_similarity\n\n    return cosine_distance\n\n\n",
  "canonical_solution": "\n    if a.shape[-1] != b.shape[-1]:\n        return ValueError\n    dist = np.dot(a, b.T) / (np.linalg.norm(a, axis=-1) * np.linalg.norm(b, axis=-1))\n    return dist\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(a = np.array([1, 1]), b = np.array([0, 1])) == cosine_distance(a = np.array([1, 1]), b = np.array([0, 1]))).all())\nassert(SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([0, 1])) == cosine_distance(a = np.array([1, 0, 0]), b = np.array([0, 1])))\nassert((SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])) == cosine_distance(a = np.array([1, 0, 0]), b = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))).all())\n"
 },
 {
  "task_id": "RWPB/119",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\ndef rotate_about_x_axis(vector, theta):\n    \"\"\"\n    Rotate a 3D vector about the x-axis by a specified angle.\n    Args:\n        - vector: numpy array of shape (3,), the 3D vector to be rotated\n        - theta: float, angle of rotation in radians\n    Returns:\n        rotated_vector: numpy array of shape (3,), the rotated 3D vector\n    \"\"\"\n",
  "solution": "\n\n\nimport math\nimport numpy as np\n\ndef rotate_about_x_axis(vector, theta):\n    \"\"\"\n    Rotate a 3D vector about the x-axis by a specified angle.\n    Args:\n        - vector: numpy array of shape (3,), the 3D vector to be rotated\n        - theta: float, angle of rotation in radians\n    Returns:\n        rotated_vector: numpy array of shape (3,), the rotated 3D vector\n    \"\"\"\n    rotation_matrix = np.array([\n        [1, 0, 0],\n        [0, math.cos(theta), -math.sin(theta)],\n        [0, math.sin(theta), math.cos(theta)]\n    ])\n    rotated_vector = np.dot(rotation_matrix, vector)\n\n    return rotated_vector\n\n\n",
  "canonical_solution": "    \n    return np.dot(np.array([\n[1, 0, 0],\n[0, np.cos(theta), -np.sin(theta)],\n[0, np.sin(theta), np.cos(theta)]        \n]), vector)\n\n\n",
  "unprocess_testcases": "vector = np.array([0, 1, 0])\ntheta = np.pi / 2\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_x_axis(vector, theta)).all())\n\nvector = np.array([1, 1, 1])\ntheta = 0\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_x_axis(vector, theta)).all())\n\nvector = np.array([1, 0, 0])\ntheta = -np.pi / 4\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_x_axis(vector, theta)).all())\n"
 },
 {
  "task_id": "RWPB/139",
  "prompt": "import torch\nimport numpy as np\n\ndef rotate_normalmap_by_angle_torch(normal_map, angle):\n    \"\"\"\n    Rotates the normals in a normal map along the y-axis by a specified angle using PyTorch.\n\n    Args:\n        normal_map (torch.Tensor): A tensor of shape (H, W, 3) representing the normal map,\n                                   where H is the height, W is the width, and each normal is a 3D vector with components in [-1, 1].\n                                   This tensor should be on a CUDA device for GPU acceleration.\n        angle (float): The rotation angle in degrees.\n\n    Returns:\n        torch.Tensor: The rotated normal map with the same shape as the input.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\nimport numpy as np\n\ndef rotate_normalmap_by_angle_torch(normal_map, angle):\n    \"\"\"\n    Rotates the normals in a normal map along the y-axis by a specified angle using PyTorch.\n\n    Args:\n        normal_map (torch.Tensor): A tensor of shape (H, W, 3) representing the normal map,\n                                   where H is the height, W is the width, and each normal is a 3D vector with components in [-1, 1].\n                                   This tensor should be on a CUDA device for GPU acceleration.\n        angle (float): The rotation angle in degrees.\n\n    Returns:\n        torch.Tensor: The rotated normal map with the same shape as the input.\n    \"\"\"\n    angle_radians = angle * np.pi / 180.0\n    rotation_matrix = torch.tensor([\n        [torch.cos(angle_radians), 0, torch.sin(angle_radians)],\n        [0, 1, 0],\n        [-torch.sin(angle_radians), 0, torch.cos(angle_radians)]\n    ])\n    rotated_normals = torch.matmul(normal_map.view(-1, 3), rotation_matrix).view(normal_map.shape)\n\n    return rotated_normals\n\n\n",
  "canonical_solution": "    \n    # Convert the angle from degrees to radians and move it to the same device as the normal_map\n    angle = torch.tensor(angle / 180 * np.pi).to(normal_map.device)\n\n    # Define the rotation matrix for rotating around the y-axis\n    R = torch.tensor([\n        [torch.cos(angle), 0, torch.sin(angle)],  # X' = X*cos(theta) + Z*sin(theta)\n        [0, 1, 0],                                # Y' = Y (no change)\n        [-torch.sin(angle), 0, torch.cos(angle)]  # Z' = -X*sin(theta) + Z*cos(theta)\n    ]).to(normal_map.device)\n\n    # Apply the rotation matrix to each normal vector in the normal map\n    # Reshape the normal map from (H, W, 3) to (H*W, 3) for matrix multiplication, then reshape back to (H, W, 3)\n    return torch.matmul(normal_map.view(-1, 3), R.T).view(normal_map.shape)\n\n\n",
  "unprocess_testcases": "normal_map = torch.rand(10, 10, 3) * 2 - 1\nangle = 0\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle_torch(normal_map, angle)).all())\n\nnormal_map = torch.tensor([[[0, 0, 1]]], dtype=torch.float32).repeat(5, 5, 1)\nangle = 90\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle_torch(normal_map, angle)).all())\n\nnormal_map = torch.rand(50, 50, 3) * 2 - 1\nangle = 360\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle_torch(normal_map, angle)).all())\n"
 },
 {
  "task_id": "RWPB/97",
  "prompt": "import numpy as np\nimport torch\n\ndef numpy_image_to_torch(image: np.ndarray) -> torch.Tensor:\n    \"\"\"\n    Converts a NumPy image array to a normalized PyTorch tensor and reorders the dimensions.\n    If the img is invalid, please return ValueError\n    \n    Parameters:\n    image (np.ndarray): The input image in NumPy array format. \n                        - For color images, the shape should be (H, W, C) where H is height, W is width, and C is the number of channels (typically 3 for RGB).\n                        - For grayscale images, the shape should be (H, W).\n\n    Returns:\n    torch.Tensor: The converted image as a PyTorch tensor with normalized values in the range [0, 1].\n                  - For color images, the shape will be (C, H, W).\n                  - For grayscale images, the shape will be (1, H, W).\n\n    Process:\n    1. Check the number of dimensions of the input image:\n       - If the image is a 3D array (color image), transpose the dimensions from (H, W, C) to (C, H, W).\n       - If the image is a 2D array (grayscale image), add a new axis to create a channel dimension, resulting in a shape of (1, H, W).\n       - If the image does not match these conditions, raise a ValueError indicating it is not a valid image.\n    2. Normalize the pixel values by dividing by 255.0 to convert the range from [0, 255] to [0, 1].\n    3. Convert the resulting NumPy array to a PyTorch tensor with dtype=torch.float.\n    \"\"\"\n",
  "solution": "\nimport numpy as np\nimport torch\n\ndef numpy_image_to_torch(image: np.ndarray) -> torch.Tensor:\n    \"\"\"\n    Converts a NumPy image array to a normalized PyTorch tensor and reorders the dimensions.\n    If the img is invalid, please return ValueError\n    \n    Parameters:\n    image (np.ndarray): The input image in NumPy array format. \n                        - For color images, the shape should be (H, W, C) where H is height, W is width, and C is the number of channels (typically 3 for RGB).\n                        - For grayscale images, the shape should be (H, W).\n\n    Returns:\n    torch.Tensor: The converted image as a PyTorch tensor with normalized values in the range [0, 1].\n                  - For color images, the shape will be (C, H, W).\n                  - For grayscale images, the shape will be (1, H, W).\n\n    Process:\n    1. Check the number of dimensions of the input image:\n       - If the image is a 3D array (color image), transpose the dimensions from (H, W, C) to (C, H, W).\n       - If the image is a 2D array (grayscale image), add a new axis to create a channel dimension, resulting in a shape of (1, H, W).\n       - If the image does not match these conditions, raise a ValueError indicating it is not a valid image.\n    2. Normalize the pixel values by dividing by 255.0 to convert the range from [0, 255] to [0, 1].\n    3. Convert the resulting NumPy array to a PyTorch tensor with dtype=torch.float.\n    \"\"\"\n    if image.ndim == 3:\n        image = np.transpose(image, (2, 0, 1))\n    elif image.ndim == 2:\n        image = np.expand_dims(image, axis=0)\n    else:\n        raise ValueError(\"Invalid image: expected 2D or 3D array, got array with shape {}\".format(image.shape))\n    image = image / 255.0\n    image = torch.from_numpy(image).float()\n\n    return image\n",
  "canonical_solution": "    \n    if image.ndim == 3:\n        image = image.transpose((2, 0, 1))  # HxWxC to CxHxW\n    elif image.ndim == 2:\n        image = image[None]  # add channel axis\n    else:\n        return ValueError\n    return torch.tensor(image / 255.0, dtype=torch.float)\n\n\n",
  "unprocess_testcases": "\n\ncolor_image = np.random.randint(0, 256, size=(100, 200, 3), dtype=np.uint8)\nassert((SOLUTION_SIGNATURE(color_image) == numpy_image_to_torch(color_image)).all())\n\ngrayscale_image = np.random.randint(0, 256, size=(50, 50), dtype=np.uint8)\nassert((SOLUTION_SIGNATURE(grayscale_image) == numpy_image_to_torch(grayscale_image)).all())\n\ninvalid_image = np.random.randint(0, 256, size=(100,), dtype=np.uint8)\nassert(SOLUTION_SIGNATURE(invalid_image) == numpy_image_to_torch(invalid_image))\n"
 },
 {
  "task_id": "RWPB/8",
  "prompt": "import numpy as np\nimport torch\n\ndef cvimg2torch(img):\n    '''Convert a img to tensor\n    input:\n        im -> ndarray uint8 HxWxC \n    return\n        tensor -> torch.tensor BxCxHxW \n    '''\n",
  "solution": "\n\n\nimport numpy as np\nimport torch\n\ndef cvimg2torch(img):\n    '''Convert a img to tensor\n    input:\n        im -> ndarray uint8 HxWxC \n    return\n        tensor -> torch.tensor BxCxHxW \n    '''\n    tensor = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0).float()\n    return tensor\n\n",
  "canonical_solution": "\n    img = img.astype(float) / 255.0\n    img = img.transpose(2, 0, 1) # NHWC -> NCHW\n    img = np.expand_dims(img, 0)\n    img = torch.from_numpy(img).float()\n    return img\n\n\n\n",
  "unprocess_testcases": "a = (np.random.rand(256, 256, 3) * 255).astype(np.uint8)\nb = np.repeat((np.random.rand(10, 10) * 255).astype(np.uint8)[:, :, np.newaxis], 3, axis=2)\nc = np.array([[[300, -10, 500], [256, 255, 0]]], dtype=np.int32)\n\nassert((SOLUTION_SIGNATURE(a) == cvimg2torch(a)).all())\nassert((SOLUTION_SIGNATURE(b) == cvimg2torch(b)).all())\nassert((SOLUTION_SIGNATURE(c) == cvimg2torch(c)).all())"
 },
 {
  "task_id": "RWPB/35",
  "prompt": "def flatten_dict(d: dict, parent_key: str = \"\", sep: str = \"/\") -> dict:\n    \"\"\"\n    Flattens a nested dictionary by collapsing its nested structure into a single layer dictionary with keys\n    representing paths, separated by a specified separator.\n\n    Args:\n        d (dict): The dictionary to flatten.\n        parent_key (str): The base path for keys in the nested dictionary. Initially left empty.\n        sep (str): The separator to use when concatenating nested keys.\n\n    Returns:\n        dict: A flattened version of the dictionary where all nested keys are transformed into single-layered keys.\n\n    Example:\n        >>> dct = {\"a\": {\"b\": 1, \"c\": {\"d\": 2}}, \"e\": 3}\n        >>> print(flatten_dict(dct))\n        {\"a/b\": 1, \"a/c/d\": 2, \"e\": 3}\n    \"\"\"\n",
  "solution": "\n\n\ndef flatten_dict(d: dict, parent_key: str = \"\", sep: str = \"/\") -> dict:\n    \"\"\"\n    Flattens a nested dictionary by collapsing its nested structure into a single layer dictionary with keys\n    representing paths, separated by a specified separator.\n\n    Args:\n        d (dict): The dictionary to flatten.\n        parent_key (str): The base path for keys in the nested dictionary. Initially left empty.\n        sep (str): The separator to use when concatenating nested keys.\n\n    Returns:\n        dict: A flattened version of the dictionary where all nested keys are transformed into single-layered keys.\n\n    Example:\n        >>> dct = {\"a\": {\"b\": 1, \"c\": {\"d\": 2}}, \"e\": 3}\n        >>> print(flatten_dict(dct))\n        {\"a/b\": 1, \"a/c/d\": 2, \"e\": 3}\n    \"\"\"\n    flattened_dict = {}\n    for key, value in d.items():\n        new_key = f\"{parent_key}{sep}{key}\" if parent_key else key\n        if isinstance(value, dict):\n            flattened_dict.update(flatten_dict(value, new_key, sep))\n        else:\n            flattened_dict[new_key] = value\n    return flattened_dict\n\n\n",
  "canonical_solution": "\n    items = []\n    for k, v in d.items():\n        # Generate the new key by concatenating the current key with the parent key using the separator\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            # Recursively flatten the dictionary and extend the list items\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            # Add the key-value pair to the items list\n            items.append((new_key, v))\n    return dict(items)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE({\"a\": {\"b\": {\"c\": {\"d\": 1}}}, \"e\": {\"f\": 2, \"g\": {\"h\": 3}}, \"i\": 4}) == flatten_dict({\"a\": {\"b\": {\"c\": {\"d\": 1}}}, \"e\": {\"f\": 2, \"g\": {\"h\": 3}}, \"i\": 4}))\nassert(SOLUTION_SIGNATURE({\"a\": {\"b\": 1, \"c\": {\"d\": 2}}, \"e\": 3}, sep='-') == flatten_dict({\"a\": {\"b\": 1, \"c\": {\"d\": 2}}, \"e\": 3}, sep='-'))\nassert(SOLUTION_SIGNATURE({\"a\": {}, \"b\": {\"c\": {}}, \"d\": 5}) == flatten_dict({\"a\": {}, \"b\": {\"c\": {}}, \"d\": 5}))\n"
 },
 {
  "task_id": "RWPB/64",
  "prompt": "import numpy as np\n\ndef moving_average(arr, window_size):\n    \"\"\"\n    Calculates the moving average of an array with a specified window size and returns both\n    the smoothed values and the residuals.\n\n    Arguments:\n    arr : numpy.ndarray\n        The input array over which the moving average is computed.\n    window_size : int\n        The number of elements in the moving average window.\n\n    Returns:\n    list of numpy.ndarray\n        The first element is an array of the moving averages, and the second element is an array of residuals.\n    \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\n\ndef moving_average(arr, window_size):\n    \"\"\"\n    Calculates the moving average of an array with a specified window size and returns both\n    the smoothed values and the residuals.\n\n    Arguments:\n    arr : numpy.ndarray\n        The input array over which the moving average is computed.\n    window_size : int\n        The number of elements in the moving average window.\n\n    Returns:\n    list of numpy.ndarray\n        The first element is an array of the moving averages, and the second element is an array of residuals.\n    \"\"\"\n    smoothed = np.convolve(arr, np.ones(window_size), 'valid') / window_size\n    residuals = arr[window_size - 1:] - smoothed\n    return [smoothed, residuals]\n\n\n",
  "canonical_solution": "\n    # Pad the input array with zeros at the beginning to handle the boundary effect\n    arr_padded = np.pad(arr, (window_size - 1, 0), \"constant\")\n\n    # Calculate the moving average using a convolution with a uniform kernel (i.e., np.ones(window_size))\n    smoothed_arr = np.convolve(arr_padded, np.ones(window_size), \"valid\") / window_size\n\n    # Calculate residuals by subtracting the smoothed array from the original array\n    residuals = arr - smoothed_arr\n\n    return [smoothed_arr, residuals]\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5, 6]), 3)[0] == moving_average(np.array([1, 2, 3, 4, 5, 6]), 3)[0]).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5, 6]), 3)[1] == moving_average(np.array([1, 2, 3, 4, 5, 6]), 3)[1]).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5, 6]), 6)[0] == moving_average(np.array([1, 2, 3, 4, 5, 6]), 6)[0]).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5, 6]), 6)[1] == moving_average(np.array([1, 2, 3, 4, 5, 6]), 6)[1]).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3]), 5)[0] == moving_average(np.array([1, 2, 3]), 5)[0]).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3]), 5)[1] == moving_average(np.array([1, 2, 3]), 5)[1]).all())\n"
 },
 {
  "task_id": "RWPB/50",
  "prompt": "import torch\n\ndef unpad_image(tensor: torch.Tensor, original_size: tuple) -> torch.Tensor:\n    \"\"\"\n    Removes padding from an image tensor to restore it to its original aspect ratio.\n    The function assumes the tensor is in the CxHxW format and has been padded symmetrically.\n\n    Args:\n        tensor (torch.Tensor): The image tensor, assumed to be in CxHxW format.\n        original_size (tuple): The original dimensions of the image as (height, width).\n\n    Returns:\n        torch.Tensor: The unpadded image tensor resized back to the original aspect ratio.\n\n    The function calculates the aspect ratio of the original image and the current tensor,\n    determining the necessary adjustments to either height or width. It then crops the tensor\n    to match the original aspect ratio, ensuring that the central part of the image is retained.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef unpad_image(tensor: torch.Tensor, original_size: tuple) -> torch.Tensor:\n    \"\"\"\n    Removes padding from an image tensor to restore it to its original aspect ratio.\n    The function assumes the tensor is in the CxHxW format and has been padded symmetrically.\n\n    Args:\n        tensor (torch.Tensor): The image tensor, assumed to be in CxHxW format.\n        original_size (tuple): The original dimensions of the image as (height, width).\n\n    Returns:\n        torch.Tensor: The unpadded image tensor resized back to the original aspect ratio.\n\n    The function calculates the aspect ratio of the original image and the current tensor,\n    determining the necessary adjustments to either height or width. It then crops the tensor\n    to match the original aspect ratio, ensuring that the central part of the image is retained.\n    \"\"\"\n    original_aspect_ratio = original_size[0] / original_size[1]\n    current_aspect_ratio = tensor.shape[1] / tensor.shape[2]\n    if original_aspect_ratio > current_aspect_ratio:\n        height = tensor.shape[1]\n        width = int(height * original_aspect_ratio)\n        tensor = torch.nn.functional.pad(tensor, (0, width - height, 0, 0))\n    else:\n        width = tensor.shape[2]\n        height = int(width / original_aspect_ratio)\n        tensor = torch.nn.functional.pad(tensor, (0, 0, 0, height - width))\n    tensor = tensor[:, :original_size[0], :original_size[1]]\n\n    return tensor\n\n\n",
  "canonical_solution": "\n    # Extract original and current dimensions\n    original_height, original_width = original_size\n    _, current_height, current_width = tensor.shape\n\n    # Calculate aspect ratios\n    original_aspect_ratio = original_width / original_height\n    current_aspect_ratio = current_width / current_height\n\n    # Determine how to adjust tensor to match original aspect ratio\n    if original_aspect_ratio > current_aspect_ratio:\n        # Original is wider than current: adjust height\n        scale_factor = current_width / original_width\n        new_height = int(original_height * scale_factor)\n        vertical_padding = (current_height - new_height) // 2\n        # Crop vertically\n        unpadded_tensor = tensor[:, vertical_padding:vertical_padding + new_height, :]\n    else:\n        # Original is taller than current: adjust width\n        scale_factor = current_height / original_height\n        new_width = int(original_width * scale_factor)\n        horizontal_padding = (current_width - new_width) // 2\n        # Crop horizontally\n        unpadded_tensor = tensor[:, :, horizontal_padding:horizontal_padding + new_width]\n\n    return unpadded_tensor\n\n\n",
  "unprocess_testcases": "a = torch.rand(3, 10, 20)\nb = torch.rand(3, 20, 10)\nc = torch.rand(3, 15, 30)\n\nassert((SOLUTION_SIGNATURE(a, (5, 10)) == unpad_image(a, (5, 10))).all())\nassert((SOLUTION_SIGNATURE(b, (10, 5)) == unpad_image(b, (10, 5))).all())\nassert((SOLUTION_SIGNATURE(c, (10, 20)) == unpad_image(c, (10, 20))).all())\n"
 },
 {
  "task_id": "RWPB/11",
  "prompt": "from typing import List\n\ndef pad_to_max_seq_length(ls: List[int], max_seq_length: int, pad_idx: int=0, pad_right: bool=True, check: bool=True):\n    \"\"\"Apply padding to an input sequence.\n\n    Args:\n        ls: List[int]: sequence to pad.\n        max_seq_length: int: max length up to which to apply padding.\n        pad_idx :int: element to use for padding.\n        pad_right: bool: True if padding is applied to right side of sequence, False to pad on left side.\n        check: bool: True if result length should be checked as under the max sequence length.\n        if result length is not equal the max sequence length, please return ValueError\n\n    Returns:\n        Sequence with specified padding applied.\n\n    \"\"\"\n",
  "solution": "\n\n\nfrom typing import List\n\ndef pad_to_max_seq_length(ls: List[int], max_seq_length: int, pad_idx: int=0, pad_right: bool=True, check: bool=True):\n    \"\"\"Apply padding to an input sequence.\n\n    Args:\n        ls: List[int]: sequence to pad.\n        max_seq_length: int: max length up to which to apply padding.\n        pad_idx :int: element to use for padding.\n        pad_right: bool: True if padding is applied to right side of sequence, False to pad on left side.\n        check: bool: True if result length should be checked as under the max sequence length.\n        if result length is not equal the max sequence length, please return ValueError\n\n    Returns:\n        Sequence with specified padding applied.\n\n    \"\"\"\n    if check and len(ls) > max_seq_length:\n        raise ValueError(\"Length of sequence is greater than max sequence length.\")\n    if pad_right:\n        return ls + [pad_idx] * (max_seq_length - len(ls))\n    else:\n        return [pad_idx] * (max_seq_length - len(ls)) + ls\n\n\n",
  "canonical_solution": "\n    padding = [pad_idx] * (max_seq_length - len(ls))\n    if pad_right:\n        result = ls + padding\n    else:\n        result = padding + ls\n\n    if check:\n        if len(result) != max_seq_length:\n            return ValueError\n    return result\n\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE([1, 2, 3], 5) == pad_to_max_seq_length([1, 2, 3], 5))\nassert(SOLUTION_SIGNATURE([1, 2, 3], 5, pad_idx=-1) == pad_to_max_seq_length([1, 2, 3], 5, pad_idx=-1))\nassert(SOLUTION_SIGNATURE([1, 2, 3], 5, pad_right=False) == pad_to_max_seq_length([1, 2, 3], 5, pad_right=False))\nassert(SOLUTION_SIGNATURE([1, 2, 3], 3) == pad_to_max_seq_length([1, 2, 3], 3))\nassert(SOLUTION_SIGNATURE([1, 2, 3], 2) == pad_to_max_seq_length([1, 2, 3], 2))\n"
 },
 {
  "task_id": "RWPB/40",
  "prompt": "import inspect\nfrom typing import Callable, List\n\ndef extract_code_from_funct(funct: Callable) -> List[str]:\n    \"\"\"\n    Extracts the body of a Python function, removing the function definition and return statement, and adjusting indentation.\n\n    Args:\n        funct (Callable): The function from which code is extracted.\n\n    Returns:\n        List[str]: A list of strings, where each string is a line of code from the function's body, adjusted for indentation.\n\n    This function utilizes Python's `inspect` module to retrieve the source code of a given function. It then processes\n    this code to remove the function's signature and any return statement at the end, adjusting the indentation of the\n    remaining lines to align with the leftmost edge.\n\n    Example:\n        def example_function(x):\n            result = x + 1\n            return result\n\n        >>> extract_code_from_funct(example_function)\n        ['result = x + 1']\n    \"\"\"\n",
  "solution": "\nimport inspect\nfrom typing import Callable, List\n\ndef extract_code_from_funct(funct: Callable) -> List[str]:\n    \"\"\"\n    Extracts the body of a Python function, removing the function definition and return statement, and adjusting indentation.\n\n    Args:\n        funct (Callable): The function from which code is extracted.\n\n    Returns:\n        List[str]: A list of strings, where each string is a line of code from the function's body, adjusted for indentation.\n\n    This function utilizes Python's `inspect` module to retrieve the source code of a given function. It then processes\n    this code to remove the function's signature and any return statement at the end, adjusting the indentation of the\n    remaining lines to align with the leftmost edge.\n\n    Example:\n        def example_function(x):\n            result = x + 1\n            return result\n\n        >>> extract_code_from_funct(example_function)\n        ['result = x + 1']\n    \"\"\"\n    source = inspect.getsource(funct)\n    lines = source.split(\"\\n\")\n    body_lines = lines[1:-1]\n    adjusted_lines = []\n    min_indent = min(len(line) - len(line.lstrip()) for line in body_lines)\n    for line in body_lines:\n        adjusted_lines.append(line[min_indent:])\n\n    return adjusted_lines\n",
  "canonical_solution": "\n    # Retrieve the source code of the function\n    source_code = inspect.getsource(funct)\n\n    # Split the source code into individual lines and remove the first line (function definition)\n    source_code_lines = source_code.splitlines()[1:]\n\n    # Calculate the number of characters of indentation in the first line of the function body\n    nident = len(source_code_lines[0]) - len(source_code_lines[0].lstrip())\n\n    # Adjust each line to remove the initial indentation and exclude the last line (usually a return statement)\n    return [line[nident:] for line in source_code_lines[:-1]]\n\n\n",
  "unprocess_testcases": "def sample_function(x):\n    y = x + 10\n    z = y * 2\n    return z\n\nassert(SOLUTION_SIGNATURE(sample_function) == extract_code_from_funct(sample_function))\n\ndef empty_function():\n    pass\nassert(SOLUTION_SIGNATURE(empty_function) == extract_code_from_funct(empty_function))\n\ndef complex_function(x):\n    if x > 0:\n        for i in range(x):\n            print(i)\n    else:\n        while x < 0:\n            x += 1\n            print(x)\n    return x\nassert(SOLUTION_SIGNATURE(complex_function) == extract_code_from_funct(complex_function))\n"
 },
 {
  "task_id": "RWPB/74",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\ndef arccos2(vector, value):\n    \"\"\"\n    Calculate the inverse cosine of a value with consideration of the vector direction.\n    Args:\n        - vector: numpy array of shape (2,), the 2D vector to consider for direction\n        - value: float, the value for which to calculate the inverse cosine\n    Returns:\n        angle: float, the inverse cosine with consideration of vector direction\n    \"\"\"\n",
  "solution": "\n\n\nimport math\nimport numpy as np\nfrom decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\ndef arccos2(vector, value):\n    \"\"\"\n    Calculate the inverse cosine of a value with consideration of the vector direction.\n    Args:\n        - vector: numpy array of shape (2,), the 2D vector to consider for direction\n        - value: float, the value for which to calculate the inverse cosine\n    Returns:\n        angle: float, the inverse cosine with consideration of vector direction\n    \"\"\"\n    angle = math.acos(value)\n    if vector[0] < 0:\n        angle = math.pi - angle\n    return angle\n\n\n",
  "canonical_solution": "    \n    if vector[0] == 0:\n        if vector[1] >= 0:\n            return 0\n        else:\n            return np.pi\n    return -np.sign(vector[0]) * np.arccos(value)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(np.array([-1, 0]), value = 0.5) == arccos2(np.array([-1, 0]), value = 0.5))\nassert(SOLUTION_SIGNATURE(np.array([1, 0]), value = 0.5) == arccos2(np.array([1, 0]), value = 0.5))\nassert(SOLUTION_SIGNATURE(np.array([0, 1]), value = -1) == arccos2(np.array([0, 1]), value = -1))\nassert(SOLUTION_SIGNATURE(np.array([0, -1]), value = -1) == arccos2(np.array([0, -1]), value = -1))\n"
 },
 {
  "task_id": "RWPB/25",
  "prompt": "import numpy as np\nimport cv2\n\ndef hole_fill(img):\n    \"\"\"\n    Performs hole filling on an image using morphological operations.\n\n    Args:\n        img (np.array): A grayscale image array where holes are to be filled.\n\n    Returns:\n        np.array: An image array with holes filled.\n\n    This function applies the flood fill algorithm to identify and fill holes in the input image.\n    The process involves padding the image, copying it for result calculation, and applying\n    bitwise operations to fill the holes based on the flood filled mask.\n    \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\nimport cv2\n\ndef hole_fill(img):\n    \"\"\"\n    Performs hole filling on an image using morphological operations.\n\n    Args:\n        img (np.array): A grayscale image array where holes are to be filled.\n\n    Returns:\n        np.array: An image array with holes filled.\n\n    This function applies the flood fill algorithm to identify and fill holes in the input image.\n    The process involves padding the image, copying it for result calculation, and applying\n    bitwise operations to fill the holes based on the flood filled mask.\n    \"\"\"\n    h, w = img.shape[:2]\n    mask = np.zeros((h+2, w+2), np.uint8)\n    result = img.copy()\n    cv2.floodFill(result, mask, (0,0), 255)\n    filled_holes = cv2.bitwise_not(result)\n    filled_img = img | filled_holes\n\n    return filled_img\n\n",
  "canonical_solution": "\n    # Pad the image to avoid boundary issues during flood fill\n    img = np.pad(img[1:-1, 1:-1], pad_width=1, mode='constant', constant_values=0)\n    img_copy = img.copy()  # Create a copy of the image for later use\n\n    # Create a mask for flood fill operation, size needs to be 2 pixels bigger than the image\n    mask = np.zeros((img.shape[0] + 2, img.shape[1] + 2), dtype=np.uint8)\n\n    # Perform flood fill from the top-left corner (0,0) with the new value 255\n    cv2.floodFill(img, mask, (0, 0), 255)\n\n    # Create inverse of the flood filled image\n    img_inverse = cv2.bitwise_not(img)\n\n    # Combine the original copy and the inverse to fill the holes\n    dst = cv2.bitwise_or(img_copy, img_inverse)\n\n    return dst\n\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(np.zeros((10, 10), dtype=np.uint8)) == hole_fill(np.zeros((10, 10), dtype=np.uint8)))\n\nimg_large_hole = np.ones((10, 10), dtype=np.uint8) * 255\nimg_large_hole[3:7, 3:7] = 0\nassert(SOLUTION_SIGNATURE(img_large_hole) == hole_fill(img_large_hole))\n\n# Create an image with both small holes on the edges and small holes scattered in the middle of the image\nimg_multiple_holes = np.ones((10, 10), dtype=np.uint8) * 255\nimg_multiple_holes[1, 1] = 0 # small holes in corners\nimg_multiple_holes[1, 8] = 0 # small holes in the other corner\nimg_multiple_holes[5, 5] = 0 # small holes in the centre\nimg_multiple_holes[8, 1] = 0 # edge holes\nimg_multiple_holes[8, 8] = 0 # holes at the edges\n\nassert(SOLUTION_SIGNATURE(img_multiple_holes) == hole_fill(img_multiple_holes))\n"
 },
 {
  "task_id": "RWPB/87",
  "prompt": "import torch\nimport numpy as np\n\n\ndef rotate_x(a, device=None):\n    \"\"\"\n    Creates a 4x4 homogeneous transformation matrix for rotation around the x-axis in 3D space.\n\n    Parameters:\n    a (float): Rotation angle in radians.\n    device (torch.device, optional): The device on which to create the tensor (e.g., 'cpu' or 'cuda'). Default is None, which means the tensor will be created on the default device.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the rotation transformation matrix around the x-axis.\n\n    Process:\n    1. Calculate the sine and cosine of the rotation angle.\n    2. Construct a 4x4 matrix where the rotation is applied to the y and z coordinates, leaving the x coordinate unchanged.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\nimport numpy as np\n\n\ndef rotate_x(a, device=None):\n    \"\"\"\n    Creates a 4x4 homogeneous transformation matrix for rotation around the x-axis in 3D space.\n\n    Parameters:\n    a (float): Rotation angle in radians.\n    device (torch.device, optional): The device on which to create the tensor (e.g., 'cpu' or 'cuda'). Default is None, which means the tensor will be created on the default device.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the rotation transformation matrix around the x-axis.\n\n    Process:\n    1. Calculate the sine and cosine of the rotation angle.\n    2. Construct a 4x4 matrix where the rotation is applied to the y and z coordinates, leaving the x coordinate unchanged.\n    \"\"\"\n    ca = torch.cos(a)\n    sa = torch.sin(a)\n    if device is not None:\n        ca = ca.to(device)\n        sa = sa.to(device)\n    return torch.tensor([\n        [1, 0, 0, 0],\n        [0, ca, -sa, 0],\n        [0, sa, ca, 0],\n        [0, 0, 0, 1]\n    ], dtype=torch.float32)\n\n\n",
  "canonical_solution": "    \n    s, c = np.sin(a), np.cos(a)\n    return torch.tensor([[1,  0, 0, 0], \n                         [0,  c, s, 0], \n                         [0, -s, c, 0], \n                         [0,  0, 0, 1]], dtype=torch.float32, device=device)\n\n\n",
  "unprocess_testcases": "a = torch.pi / 4\nassert((SOLUTION_SIGNATURE(a) == rotate_x(a)).all())\n\na = 0\nassert((SOLUTION_SIGNATURE(a) == rotate_x(a)).all())\n\na = 2 * torch.pi\nassert((SOLUTION_SIGNATURE(a) == rotate_x(a)).all())\n"
 },
 {
  "task_id": "RWPB/129",
  "prompt": "import torch\nimport torch.nn.functional as F\n\n\ndef scale_masks(masks, shape, padding=True):\n    \"\"\"\n    Rescale segment masks to shape.\n\n    Args:\n        masks (torch.Tensor): (N, C, H, W).\n        shape (tuple): Height and width.\n        padding (bool): If True, assuming the boxes is based on image augmented by yolo style. If False then do regular\n            rescaling.\n    \"\"\"\n",
  "solution": "\n\n\ndef scale_masks(masks, shape, padding=True):\n    \"\"\"\n    Rescale segment masks to shape.\n\n    Args:\n        masks (torch.Tensor): (N, C, H, W).\n        shape (tuple): Height and width.\n        padding (bool): If True, assuming the boxes is based on image augmented by yolo style. If False then do regular\n            rescaling.\n    \"\"\"\n    if padding:\n        masks = F.interpolate(masks, size=shape, mode=\"nearest\")\n    else:\n        masks = F.interpolate(masks, size=shape, mode=\"nearest\")\n    return masks\n\n\n",
  "canonical_solution": "    \n    mh, mw = masks.shape[2:]\n    gain = min(mh / shape[0], mw / shape[1])  # gain  = old / new\n    pad = [mw - shape[1] * gain, mh - shape[0] * gain]  # wh padding\n    if padding:\n        pad[0] /= 2\n        pad[1] /= 2\n    top, left = (int(pad[1]), int(pad[0])) if padding else (0, 0)  # y, x\n    bottom, right = (int(mh - pad[1]), int(mw - pad[0]))\n    masks = masks[..., top:bottom, left:right]\n\n    masks = F.interpolate(masks, shape, mode=\"bilinear\", align_corners=False)  # NCHW\n    return masks\n\n\n",
  "unprocess_testcases": "masks = torch.rand(2, 3, 100, 200)\ntarget_shape = (50, 100)\nassert((SOLUTION_SIGNATURE(masks, target_shape) == scale_masks(masks, target_shape)).all())\n\nmasks = torch.rand(1, 1, 150, 300)\ntarget_shape = (100, 100)\nassert((SOLUTION_SIGNATURE(masks, target_shape) == scale_masks(masks, target_shape)).all())\n\nmasks = torch.rand(3, 3, 200, 200)\ntarget_shape = (210, 210)\nassert((SOLUTION_SIGNATURE(masks, target_shape) == scale_masks(masks, target_shape)).all())\n"
 },
 {
  "task_id": "RWPB/138",
  "prompt": "import numpy as np\n\ndef rotate_normalmap_by_angle(normal_map: np.ndarray, angle: float):\n    \"\"\"\n    Rotates the normals in a normal map along the y-axis by a specified angle.\n\n    Args:\n        normal_map (np.ndarray): An array of shape (H, W, 3) representing the normal map,\n                                 where H is the height, W is the width, and each normal is a 3D vector with components in [-1, 1].\n        angle (float): The rotation angle in degrees.\n\n    Returns:\n        np.ndarray: The rotated normal map with the same shape as the input.\n    \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\nimport math\n\ndef rotate_normalmap_by_angle(normal_map: np.ndarray, angle: float):\n    \"\"\"\n    Rotates the normals in a normal map along the y-axis by a specified angle.\n\n    Args:\n        normal_map (np.ndarray): An array of shape (H, W, 3) representing the normal map,\n                                 where H is the height, W is the width, and each normal is a 3D vector with components in [-1, 1].\n        angle (float): The rotation angle in degrees.\n\n    Returns:\n        np.ndarray: The rotated normal map with the same shape as the input.\n    \"\"\"\n    angle_radians = math.radians(angle)\n    rotation_matrix = np.array([\n        [math.cos(angle_radians), 0, math.sin(angle_radians)],\n        [0, 1, 0],\n        [-math.sin(angle_radians), 0, math.cos(angle_radians)]\n    ])\n    rotated_normal_map = np.zeros_like(normal_map)\n    for i in range(normal_map.shape[0]):\n        for j in range(normal_map.shape[1]):\n            rotated_normal_map[i, j] = np.dot(rotation_matrix, normal_map[i, j])\n\n    return rotated_normal_map\n\n\n",
  "canonical_solution": "    \n    # Convert the angle from degrees to radians\n    angle = angle / 180 * np.pi\n    # Define the rotation matrix for rotating around the y-axis\n    R = np.array([\n        [np.cos(angle), 0, np.sin(angle)],  # X' = X*cos(theta) + Z*sin(theta)\n        [0, 1, 0],                          # Y' = Y (no change)\n        [-np.sin(angle), 0, np.cos(angle)]  # Z' = -X*sin(theta) + Z*cos(theta)\n    ])\n    # Apply the rotation matrix to each normal vector in the normal map\n    # Reshape the normal map from (H, W, 3) to (H*W, 3) for matrix multiplication, then reshape back to (H, W, 3)\n    return np.dot(normal_map.reshape(-1, 3), R.T).reshape(normal_map.shape)\n\n\n",
  "unprocess_testcases": "normal_map = np.array([[[0, 0, 1]]])\nangle = 90\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle(normal_map, angle)).all())\n\nnormal_map = np.array([[[1, 0, 0], [0, 1, 0], [0, 0, 1]]])\nangle = 0\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle(normal_map, angle)).all())\n\nnormal_map = np.array([[[1, 0, 0], [0, 1, 0], [0, 0, 1]]])\nangle = 720  # Two full rotations\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle(normal_map, angle)).all())\n"
 },
 {
  "task_id": "RWPB/96",
  "prompt": "import numpy as np\n\ndef movmean(arr, n_avg):\n  \"\"\"\n  Compute the moving average of an array.\n  Args:\n      - arr: numpy.ndarray, the input array\n      - n_avg: int, the number of elements to include in the moving average\n  Returns:\n      mov_avg: numpy.ndarray, the moving average of the input array\n\n  If the windows size is bigger than the length of array, the code should return a empty array.\n  \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\n\ndef movmean(arr, n_avg):\n  \"\"\"\n  Compute the moving average of an array.\n  Args:\n      - arr: numpy.ndarray, the input array\n      - n_avg: int, the number of elements to include in the moving average\n  Returns:\n      mov_avg: numpy.ndarray, the moving average of the input array\n\n  If the windows size is bigger than the length of array, the code should return a empty array.\n  \"\"\"\n  if n_avg > len(arr):\n      return np.array([])\n  mov_avg = np.convolve(arr, np.ones(n_avg)/n_avg, mode='valid')\n\n  return mov_avg\n\n",
  "canonical_solution": "  \n  sums = np.cumsum(arr, dtype=float)\n  sums[n_avg:] -= sums[:-n_avg]\n  return sums[n_avg - 1 :] / n_avg\n\n\n",
  "unprocess_testcases": "arr = [1, 2, 3, 4, 5]\nn_avg = 3\nassert((SOLUTION_SIGNATURE(arr, n_avg) == movmean(arr, n_avg)).all())\n\narr = [1, 2, 3, 4, 5]\nn_avg = 5\nassert((SOLUTION_SIGNATURE(arr, n_avg) == movmean(arr, n_avg)).all())\n\narr = [1, 2, 3]\nn_avg = 4\nassert((SOLUTION_SIGNATURE(arr, n_avg) == movmean(arr, n_avg)).all())\n"
 },
 {
  "task_id": "RWPB/9",
  "prompt": "def is_byte_token(s: str) -> bool:\n  \"\"\"Returns True if s is a byte string like \"<0xAB>\".\n    input:\n        s: string\n    return\n        bool: Ture or False\n  \"\"\"\n  # Bytes look like \"<0xAB>\".\n",
  "solution": "\ndef is_byte_token(s: str) -> bool:\n  \"\"\"Returns True if s is a byte string like \"<0xAB>\".\n    input:\n        s: string\n    return\n        bool: Ture or False\n  \"\"\"\n  return s.startswith(\"<0x\") and s.endswith(\">\")\n",
  "canonical_solution": "  \n  if len(s) != 6 or s[0:3] != \"<0x\" or s[-1] != \">\":\n    return False\n  return True\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(\"<0xAB>\") == is_byte_token(\"<0xAB>\"))\nassert(SOLUTION_SIGNATURE(\"<0xG1>\") == is_byte_token(\"<0xG1>\"))\nassert(SOLUTION_SIGNATURE(\"0xAB>\") == is_byte_token(\"0xAB>\"))\nassert(SOLUTION_SIGNATURE(\"<0xAB\") == is_byte_token(\"<0xAB\"))\n"
 },
 {
  "task_id": "RWPB/34",
  "prompt": "import numpy as np\n\ndef pad_tokens(tokens: np.ndarray, context_window: int, pad_token: int) -> np.ndarray:\n    \"\"\"\n    Pads or truncates a sequence of tokens to a specified length determined by the context window.\n\n    Args:\n        tokens (np.ndarray): An array of tokenized data, possibly with multiple sequences.\n                             The shape should typically be (batch_size, sequence_length) or (sequence_length,).\n        context_window (int): The target number of tokens in a sequence after padding or truncation.\n                              The final sequence length will be `context_window + 1`.\n        pad_token (int): The token used to pad shorter sequences.\n\n    Returns:\n        np.ndarray: An array where each sequence is exactly `context_window + 1` tokens long.\n\n    This function modifies the length of sequences in a batch of tokenized data:\n    - If a sequence is longer than `context_window + 1`, it gets truncated to this length.\n    - If a sequence is shorter, it gets padded with `pad_token` until it reaches the length of `context_window + 1`.\n    \"\"\"\n",
  "solution": "\nimport numpy as np\n\ndef pad_tokens(tokens: np.ndarray, context_window: int, pad_token: int) -> np.ndarray:\n    \"\"\"\n    Pads or truncates a sequence of tokens to a specified length determined by the context window.\n\n    Args:\n        tokens (np.ndarray): An array of tokenized data, possibly with multiple sequences.\n                             The shape should typically be (batch_size, sequence_length) or (sequence_length,).\n        context_window (int): The target number of tokens in a sequence after padding or truncation.\n                              The final sequence length will be `context_window + 1`.\n        pad_token (int): The token used to pad shorter sequences.\n\n    Returns:\n        np.ndarray: An array where each sequence is exactly `context_window + 1` tokens long.\n\n    This function modifies the length of sequences in a batch of tokenized data:\n    - If a sequence is longer than `context_window + 1`, it gets truncated to this length.\n    - If a sequence is shorter, it gets padded with `pad_token` until it reaches the length of `context_window + 1`.\n    \"\"\"\n    if tokens.ndim == 1:\n        tokens = tokens[None, :]\n    padded_tokens = []\n    for sequence in tokens:\n        if len(sequence) > context_window + 1:\n            sequence = sequence[:context_window + 1]\n        else:\n            sequence = np.pad(sequence, (0, context_window + 1 - len(sequence)), 'constant', constant_values=pad_token)\n        padded_tokens.append(sequence)\n    return np.array(padded_tokens)\n",
  "canonical_solution": "\n    # Determine the target length for each sequence\n    target_length = context_window + 1\n    current_length = tokens.shape[-1]\n\n    # Truncate if the current length is greater than the target length\n    if current_length > target_length:\n        tokens = tokens[..., :target_length]\n    # Pad if the current length is less than the target length\n    elif current_length < target_length:\n        # Calculate the required number of padding elements\n        padding_size = target_length - current_length\n        # Create a padding array filled with the pad_token\n        padding = np.full(tokens.shape[:-1] + (padding_size,), pad_token)\n        # Concatenate the original tokens with the padding array\n        tokens = np.concatenate([tokens, padding], axis=-1)\n\n    # Ensure the final tokens array is of the correct shape\n    assert tokens.shape[-1] == target_length, \"Final token sequence length does not match the target length.\"\n\n    return tokens\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5, 6, 7]), 4, 0) == pad_tokens(np.array([1, 2, 3, 4, 5, 6, 7]), 4, 0)).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3]), 5, 0) == pad_tokens(np.array([1, 2, 3]), 5, 0)).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5]), 4, 0) == pad_tokens(np.array([1, 2, 3, 4, 5]), 4, 0)).all())\n"
 },
 {
  "task_id": "RWPB/65",
  "prompt": "import itertools as it\nimport numpy as np\n\n\ndef merge_dicts_recursively(*dicts):\n    \"\"\"\n    Create a dictionary whose keys are the union of all input dictionaries. \n    The value for each key is based on the first dictionary in the list with that key.\n    Dictionaries later in the list have higher priority.\n    When values are dictionaries, it is applied recursively.\n    Args:\n        - *dicts: tuple of dicts, the dictionaries to merge\n    Returns:\n        result: dict, the merged dictionary\n    \"\"\"\n",
  "solution": "\n\n\ndef merge_dicts_recursively(*dicts):\n    \"\"\"\n    Create a dictionary whose keys are the union of all input dictionaries. \n    The value for each key is based on the first dictionary in the list with that key.\n    Dictionaries later in the list have higher priority.\n    When values are dictionaries, it is applied recursively.\n    Args:\n        - *dicts: tuple of dicts, the dictionaries to merge\n    Returns:\n        result: dict, the merged dictionary\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            if k in result and isinstance(v, dict) and isinstance(result[k], dict):\n                result[k] = merge_dicts_recursively(result[k], v)\n            else:\n                result[k] = v\n    return result\n\n\n",
  "canonical_solution": "    \n    result = dict()\n    all_items = it.chain(*[d.items() for d in dicts])\n    for key, value in all_items:\n        if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n            result[key] = merge_dicts_recursively(result[key], value)\n        else:\n            result[key] = value\n    return result\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE({'a': 1, 'b': 2}, {'b': 3, 'c': 4}) == merge_dicts_recursively({'a': 1, 'b': 2}, {'b': 3, 'c': 4}))\nassert(SOLUTION_SIGNATURE({'a': 1, 'nested': {'x': 10, 'y': 20}}, {'b': 2, 'nested': {'y': 30, 'z': 40}}) == merge_dicts_recursively({'a': 1, 'nested': {'x': 10, 'y': 20}}, {'b': 2, 'nested': {'y': 30, 'z': 40}}))\nassert(SOLUTION_SIGNATURE({'a': [1, 2, 3], 'b': 'hello'}, {'a': 'not a list', 'b': {'nested': 'dict'}}) == merge_dicts_recursively({'a': [1, 2, 3], 'b': 'hello'}, {'a': 'not a list', 'b': {'nested': 'dict'}}))\n"
 },
 {
  "task_id": "RWPB/51",
  "prompt": "import torch\nimport torch.nn.functional as F\n\n\ndef _ssim(img1, img2, window, window_size, channel, size_average=True):\n    \"\"\"\n    Calculate the Structural Similarity Index (SSIM) between two images.\n\n    Arguments:\n    img1, img2 : torch.Tensor\n        The two images between which SSIM is to be computed. They should have the same dimensions.\n    window : torch.Tensor\n        The window tensor used for mean and variance calculation, typically a Gaussian window.\n    window_size : int\n        The size of the Gaussian window, used here for padding calculations.\n    channel : int\n        The number of channels in img1 and img2 (e.g., 3 for RGB, 1 for grayscale).\n    size_average : bool, optional\n        If True (default), returns the mean SSIM over all elements. If False, returns a tensor of SSIM values for each channel.\n\n    Returns:\n    torch.Tensor\n        SSIM index as a float or as a tensor based on 'size_average' parameter.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\nimport torch.nn.functional as F\n\n\ndef _ssim(img1, img2, window, window_size, channel, size_average=True):\n    \"\"\"\n    Calculate the Structural Similarity Index (SSIM) between two images.\n\n    Arguments:\n    img1, img2 : torch.Tensor\n        The two images between which SSIM is to be computed. They should have the same dimensions.\n    window : torch.Tensor\n        The window tensor used for mean and variance calculation, typically a Gaussian window.\n    window_size : int\n        The size of the Gaussian window, used here for padding calculations.\n    channel : int\n        The number of channels in img1 and img2 (e.g., 3 for RGB, 1 for grayscale).\n    size_average : bool, optional\n        If True (default), returns the mean SSIM over all elements. If False, returns a tensor of SSIM values for each channel.\n\n    Returns:\n    torch.Tensor\n        SSIM index as a float or as a tensor based on'size_average' parameter.\n    \"\"\"\n    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1 * mu2\n    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n    C1 = (0.01 * 255) ** 2\n    C2 = (0.03 * 255) ** 2\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n    if size_average:\n        return ssim_map.mean()\n    else:\n        return ssim_map.mean(1).mean(1).mean(1)\n\n\n",
  "canonical_solution": "\n    # Apply convolution to calculate the mean of img1 and img2 using the specified window\n    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n\n    # Compute the squares of means\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    # Compute the mean of products\n    mu1_mu2 = mu1 * mu2\n\n    # Calculate variance of img1 and img2, and the covariance between them\n    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n\n    # Constants for stability (avoid division by zero)\n    C1 = 0.01 ** 2\n    C2 = 0.03 ** 2\n\n    # Compute the SSIM index\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n\n    # Return the average SSIM over all pixels if size_average is True, else return SSIM map\n    if size_average:\n        return ssim_map.mean()  # average over all channels and pixels\n    else:\n        return ssim_map.mean(1).mean(1).mean(1)  # average over each channel separately\n\n\n",
  "unprocess_testcases": "def gaussian_window(channel, window_size):\n    \"\"\" Creates a Gaussian window tensor for given channel and window size. \"\"\"\n    # Ensure the operation is done with tensors\n    _1D_window = torch.exp(-torch.arange(window_size).float()**2 / (2 * window_size**2))\n    _1D_window = _1D_window / _1D_window.sum()  # Normalize the window\n    _2D_window = _1D_window[:, None] * _1D_window[None, :]  # Create 2D Gaussian window\n    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n    return window\n\n\nimg = torch.rand(1, 3, 256, 256)\nassert((SOLUTION_SIGNATURE(img, img.clone(), gaussian_window(3, 11), 11, 3) == _ssim(img, img.clone(), gaussian_window(3, 11), 11, 3)).all())\n\nimg1 = torch.zeros(1, 1, 512, 512)  # Black image\nimg2 = torch.ones(1, 1, 512, 512)   # White image\nwindow = gaussian_window(1, 25)\nassert((SOLUTION_SIGNATURE(img1, img2, window, 25, 1, False) == _ssim(img1, img2, window, 25, 1, False)).all())\n\nimg1 = torch.rand(1, 1, 300, 300)\nimg2 = torch.rand(1, 1, 300, 300)\nwindow = gaussian_window(1, 25)\nassert((SOLUTION_SIGNATURE(img1, img2, window, 11, 1) == _ssim(img1, img2, window, 11, 1)).all())\n"
 },
 {
  "task_id": "RWPB/10",
  "prompt": "import torch\n\ndef euler2rot(euler_angle):\n    \"\"\"\n    Converts Euler angles to a rotation matrix.\n\n    Args:\n    euler_angle (torch.Tensor): Tensor of shape [batch_size, 3] containing Euler angles.\n\n    Returns:\n    torch.Tensor: Rotation matrix of shape [batch_size, 3, 3].\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef euler2rot(euler_angle):\n    \"\"\"\n    Converts Euler angles to a rotation matrix.\n\n    Args:\n    euler_angle (torch.Tensor): Tensor of shape [batch_size, 3] containing Euler angles.\n\n    Returns:\n    torch.Tensor: Rotation matrix of shape [batch_size, 3, 3].\n    \"\"\"\n    batch_size = euler_angle.shape[0]\n    cos_x = torch.cos(euler_angle[:, 0]).unsqueeze(1)\n    sin_x = torch.sin(euler_angle[:, 0]).unsqueeze(1)\n    cos_y = torch.cos(euler_angle[:, 1]).unsqueeze(1)\n    sin_y = torch.sin(euler_angle[:, 1]).unsqueeze(1)\n    cos_z = torch.cos(euler_angle[:, 2]).unsqueeze(1)\n    sin_z = torch.sin(euler_angle[:, 2]).unsqueeze(1)\n    rot_x = torch.zeros(batch_size, 3, 3).to(euler_angle.device)\n    rot_x[:, 0, 0] = 1\n    rot_x[:, 1, 1] = cos_x[:, 0]\n    rot_x[:, 1, 2] = -sin_x[:, 0]\n    rot_x[:, 2, 1] = sin_x[:, 0]\n    rot_x[:, 2, 2] = cos_x[:, 0]\n    rot_y = torch.zeros(batch_size, 3, 3).to(euler_angle.device)\n    rot_y[:, 0, 0] = cos_y[:, 0]\n    rot_y[:, 0, 2] = sin_y[:, 0]\n    rot_y[:, 1, 1] = 1\n    rot_y[:, 2, 0] = -sin_y[:, 0]\n    rot_y[:, 2, 2] = cos_y[:, 0]\n    rot_z = torch.zeros(batch_size, 3, 3).to(euler_angle.device)\n    rot_z[:, 0, 0] = cos_z[:, 0]\n    rot_z[:, 0, 1] = -sin_z[:, 0]\n    rot_z[:, 1, 0] = sin_z[:, 0]\n    rot_z[:, 1, 1] = cos_z[:, 0]\n    rot_z[:, 2, 2] = 1\n    rot_mat = torch.matmul(torch.matmul(rot_x, rot_y), rot_z)\n\n    return rot_mat\n\n\n",
  "canonical_solution": "\n    batch_size = euler_angle.shape[0]\n    theta = euler_angle[:, 0].reshape(-1, 1, 1)\n    phi = euler_angle[:, 1].reshape(-1, 1, 1)\n    psi = euler_angle[:, 2].reshape(-1, 1, 1)\n    one = torch.ones((batch_size, 1, 1), dtype=torch.float32, device=euler_angle.device)\n    zero = torch.zeros((batch_size, 1, 1), dtype=torch.float32, device=euler_angle.device)\n    rot_x = torch.cat((\n        torch.cat((one, zero, zero), 1),\n        torch.cat((zero, theta.cos(), theta.sin()), 1),\n        torch.cat((zero, -theta.sin(), theta.cos()), 1),\n    ), 2)\n    rot_y = torch.cat((\n        torch.cat((phi.cos(), zero, -phi.sin()), 1),\n        torch.cat((zero, one, zero), 1),\n        torch.cat((phi.sin(), zero, phi.cos()), 1),\n    ), 2)\n    rot_z = torch.cat((\n        torch.cat((psi.cos(), -psi.sin(), zero), 1),\n        torch.cat((psi.sin(), psi.cos(), zero), 1),\n        torch.cat((zero, zero, one), 1)\n    ), 2)\n    return torch.bmm(rot_x, torch.bmm(rot_y, rot_z))\n\n\n",
  "unprocess_testcases": "a = torch.tensor([[0.0, 0.0, 0.0]])\nb = torch.tensor([[0.0, 0.0, torch.pi / 4]])\nc = torch.tensor([[torch.pi / 2, torch.pi / 4, torch.pi / 6]])\n\nassert((SOLUTION_SIGNATURE(a) == euler2rot(a)).all())\nassert((SOLUTION_SIGNATURE(b) == euler2rot(b)).all())\nassert((SOLUTION_SIGNATURE(c) == euler2rot(c)).all())\n"
 },
 {
  "task_id": "RWPB/41",
  "prompt": "def merge_short_sentences_en(sens):\n    \"\"\"Avoid short sentences by merging them with the following sentence.\n\n    Args:\n        List[str]: list of input sentences.\n\n    Returns:\n        List[str]: list of output sentences.\n    \"\"\"\n",
  "solution": "\ndef merge_short_sentences_en(sens):\n    \"\"\"Avoid short sentences by merging them with the following sentence.\n\n    Args:\n        List[str]: list of input sentences.\n\n    Returns:\n        List[str]: list of output sentences.\n    \"\"\"\n    output_sens = []\n    for i, sen in enumerate(sens):\n        if len(sen) < 10:\n            output_sens.append(f\"{sen} {sens[i + 1]}\")\n        else:\n            output_sens.append(sen)\n    return output_sens\n",
  "canonical_solution": "\n    sens_out = []\n    for s in sens:\n        # If the previous sentense is too short, merge them with\n        # the current sentence.\n        if len(sens_out) > 0 and len(sens_out[-1].split(\" \")) <= 2:\n            sens_out[-1] = sens_out[-1] + \" \" + s\n        else:\n            sens_out.append(s)\n    try:\n        if len(sens_out[-1].split(\" \")) <= 2:\n            sens_out[-2] = sens_out[-2] + \" \" + sens_out[-1]\n            sens_out.pop(-1)\n    except:\n        pass\n    return sens_out\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE([\"I am.\", \"Here now.\", \"John is going to the store.\", \"Oh.\", \"Really?\", \"That's interesting.\"]) == merge_short_sentences_en([\"I am.\", \"Here now.\", \"John is going to the store.\", \"Oh.\", \"Really?\", \"That's interesting.\"]))\nassert(SOLUTION_SIGNATURE([\"This is a proper sentence.\", \"Here is another example.\", \"Each of these sentences is long enough.\"]) == merge_short_sentences_en([\"This is a proper sentence.\", \"Here is another example.\", \"Each of these sentences is long enough.\"]))\nassert(SOLUTION_SIGNATURE([\"Weather is bad.\", \"Indeed.\", \"Very cold.\"]) == merge_short_sentences_en([\"Weather is bad.\", \"Indeed.\", \"Very cold.\"]))\n"
 },
 {
  "task_id": "RWPB/75",
  "prompt": "import numpy as np\n\ndef align_vector_to_another(a=np.array([0, 0, 1]), b=np.array([1, 0, 0])):\n    \"\"\"\n    Aligns vector 'a' to vector 'b' using an axis-angle rotation.\n\n    Parameters:\n    a (np.array): The initial vector to be aligned. Default is np.array([0, 0, 1]).\n    b (np.array): The target vector to align with. Default is np.array([1, 0, 0]).\n\n    Returns:\n    tuple: A tuple containing:\n        - axis_ (np.array): The normalized axis of rotation.\n        - angle (float): The angle of rotation in radians.\n\n    Process:\n    1. Check if vectors 'a' and 'b' are already aligned. If they are, return (None, None).\n    2. Compute the cross product of 'a' and 'b' to find the rotation axis.\n    3. Normalize the rotation axis to get a unit vector.\n    4. Calculate the angle between 'a' and 'b' using the arccosine of their dot product.\n\n    If 'a' and 'b' are not aligned, the function returns the necessary rotation axis and angle to align 'a' with 'b'.\n    \"\"\"\n",
  "solution": "\nimport numpy as np\n\ndef align_vector_to_another(a=np.array([0, 0, 1]), b=np.array([1, 0, 0])):\n    \"\"\"\n    Aligns vector 'a' to vector 'b' using an axis-angle rotation.\n\n    Parameters:\n    a (np.array): The initial vector to be aligned. Default is np.array([0, 0, 1]).\n    b (np.array): The target vector to align with. Default is np.array([1, 0, 0]).\n\n    Returns:\n    tuple: A tuple containing:\n        - axis_ (np.array): The normalized axis of rotation.\n        - angle (float): The angle of rotation in radians.\n\n    Process:\n    1. Check if vectors 'a' and 'b' are already aligned. If they are, return (None, None).\n    2. Compute the cross product of 'a' and 'b' to find the rotation axis.\n    3. Normalize the rotation axis to get a unit vector.\n    4. Calculate the angle between 'a' and 'b' using the arccosine of their dot product.\n\n    If 'a' and 'b' are not aligned, the function returns the necessary rotation axis and angle to align 'a' with 'b'.\n    \"\"\"\n    if np.allclose(a, b):\n        return (None, None)\n    axis_ = np.cross(a, b)\n    axis_ = axis_ / np.linalg.norm(axis_)\n    angle = np.arccos(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n\n    return (axis_, angle)\n",
  "canonical_solution": "    \n    if np.array_equal(a, b):\n        return None, None\n    axis_ = np.cross(a, b)\n    axis_ = axis_ / np.linalg.norm(axis_)\n    angle = np.arccos(np.dot(a, b))\n\n    return axis_, angle\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([1, 0, 0]))[0] == align_vector_to_another(a = np.array([1, 0, 0]), b = np.array([1, 0, 0]))[0]))\nassert((SOLUTION_SIGNATURE(a = np.array([0, 0, 1]), b = np.array([1, 0, 0]))[0] == align_vector_to_another(a = np.array([0, 0, 1]), b = np.array([1, 0, 0]))[0]).all())\nassert((SOLUTION_SIGNATURE(a = np.array([1, 0, 1]), b = np.array([-1, 0, 0]))[0] == align_vector_to_another(a = np.array([1, 0, 1]), b = np.array([-1, 0, 0]))[0]).all())\nassert((SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([1, 0, 1]))[0] == align_vector_to_another(a = np.array([1, 0, 0]), b = np.array([1, 0, 1]))[0]).all())\nassert((SOLUTION_SIGNATURE(a = np.array([1, 1, 1]), b = np.array([-1, 1, -1]))[0] == align_vector_to_another(a = np.array([1, 1, 1]), b = np.array([-1, 1, -1]))[0]).all())\nassert(SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([1, 0, 0]))[1] == align_vector_to_another(a = np.array([1, 0, 0]), b = np.array([1, 0, 0]))[1])\nassert(SOLUTION_SIGNATURE(a = np.array([0, 0, 1]), b = np.array([1, 0, 0]))[1] == align_vector_to_another(a = np.array([0, 0, 1]), b = np.array([1, 0, 0]))[1])\nassert(SOLUTION_SIGNATURE(a = np.array([1, 0, 1]), b = np.array([-1, 0, 0]))[1] == align_vector_to_another(a = np.array([1, 0, 1]), b = np.array([-1, 0, 0]))[1])\nassert(SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([1, 0, 1]))[1] == align_vector_to_another(a = np.array([1, 0, 0]), b = np.array([1, 0, 1]))[1])\nassert(SOLUTION_SIGNATURE(a = np.array([1, 1, 1]), b = np.array([-1, 1, -1]))[1] == align_vector_to_another(a = np.array([1, 1, 1]), b = np.array([-1, 1, -1]))[1])\n"
 },
 {
  "task_id": "RWPB/24",
  "prompt": "import re\nimport hashlib\n\n\ndef check(x, checksum_token):\n    \"\"\"\n    Verifies if the provided checksum in a given string matches the SHA-256 hash of the string with a replaced checksum.\n\n    Parameters\n    ----------\n    x : str\n        The input string that contains a checksum to be verified.\n    checksum_token : str\n        A placeholder token that will replace the current checksum in the string for hashing.\n\n    Returns\n    -------\n    bool\n        True if the checksum is correct, False otherwise.\n\n    Notes\n    -----\n    This function uses a regular expression to find the checksum in the input string `x`.\n    It then replaces the checksum with `checksum_token`, hashes the resulting string using SHA-256,\n    and compares the hash with the original checksum. If they match, the function returns True,\n    indicating that the checksum is valid. If they do not match or if no checksum is found,\n    the function returns False.\n    \"\"\"\n",
  "solution": "\n\n\ndef check(x, checksum_token):\n    \"\"\"\n    Verifies if the provided checksum in a given string matches the SHA-256 hash of the string with a replaced checksum.\n\n    Parameters\n    ----------\n    x : str\n        The input string that contains a checksum to be verified.\n    checksum_token : str\n        A placeholder token that will replace the current checksum in the string for hashing.\n\n    Returns\n    -------\n    bool\n        True if the checksum is correct, False otherwise.\n\n    Notes\n    -----\n    This function uses a regular expression to find the checksum in the input string `x`.\n    It then replaces the checksum with `checksum_token`, hashes the resulting string using SHA-256,\n    and compares the hash with the original checksum. If they match, the function returns True,\n    indicating that the checksum is valid. If they do not match or if no checksum is found,\n    the function returns False.\n    \"\"\"\n    pattern = r\"(?<=\\[)(\\w{64})(?=\\])\"\n    match = re.search(pattern, x)\n    if match:\n        checksum = match.group(1)\n        x = x.replace(checksum, checksum_token)\n        hash_object = hashlib.sha256(x.encode())\n        hex_digest = hash_object.hexdigest()\n        return hex_digest == checksum\n    return False\n\n\n",
  "canonical_solution": "\n\n    # Pre-compiled regular expression for matching and replacing checksums\n    re_checksum = re.compile(r'\"Checksum\": \"([0-9a-fA-F]{64})\"')\n\n    # Search for the checksum in the input string\n    m = re.search(re_checksum, x)\n    if not m:\n        return False\n\n    # Replace the checksum with the provided token\n    replaced = re.sub(re_checksum, f'\"Checksum\": \"{checksum_token}\"', x)\n\n    # Calculate the SHA-256 hash of the modified string\n    h = hashlib.sha256(replaced.encode(\"utf8\"))\n\n    # Compare the calculated hash with the original checksum\n    return h.hexdigest() == m.group(1)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE('{\"Data\": \"Example data\", \"Checksum\": \"0b3ad42a156a99abcce760eee811fcfcbb806126697b77c5eb9638e20c455439\"}', checksum_token = \"TOKEN\") == check('{\"Data\": \"Example data\", \"Checksum\": \"0b3ad42a156a99abcce760eee811fcfcbb806126697b77c5eb9638e20c455439\"}', checksum_token = \"TOKEN\"))\nassert(SOLUTION_SIGNATURE('{\"Data\": \"Example data\", \"Checksum\": \"f03a9dcfea7e161acf569d267c75034617de5779b416ea8e4aea18d52318dd4b\"}', checksum_token = \"Apple\") == check('{\"Data\": \"Example data\", \"Checksum\": \"f03a9dcfea7e161acf569d267c75034617de5779b416ea8e4aea18d52318dd4b\"}', checksum_token = \"Apple\"))\nassert(SOLUTION_SIGNATURE('{\"Data\": \"Example data\", \"Checksum\": \"f03a9dcfea7e161acf569d267c75034617de5779b416ea8e4aea18d52318dd4b\"}', checksum_token = \"LLMs\") == check('{\"Data\": \"Example data\", \"Checksum\": \"f03a9dcfea7e161acf569d267c75034617de5779b416ea8e4aea18d52318dd4b\"}', checksum_token = \"LLMs\"))\n"
 },
 {
  "task_id": "RWPB/86",
  "prompt": "import torch\n\ndef inverse_sigmoid(x):\n    \"\"\"\n    Computes the inverse sigmoid of a tensor.\n\n    Args:\n    x (torch.Tensor): Input tensor.\n\n    Returns:\n    torch.Tensor: Tensor with the inverse sigmoid applied.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef inverse_sigmoid(x):\n    \"\"\"\n    Computes the inverse sigmoid of a tensor.\n\n    Args:\n    x (torch.Tensor): Input tensor.\n\n    Returns:\n    torch.Tensor: Tensor with the inverse sigmoid applied.\n    \"\"\"\n    return torch.log(x / (1 - x))\n\n\n",
  "canonical_solution": "    \n    return torch.log(x / (1 - x))\n\n",
  "unprocess_testcases": "x = torch.tensor([0.2, 0.5, 0.8])\nassert((SOLUTION_SIGNATURE(x) == inverse_sigmoid(x)).all())\n\nx = torch.tensor([0.001, 0.999])\nassert((SOLUTION_SIGNATURE(x) == inverse_sigmoid(x)).all())\n\nx = torch.tensor([0.0, 1.0])\nassert((SOLUTION_SIGNATURE(x) == inverse_sigmoid(x)).all())\n"
 },
 {
  "task_id": "RWPB/128",
  "prompt": "import numpy as np\nimport torch\n\n\ndef xywh2xyxy(x):\n    \"\"\"\n    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n    top-left corner and (x2, y2) is the bottom-right corner.\n\n    Args:\n        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.\n\n    Returns:\n        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.\n    \"\"\"\n",
  "solution": "\n\n\ndef xywh2xyxy(x):\n    \"\"\"\n    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n    top-left corner and (x2, y2) is the bottom-right corner.\n\n    Args:\n        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.\n\n    Returns:\n        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.\n    \"\"\"\n    if isinstance(x, np.ndarray):\n        y = np.zeros_like(x)\n    elif isinstance(x, torch.Tensor):\n        y = torch.zeros_like(x)\n    else:\n        raise ValueError(\"Input must be a numpy array or a torch tensor.\")\n    y[:, 0] = x[:, 0] - x[:, 2] / 2  # x1 = x - width / 2\n    y[:, 1] = x[:, 1] - x[:, 3] / 2  # y1 = y - height / 2\n    y[:, 2] = x[:, 0] + x[:, 2] / 2  # x2 = x + width / 2\n    y[:, 3] = x[:, 1] + x[:, 3] / 2  # y2 = y + height / 2\n    return y\n\n\n",
  "canonical_solution": "    \n    assert x.shape[-1] == 4, f\"input shape last dimension expected 4 but input shape is {x.shape}\"\n    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\n    dw = x[..., 2] / 2  # half-width\n    dh = x[..., 3] / 2  # half-height\n    y[..., 0] = x[..., 0] - dw  # top left x\n    y[..., 1] = x[..., 1] - dh  # top left y\n    y[..., 2] = x[..., 0] + dw  # bottom right x\n    y[..., 3] = x[..., 1] + dh  # bottom right y\n    return y\n\n",
  "unprocess_testcases": "x = np.array([[10, 20, 30, 40]])\nassert((SOLUTION_SIGNATURE(x) == xywh2xyxy(x)).all())\n\nx = np.array([[10, 10, 20, 20], [30, 40, 10, 10], [50, 60, 30, 40]])\nassert((SOLUTION_SIGNATURE(x) == xywh2xyxy(x)).all())\n\nx = torch.tensor([[15, 25, 0, 35]])\nassert((SOLUTION_SIGNATURE(x) == xywh2xyxy(x)).all())\n"
 },
 {
  "task_id": "RWPB/108",
  "prompt": "import torch\n\ndef norm_grid(v_grid):\n    \"\"\"Normalize the grid values to the range [-1, 1].\n\n    Args:\n        v_grid (torch.Tensor): The input grid of shape (B, 2, H, W).\n\n    Returns:\n        torch.Tensor: The normalized grid of shape (B, H, W, 2).\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef norm_grid(v_grid):\n    \"\"\"Normalize the grid values to the range [-1, 1].\n\n    Args:\n        v_grid (torch.Tensor): The input grid of shape (B, 2, H, W).\n\n    Returns:\n        torch.Tensor: The normalized grid of shape (B, H, W, 2).\n    \"\"\"\n    v_grid_norm = v_grid.clone()\n    v_grid_norm[:, 0, :, :] = 2.0 * (v_grid_norm[:, 0, :, :] / (v_grid_norm.shape[2] - 1) - 0.5)\n    v_grid_norm[:, 1, :, :] = 2.0 * (v_grid_norm[:, 1, :, :] / (v_grid_norm.shape[3] - 1) - 0.5)\n    v_grid_norm = v_grid_norm.permute(0, 2, 3, 1)\n\n    return v_grid_norm\n\n\n",
  "canonical_solution": "    \n    _, _, H, W = v_grid.size()\n\n    # scale grid to [-1,1]\n    v_grid_norm = torch.zeros_like(v_grid)\n    v_grid_norm[:, 0, :, :] = 2.0 * v_grid[:, 0, :, :] / (W - 1) - 1.0\n    v_grid_norm[:, 1, :, :] = 2.0 * v_grid[:, 1, :, :] / (H - 1) - 1.0\n    return v_grid_norm.permute(0, 2, 3, 1)  # BHW2\n\n\n",
  "unprocess_testcases": "v_grid = torch.rand((2, 2, 4, 4))\nassert((SOLUTION_SIGNATURE(v_grid) == norm_grid(v_grid)).all())\n\nv_grid = torch.tensor([[[[0, 1], [0, 1]], [[0, 0], [1, 1]]]])\nassert((SOLUTION_SIGNATURE(v_grid) == norm_grid(v_grid)).all())\n\nv_grid = torch.rand((1, 2, 3, 5))\nassert((SOLUTION_SIGNATURE(v_grid) == norm_grid(v_grid)).all())\n"
 },
 {
  "task_id": "RWPB/55",
  "prompt": "import numpy as np\nimport torch\n\n\ndef numpy_to_pytorch(x):\n    \"\"\"\n    Converts a numpy array to a PyTorch tensor with specific preprocessing.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        A numpy array to be converted to a PyTorch tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        A PyTorch tensor with the same underlying data as the input numpy array,\n        normalized and reshaped to have a batch dimension.\n\n    Notes\n    -----\n    This function performs the following operations on the input numpy array:\n    - Converts the data type to np.float32.\n    - Normalizes the values by dividing by 255.0, which is often used for image pixel values.\n    - Adds a batch dimension by prepending a singleton dimension (None).\n    - Creates a contiguous array to ensure the data layout is compatible with PyTorch.\n    - Converts the numpy array to a PyTorch tensor using torch.from_numpy.\n    - Casts the resulting tensor to a float type.\n\n    This is a common preprocessing step when preparing numpy arrays for use with PyTorch models,\n    especially in computer vision tasks where image data is involved.\n    \"\"\"\n",
  "solution": "\n\n\ndef numpy_to_pytorch(x):\n    x = x.astype(np.float32)\n    x /= 255.0\n    x = np.expand_dims(x, axis=0)\n    x = np.ascontiguousarray(x)\n    x = torch.from_numpy(x)\n    x = x.float()\n\n    return x\n\n\n",
  "canonical_solution": "    \n    # Convert numpy array to float32 and normalize by dividing by 255.0\n    y = x.astype(np.float32) / 255.0\n\n    # Add a batch dimension by prepending a singleton dimension\n    y = y[None]\n\n    # Ensure the array is contiguous in memory for compatibility with PyTorch\n    y = np.ascontiguousarray(y.copy())\n\n    # Convert the numpy array to a PyTorch tensor and cast it to float\n    y = torch.from_numpy(y).float()\n\n    return y\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([[10, 20, 30], [40, 50, 60]])) == numpy_to_pytorch(np.array([[10, 20, 30], [40, 50, 60]]))).all())\nassert((SOLUTION_SIGNATURE(np.array([[255]])) == numpy_to_pytorch(np.array([[255]]))).all())\nassert((SOLUTION_SIGNATURE(np.array([[[255, 0, 0], [0, 255, 0], [0, 0, 255]], [[100, 100, 100], [150, 150, 150], [200, 200, 200]], [[50, 25, 75], [125, 75, 175], [225, 125, 25]]])) == numpy_to_pytorch(np.array([[[255, 0, 0], [0, 255, 0], [0, 0, 255]], [[100, 100, 100], [150, 150, 150], [200, 200, 200]], [[50, 25, 75], [125, 75, 175], [225, 125, 25]]]))).all())\n"
 },
 {
  "task_id": "RWPB/92",
  "prompt": "import math\nimport numpy as np\nfrom decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\ndef round(number, num_decimal_places, leave_int=False):\n    '''\n    Purpose: round a number to the specified number of decimal places. Existing \n        round functions may not round correctly so that's why I built my own.\n    Args:\n        - number: float, number to round\n        - num_decimal_places: int, number of decimal places to round number to\n        - leave_int: bool, whether to leave integers unchanged or convert them to floats\n\n    Returns:\n        - float: rounded number\n    '''\n",
  "solution": "\n\n\ndef round(number, num_decimal_places, leave_int=False):\n    '''\n    Purpose: round a number to the specified number of decimal places. Existing \n        round functions may not round correctly so that's why I built my own.\n    Args:\n        - number: float, number to round\n        - num_decimal_places: int, number of decimal places to round number to\n        - leave_int: bool, whether to leave integers unchanged or convert them to floats\n\n    Returns:\n        - float: rounded number\n    '''\n    if leave_int and number == int(number):\n        return number\n    if num_decimal_places == 0:\n        return int(number)\n    decimal_places = 10 ** num_decimal_places\n    rounded_number = Decimal(number * decimal_places).quantize(Decimal('1'), rounding=ROUND_HALF_UP) / decimal_places\n    return float(rounded_number)\n\n\n",
  "canonical_solution": "    \n    if leave_int and int(number) == number:\n        return number\n    decimal_str = '1.'\n    for decimal_place in range(num_decimal_places):\n        decimal_str += '1'\n    return float(Decimal(str(number)).quantize(Decimal(decimal_str), rounding=ROUND_HALF_UP))\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(3.14159265, 4) == round(3.14159265, 4))\nassert(SOLUTION_SIGNATURE(200, 2, leave_int=True) == round(200, 2, leave_int=True))\nassert(SOLUTION_SIGNATURE(2.789, 0) == round(2.789, 0))\n"
 },
 {
  "task_id": "RWPB/61",
  "prompt": "import torch\nimport torch.nn.functional as F\n\n\ndef layer_norm_ref(x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False):\n    \"\"\"\n    Applies Layer Normalization to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        The input tensor.\n    weight : torch.Tensor\n        The weight tensor for scaling the normalized input.\n    bias : torch.Tensor or None\n        The bias tensor for shifting the normalized input. If None, no bias is added.\n    residual : torch.Tensor or None\n        The residual tensor to be added to the normalized input. If None, no residual is added.\n    eps : float, optional\n        A small constant added for numerical stability.\n    prenorm : bool, optional\n        If True, the function returns both the normalized output and the original input.\n    upcast : bool, optional\n        If True, upcasts all tensors to float type for computation.\n\n    Returns\n    -------\n    torch.Tensor or tuple of torch.Tensor\n        The layer-normalized output tensor. If `prenorm` is True, returns a tuple\n        containing the output tensor and the original input tensor.\n\n    Notes\n    -----\n    Layer Normalization normalizes the input tensor by subtracting the mean and dividing by\n    the standard deviation of each individual layer. This function performs the normalization,\n    applies the weight and bias, and optionally adds a residual connection. If `upcast` is True,\n    all tensors are upcast to float type before the operations for precision.\n    \"\"\"\n",
  "solution": "\n\n\ndef layer_norm_ref(x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False):\n    \"\"\"\n    Applies Layer Normalization to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        The input tensor.\n    weight : torch.Tensor\n        The weight tensor for scaling the normalized input.\n    bias : torch.Tensor or None\n        The bias tensor for shifting the normalized input. If None, no bias is added.\n    residual : torch.Tensor or None\n        The residual tensor to be added to the normalized input. If None, no residual is added.\n    eps : float, optional\n        A small constant added for numerical stability.\n    prenorm : bool, optional\n        If True, the function returns both the normalized output and the original input.\n    upcast : bool, optional\n        If True, upcasts all tensors to float type for computation.\n\n    Returns\n    -------\n    torch.Tensor or tuple of torch.Tensor\n        The layer-normalized output tensor. If `prenorm` is True, returns a tuple\n        containing the output tensor and the original input tensor.\n\n    Notes\n    -----\n    Layer Normalization normalizes the input tensor by subtracting the mean and dividing by\n    the standard deviation of each individual layer. This function performs the normalization,\n    applies the weight and bias, and optionally adds a residual connection. If `upcast` is True,\n    all tensors are upcast to float type before the operations for precision.\n    \"\"\"\n    if upcast:\n        x = x.float()\n        weight = weight.float()\n        if bias is not None:\n            bias = bias.float()\n        if residual is not None:\n            residual = residual.float()\n    mean = x.mean(-1, keepdim=True)\n    std = x.std(-1, keepdim=True)\n    normalized = (x - mean) / (std + eps)\n    output = F.linear(normalized, weight, bias)\n    if residual is not None:\n        output += residual\n    if prenorm:\n        return output, x\n    return output\n\n\n",
  "canonical_solution": "\n    # Preserve the original dtype\n    dtype = x.dtype\n\n    # Upcast weight, bias, and x to float if necessary\n    if upcast:\n        weight = weight.float()\n        bias = bias.float() if bias is not None else None\n        x = x.float()\n        residual = residual.float() if residual is not None else residual\n\n    # Add residual to input if provided\n    if residual is not None:\n        x = (x + residual).to(dtype)\n\n    # Apply layer normalization\n    # Note: It was x.to(weight.dtype) but it should be x because F.layer_norm expects the input tensor x.\n    out = F.layer_norm(x, x.shape[-1:], weight=weight, bias=bias, eps=eps).to(dtype)\n\n    # Return the output only or the output with the original input if prenorm is True\n    return out if not prenorm else (out, x)\n\n\n",
  "unprocess_testcases": "a = torch.rand(10, 5)\nb = torch.rand(5,)\n\nassert((SOLUTION_SIGNATURE(a, b, b) == layer_norm_ref(a, b, b)).all())\nassert((SOLUTION_SIGNATURE(a, b, b, a, 1e-5, False, True) == layer_norm_ref(a, b, b, a, 1e-5, False, True)).all())\nassert((SOLUTION_SIGNATURE(a, b, None, None, 1e-6, True, False)[0] == layer_norm_ref(a, b, None, None, 1e-6, True, False)[0]).all())\nassert((SOLUTION_SIGNATURE(a, b, None, None, 1e-6, True, False)[1] == layer_norm_ref(a, b, None, None, 1e-6, True, False)[1]).all())\nassert((SOLUTION_SIGNATURE(a, b, None, b, 1e-6, True, False)[0] == layer_norm_ref(a, b, None, b, 1e-6, True, False)[0]).all())\nassert((SOLUTION_SIGNATURE(a, b, None, b, 1e-6, True, False)[1] == layer_norm_ref(a, b, None, b, 1e-6, True, False)[1]).all())\nassert((SOLUTION_SIGNATURE(a, b, None) == layer_norm_ref(a, b, None)).all())\n"
 },
 {
  "task_id": "RWPB/30",
  "prompt": "import numpy as np\n\ndef get_confusion_matrix(gt_label, pred_label, num_classes):\n    \"\"\"\n    Calculates the confusion matrix from the ground truth and predicted labels.\n    If the length of gt_label is not equal to the length of pred_label, please return ValueError \n\n    Args:\n        gt_label (np.array): An array of ground truth labels.\n        pred_label (np.array): An array of predicted labels from the model.\n        num_classes (int): The number of classes in the dataset.\n\n    Returns:\n        np.array: A 2D array (matrix) of shape (num_classes, num_classes) representing the confusion matrix,\n                  where each row corresponds to the true classes and each column to the predicted classes.\n\n    The function calculates a confusion matrix which helps in evaluating the accuracy of a classification model.\n    It does so by cross-tabulating each class label with its corresponding prediction, thus providing insight\n    into misclassifications.\n    \"\"\"\n",
  "solution": "\nimport numpy as np\n\ndef get_confusion_matrix(gt_label, pred_label, num_classes):\n    \"\"\"\n    Calculates the confusion matrix from the ground truth and predicted labels.\n    If the length of gt_label is not equal to the length of pred_label, please return ValueError\n\n    Args:\n        gt_label (np.array): An array of ground truth labels.\n        pred_label (np.array): An array of predicted labels from the model.\n        num_classes (int): The number of classes in the dataset.\n\n    Returns:\n        np.array: A 2D array (matrix) of shape (num_classes, num_classes) representing the confusion matrix,\n                  where each row corresponds to the true classes and each column to the predicted classes.\n\n    The function calculates a confusion matrix which helps in evaluating the accuracy of a classification model.\n    It does so by cross-tabulating each class label with its corresponding prediction, thus providing insight\n    into misclassifications.\n    \"\"\"\n    if len(gt_label)!= len(pred_label):\n        raise ValueError(\"The length of gt_label and pred_label must be equal.\")\n    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n    for i in range(len(gt_label)):\n        true_class = gt_label[i]\n        predicted_class = pred_label[i]\n        confusion_matrix[true_class, predicted_class] += 1\n\n    return confusion_matrix\n",
  "canonical_solution": "\n    if gt_label.shape[-1] != pred_label.shape[-1]:\n        return ValueError\n    \n    # Compute the linear index for each element of the confusion matrix\n    index = (gt_label * num_classes + pred_label).astype('int32')\n\n    # Count the occurrence of each index\n    label_count = np.bincount(index, minlength=num_classes*num_classes)\n\n    # Initialize the confusion matrix with zeros\n    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n\n    # Fill the confusion matrix with counts\n    for i_label in range(num_classes):\n        for i_pred_label in range(num_classes):\n            cur_index = i_label * num_classes + i_pred_label\n            confusion_matrix[i_label, i_pred_label] = label_count[cur_index]\n\n    return confusion_matrix\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([0, 1, 2, 1, 0]), np.array([0, 2, 2, 1, 0]), 3) == get_confusion_matrix(np.array([0, 1, 2, 1, 0]), np.array([0, 2, 2, 1, 0]), 3)).all())\nassert((SOLUTION_SIGNATURE(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0]), 1) == get_confusion_matrix(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0]), 1)).all())\nassert(SOLUTION_SIGNATURE(np.array([0, 1, 2, 3]), np.array([0, 1, 2]), 4) == get_confusion_matrix(np.array([0, 1, 2, 3]), np.array([0, 1, 2]), 4))\n"
 },
 {
  "task_id": "RWPB/20",
  "prompt": "import math\n\n\ndef get_line_angle(x1, y1, x2, y2):\n    '''\n    Calculates the angle of a line segment relative to the horizontal axis.\n\n    Parameters\n    ----------\n    x1, y1 : float\n        Coordinates of the first point (x1, y1).\n    x2, y2 : float\n        Coordinates of the second point (x2, y2).\n\n    Return\n    ------\n    float\n        The angle of the line segment from point (x1, y1) to (x2, y2) in degrees.\n\n    The function computes the slope of the line segment and then uses the arctangent\n    function to determine the angle in radians. This angle is converted to degrees.\n    The function includes handling for vertical lines where the slope would be undefined.\n    '''\n",
  "solution": "\n\n\ndef get_line_angle(x1, y1, x2, y2):\n    '''\n    Calculates the angle of a line segment relative to the horizontal axis.\n\n    Parameters\n    ----------\n    x1, y1 : float\n        Coordinates of the first point (x1, y1).\n    x2, y2 : float\n        Coordinates of the second point (x2, y2).\n\n    Return\n    ------\n    float\n        The angle of the line segment from point (x1, y1) to (x2, y2) in degrees.\n\n    The function computes the slope of the line segment and then uses the arctangent\n    function to determine the angle in radians. This angle is converted to degrees.\n    The function includes handling for vertical lines where the slope would be undefined.\n    '''\n    if x1 == x2:\n        angle = math.pi / 2  # 90 degrees\n    else:\n        slope = (y2 - y1) / (x2 - x1)\n        angle = math.atan(slope)\n    return math.degrees(angle)\n\n\n",
  "canonical_solution": "\n    # Check for a vertical line to avoid division by zero\n    if x2 == x1:\n        return 90 if y2 > y1 else -90  # Directly return 90 or -90 degrees depending on the direction\n\n    # Calculate the slope of the line\n    slope = (y2 - y1) / (x2 - x1)\n\n    # Calculate the angle in radians\n    angle_radians = math.atan(slope)\n\n    # Convert the angle to degrees\n    angle_degrees = math.degrees(angle_radians)\n\n    return angle_degrees\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(0, 10, 10, 0) == get_line_angle(0, 10, 10, 0))\nassert(SOLUTION_SIGNATURE(5, 5, 5, 10) == get_line_angle(5, 5, 5, 10))\nassert(SOLUTION_SIGNATURE(0, 0, 10, 0) == get_line_angle(0, 0, 10, 0))\n"
 },
 {
  "task_id": "RWPB/71",
  "prompt": "from math import factorial\n\n\ndef getLambertCoefs(numOfLambertCoefs):\n    \"\"\"\n    Compute the Lambert coefficients for a given number of coefficients.\n    Args:\n        - numOfLambertCoefs: int, the number of coefficients\n    Returns:\n        lambert_coefs: list, the computed Lambert coefficients\n    \"\"\"\n",
  "solution": "\n\n\ndef getLambertCoefs(numOfLambertCoefs):\n    \"\"\"\n    Compute the Lambert coefficients for a given number of coefficients.\n    Args:\n        - numOfLambertCoefs: int, the number of coefficients\n    Returns:\n        lambert_coefs: list, the computed Lambert coefficients\n    \"\"\"\n    lambert_coefs = []\n    for i in range(numOfLambertCoefs):\n        lambert_coefs.append(factorial(i) / factorial(i + 1))\n    return lambert_coefs\n\n\n",
  "canonical_solution": "    \n    maxFactorial = factorial(numOfLambertCoefs-1)\n    return [maxFactorial*i**(i-1)//factorial(i) for i in range(1,numOfLambertCoefs)]\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(5) == getLambertCoefs(5))\nassert(SOLUTION_SIGNATURE(1) == getLambertCoefs(1))\nassert(SOLUTION_SIGNATURE(20) == getLambertCoefs(20))\nassert(SOLUTION_SIGNATURE(12) == getLambertCoefs(12))\nassert(SOLUTION_SIGNATURE(15) == getLambertCoefs(15))\n"
 },
 {
  "task_id": "RWPB/82",
  "prompt": "import importlib.util\nimport numpy as np\nimport torch\n\ndef is_tensor(x):\n    \"\"\"\n    Determines if the input `x` is a `torch.Tensor` or a `np.ndarray`.\n\n    Parameters:\n    x : any type\n        The input variable to be checked.\n\n    Returns:\n    bool\n        Returns True if `x` is either a `torch.Tensor` (if the PyTorch library is available) \n        or a `np.ndarray`. Otherwise, returns False.\n\n    Process:\n    1. Checks if the PyTorch library is available using `importlib.util.find_spec(\"torch\")`.\n    2. If PyTorch is available, imports the library and checks if `x` is an instance of `torch.Tensor`.\n    3. If `x` is not a `torch.Tensor` or if PyTorch is not available, it then checks if `x` is an instance of `np.ndarray`.\n    \"\"\"\n",
  "solution": "\nimport importlib.util\nimport numpy as np\nimport torch\n\ndef is_tensor(x):\n    \"\"\"\n    Determines if the input `x` is a `torch.Tensor` or a `np.ndarray`.\n\n    Parameters:\n    x : any type\n        The input variable to be checked.\n\n    Returns:\n    bool\n        Returns True if `x` is either a `torch.Tensor` (if the PyTorch library is available) \n        or a `np.ndarray`. Otherwise, returns False.\n\n    Process:\n    1. Checks if the PyTorch library is available using `importlib.util.find_spec(\"torch\")`.\n    2. If PyTorch is available, imports the library and checks if `x` is an instance of `torch.Tensor`.\n    3. If `x` is not a `torch.Tensor` or if PyTorch is not available, it then checks if `x` is an instance of `np.ndarray`.\n    \"\"\"\n    if importlib.util.find_spec(\"torch\") is not None:\n        import torch\n        return isinstance(x, torch.Tensor)\n    else:\n        return isinstance(x, np.ndarray)\n",
  "canonical_solution": "    \n    if importlib.util.find_spec(\"torch\") is not None:\n        import torch\n\n        if isinstance(x, torch.Tensor):\n            return True\n\n    return isinstance(x, np.ndarray)\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(np.array([1, 2, 3])) == is_tensor(np.array([1, 2, 3])))\nassert(SOLUTION_SIGNATURE([1, 2, 3]) == is_tensor([1, 2, 3]))\nassert(SOLUTION_SIGNATURE(torch.tensor([1, 2, 3])) == is_tensor(torch.tensor([1, 2, 3])))\nassert(SOLUTION_SIGNATURE(0) == is_tensor(0))\n"
 },
 {
  "task_id": "RWPB/45",
  "prompt": "import torch\n\ndef invalid_to_zeros(arr, valid_mask, ndim=999):\n    \"\"\"\n    Replaces invalid entries in an array with zeros, based on a validity mask.\n\n    Parameters\n    ----------\n    arr : torch.Tensor\n        The input tensor with potentially invalid entries.\n    valid_mask : torch.Tensor (boolean)\n        A boolean mask with the same number of elements as 'arr', where True indicates\n        a valid entry and False indicates an invalid entry that should be replaced with zero.\n    ndim : int, optional\n        The desired number of dimensions for the output tensor. If 'arr' has more than\n        'ndim' dimensions, it will be flattened accordingly. Default is 999, meaning no flattening.\n\n    Returns\n    -------\n    torch.Tensor\n        The input tensor 'arr' with invalid entries replaced by zeros.\n    int or torch.Tensor\n        The number of non-zero entries in the validity mask (nnz), which corresponds to the\n        number of valid points per image.\n\n    Notes\n    -----\n    This function is particularly useful in the context of point cloud processing or similar\n    applications where some data points may be invalid or missing. By setting these invalid\n    points to zero, the resulting tensor can be used for further processing, such as neural\n    network input.\n\n    If 'valid_mask' is provided, the function updates 'arr' in-place to zero out the invalid\n    entries. It also calculates the number of non-zero entries in the mask (nnz), which is\n    returned separately. If 'valid_mask' is None, it assumes all entries are valid and calculates\n    nnz based on the total number of elements in 'arr' divided by the first dimension size.\n\n    If 'arr' has more dimensions than specified by 'ndim', the function flattens it to the\n    desired number of dimensions.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef invalid_to_zeros(arr, valid_mask, ndim=999):\n    \"\"\"\n    Replaces invalid entries in an array with zeros, based on a validity mask.\n\n    Parameters\n    ----------\n    arr : torch.Tensor\n        The input tensor with potentially invalid entries.\n    valid_mask : torch.Tensor (boolean)\n        A boolean mask with the same number of elements as 'arr', where True indicates\n        a valid entry and False indicates an invalid entry that should be replaced with zero.\n    ndim : int, optional\n        The desired number of dimensions for the output tensor. If 'arr' has more than\n        'ndim' dimensions, it will be flattened accordingly. Default is 999, meaning no flattening.\n\n    Returns\n    -------\n    torch.Tensor\n        The input tensor 'arr' with invalid entries replaced by zeros.\n    int or torch.Tensor\n        The number of non-zero entries in the validity mask (nnz), which corresponds to the\n        number of valid points per image.\n\n    Notes\n    -----\n    This function is particularly useful in the context of point cloud processing or similar\n    applications where some data points may be invalid or missing. By setting these invalid\n    points to zero, the resulting tensor can be used for further processing, such as neural\n    network input.\n\n    If 'valid_mask' is provided, the function updates 'arr' in-place to zero out the invalid\n    entries. It also calculates the number of non-zero entries in the mask (nnz), which is\n    returned separately. If 'valid_mask' is None, it assumes all entries are valid and calculates\n    nnz based on the total number of elements in 'arr' divided by the first dimension size.\n\n    If 'arr' has more dimensions than specified by 'ndim', the function flattens it to the\n    desired number of dimensions.\n    \"\"\"\n    if valid_mask is not None:\n        arr[~valid_mask] = 0\n        nnz = valid_mask.sum()\n    else:\n        nnz = arr.shape[0] // arr.shape[1]\n    if arr.ndim > ndim:\n        arr = arr.flatten(ndim)\n\n    return arr, nnz\n\n\n",
  "canonical_solution": "\n    if valid_mask is not None:\n        arr = arr.clone()  # Create a copy of arr to avoid modifying the original tensor\n        arr[~valid_mask] = 0  # Set invalid entries (where mask is False) to zero\n        nnz = valid_mask.view(len(valid_mask), -1).sum(1)  # Calculate the number of valid points per image\n    else:\n        # If no valid_mask is provided, calculate nnz based on total elements\n        nnz = arr.numel() // len(arr) if len(arr) else 0\n\n    # If the number of dimensions in arr is greater than ndim, flatten it\n    if arr.ndim > ndim:\n        if ndim > 0:\n            arr = arr.reshape([arr.shape[0], -1])[:ndim]\n        else:\n            arr = arr.flatten()\n\n    return arr, nnz\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([1, 2, 3, 4]), valid_mask = torch.tensor([True, False, True, True]))[0] == invalid_to_zeros(torch.tensor([1, 2, 3, 4]), valid_mask = torch.tensor([True, False, True, True]))[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 2, 3, 4]), valid_mask = torch.tensor([True, False, True, True]))[1] == invalid_to_zeros(torch.tensor([1, 2, 3, 4]), valid_mask = torch.tensor([True, False, True, True]))[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 2, 3, 4]), torch.tensor([False, False, False, False]))[0] == invalid_to_zeros(torch.tensor([1, 2, 3, 4]), torch.tensor([False, False, False, False]))[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 2, 3, 4]), torch.tensor([False, False, False, False]))[1] == invalid_to_zeros(torch.tensor([1, 2, 3, 4]), torch.tensor([False, False, False, False]))[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]), torch.tensor([[[True, True], [False, True]], [[True, False], [True, True]]]), 1)[0] == invalid_to_zeros(torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]), torch.tensor([[[True, True], [False, True]], [[True, False], [True, True]]]), 1)[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]), torch.tensor([[[True, True], [False, True]], [[True, False], [True, True]]]), 1)[1] == invalid_to_zeros(torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]), torch.tensor([[[True, True], [False, True]], [[True, False], [True, True]]]), 1)[1]).all())\n"
 },
 {
  "task_id": "RWPB/14",
  "prompt": "import numpy as np\n\ndef align_vector_to_another(a=np.array([0, 0, 1]), b=np.array([1, 0, 0])):\n    '''Aligns vector a to vector b with axis angle rotation.\n    If array a is equal array b, please return (None, None)\n    Args:\n        - a: numpy.array. the first vector. \n        - b: numpy.array. the second vector.\n    Return:\n        - axis: numpy.array. the axis of vector\n        - angle: numpy.array. the angle of vector\n    '''\n",
  "solution": "\nimport numpy as np\n\ndef align_vector_to_another(a=np.array([0, 0, 1]), b=np.array([1, 0, 0])):\n    '''Aligns vector a to vector b with axis angle rotation.\n    If array a is equal array b, please return (None, None)\n    Args:\n        - a: numpy.array. the first vector. \n        - b: numpy.array. the second vector.\n    Return:\n        - axis: numpy.array. the axis of vector\n        - angle: numpy.array. the angle of vector\n    '''\n    if np.array_equal(a, b):\n        return (None, None)\n    axis = np.cross(a, b)\n    angle = np.arccos(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n    return (axis, angle)\n",
  "canonical_solution": "\n    if np.array_equal(a, b):\n        return None, None\n    axis_ = np.cross(a, b)\n    axis_ = axis_ / np.linalg.norm(axis_)\n    angle = np.arccos(np.dot(a, b))\n\n    return axis_, angle\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(a = [1, 0, 0], b = [1, 0, 0])[0] == align_vector_to_another(a = [1, 0, 0], b = [1, 0, 0])[0])\nassert((SOLUTION_SIGNATURE(a = [0, 0, 1], b = [0, 1, 0])[0] == align_vector_to_another(a = [0, 0, 1], b = [0, 1, 0])[0]).all())\nassert((SOLUTION_SIGNATURE(a = [1, 1, 0], b = [-1, 0, 0])[0] == align_vector_to_another(a = [1, 1, 0], b = [-1, 0, 0])[0]).all())\nassert(SOLUTION_SIGNATURE(a = [1, 0, 0], b = [1, 0, 0])[1] == align_vector_to_another(a = [1, 0, 0], b = [1, 0, 0])[1])\nassert((SOLUTION_SIGNATURE(a = [0, 0, 1], b = [0, 1, 0])[1] == align_vector_to_another(a = [0, 0, 1], b = [0, 1, 0])[1]).all())\nassert((SOLUTION_SIGNATURE(a = [1, 1, 0], b = [-1, 0, 0])[1] == align_vector_to_another(a = [1, 1, 0], b = [-1, 0, 0])[1]).all())\n"
 },
 {
  "task_id": "RWPB/118",
  "prompt": "import numpy as np\n\ndef filter_nan(ts, vs):\n  \"\"\"\n  Filter NaN values from a time series.\n  Args:\n      - ts: numpy.ndarray, the time series\n      - vs: numpy.ndarray, the values corresponding to the time series\n  Returns:\n      filtered_ts: numpy.ndarray, the time series with NaN values removed\n      filtered_vs: numpy.ndarray, the corresponding values with NaN values removed\n  \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\n\ndef filter_nan(ts, vs):\n  \"\"\"\n  Filter NaN values from a time series.\n  Args:\n      - ts: numpy.ndarray, the time series\n      - vs: numpy.ndarray, the values corresponding to the time series\n  Returns:\n      filtered_ts: numpy.ndarray, the time series with NaN values removed\n      filtered_vs: numpy.ndarray, the corresponding values with NaN values removed\n  \"\"\"\n  mask = np.isnan(vs)\n  filtered_ts = ts[~mask]\n  filtered_vs = vs[~mask]\n  return filtered_ts, filtered_vs\n\n",
  "canonical_solution": "  \n  good_vs = np.logical_not(np.isnan(vs))\n  return ts[good_vs], vs[good_vs]\n\n\n",
  "unprocess_testcases": "ts = np.array([1, 2, 3, 4, 5])\nvs = np.array([1.1, 2.2, 3.3, 4.4, 5.5])\nassert((SOLUTION_SIGNATURE(ts, vs)[0] == filter_nan(ts, vs)[0]).all())\nassert((SOLUTION_SIGNATURE(ts, vs)[1] == filter_nan(ts, vs)[1]).all())\n\nts = np.array([1, 2, 3, 4, 5])\nvs = np.array([np.nan, np.nan, np.nan, np.nan, np.nan])\nassert((SOLUTION_SIGNATURE(ts, vs)[0] == filter_nan(ts, vs)[0]).all())\nassert((SOLUTION_SIGNATURE(ts, vs)[1] == filter_nan(ts, vs)[1]).all())\n\nts = np.array([1, 2, 3, 4, 5])\nvs = np.array([1.1, np.nan, 3.3, np.nan, 5.5])\nassert((SOLUTION_SIGNATURE(ts, vs)[0] == filter_nan(ts, vs)[0]).all())\nassert((SOLUTION_SIGNATURE(ts, vs)[1] == filter_nan(ts, vs)[1]).all())\n"
 },
 {
  "task_id": "RWPB/6",
  "prompt": "def format_seconds(seconds: float) -> str:\n    \"\"\"Converts a float number (seconds) to a string in the format:\n\n    - HH:MM:SS.SSS when greater than or equal to one hour\n    - MM:SS.SSS when less than one hour\n\n    Args:\n        seconds: The float number to convert (in seconds).\n\n    Returns:\n        str: The formatted string representation of the time.\n    \"\"\"\n",
  "solution": "\n\n\ndef format_seconds(seconds: float) -> str:\n    \"\"\"Converts a float number (seconds) to a string in the format:\n\n    - HH:MM:SS.SSS when greater than or equal to one hour\n    - MM:SS.SSS when less than one hour\n\n    Args:\n        seconds: The float number to convert (in seconds).\n\n    Returns:\n        str: The formatted string representation of the time.\n    \"\"\"\n    hours = int(seconds // 3600)\n    minutes = int((seconds % 3600) // 60)\n    seconds = int(seconds % 60)\n    milliseconds = int((seconds - int(seconds)) * 1000)\n    if hours > 0:\n        return f\"{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}\"\n    else:\n        return f\"{minutes:02d}:{seconds:02d}.{milliseconds:03d}\"\n\n\n",
  "canonical_solution": "    \n    hours = int(seconds // 3600)  # Extract whole hours\n    minutes = int((seconds % 3600) // 60)  # Extract whole minutes\n    seconds = seconds % 60  # Extract remaining seconds\n\n    # Use f-strings for clean formatting\n    if hours > 0:\n        return f\"{hours:02d}:{minutes:02d}:{seconds:06.3f}\"\n    else:\n        return f\"{minutes:02d}:{seconds:06.3f}\"\n    \n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(90.123) == format_seconds(90.123))\nassert(SOLUTION_SIGNATURE(3600.0) == format_seconds(3600.0))\nassert(SOLUTION_SIGNATURE(5436.789) == format_seconds(5436.789))"
 },
 {
  "task_id": "RWPB/99",
  "prompt": "import math\n\ndef focal2fov(focal, pixels):\n    \"\"\"\n    Converts focal length to field of view (FOV).\n    If focal is zero, the code should return KeyError\n\n    Args:\n    focal (float): Focal length.\n    pixels (int): Number of pixels.\n\n    Returns:\n    float: Field of view in radians.\n    \"\"\"\n",
  "solution": "\n\n\nimport math\n\ndef focal2fov(focal, pixels):\n    \"\"\"\n    Converts focal length to field of view (FOV).\n    If focal is zero, the code should return KeyError\n\n    Args:\n    focal (float): Focal length.\n    pixels (int): Number of pixels.\n\n    Returns:\n    float: Field of view in radians.\n    \"\"\"\n    if focal == 0:\n        raise KeyError(\"Focal length cannot be zero.\")\n    return 2 * math.atan(pixels / (2 * focal))\n\n",
  "canonical_solution": "    \n    if int(focal) == 0:\n        return KeyError\n    return 2 * math.atan(pixels / (2 * focal))\n\n",
  "unprocess_testcases": "\nfocal = 49.1\npixels = 50000\nassert(SOLUTION_SIGNATURE(focal, pixels) == focal2fov(focal, pixels))\n\nfocal = 50.0\npixels = 1920\nassert(SOLUTION_SIGNATURE(focal, pixels) == focal2fov(focal, pixels))\n\nfocal = 1.0\npixels = 1920\nassert(SOLUTION_SIGNATURE(focal, pixels) == focal2fov(focal, pixels))\n\nfocal = 0\npixels = 2048\nassert(SOLUTION_SIGNATURE(focal, pixels) == focal2fov(focal, pixels))\n"
 },
 {
  "task_id": "RWPB/137",
  "prompt": "import numpy as np\nimport torch\nfrom sklearn.linear_model import LinearRegression\n\ndef fit_params(\n    x,\n    y,\n    fun,\n    a_range=(-10, 10),\n    b_range=(-10, 10),\n    grid_number=101,\n    iteration=3,\n    verbose=True,\n):\n    \"\"\"\n    fit a, b, c, d such that\n\n    .. math::\n        |y-(cf(ax+b)+d)|^2\n\n    is minimized. Both x and y are 1D array. Sweep a and b, find the best fitted model.\n\n    Args:\n    -----\n        x : 1D array\n            x values\n        y : 1D array\n            y values\n        fun : function\n            symbolic function\n        a_range : tuple\n            sweeping range of a\n        b_range : tuple\n            sweeping range of b\n        grid_num : int\n            number of steps along a and b\n        iteration : int\n            number of zooming in\n        verbose : bool\n            print extra information if True\n\n    Returns:\n    --------\n        a_best : float\n            best fitted a\n        b_best : float\n            best fitted b\n        c_best : float\n            best fitted c\n        d_best : float\n            best fitted d\n        r2_best : float\n            best r2 (coefficient of determination)\n\n    Example\n    -------\n    >>> num = 100\n    >>> x = torch.linspace(-1,1,steps=num)\n    >>> noises = torch.normal(0,1,(num,)) * 0.02\n    >>> y = 5.0*torch.sin(3.0*x + 2.0) + 0.7 + noises\n    >>> fit_params(x, y, torch.sin)\n    r2 is 0.9999727010726929\n    (tensor([2.9982, 1.9996, 5.0053, 0.7011]), tensor(1.0000))\n    \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\nimport torch\nfrom sklearn.linear_model import LinearRegression\n\ndef fit_params(\n    x,\n    y,\n    fun,\n    a_range=(-10, 10),\n    b_range=(-10, 10),\n    grid_number=101,\n    iteration=3,\n    verbose=True,\n):\n    \"\"\"\n    fit a, b, c, d such that\n\n   .. math::\n        |y-(cf(ax+b)+d)|^2\n\n    is minimized. Both x and y are 1D array. Sweep a and b, find the best fitted model.\n\n    Args:\n    -----\n        x : 1D array\n            x values\n        y : 1D array\n            y values\n        fun : function\n            symbolic function\n        a_range : tuple\n            sweeping range of a\n        b_range : tuple\n            sweeping range of b\n        grid_num : int\n            number of steps along a and b\n        iteration : int\n            number of zooming in\n        verbose : bool\n            print extra information if True\n\n    Returns:\n    --------\n        a_best : float\n            best fitted a\n        b_best : float\n            best fitted b\n        c_best : float\n            best fitted c\n        d_best : float\n            best fitted d\n        r2_best : float\n            best r2 (coefficient of determination)\n\n    Example\n    -------\n    >>> num = 100\n    >>> x = torch.linspace(-1,1,steps=num)\n    >>> noises = torch.normal(0,1,(num,)) * 0.02\n    >>> y = 5.0*torch.sin(3.0*x + 2.0) + 0.7 + noises\n    >>> fit_params(x, y, torch.sin)\n    r2 is 0.9999727010726929\n    (tensor([2.9982, 1.9996, 5.0053, 0.7011]), tensor(1.0000))\n    \"\"\"\n    a_min, a_max = a_range\n    b_min, b_max = b_range\n    a_grid = np.linspace(a_min, a_max, grid_number)\n    b_grid = np.linspace(b_min, b_max, grid_number)\n    a_best = None\n    b_best = None\n    c_best = None\n    d_best = None\n    r2_best = -np.inf\n    for a in a_grid:\n        for b in b_grid:\n            x_new = a * x + b\n            x_new = x_new.reshape(-1, 1)\n            y_new = y.reshape(-1, 1)\n            reg = LinearRegression().fit(x_new, y_new)\n            c = reg.coef_[0][0]\n            d = reg.intercept_[0]\n            y_pred = c * fun(a * x + b) + d\n            r2 = reg.score(x_new, y_new)\n            if r2 > r2_best:\n                a_best = a\n                b_best = b\n                c_best = c\n                d_best = d\n                r2_best = r2\n    if verbose:\n        print(f\"r2 is {r2_best}\")\n    return (a_best, b_best, c_best, d_best), r2_best\n\n\n",
  "canonical_solution": "    \n    # fit a, b, c, d such that y=c*fun(a*x+b)+d; both x and y are 1D array.\n    # sweep a and b, choose the best fitted model\n    for _ in range(iteration):\n        a_ = torch.linspace(a_range[0], a_range[1], steps=grid_number)\n        b_ = torch.linspace(b_range[0], b_range[1], steps=grid_number)\n        a_grid, b_grid = torch.meshgrid(a_, b_, indexing=\"ij\")\n        post_fun = fun(\n            a_grid[None, :, :] * x[:, None, None] + b_grid[None, :, :]\n        )\n        x_mean = torch.mean(post_fun, dim=[0], keepdim=True)\n        y_mean = torch.mean(y, dim=[0], keepdim=True)\n        numerator = (\n            torch.sum((post_fun - x_mean) * (y - y_mean)[:, None, None], dim=0)\n            ** 2\n        )\n        denominator = torch.sum((post_fun - x_mean) ** 2, dim=0) * torch.sum(\n            (y - y_mean)[:, None, None] ** 2, dim=0\n        )\n        r2 = numerator / (denominator + 1e-4)\n        r2 = torch.nan_to_num(r2)\n\n        best_id = torch.argmax(r2)\n        a_id, b_id = (\n            torch.div(best_id, grid_number, rounding_mode=\"floor\"),\n            best_id % grid_number,\n        )\n\n        if (\n            a_id == 0\n            or a_id == grid_number - 1\n            or b_id == 0\n            or b_id == grid_number - 1\n        ):\n            if _ == 0 and verbose:\n                pass\n            if a_id == 0:\n                a_arange = [a_[0], a_[1]]  # noqa\n            if a_id == grid_number - 1:\n                a_arange = [a_[-2], a_[-1]]  # noqa\n            if b_id == 0:\n                b_arange = [b_[0], b_[1]]  # noqa\n            if b_id == grid_number - 1:\n                b_arange = [b_[-2], b_[-1]]  # noqa\n\n        else:\n            a_range = [a_[a_id - 1], a_[a_id + 1]]\n            b_range = [b_[b_id - 1], b_[b_id + 1]]\n\n    a_best = a_[a_id]\n    b_best = b_[b_id]\n    post_fun = fun(a_best * x + b_best)\n    r2_best = r2[a_id, b_id]\n\n\n    post_fun = torch.nan_to_num(post_fun)\n    reg = LinearRegression().fit(\n        post_fun[:, None].detach().numpy(), y.detach().numpy()\n    )\n    c_best = torch.from_numpy(reg.coef_)[0]\n    d_best = torch.from_numpy(np.array(reg.intercept_))\n    return torch.stack([a_best, b_best, c_best, d_best]), r2_best\n\n\n\n\n",
  "unprocess_testcases": "x = torch.linspace(-1, 1, steps=100)\ny = 5.0 * torch.sin(3.0 * x + 2.0) + 0.7 + torch.normal(0, 1, (100,)) * 0.02\nassert((SOLUTION_SIGNATURE(x, y, torch.sin)[0] == fit_params(x, y, torch.sin)[0]).all())\nassert((SOLUTION_SIGNATURE(x, y, torch.sin)[1] == fit_params(x, y, torch.sin)[1]).all())\n\n\nx = torch.linspace(-2, 2, steps=50)\ny = 4.0 * torch.cos(2.0 * x + 1.0) + 1.0 + torch.normal(0, 1, (50,)) * 0.05\nassert((SOLUTION_SIGNATURE(x, y, torch.cos, a_range=(-2, 2), b_range=(-2, 2), grid_number=5)[0] == fit_params(x, y, torch.cos, a_range=(-2, 2), b_range=(-2, 2), grid_number=5)[0]).all())\nassert((SOLUTION_SIGNATURE(x, y, torch.cos, a_range=(-2, 2), b_range=(-2, 2), grid_number=5)[1] == fit_params(x, y, torch.cos, a_range=(-2, 2), b_range=(-2, 2), grid_number=5)[1]).all())\n\nx = torch.linspace(-3, 3, steps=150)\ny = 2.0 * torch.tanh(0.5 * x + 3.0) + 0.5 + torch.normal(0, 1, (150,)) * 0.1\nassert((SOLUTION_SIGNATURE(x, y, torch.tanh, iteration=5)[0] == fit_params(x, y, torch.tanh, iteration=5)[0]).all())\nassert((SOLUTION_SIGNATURE(x, y, torch.tanh, iteration=5)[1] == fit_params(x, y, torch.tanh, iteration=5)[1]).all())\n"
 },
 {
  "task_id": "RWPB/103",
  "prompt": "MAX_VAL = (1 << 256) - 1\n\n\n\n# does not revert on overflow\ndef unsafeAdd(x, y):\n    \"\"\"\n    Add two integers, ignoring overflow.\n    Args:\n        - x: int, first operand\n        - y: int, second operand\n    Returns:\n        result: int, the sum of x and y, with overflow ignored\n    \"\"\"\n",
  "solution": "\n\n\ndef unsafeAdd(x, y):\n    \"\"\"\n    Add two integers, ignoring overflow.\n    Args:\n        - x: int, first operand\n        - y: int, second operand\n    Returns:\n        result: int, the sum of x and y, with overflow ignored\n    \"\"\"\n    return (x + y) % MAX_VAL\n\n",
  "canonical_solution": "    \n    return (x + y) & MAX_VAL\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(10, 20) == unsafeAdd(10, 20))\nassert(SOLUTION_SIGNATURE(MAX_VAL - 5, 10) == unsafeAdd(MAX_VAL - 5, 10))\nassert(SOLUTION_SIGNATURE(MAX_VAL // 2, (MAX_VAL // 2) + 1) == unsafeAdd(MAX_VAL // 2, (MAX_VAL // 2) + 1))\n"
 },
 {
  "task_id": "RWPB/113",
  "prompt": "import torch\n\ndef _linspace(start: torch.Tensor, stop: torch.Tensor, num: int):\n    \"\"\"\n    Generate a sequence of evenly spaced values between start and stop tensors.\n\n    Args:\n        start (torch.Tensor): The starting values of the sequence.\n        stop (torch.Tensor): The stopping values of the sequence.\n        num (int): The number of values to generate.\n\n    Returns:\n        torch.Tensor: A tensor containing `num` evenly spaced values between `start` and `stop`.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef _linspace(start: torch.Tensor, stop: torch.Tensor, num: int):\n    \"\"\"\n    Generate a sequence of evenly spaced values between start and stop tensors.\n\n    Args:\n        start (torch.Tensor): The starting values of the sequence.\n        stop (torch.Tensor): The stopping values of the sequence.\n        num (int): The number of values to generate.\n\n    Returns:\n        torch.Tensor: A tensor containing `num` evenly spaced values between `start` and `stop`.\n    \"\"\"\n    return torch.linspace(start, stop, num)\n\n\n",
  "canonical_solution": "    \n    if num == 1:\n        return start.clone()\n\n    steps = torch.linspace(0, 1, num, dtype=start.dtype, device=start.device)\n\n    steps = steps.view(*([1] * (start.dim() - 1)), num)\n    out = start.unsqueeze(-1) + (stop - start).unsqueeze(-1) * steps\n    return out\n\n\n",
  "unprocess_testcases": "start = torch.tensor([0.])\nstop = torch.tensor([10.])\nnum = 5\nassert((SOLUTION_SIGNATURE(start, stop, num) == _linspace(start, stop, num)).all())\n\nstart = torch.tensor([1., 2., 3.])\nstop = torch.tensor([1., 2., 3.])\nnum = 1\nassert((SOLUTION_SIGNATURE(start, stop, num) == _linspace(start, stop, num)).all())\n\nstart = torch.tensor([0, 10], dtype=torch.int32)\nstop = torch.tensor([5, 15], dtype=torch.int32)\nnum = 3\nassert((SOLUTION_SIGNATURE(start, stop, num) == _linspace(start, stop, num)).all())\n\nstart = torch.tensor([5., 5., 5.])\nstop = torch.tensor([5., 5., 5.])\nnum = 3\nassert((SOLUTION_SIGNATURE(start, stop, num) == _linspace(start, stop, num)).all())\n\nstart = torch.tensor([[0., 0.], [10., 10.]])\nstop = torch.tensor([[5., 10.], [15., 20.]])\nnum = 4\nassert((SOLUTION_SIGNATURE(start, stop, num) == _linspace(start, stop, num)).all())\n"
 },
 {
  "task_id": "RWPB/127",
  "prompt": "import torch\n\ndef shift_dim(x, src_dim=-1, dest_dim=-1, make_contiguous=True):\n    \"\"\"\n    Shifts a specified dimension of a tensor to a new position.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        src_dim (int): The original position of the dimension to move. Negative values count from the end.\n        dest_dim (int): The destination position for the dimension. Negative values count from the end.\n        make_contiguous (bool): If True, returns a contiguous tensor. This can be necessary after a permute\n                                operation because permute can lead to non-contiguous memory layout, potentially\n                                affecting performance.\n\n    Returns:\n        torch.Tensor: A tensor with the specified dimension shifted to the new position.\n\n    Example:\n        # Assuming x is a tensor with shape (batch, channels, time, height, width)\n        # and we want to move the 'channels' dimension to the end:\n        shifted_tensor = shift_dim(x, 1, -1)\n        # shifted_tensor will have the shape (batch, time, height, width, channels)\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef shift_dim(x, src_dim=-1, dest_dim=-1, make_contiguous=True):\n    \"\"\"\n    Shifts a specified dimension of a tensor to a new position.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        src_dim (int): The original position of the dimension to move. Negative values count from the end.\n        dest_dim (int): The destination position for the dimension. Negative values count from the end.\n        make_contiguous (bool): If True, returns a contiguous tensor. This can be necessary after a permute\n                                operation because permute can lead to non-contiguous memory layout, potentially\n                                affecting performance.\n\n    Returns:\n        torch.Tensor: A tensor with the specified dimension shifted to the new position.\n\n    Example:\n        # Assuming x is a tensor with shape (batch, channels, time, height, width)\n        # and we want to move the 'channels' dimension to the end:\n        shifted_tensor = shift_dim(x, 1, -1)\n        # shifted_tensor will have the shape (batch, time, height, width, channels)\n    \"\"\"\n    if src_dim < 0:\n        src_dim += x.dim()\n    if dest_dim < 0:\n        dest_dim += x.dim()\n    perm = list(range(x.dim()))\n    perm.insert(dest_dim, perm.pop(src_dim))\n    shifted_tensor = x.permute(perm)\n    if make_contiguous:\n        shifted_tensor = shifted_tensor.contiguous()\n\n    return shifted_tensor\n\n",
  "canonical_solution": "    \n    n_dims = len(x.shape)  # Number of dimensions in the input tensor\n\n    # Adjust src_dim and dest_dim to be positive if they are given as negative\n    if src_dim < 0:\n        src_dim += n_dims\n    if dest_dim < 0:\n        dest_dim += n_dims\n\n    # Ensure the source and destination dimensions are within the valid range\n    assert 0 <= src_dim < n_dims and 0 <= dest_dim < n_dims, \"Source and destination dimensions must be within the valid range\"\n\n    # Create a list of dimensions except for the source dimension\n    dims = list(range(n_dims))\n    del dims[src_dim]  # Remove the source dimension from the list\n\n    # Build a new permutation of dimensions\n    permutation = []\n    ctr = 0\n    for i in range(n_dims):\n        if i == dest_dim:\n            permutation.append(src_dim)  # Insert the source dimension at the destination index\n        else:\n            permutation.append(dims[ctr])  # Fill in the rest of dimensions\n            ctr += 1\n\n    # Permute the dimensions of the tensor according to the new order\n    x = x.permute(permutation)\n\n    # Make the tensor contiguous if required\n    if make_contiguous:\n        x = x.contiguous()\n\n    return x\n\n\n",
  "unprocess_testcases": "tensor = torch.randn(2, 3, 4, 5)\nassert((SOLUTION_SIGNATURE(tensor, src_dim=1, dest_dim=-1) == shift_dim(tensor, src_dim=1, dest_dim=-1)).all())\n\ntensor = torch.randn(3, 4, 5)\nassert((SOLUTION_SIGNATURE(tensor, src_dim=-3, dest_dim=0, make_contiguous=False) == shift_dim(tensor, src_dim=-3, dest_dim=0, make_contiguous=False)).all())\n\n\ntensor = torch.randn(1, 2, 3, 4, 5)\nassert((SOLUTION_SIGNATURE(tensor, src_dim=0, dest_dim=4, make_contiguous=True) == shift_dim(tensor, src_dim=0, dest_dim=4, make_contiguous=True)).all())\n"
 },
 {
  "task_id": "RWPB/89",
  "prompt": "import math\n\n\ndef format_ratio(ratio):\n    \"\"\"\n    Formats a ratio into a percentage string\n\n    Args:\n        - ratio: float, ratio to be formatted\n    Returns:\n        formatted_ratio: str, formatted ratio in percentage\n\n    Examples:\n        - input: 3.1245133412\n        - output: percentage:3.124\n    \"\"\"\n",
  "solution": "\n\n\ndef format_ratio(ratio):\n    \"\"\"\n    Formats a ratio into a percentage string\n\n    Args:\n        - ratio: float, ratio to be formatted\n    Returns:\n        formatted_ratio: str, formatted ratio in percentage\n\n    Examples:\n        - input: 3.1245133412\n        - output: percentage:3.124\n    \"\"\"\n    formatted_ratio = f\"percentage:{ratio:.3f}\"\n    return formatted_ratio\n\n\n",
  "canonical_solution": "    \n    percentage = ratio * 100\n    return f\"{percentage:3.3f}%\"\n\n\n",
  "unprocess_testcases": "assert(format(3.14) == format(3.14))\nassert(format(6.29) == format(6.29))\nassert(format(0) == format(0))\n"
 },
 {
  "task_id": "RWPB/2",
  "prompt": "import torch\nfrom scipy import special, pi\n\ndef logbessel_I_scipy(nu, z, check = True):\n\t'''\n\tPytorch version of scipy computation of modified Bessel functions \n\tof the 1st kind I(nu,z).\n\tParameters\n\t----------\n\tnu: positive int, float\n\t\tOrder of modified Bessel function of 1st kind.\n\tz: int/float or tensor, shape (N,) \n\t\tArgument of Bessel function.\n\tcheck: bool\n\t\tIf True, check if argument of log is non zero.\n\t\n\tReturn\n\t------\n\tresult: tensor, shape (N,)\n\t'''\n",
  "solution": "\nimport torch\nfrom scipy import special, pi\n\ndef logbessel_I_scipy(nu, z, check = True):\n\t'''\n\tPytorch version of scipy computation of modified Bessel functions \n\tof the 1st kind I(nu,z).\n\tParameters\n\t----------\n\tnu: positive int, float\n\t\tOrder of modified Bessel function of 1st kind.\n\tz: int/float or tensor, shape (N,) \n\t\tArgument of Bessel function.\n\tcheck: bool\n\t\tIf True, check if argument of log is non zero.\n\t\n\tReturn\n\t------\n\tresult: tensor, shape (N,)\n\t'''\n\tif check:\n\t\tif torch.any(z == 0):\n\t\t\traise ValueError(\"Argument of log cannot be zero!\")\n\tresult = torch.log(special.iv(nu, z))\n\treturn result\n",
  "canonical_solution": "\n\tif not isinstance(z, torch.Tensor):\n\t\tz = torch.tensor(z)\n\tz = z.reshape(-1)\n\n\tresult = special.ive(nu, z)\n\tif check:\n\t\tassert len(result[ result == 0]) == 0\n\tresult = torch.log(result) + z\n\treturn result\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(nu=1, z=5.0) == logbessel_I_scipy(nu=1, z=5.0)).all())\nassert((SOLUTION_SIGNATURE(nu=0.5, z=torch.tensor([0.1, 1.0, 10.0])) == logbessel_I_scipy(nu=0.5, z=torch.tensor([0.1, 1.0, 10.0]))).all())\nassert((SOLUTION_SIGNATURE(nu=-0.0001, z=0.0001) == logbessel_I_scipy(nu=-0.0001, z=0.0001)).all())"
 },
 {
  "task_id": "RWPB/107",
  "prompt": "import torch\n\ndef pad_camera_extrinsics_4x4(extrinsics):\n    \"\"\"\n    Pads a given extrinsics matrix to a 4x4 matrix if it is not already.\n\n    Args:\n        extrinsics (torch.Tensor): Input extrinsics matrix of shape (..., 3, 4).\n\n    Returns:\n        torch.Tensor: Padded extrinsics matrix of shape (..., 4, 4).\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef pad_camera_extrinsics_4x4(extrinsics):\n    \"\"\"\n    Pads a given extrinsics matrix to a 4x4 matrix if it is not already.\n\n    Args:\n        extrinsics (torch.Tensor): Input extrinsics matrix of shape (..., 3, 4).\n\n    Returns:\n        torch.Tensor: Padded extrinsics matrix of shape (..., 4, 4).\n    \"\"\"\n    if extrinsics.shape[-2:] == (3, 4):\n        padded_extrinsics = torch.zeros((*extrinsics.shape[:-2], 4, 4), dtype=extrinsics.dtype, device=extrinsics.device)\n        padded_extrinsics[..., :3, :4] = extrinsics\n        padded_extrinsics[..., 3, 3] = 1.0\n        return padded_extrinsics\n    else:\n        return extrinsics\n\n\n",
  "canonical_solution": "    \n    if extrinsics.shape[-2] == 4:\n        return extrinsics\n    padding = torch.tensor([[0, 0, 0, 1]]).to(extrinsics)\n    if extrinsics.ndim == 3:\n        padding = padding.unsqueeze(0).repeat(extrinsics.shape[0], 1, 1)\n    extrinsics = torch.cat([extrinsics, padding], dim=-2)\n    return extrinsics\n\n\n",
  "unprocess_testcases": "extrinsics = torch.rand((2, 4, 4))\nassert((SOLUTION_SIGNATURE(extrinsics) == pad_camera_extrinsics_4x4(extrinsics)).all())\n\nextrinsics = torch.rand((3, 3, 4))\nassert((SOLUTION_SIGNATURE(extrinsics) == pad_camera_extrinsics_4x4(extrinsics)).all())\n\nextrinsics = torch.rand((3, 4))\nassert((SOLUTION_SIGNATURE(extrinsics) == pad_camera_extrinsics_4x4(extrinsics)).all())\n\nextrinsics = torch.rand((2, 1, 4))\nassert((SOLUTION_SIGNATURE(extrinsics) == pad_camera_extrinsics_4x4(extrinsics)).all())\n"
 },
 {
  "task_id": "RWPB/133",
  "prompt": "from PIL import Image\n\ndef divide_to_patches(image, patch_size):\n    \"\"\"\n    Divides an image into patches of a specified size.\n\n    Args:\n        image (PIL.Image.Image): The input image.\n        patch_size (int): The size of each patch.\n\n    Returns:\n        list: A list of PIL.Image.Image objects representing the patches.\n    \"\"\"\n",
  "solution": "\n\n\nfrom PIL import Image\n\ndef divide_to_patches(image, patch_size):\n    \"\"\"\n    Divides an image into patches of a specified size.\n\n    Args:\n        image (PIL.Image.Image): The input image.\n        patch_size (int): The size of each patch.\n\n    Returns:\n        list: A list of PIL.Image.Image objects representing the patches.\n    \"\"\"\n    width, height = image.size\n    num_rows = height // patch_size\n    num_cols = width // patch_size\n    patches = []\n    for row in range(num_rows):\n        for col in range(num_cols):\n            patch = image.crop((col * patch_size, row * patch_size, (col + 1) * patch_size, (row + 1) * patch_size))\n            patches.append(patch)\n    return patches\n\n",
  "canonical_solution": "    \n    patches = []\n    width, height = image.size\n    for i in range(0, height, patch_size):\n        for j in range(0, width, patch_size):\n            box = (j, i, j + patch_size, i + patch_size)\n            patch = image.crop(box)\n            patches.append(patch)\n\n    return patches\n\n\n",
  "unprocess_testcases": "img = Image.new('RGB', (256, 256))\npatch_size = 64\nassert(SOLUTION_SIGNATURE(img, patch_size) == divide_to_patches(img, patch_size))\n\nimg = Image.new('RGB', (300, 150))\npatch_size = 50\nassert(SOLUTION_SIGNATURE(img, patch_size) == divide_to_patches(img, patch_size))\n\nimg = Image.new('RGB', (30, 30))\npatch_size = 50\nassert(SOLUTION_SIGNATURE(img, patch_size) == divide_to_patches(img, patch_size))\n"
 },
 {
  "task_id": "RWPB/123",
  "prompt": "import torch\nimport torch.nn as nn\n\n\ndef fuse_deconv_and_bn(deconv, bn):\n    \"\"\"\n    Fuses a ConvTranspose2d layer with a BatchNorm2d layer into a single ConvTranspose2d layer.\n\n    Arguments:\n    deconv : torch.nn.ConvTranspose2d\n        The convolutional transpose layer to be fused.\n    bn : torch.nn.BatchNorm2d\n        The batch normalization layer to be fused.\n\n    Returns:\n    torch.nn.ConvTranspose2d\n        The resulting fused ConvTranspose2d layer.\n    \"\"\"\n",
  "solution": "\n\n\ndef fuse_deconv_and_bn(deconv, bn):\n    \"\"\"\n    Fuses a ConvTranspose2d layer with a BatchNorm2d layer into a single ConvTranspose2d layer.\n\n    Arguments:\n    deconv : torch.nn.ConvTranspose2d\n        The convolutional transpose layer to be fused.\n    bn : torch.nn.BatchNorm2d\n        The batch normalization layer to be fused.\n\n    Returns:\n    torch.nn.ConvTranspose2d\n        The resulting fused ConvTranspose2d layer.\n    \"\"\"\n    fused_weight = deconv.weight.clone()\n    fused_bias = torch.zeros_like(fused_weight[0])\n\n    for i in range(len(fused_weight)):\n        fused_weight[i] = fused_weight[i] * bn.weight[i] / bn.running_var[i]\n        fused_bias[i] = deconv.bias[i] - bn.running_mean[i] * fused_weight[i] / bn.running_var[i]\n\n    fused_deconv = nn.ConvTranspose2d(\n        in_channels=deconv.in_channels,\n        out_channels=deconv.out_channels,\n        kernel_size=deconv.kernel_size,\n        stride=deconv.stride,\n        padding=deconv.padding,\n        output_padding=deconv.output_padding,\n        dilation=deconv.dilation,\n        groups=deconv.groups,\n        bias=True,\n    )\n    fused_deconv.weight = nn.Parameter(fused_weight)\n    fused_deconv.bias = nn.Parameter(fused_bias)\n\n    return fused_deconv\n\n\n",
  "canonical_solution": "    \n    # Create a new ConvTranspose2d layer with the same parameters as the original but with bias enabled\n    fuseddconv = nn.ConvTranspose2d(\n        in_channels=deconv.in_channels,\n        out_channels=deconv.out_channels,\n        kernel_size=deconv.kernel_size,\n        stride=deconv.stride,\n        padding=deconv.padding,\n        output_padding=deconv.output_padding,\n        dilation=deconv.dilation,\n        groups=deconv.groups,\n        bias=True  # Always true to accommodate the fused bias\n    ).requires_grad_(False).to(deconv.weight.device)  # Disable grad and move to the same device\n\n    # Prepare the weights for fusion\n    w_deconv = deconv.weight.clone().view(deconv.out_channels, -1)\n    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))\n    fuseddconv.weight.copy_(\n        torch.mm(w_bn, w_deconv).view(fuseddconv.weight.shape))  # Apply the BN transformation to weights\n\n    # Prepare and fuse the biases\n    b_conv = torch.zeros(deconv.weight.shape[1], device=deconv.weight.device) if deconv.bias is None else deconv.bias\n    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n    fuseddconv.bias.copy_(\n        torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)  # Combine biases from BN and deconv layers\n\n    return fuseddconv\n\n\n",
  "unprocess_testcases": "a = torch.randn(20, 3, 50, 100)\ndeconv = nn.ConvTranspose2d(3, 3, 3, stride=2, padding=1, output_padding=1, dilation=1, groups=1, bias=None)\nbn = nn.BatchNorm2d(3)\nassert((SOLUTION_SIGNATURE(deconv, bn)(a) == fuse_deconv_and_bn(deconv, bn)(a)).all())\n\ninitial_bias = torch.randn(3)\ndeconv = nn.ConvTranspose2d(3, 3, 3, stride=2, padding=1, output_padding=1, dilation=1, groups=1, bias=True)\ndeconv.bias.data = initial_bias.clone()\nbn = nn.BatchNorm2d(3)\nassert((SOLUTION_SIGNATURE(deconv, bn)(a) == fuse_deconv_and_bn(deconv, bn)(a)).all())\n\ndeconv = nn.ConvTranspose2d(3, 4, (5, 5), (2, 2), (1, 1), output_padding=(1, 1), dilation=(2, 2))\nbn = nn.BatchNorm2d(4)\nassert((SOLUTION_SIGNATURE(deconv, bn)(a) == fuse_deconv_and_bn(deconv, bn)(a)).all())\n\n"
 },
 {
  "task_id": "RWPB/117",
  "prompt": "import os.path\n\ndef path_spliter(path: str):\n    \"\"\"\n    Splits the given path into its components.\n\n    Args:\n        path (str): The path to split.\n\n    Returns:\n        list: A list of path components.\n    \"\"\"\n",
  "solution": "\n\n\nimport os.path\n\ndef path_spliter(path: str):\n    \"\"\"\n    Splits the given path into its components.\n\n    Args:\n        path (str): The path to split.\n\n    Returns:\n        list: A list of path components.\n    \"\"\"\n    return os.path.split(path)\n\n\n",
  "canonical_solution": "    \n    res = []\n    if path == '':\n        return []\n\n    while path != '' and path != '/':\n        res.append(os.path.split(path)[1])\n        path = os.path.split(path)[0]\n    if path != '':\n        res.append(path)\n    \n\n    return res[::-1]\n\n\n",
  "unprocess_testcases": "str = '/usr/bin/python'\nassert(SOLUTION_SIGNATURE(str) == path_spliter(str))\n\nstr = '../folder/subfolder/file.txt'\nassert(SOLUTION_SIGNATURE(str) == path_spliter(str))\n\nstr = './../xx/yy/./../yty/file.txt'\nassert(SOLUTION_SIGNATURE(str) == path_spliter(str))\n\nstr = ''\nassert(SOLUTION_SIGNATURE(str) == path_spliter(str))\n"
 },
 {
  "task_id": "RWPB/3",
  "prompt": "import numpy as np\n\ndef get_focalLength_from_fieldOfView(fov=60, img_size=512):\n    \"\"\"\n    Compute the focal length of the camera lens by assuming a certain FOV for the entire image\n    Args:\n        - fov: float, expressed in degree\n        - img_size: int\n    Return:\n        focal: float\n    \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\n\ndef get_focalLength_from_fieldOfView(fov=60, img_size=512):\n    \"\"\"\n    Compute the focal length of the camera lens by assuming a certain FOV for the entire image\n    Args:\n        - fov: float, expressed in degree\n        - img_size: int\n    Return:\n        focal: float\n    \"\"\"\n    fov_rad = fov * np.pi / 180\n    focal = img_size / (2 * np.tan(fov_rad / 2))\n    return focal\n\n\n",
  "canonical_solution": "    \n    focal = img_size / (2 * np.tan(np.radians(fov) /2))\n    return focal\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(fov = 60, img_size = 512) == get_focalLength_from_fieldOfView(fov = 60, img_size = 512))\nassert(SOLUTION_SIGNATURE(fov = 1, img_size = 512) == get_focalLength_from_fieldOfView(fov = 1, img_size = 512))\nassert(SOLUTION_SIGNATURE(fov = 120, img_size = 2048) == get_focalLength_from_fieldOfView(fov = 120, img_size = 2048))"
 },
 {
  "task_id": "RWPB/106",
  "prompt": "import numpy as np\n\ndef interpolate_features(features,\n                         input_rate,\n                         output_rate,\n                         output_len):\n    \"\"\"\n    Interpolate DeepSpeech features.\n\n    Parameters\n    ----------\n    features : np.array\n        DeepSpeech features.\n    input_rate : int\n        input rate (FPS).\n    output_rate : int\n        Output rate (FPS).\n    output_len : int\n        Output data length.\n\n    Returns\n    -------\n    np.array\n        Interpolated data.\n    \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\n\ndef interpolate_features(features,\n                         input_rate,\n                         output_rate,\n                         output_len):\n    \"\"\"\n    Interpolate DeepSpeech features.\n\n    Parameters\n    ----------\n    features : np.array\n        DeepSpeech features.\n    input_rate : int\n        input rate (FPS).\n    output_rate : int\n        Output rate (FPS).\n    output_len : int\n        Output data length.\n\n    Returns\n    -------\n    np.array\n        Interpolated data.\n    \"\"\"\n    output_shape = (output_len, features.shape[1])\n    output = np.zeros(output_shape)\n    input_len = features.shape[0]\n    input_times = np.linspace(0, input_len - 1, input_len)\n    output_times = np.linspace(0, input_len - 1, output_len)\n    for i in range(output_shape[1]):\n        output[:, i] = np.interp(output_times, input_times, features[:, i])\n\n    return output\n\n\n",
  "canonical_solution": "    \n    input_len = features.shape[0]\n    num_features = features.shape[1]\n    input_timestamps = np.arange(input_len) / float(input_rate)\n    output_timestamps = np.arange(output_len) / float(output_rate)\n    output_features = np.zeros((output_len, num_features))\n    for feature_idx in range(num_features):\n        output_features[:, feature_idx] = np.interp(\n            x=output_timestamps,\n            xp=input_timestamps,\n            fp=features[:, feature_idx])\n    return output_features\n\n\n",
  "unprocess_testcases": "features = np.random.rand(10, 2)\ninput_rate = 2\noutput_rate = 2\noutput_len = 5\nassert((SOLUTION_SIGNATURE(features, input_rate, output_rate, output_len) == interpolate_features(features, input_rate, output_rate, output_len)).all())\n\nfeatures = np.random.rand(10, 3)\ninput_rate = 5\noutput_rate = 5\noutput_len = 0\nassert((SOLUTION_SIGNATURE(features, input_rate, output_rate, output_len) == interpolate_features(features, input_rate, output_rate, output_len)).all())\n\nfeatures = np.random.rand(15, 4)\ninput_rate = 3\noutput_rate = 4\noutput_len = 10\nassert((SOLUTION_SIGNATURE(features, input_rate, output_rate, output_len) == interpolate_features(features, input_rate, output_rate, output_len)).all())\n"
 },
 {
  "task_id": "RWPB/132",
  "prompt": "import torch\n\n\ndef B_batch(x, grid, k=0, extend=True, device=\"cpu\"):\n    \"\"\"\n    evaludate x on B-spline bases\n\n    Args:\n    -----\n        x : 2D torch.tensor\n            inputs, shape (number of splines, number of samples)\n        grid : 2D torch.tensor\n            grids, shape (number of splines, number of grid points)\n        k : int\n            the piecewise polynomial order of splines.\n        extend : bool\n            If True, k points are extended on both ends. If False, no extension (zero boundary condition). Default: True\n        device : str\n            devicde\n\n    Returns:\n    --------\n        spline values : 3D torch.tensor\n            shape (number of splines, number of B-spline bases (coeffcients), number of samples). The numbef of B-spline bases = number of grid points + k - 1.\n\n    Example\n    -------\n    >>> num_spline = 5\n    >>> num_sample = 100\n    >>> num_grid_interval = 10\n    >>> k = 3\n    >>> x = torch.normal(0,1,size=(num_spline, num_sample))\n    >>> grids = torch.einsum('i,j->ij', torch.ones(num_spline,), torch.linspace(-1,1,steps=num_grid_interval+1))\n    >>> B_batch(x, grids, k=k).shape\n    torch.Size([5, 13, 100])\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\n\ndef B_batch(x, grid, k=0, extend=True, device=\"cpu\"):\n    \"\"\"\n    evaludate x on B-spline bases\n\n    Args:\n    -----\n        x : 2D torch.tensor\n            inputs, shape (number of splines, number of samples)\n        grid : 2D torch.tensor\n            grids, shape (number of splines, number of grid points)\n        k : int\n            the piecewise polynomial order of splines.\n        extend : bool\n            If True, k points are extended on both ends. If False, no extension (zero boundary condition). Default: True\n        device : str\n            devicde\n\n    Returns:\n    --------\n        spline values : 3D torch.tensor\n            shape (number of splines, number of B-spline bases (coeffcients), number of samples). The numbef of B-spline bases = number of grid points + k - 1.\n\n    Example\n    -------\n    >>> num_spline = 5\n    >>> num_sample = 100\n    >>> num_grid_interval = 10\n    >>> k = 3\n    >>> x = torch.normal(0,1,size=(num_spline, num_sample))\n    >>> grids = torch.einsum('i,j->ij', torch.ones(num_spline,), torch.linspace(-1,1,steps=num_grid_interval+1))\n    >>> B_batch(x, grids, k=k).shape\n    torch.Size([5, 13, 100])\n    \"\"\"\n    num_spline, num_sample = x.shape\n    num_grid_interval = grid.shape[1] - 1\n    num_grid_point = num_grid_interval + 1\n    num_B_spline_basis = num_grid_point + k - 1\n    B_spline_basis = torch.zeros(num_spline, num_B_spline_basis, num_sample, device=device)\n    for i in range(num_spline):\n        for j in range(num_B_spline_basis):\n            for l in range(num_sample):\n                B_spline_basis[i, j, l] = B_spline(x[i, l], grid[i, :], j, k, extend)\n\n    return B_spline_basis\n\n\ndef B_spline(x, grid, j, k=0, extend=True):\n    \"\"\"\n    evaludate x on B-spline bases\n\n    Args:\n    -----\n        x : float\n            input\n        grid : 1D torch.tensor\n            grids, shape (number of grid points)\n        j : int\n            the jth B-spline basis\n        k : int\n            the piecewise polynomial order of splines.\n        extend : bool\n            If True, k points are extended on both ends. If False, no extension (zero boundary condition). Default: True\n\n    Returns:\n    --------\n        spline value : float\n\n    Example\n    -------\n    >>> num_grid_interval = 10\n    >>> k = 3\n    >>> x = 0.5\n    >>> grid = torch.linspace(-1,1,steps=num_grid_interval+1)\n    >>> B_spline(x, grid, 5, k=k)\n    0.25\n    \"\"\"\n    num_grid_interval = grid.shape[0] - 1\n    num_grid_point = num_grid_interval + 1\n    num_B_spline_basis = num_grid_point + k - 1\n    if j < 0 or j >= num_B_spline_basis:\n        return 0\n    if k == 0:\n        if grid[j] <= x < grid[j + 1]:\n            return 1\n        else:\n            return 0\n    if extend:\n        if j < k - 1:\n            return (x - grid[j]) / (grid[j + k - 1] - grid[j]) * B_spline(x, grid, j, k - 1, extend) + (\n                grid[j + k] - x\n            ) / (grid[j + k] - grid[j + 1]) * B_spline(x, grid, j + 1, k - 1, extend)\n        elif j > num_B_spline_basis - k:\n            return (grid[j + k - 1] - x) / (grid[j + k - 1] - grid[j]) * B_spline(x, grid, j, k - 1, extend) + (\n                x - grid[j + 1]\n            ) / (grid[j + k] - grid[j + 1]) * B_spline(x, grid, j + 1, k -",
  "canonical_solution": "\n    # x shape: (size, x); grid shape: (size, grid)\n    def extend_grid(grid, k_extend=0):\n        # pad k to left and right\n        # grid shape: (batch, grid)\n        h = (grid[:, [-1]] - grid[:, [0]]) / (grid.shape[1] - 1)\n\n        for i in range(k_extend):\n            grid = torch.cat([grid[:, [0]] - h, grid], dim=1)\n            grid = torch.cat([grid, grid[:, [-1]] + h], dim=1)\n        grid = grid.to(device)\n        return grid\n\n    if extend:\n        grid = extend_grid(grid, k_extend=k)\n\n    grid = grid.unsqueeze(dim=2).to(device)\n    x = x.unsqueeze(dim=1).to(device)\n\n    if k == 0:\n        value = (x >= grid[:, :-1]) * (x < grid[:, 1:])\n    else:\n        B_km1 = B_batch(\n            x[:, 0], grid=grid[:, :, 0], k=k - 1, extend=False, device=device\n        )\n        value = (x - grid[:, : -(k + 1)]) / (\n            grid[:, k:-1] - grid[:, : -(k + 1)]\n        ) * B_km1[:, :-1] + (grid[:, k + 1 :] - x) / (\n            grid[:, k + 1 :] - grid[:, 1:(-k)]\n        ) * B_km1[\n            :, 1:\n        ]\n    return value\n\n\n",
  "unprocess_testcases": "num_splines = 3\nnum_samples = 50\nnum_grid_points = 8\nk = 2\nx = torch.normal(0, 1, size=(num_splines, num_samples))\ngrids = torch.einsum('i,j->ij', torch.ones(num_splines,), torch.linspace(-1, 1, steps=num_grid_points))\nassert((SOLUTION_SIGNATURE(x, grids, k=k) == B_batch(x, grids, k=k)).all())\n\nnum_splines = 2\nnum_samples = 30\nnum_grid_points = 5\nk = 0\nx = torch.linspace(-1, 1, steps=num_samples).repeat(num_splines, 1)\ngrids = torch.einsum('i,j->ij', torch.ones(num_splines,), torch.linspace(-1, 1, steps=num_grid_points))\nassert((SOLUTION_SIGNATURE(x, grids, k=k, extend=False) == B_batch(x, grids, k=k, extend=False)).all())\n\n\nnum_splines = 4\nnum_samples = 100\nnum_grid_points = 10\nk = 4\nx = torch.cat([\n    torch.linspace(-1.5, -1, steps=num_samples // 2),\n    torch.linspace(1, 1.5, steps=num_samples // 2)\n]).repeat(num_splines, 1)\ngrids = torch.einsum('i,j->ij', torch.ones(num_splines,), torch.linspace(-1, 1, steps=num_grid_points))\nassert((SOLUTION_SIGNATURE(x, grids, k=k, extend=False) == B_batch(x, grids, k=k, extend=False)).all())\n"
 },
 {
  "task_id": "RWPB/122",
  "prompt": "import torch\n\ndef view_range(x, i, j, shape):\n    \"\"\"\n    Reshapes a portion of a tensor from dimension i (inclusive) to dimension j (exclusive) into the specified shape.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        i (int): The start dimension for reshaping, inclusive. Negative values count from the end.\n        j (int): The end dimension for reshaping, exclusive. If None, extends to the end of dimensions. Negative values count from the end.\n        shape (tuple): The target shape to apply between dimensions i and j.\n\n    Returns:\n        torch.Tensor: A tensor reshaped from dimension i to j according to the specified shape.\n\n    Example:\n        # Assuming x has shape (batch, thw, channels)\n        # To reshape 'thw' into (t, h, w):\n        reshaped_x = view_range(x, 1, 2, (t, h, w))\n        # reshaped_x will have the shape (batch, t, h, w, channels)\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef view_range(x, i, j, shape):\n    \"\"\"\n    Reshapes a portion of a tensor from dimension i (inclusive) to dimension j (exclusive) into the specified shape.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        i (int): The start dimension for reshaping, inclusive. Negative values count from the end.\n        j (int): The end dimension for reshaping, exclusive. If None, extends to the end of dimensions. Negative values count from the end.\n        shape (tuple): The target shape to apply between dimensions i and j.\n\n    Returns:\n        torch.Tensor: A tensor reshaped from dimension i to j according to the specified shape.\n\n    Example:\n        # Assuming x has shape (batch, thw, channels)\n        # To reshape 'thw' into (t, h, w):\n        reshaped_x = view_range(x, 1, 2, (t, h, w))\n        # reshaped_x will have the shape (batch, t, h, w, channels)\n    \"\"\"\n    if i < 0:\n        i = len(x.shape) + i\n    if j is None:\n        j = len(x.shape)\n    elif j < 0:\n        j = len(x.shape) + j\n    return x.view(*x.shape[:i], *shape, *x.shape[j:])\n\n\n",
  "canonical_solution": "    \n    shape = tuple(shape)  # Ensure the shape is a tuple, necessary for concatenation with other shapes\n\n    n_dims = len(x.shape)  # Total number of dimensions in the input tensor\n\n    # Adjust indices for negative values\n    if i < 0:\n        i += n_dims\n    if j is None:\n        j = n_dims\n    elif j < 0:\n        j += n_dims\n\n    # Ensure the indices are valid\n    assert 0 <= i < j <= n_dims, \"Indices i and j must define a valid range of dimensions\"\n\n    # Get the current shape of the tensor\n    x_shape = x.shape\n    # Construct the new shape by combining slices of the old shape with the new specified shape\n    target_shape = x_shape[:i] + shape + x_shape[j:]\n\n    # Reshape the tensor to the target shape\n    return x.view(target_shape)\n\n",
  "unprocess_testcases": "x = torch.randn(4, 3, 2, 5)\ni = 1\nj = 3\nshape = (3, 2)\nassert((SOLUTION_SIGNATURE(x, i, j, shape) == view_range(x, i, j, shape)).all())\n\nx = torch.randn(4, 3, 2, 5, 6)\ni = -4\nj = -2\nshape = (6,)\nassert((SOLUTION_SIGNATURE(x, i, j, shape) == view_range(x, i, j, shape)).all())\n\nx = torch.randn(4, 3, 2)\ni = 1\nj = None\nshape = (6,)\nassert((SOLUTION_SIGNATURE(x, i, j, shape) == view_range(x, i, j, shape)).all())\n"
 },
 {
  "task_id": "RWPB/116",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\n\ndef round_nearest_multiple(number, a, direction='standard'):\n    \"\"\"\n    Rounds a number to the nearest multiple of a. The returned number will have the\n    same precision as a.\n\n    Args:\n        - number: float, the number to be rounded.\n        - a: float, the multiple to round to.\n        - direction: str, optional, specifying the rounding direction ('standard', 'down', or 'up').\n                     Default is 'standard'.\n\n    Returns:\n        rounded_number: float, the rounded number to the nearest multiple of a.\n    \"\"\"\n",
  "solution": "\n\n\ndef round_nearest_multiple(number, a, direction='standard'):\n    \"\"\"\n    Rounds a number to the nearest multiple of a. The returned number will have the\n    same precision as a.\n\n    Args:\n        - number: float, the number to be rounded.\n        - a: float, the multiple to round to.\n        - direction: str, optional, specifying the rounding direction ('standard', 'down', or 'up').\n                     Default is'standard'.\n\n    Returns:\n        rounded_number: float, the rounded number to the nearest multiple of a.\n    \"\"\"\n    if direction =='standard':\n        rounded_number = round(number / a) * a\n    elif direction == 'down':\n        rounded_number = math.floor(number / a) * a\n    elif direction == 'up':\n        rounded_number = math.ceil(number / a) * a\n    else:\n        raise ValueError(\"Invalid direction. Please choose from'standard', 'down', or 'up'.\")\n\n    return rounded_number\n\n\n",
  "canonical_solution": "    \n    if direction == 'down':\n        return round(math.floor(number / a) * a, -int(math.floor(math.log10(a))))\n    elif direction == 'up':\n        return round(math.ceil(number / a) * a, -int(math.floor(math.log10(a))))\n    elif direction == 'standard':\n        return round(number, -int(math.floor(math.log10(a))))\n\n\n\n",
  "unprocess_testcases": "number = 14.7\na = 5\nassert(SOLUTION_SIGNATURE(number, a) == round_nearest_multiple(number, a))\n\nnumber = 9.8\na = 2\ndirection = 'down'\nassert(SOLUTION_SIGNATURE(number, a, direction) == round_nearest_multiple(number, a, direction))\n\nnumber = 7.1\na = 0.5\ndirection = 'up'\nassert(SOLUTION_SIGNATURE(number, a, direction) == round_nearest_multiple(number, a, direction))"
 },
 {
  "task_id": "RWPB/7",
  "prompt": "from importlib import import_module\n\ndef dynamic_import_function(function_path):\n    '''Dynamically import a function from a path string by using import_module() (e.g., \"module.submodule.my_function\")\n    if no match distribution found, please return ModuleNotFoundError\n    Args:\n        a function path string:string\n    Returns:\n        function (e.g., my_function)\n    '''\n",
  "solution": "\n\n\ndef dynamic_import_function(function_path):\n    '''Dynamically import a function from a path string by using import_module() (e.g., \"module.submodule.my_function\")\n    if no match distribution found, please return ModuleNotFoundError\n    Args:\n        a function path string:string\n    Returns:\n        function (e.g., my_function)\n    '''\n    try:\n        module_path, function_name = function_path.rsplit('.', 1)\n        module = import_module(module_path)\n        return getattr(module, function_name)\n    except (ImportError, AttributeError):\n        return ModuleNotFoundError\n\n",
  "canonical_solution": "    \n    try:\n        module_path, function_name = function_path.rsplit(\".\", 1)\n        module = import_module(module_path)\n        function = getattr(module, function_name)\n    except:\n        return ModuleNotFoundError\n    return function\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE('math.sqrt') == dynamic_import_function('math.sqrt'))\nassert(SOLUTION_SIGNATURE('os.path.join') == dynamic_import_function('os.path.join'))\nassert(SOLUTION_SIGNATURE('nonexistent.mod.func') == dynamic_import_function('nonexistent.mod.func'))\n"
 },
 {
  "task_id": "RWPB/98",
  "prompt": "import numpy as np\n\n\ndef exp_smooth(vals, alpha):\n  \"\"\"\n  Exponential smoothing function.\n  Smooths a sequence of values using exponential smoothing.\n  If input array is an empty, the code should return KeyError\n  Args:\n      - vals: list or numpy.ndarray, input sequence of values\n      - alpha: float, smoothing factor\n  Returns:\n      smoothed: numpy.ndarray, smoothed sequence of values\n  \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\n\n\ndef exp_smooth(vals, alpha):\n  \"\"\"\n  Exponential smoothing function.\n  Smooths a sequence of values using exponential smoothing.\n  If input array is an empty, the code should return KeyError\n  Args:\n      - vals: list or numpy.ndarray, input sequence of values\n      - alpha: float, smoothing factor\n  Returns:\n      smoothed: numpy.ndarray, smoothed sequence of values\n  \"\"\"\n  if len(vals) == 0:\n      raise KeyError(\"Input array is empty\")\n  smoothed = np.zeros(len(vals))\n  smoothed[0] = vals[0]\n  for i in range(1, len(vals)):\n      smoothed[i] = alpha * vals[i] + (1 - alpha) * smoothed[i - 1]\n\n  return smoothed\n\n\n",
  "canonical_solution": "  \n  if len(vals) == 0:\n    return KeyError\n  smooth = np.array(vals[0], dtype=float)\n  smoothed = [smooth.copy()]\n\n  for i in range(1, len(vals)):\n    smooth += -alpha * (smooth - vals[i])\n    smoothed.append(smooth.copy())\n\n  return np.array(smoothed)\n\n\n",
  "unprocess_testcases": "vals = [1, 2, 3, 4, 5]\nalpha = 0.5\nassert((SOLUTION_SIGNATURE(vals, alpha) == exp_smooth(vals, alpha)).all())\n\nvals = [67, 576, 12, 4, 5, 34, 123, 67, 32, 34]\nalpha = 0.9\nassert((SOLUTION_SIGNATURE(vals, alpha) == exp_smooth(vals, alpha)).all())\n\nvals = [10, 10, 10, 10, 10]\nalpha = 0.2\nassert((SOLUTION_SIGNATURE(vals, alpha) == exp_smooth(vals, alpha)).all())\n\nvals = []\nalpha = 0.3\nassert(SOLUTION_SIGNATURE(vals, alpha) == exp_smooth(vals, alpha))\n"
 },
 {
  "task_id": "RWPB/136",
  "prompt": "import torch\n\ndef slerp(v0: torch.Tensor, v1: torch.Tensor, t: float, DOT_THRESHOLD: float = 0.9995) -> torch.Tensor:\n    \"\"\"\n    Performs spherical linear interpolation between two vectors.\n\n    Args:\n        v0 (torch.Tensor): The starting vector of the interpolation.\n        v1 (torch.Tensor): The ending vector of the interpolation.\n        t (float): The interpolation parameter, where 0 <= t <= 1.\n        DOT_THRESHOLD (float): A threshold for the cosine of the angle between v0 and v1 to handle numerical instabilities.\n\n    Returns:\n        torch.Tensor: The interpolated vector at parameter t along the great circle joining v0 and v1.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef slerp(v0: torch.Tensor, v1: torch.Tensor, t: float, DOT_THRESHOLD: float = 0.9995) -> torch.Tensor:\n    \"\"\"\n    Performs spherical linear interpolation between two vectors.\n\n    Args:\n        v0 (torch.Tensor): The starting vector of the interpolation.\n        v1 (torch.Tensor): The ending vector of the interpolation.\n        t (float): The interpolation parameter, where 0 <= t <= 1.\n        DOT_THRESHOLD (float): A threshold for the cosine of the angle between v0 and v1 to handle numerical instabilities.\n\n    Returns:\n        torch.Tensor: The interpolated vector at parameter t along the great circle joining v0 and v1.\n    \"\"\"\n    assert isinstance(v0, torch.Tensor) and v0.ndim == 1, \"v0 must be a 1-dimensional torch.Tensor\"\n    assert isinstance(v1, torch.Tensor) and v1.ndim == 1, \"v1 must be a 1-dimensional torch.Tensor\"\n    assert isinstance(t, (int, float)) and 0 <= t <= 1, \"t must be a float between 0 and 1\"\n    assert isinstance(DOT_THRESHOLD, float), \"DOT_THRESHOLD must be a float\"\n    dot = torch.dot(v0, v1)\n    if dot > DOT_THRESHOLD:\n        return (1 - t) * v0 + t * v1\n    theta = torch.acos(torch.clamp(dot, -1, 1))\n    v0_scaled = v0 * torch.sin((1 - t) * theta) / torch.sin(theta)\n    v1_scaled = v1 * torch.sin(t * theta) / torch.sin(theta)\n    return v0_scaled + v1_scaled\n\n",
  "canonical_solution": "    \n    # Normalize the input vectors to unit vectors\n    u0 = v0 / v0.norm()\n    u1 = v1 / v1.norm()\n    # Compute the cosine of the angle between the vectors\n    dot = (u0 * u1).sum()\n    # Check if the vectors are almost parallel\n    if dot.abs() > DOT_THRESHOLD:\n        # If vectors are nearly parallel, fall back to linear interpolation\n        return (1.0 - t) * v0 + t * v1\n    # Compute the angle between the vectors using arccos\n    omega = dot.acos()\n    # Perform spherical linear interpolation\n    return (((1.0 - t) * omega).sin() * v0 + (t * omega).sin() * v1) / omega.sin()\n\n",
  "unprocess_testcases": "v0 = torch.tensor([1.0, 0.0, 0.0])\nv1 = torch.tensor([0.0, 1.0, 0.0])\nt = 0.5\nassert((SOLUTION_SIGNATURE(v0, v1, t) == slerp(v0, v1, t)).all())\n\nv0 = torch.tensor([1.0, 0.0, 0.0])\nv1 = torch.tensor([1.0, 0.0, 0.0])\nt = 0.3\nassert((SOLUTION_SIGNATURE(v0, v1, t) == slerp(v0, v1, t)).all())\n\nv0 = torch.tensor([0.9995, 0.01, 0.0])\nv1 = torch.tensor([1.0, 0.0, 0.0])\nt = 0.7\nassert((SOLUTION_SIGNATURE(v0, v1, t) == slerp(v0, v1, t)).all())\n"
 },
 {
  "task_id": "RWPB/102",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\n\ndef rotate_about_z_axis(vector, theta):\n    \"\"\"\n    Rotate a 3D vector about the z-axis by a specified angle.\n    Args:\n        - vector: numpy array of shape (3,), the 3D vector to be rotated\n        - theta: float, angle of rotation in radians\n    Returns:\n        rotated_vector: numpy array of shape (3,), the rotated 3D vector\n    \"\"\"\n",
  "solution": "\n\n\nimport math\nimport numpy as np\n\ndef rotate_about_z_axis(vector, theta):\n    \"\"\"\n    Rotate a 3D vector about the z-axis by a specified angle.\n    Args:\n        - vector: numpy array of shape (3,), the 3D vector to be rotated\n        - theta: float, angle of rotation in radians\n    Returns:\n        rotated_vector: numpy array of shape (3,), the rotated 3D vector\n    \"\"\"\n    rotation_matrix = np.array([\n        [math.cos(theta), -math.sin(theta), 0],\n        [math.sin(theta), math.cos(theta), 0],\n        [0, 0, 1]\n    ])\n    rotated_vector = np.dot(rotation_matrix, vector)\n\n    return rotated_vector\n\n",
  "canonical_solution": "\n    return np.dot(np.array([\n[np.cos(theta), -np.sin(theta), 0],\n[np.sin(theta), np.cos(theta), 0],\n[0, 0, 1]\n]), vector)\n\n",
  "unprocess_testcases": "vector = np.array([1, 0, 0])\ntheta = 0\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_z_axis(vector, theta)).all())\n\nvector = np.array([1, 0, 0])\ntheta = np.pi / 2\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_z_axis(vector, theta)).all())\n\nvector = np.array([1, 2, 3])\ntheta = np.pi / 4\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_z_axis(vector, theta)).all())\n\n"
 },
 {
  "task_id": "RWPB/112",
  "prompt": "def submatrix(matrix, col):\n    \"\"\"\n    Extracts the submatrix formed by deleting a specified column from the given matrix.\n\n    Args:\n        - matrix: list of lists, the input matrix.\n        - col: int, the index of the column to be removed.\n\n    Returns:\n        sub_matrix: list of lists, the submatrix obtained after removing the specified column.\n    \"\"\"\n",
  "solution": "\n\n\ndef submatrix(matrix, col):\n    \"\"\"\n    Extracts the submatrix formed by deleting a specified column from the given matrix.\n\n    Args:\n        - matrix: list of lists, the input matrix.\n        - col: int, the index of the column to be removed.\n\n    Returns:\n        sub_matrix: list of lists, the submatrix obtained after removing the specified column.\n    \"\"\"\n    sub_matrix = [row[:col] + row[col+1:] for row in matrix]\n    return sub_matrix\n\n\n",
  "canonical_solution": "    \n    return [row[:col] + row[col + 1:] for row in matrix[1:]]\n\n\n\n",
  "unprocess_testcases": "matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ncol = 1\nassert(SOLUTION_SIGNATURE(matrix, col) == submatrix(matrix, col))\n\nmatrix = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]\ncol = 0\nassert(SOLUTION_SIGNATURE(matrix, col) == submatrix(matrix, col))\n\nmatrix = [[1, 2, 3], [4, 5], [6]]\ncol = 2\nassert(SOLUTION_SIGNATURE(matrix, col) == submatrix(matrix, col))\n\nmatrix = [[1, 5, 10], [4, 5], [6]]\ncol = 1\nassert(SOLUTION_SIGNATURE(matrix, col) == submatrix(matrix, col))\n"
 },
 {
  "task_id": "RWPB/126",
  "prompt": "import torch\n\ndef crop_mask(masks, boxes):\n    \"\"\"\n    Crops each mask in a batch of masks to the specified bounding boxes.\n\n    Args:\n        masks (torch.Tensor): A tensor of shape [n, h, w] where 'n' is the number of masks, and 'h' and 'w' are the height and width of the masks.\n        boxes (torch.Tensor): A tensor of shape [n, 4] representing the bounding boxes for each mask. The boxes are in the format [x1, y1, x2, y2].\n\n    Returns:\n        torch.Tensor: A tensor of the same shape as 'masks' but with each mask cropped to its corresponding bounding box.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef crop_mask(masks, boxes):\n    \"\"\"\n    Crops each mask in a batch of masks to the specified bounding boxes.\n\n    Args:\n        masks (torch.Tensor): A tensor of shape [n, h, w] where 'n' is the number of masks, and 'h' and 'w' are the height and width of the masks.\n        boxes (torch.Tensor): A tensor of shape [n, 4] representing the bounding boxes for each mask. The boxes are in the format [x1, y1, x2, y2].\n\n    Returns:\n        torch.Tensor: A tensor of the same shape as'masks' but with each mask cropped to its corresponding bounding box.\n    \"\"\"\n    cropped_masks = []\n    for i in range(masks.shape[0]):\n        mask = masks[i]\n        box = boxes[i]\n        cropped_mask = mask[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n        cropped_masks.append(cropped_mask)\n    return torch.stack(cropped_masks)\n\n\n",
  "canonical_solution": "    \n    _, h, w = masks.shape  # Get the dimensions of the masks\n    # Split the bounding box tensor into x1, y1, x2, y2 components. Each is expanded to have shape [n, 1, 1].\n    x1, y1, x2, y2 = torch.chunk(boxes[:, :, None], 4, dim=1)\n    # Generate tensors representing the indices of each column and each row in the image, and broadcast them to match dimensions\n    r = torch.arange(w, device=masks.device, dtype=x1.dtype)[None, None, :]  # row indices, shape [1, 1, w]\n    c = torch.arange(h, device=masks.device, dtype=y1.dtype)[None, :, None]  # column indices, shape [1, h, 1]\n\n    # Generate a mask by comparing indices with the bounding box coordinates\n    # The comparison results in a binary mask for each bounding box, which when multiplied with the original mask, crops it.\n    return masks * ((r >= x1) * (r < x2) * (c >= y1) * (c < y2))\n\n",
  "unprocess_testcases": "masks = torch.ones((3, 5, 5))\nboxes = torch.tensor([[1, 1, 4, 4], [0, 0, 3, 3], [2, 2, 5, 5]])\nassert((SOLUTION_SIGNATURE(masks, boxes) == crop_mask(masks, boxes)).all())\n\nmasks = torch.ones((2, 5, 5))\nboxes = torch.tensor([[0, 0, 5, 5], [-1, -1, 6, 6]])\nassert((SOLUTION_SIGNATURE(masks, boxes) == crop_mask(masks, boxes)).all())\n\nmasks = torch.tensor([\n    [[1, 0, 1, 0, 1, 0],\n     [0, 1, 0, 1, 0, 1],\n     [1, 0, 1, 0, 1, 0],\n     [0, 1, 0, 1, 0, 1],\n     [1, 0, 1, 0, 1, 0],\n     [0, 1, 0, 1, 0, 1]],\n    [[0, 1, 0, 1, 0, 1],\n     [1, 0, 1, 0, 1, 0],\n     [0, 1, 0, 1, 0, 1],\n     [1, 0, 1, 0, 1, 0],\n     [0, 1, 0, 1, 0, 1],\n     [1, 0, 1, 0, 1, 0]]\n])\nboxes = torch.tensor([[1, 1, 3, 3], [3, 3, 6, 6]])\nassert((SOLUTION_SIGNATURE(masks, boxes) == crop_mask(masks, boxes)).all())\n"
 },
 {
  "task_id": "RWPB/88",
  "prompt": "import torch\nimport numpy as np\n\ndef rotate_y(a, device=None):\n    \"\"\"\n    Creates a 4x4 homogeneous transformation matrix for rotation around the y-axis in 3D space.\n\n    Parameters:\n    a (float): Rotation angle in radians.\n    device (torch.device, optional): The device on which to create the tensor (e.g., 'cpu' or 'cuda'). Default is None, which means the tensor will be created on the default device.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the rotation transformation matrix around the y-axis.\n\n    Process:\n    1. Calculate the sine and cosine of the rotation angle.\n    2. Construct a 4x4 matrix where the rotation is applied to the x and z coordinates, leaving the y coordinate unchanged.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\nimport numpy as np\n\ndef rotate_y(a, device=None):\n    \"\"\"\n    Creates a 4x4 homogeneous transformation matrix for rotation around the y-axis in 3D space.\n\n    Parameters:\n    a (float): Rotation angle in radians.\n    device (torch.device, optional): The device on which to create the tensor (e.g., 'cpu' or 'cuda'). Default is None, which means the tensor will be created on the default device.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the rotation transformation matrix around the y-axis.\n\n    Process:\n    1. Calculate the sine and cosine of the rotation angle.\n    2. Construct a 4x4 matrix where the rotation is applied to the x and z coordinates, leaving the y coordinate unchanged.\n    \"\"\"\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    s = torch.sin(a)\n    c = torch.cos(a)\n    R = torch.tensor([\n        [c, 0, -s, 0],\n        [0, 1, 0, 0],\n        [s, 0, c, 0],\n        [0, 0, 0, 1]\n    ], device=device)\n\n    return R\n\n",
  "canonical_solution": "    \n    s, c = np.sin(a), np.cos(a)\n    return torch.tensor([[ c, 0, s, 0], \n                         [ 0, 1, 0, 0], \n                         [-s, 0, c, 0], \n                         [ 0, 0, 0, 1]], dtype=torch.float32, device=device)\n\n",
  "unprocess_testcases": "a = torch.pi / 4\nassert((SOLUTION_SIGNATURE(a) == rotate_y(a)).all())\n\na = 0\nassert((SOLUTION_SIGNATURE(a) == rotate_y(a)).all())\n\na = 2 * torch.pi\nassert((SOLUTION_SIGNATURE(a) == rotate_y(a)).all())\n"
 },
 {
  "task_id": "RWPB/58",
  "prompt": "def unpad_image_shape(current_height, current_width, original_size):\n    \"\"\"\n    Calculates the new shape of an image after unpading a resized and padded PyTorch tensor.\n\n    Parameters\n    ----------\n    current_height : int\n        The current height of the padded image.\n    current_width : int\n        The current width of the padded image.\n    original_size : tuple\n        A tuple representing the original size of the image in the format (original_width, original_height).\n\n    Returns\n    -------\n    tuple\n        The new shape of the image after unpadding, in the format (new_height, new_width).\n\n    Notes\n    -----\n    This function first calculates the aspect ratios of the original and current images.\n    It then determines how to unpad the image based on the aspect ratios to maintain the original aspect ratio.\n\n    If the original aspect ratio is greater than the current aspect ratio, it means the image has been made wider.\n    The function calculates the new height by scaling the original height with the same factor that the width has been scaled.\n    It then computes the padding needed to center the original aspect ratio within the current dimensions.\n\n    Conversely, if the original aspect ratio is less than the current aspect ratio, the image has been made taller.\n    The function calculates the new width and computes the necessary padding to center the image vertically.\n\n    The function returns the new dimensions of the image after unpadding, which can be used to trim the padded image back to its original shape.\n    \"\"\"\n",
  "solution": "\nimport unittest\n\nclass UnpadImageShapeTestCase(unittest.TestCase):\n    ",
  "canonical_solution": "\n    original_width, original_height = original_size\n\n    # Calculate aspect ratios\n    original_aspect_ratio = original_width / original_height\n    current_aspect_ratio = current_width / current_height\n\n    # Check which dimension has been altered more and unpad accordingly\n    if original_aspect_ratio > current_aspect_ratio:\n        # The image has been made wider, so calculate new height and padding\n        scale_factor = current_width / original_width\n        new_height = int(original_height * scale_factor)\n        padding = (current_height - new_height) // 2\n        new_shape = (current_height - 2 * padding, current_width)\n    else:\n        # The image has been made taller, so calculate new width and padding\n        scale_factor = current_height / original_height\n        new_width = int(original_width * scale_factor)\n        padding = (current_width - new_width) // 2\n        new_shape = (current_height, current_width - 2 * padding)\n\n    return new_shape\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(current_height=1000, current_width=2000, original_size=(1000, 500))[0] == unpad_image_shape(current_height=1000, current_width=2000, original_size=(1000, 500))[0])\nassert(SOLUTION_SIGNATURE(current_height=2000, current_width=1000, original_size=(500, 1000))[0] == unpad_image_shape(current_height=2000, current_width=1000, original_size=(500, 1000))[0])\nassert(SOLUTION_SIGNATURE(current_height=1200, current_width=1600, original_size=(600, 800))[0] == unpad_image_shape(current_height=1200, current_width=1600, original_size=(600, 800))[0])\nassert(SOLUTION_SIGNATURE(current_height=100, current_width=160, original_size=(5, 1))[0] == unpad_image_shape(current_height=100, current_width=160, original_size=(5, 1))[0])\nassert(SOLUTION_SIGNATURE(current_height=1231, current_width=4123, original_size=(76, 2))[0] == unpad_image_shape(current_height=1231, current_width=4123, original_size=(76, 2))[0])\nassert(SOLUTION_SIGNATURE(current_height=1000, current_width=2000, original_size=(1000, 500))[1] == unpad_image_shape(current_height=1000, current_width=2000, original_size=(1000, 500))[1])\nassert(SOLUTION_SIGNATURE(current_height=2000, current_width=1000, original_size=(500, 1000))[1] == unpad_image_shape(current_height=2000, current_width=1000, original_size=(500, 1000))[1])\nassert(SOLUTION_SIGNATURE(current_height=1200, current_width=1600, original_size=(600, 800))[1] == unpad_image_shape(current_height=1200, current_width=1600, original_size=(600, 800))[1])\nassert(SOLUTION_SIGNATURE(current_height=100, current_width=160, original_size=(5, 1))[1] == unpad_image_shape(current_height=100, current_width=160, original_size=(5, 1))[1])\nassert(SOLUTION_SIGNATURE(current_height=1231, current_width=4123, original_size=(76, 2))[1] == unpad_image_shape(current_height=1231, current_width=4123, original_size=(76, 2))[1])\n"
 },
 {
  "task_id": "RWPB/105",
  "prompt": "import torch\n\ndef sample_from_3dgrid(grid: torch.Tensor, coordinates: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Samples features from a 3D grid based on 3D coordinates.\n\n    Args:\n        grid (torch.Tensor): 3D grid with shape (1, channels, H, W, D).\n        coordinates (torch.Tensor): 3D coordinates to sample from with shape (batch_size, num_points_per_batch, 3).\n\n    Returns:\n        torch.Tensor: Sampled features with shape (batch_size, num_points_per_batch, feature_channels).\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef sample_from_3dgrid(grid: torch.Tensor, coordinates: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Samples features from a 3D grid based on 3D coordinates.\n\n    Args:\n        grid (torch.Tensor): 3D grid with shape (1, channels, H, W, D).\n        coordinates (torch.Tensor): 3D coordinates to sample from with shape (batch_size, num_points_per_batch, 3).\n\n    Returns:\n        torch.Tensor: Sampled features with shape (batch_size, num_points_per_batch, feature_channels).\n    \"\"\"\n    batch_size, num_points_per_batch, _ = coordinates.shape\n    grid = grid.squeeze(0)\n    grid_height, grid_width, grid_depth = grid.shape\n    coordinates_x = coordinates[:, :, 0]\n    coordinates_y = coordinates[:, :, 1]\n    coordinates_z = coordinates[:, :, 2]\n    coordinates_x_clipped = torch.clip(coordinates_x, 0, grid_width - 1)\n    coordinates_y_clipped = torch.clip(coordinates_y, 0, grid_height - 1)\n    coordinates_z_clipped = torch.clip(coordinates_z, 0, grid_depth - 1)\n    coordinates_x_floor = torch.floor(coordinates_x_clipped).long()\n    coordinates_y_floor = torch.floor(coordinates_y_clipped).long()\n    coordinates_z_floor = torch.floor(coordinates_z_clipped).long()\n    coordinates_x_ceil = torch.ceil(coordinates_x_clipped).long()\n    coordinates_y_ceil = torch.ceil(coordinates_y_clipped).long()\n    coordinates_z_ceil = torch.ceil(coordinates_z_clipped).long()\n    features_floor = grid[:, coordinates_y_floor, coordinates_x_floor, coordinates_z_floor]\n    features_ceil = grid[:, coordinates_y_ceil, coordinates_x_ceil, coordinates_z_ceil]\n    features = torch.stack([features_floor, features_ceil], dim=-1)\n    return features\n\n\n",
  "canonical_solution": "    \n    batch_size, n_coords, n_dims = coordinates.shape\n    sampled_features = torch.nn.functional.grid_sample(\n        grid.expand(batch_size, -1, -1, -1, -1),\n        coordinates.reshape(batch_size, 1, 1, -1, n_dims),\n        mode='bilinear', \n        padding_mode='zeros', \n        align_corners=False,\n    )\n    N, C, H, W, D = sampled_features.shape\n    sampled_features = sampled_features.permute(0, 4, 3, 2, 1).reshape(N, H*W*D, C)\n    return sampled_features\n\n\n",
  "unprocess_testcases": "grid1 = torch.rand((1, 3, 4, 4, 4))  # 3D grid with 3 channels, shape (1, 3, 4, 4, 4)\ncoordinates1 = torch.tensor([[[1.0, 1.0, 1.0], [2.5, 2.5, 2.5]]])  # 3D coordinates, shape (1, 2, 3)\nassert((SOLUTION_SIGNATURE(grid1, coordinates1) == sample_from_3dgrid(grid1, coordinates1)).all())\n\ngrid2 = torch.rand((1, 2, 5, 5, 5))  # 3D grid with 2 channels, shape (1, 2, 5, 5, 5)\ncoordinates2 = torch.tensor([[[6.0, 6.0, 6.0], [-1.0, -1.0, -1.0]]])  # Out of bounds coordinates, shape (1, 2, 3)\nassert((SOLUTION_SIGNATURE(grid2, coordinates2) == sample_from_3dgrid(grid2, coordinates2)).all())\n\ngrid3 = torch.rand((1, 4, 6, 6, 6))  # 3D grid with 4 channels, shape (1, 4, 6, 6, 6)\ncoordinates3 = torch.tensor([\n    [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0]],\n    [[0.5, 0.5, 0.5], [4.5, 4.5, 4.5], [5.5, 5.5, 5.5]]\n])  # 3D coordinates, shape (2, 3, 3)\nassert((SOLUTION_SIGNATURE(grid3, coordinates3) == sample_from_3dgrid(grid3, coordinates3)).all())\n"
 },
 {
  "task_id": "RWPB/131",
  "prompt": "import torch\nimport numpy as np\n\ndef box_iou(box1, box2, eps=1e-7):\n    \"\"\"\n    Calculate intersection-over-union (IoU) of boxes. Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n    Based on https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n\n    Args:\n        box1 (torch.Tensor): A tensor of shape (N, 4) representing N bounding boxes.\n        box2 (torch.Tensor): A tensor of shape (M, 4) representing M bounding boxes.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n\n    Returns:\n        (torch.Tensor): An NxM tensor containing the pairwise IoU values for every element in box1 and box2.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\nimport numpy as np\n\ndef box_iou(box1, box2, eps=1e-7):\n    \"\"\"\n    Calculate intersection-over-union (IoU) of boxes. Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n    Based on https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n\n    Args:\n        box1 (torch.Tensor): A tensor of shape (N, 4) representing N bounding boxes.\n        box2 (torch.Tensor): A tensor of shape (M, 4) representing M bounding boxes.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n\n    Returns:\n        (torch.Tensor): An NxM tensor containing the pairwise IoU values for every element in box1 and box2.\n    \"\"\"\n    if box1.numel() == 0 or box2.numel() == 0:\n        return torch.empty((0, ), dtype=torch.float32)\n\n    area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n    area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n    lt = torch.max(box1[:, None, :2], box2[:, :2])  # [N,M,2]\n    rb = torch.min(box1[:, None, 2:], box2[:, 2:])  # [N,M,2]\n\n    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n\n    iou = inter / (area1[:, None] + area2 - inter + eps)\n    return iou\n\n\n",
  "canonical_solution": "\n    # NOTE: need float32 to get accurate iou values\n    box1 = torch.as_tensor(box1, dtype=torch.float32)\n    box2 = torch.as_tensor(box2, dtype=torch.float32)\n    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n    (a1, a2), (b1, b2) = box1.unsqueeze(1).chunk(2, 2), box2.unsqueeze(0).chunk(2, 2)\n    inter = (torch.min(a2, b2) - torch.max(a1, b1)).clamp_(0).prod(2)\n\n    # IoU = inter / (area1 + area2 - inter)\n    return inter / ((a2 - a1).prod(2) + (b2 - b1).prod(2) - inter + eps)\n\n",
  "unprocess_testcases": "box1 = np.array([[0, 0, 2, 2]])\nbox2 = np.array([[1, 1, 3, 3]])\nassert((SOLUTION_SIGNATURE(box1, box2) == box_iou(box1, box2)).all())\n\nbox1 = np.array([[0, 0, 1, 1]])\nbox2 = np.array([[2, 2, 3, 3]])\nassert((SOLUTION_SIGNATURE(box1, box2) == box_iou(box1, box2)).all())\n\nbox1 = np.array([[0, 0, 3, 3], [4, 4, 5, 5]])\nbox2 = np.array([[1, 1, 4, 4], [0, 0, 2, 2]])\nassert((SOLUTION_SIGNATURE(box1, box2) == box_iou(box1, box2)).all())\n\nbox1 = np.array([[0, 0, 3, 3], [4, 4, 5, 5]])\nbox2 = np.array([[1, 1, 4, 4], [0, 0, 2, 2]])\nassert((SOLUTION_SIGNATURE(box1, box2, 2e-5) == box_iou(box1, box2, 2e-5)).all())\n\nbox1 = np.array([[1, 3, 3, 6], [7, 7, 2, 6]])\nbox2 = np.array([[9, 3, 5, 4], [0, 1, 3, 8]])\nassert((SOLUTION_SIGNATURE(box1, box2) == box_iou(box1, box2)).all())\n"
 },
 {
  "task_id": "RWPB/121",
  "prompt": "def isprime(n):\n    \"\"\"\n    Check if a number is a prime number.\n\n    Args:\n        - n: int, number to be checked for primality\n\n    Returns:\n        - bool: True if n is prime, False otherwise\n    \"\"\"\n",
  "solution": "\ndef isprime(n):\n    \"\"\"\n    Check if a number is a prime number.\n\n    Args:\n        - n: int, number to be checked for primality\n\n    Returns:\n        - bool: True if n is prime, False otherwise\n    \"\"\"\n    if n <= 1:\n        return False\n    for i in range(2, int(n**0.5) + 1):\n        if n % i == 0:\n            return False\n    return True\n",
  "canonical_solution": "    \n    if n == 2:\n        return True\n    if n % 2 == 0:\n        return False\n\n    max = n**0.5+1\n    i = 3\n    \n    while i <= max:\n        if n % i == 0:\n            return False\n        i+=2\n    return True\n\n\n",
  "unprocess_testcases": "n = 2\nassert(SOLUTION_SIGNATURE(n) == isprime(n))\n\nn = 4\nassert(SOLUTION_SIGNATURE(n) == isprime(n))\n\nn = 29\nassert(SOLUTION_SIGNATURE(n) == isprime(n))\n"
 },
 {
  "task_id": "RWPB/115",
  "prompt": "import torch\nimport torch.nn as nn\n\ndef compute_tri_normal(geometry, tris):\n    \"\"\"\n    Compute the normal vectors for triangles defined by vertex indices.\n\n    Args:\n        geometry (torch.Tensor): A tensor of shape (batch_size, num_points, 3) representing the 3D coordinates of points.\n        tris (torch.Tensor): A tensor of shape (num_tris, 3) representing the vertex indices that form triangles.\n\n    Returns:\n        torch.Tensor: A tensor of shape (batch_size, num_tris, 3) representing the normal vectors of the triangles.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\nimport torch.nn as nn\n\ndef compute_tri_normal(geometry, tris):\n    \"\"\"\n    Compute the normal vectors for triangles defined by vertex indices.\n\n    Args:\n        geometry (torch.Tensor): A tensor of shape (batch_size, num_points, 3) representing the 3D coordinates of points.\n        tris (torch.Tensor): A tensor of shape (num_tris, 3) representing the vertex indices that form triangles.\n\n    Returns:\n        torch.Tensor: A tensor of shape (batch_size, num_tris, 3) representing the normal vectors of the triangles.\n    \"\"\"\n    batch_size, num_points, _ = geometry.shape\n    num_tris = tris.shape[0]\n    tri_vertices = geometry[:, tris, :]  # (batch_size, num_tris, 3, 3)\n    v1 = tri_vertices[:, :, 1, :] - tri_vertices[:, :, 0, :]  # (batch_size, num_tris, 3)\n    v2 = tri_vertices[:, :, 2, :] - tri_vertices[:, :, 0, :]  # (batch_size, num_tris, 3)\n    cross_product = torch.cross(v1, v2, dim=-1)  # (batch_size, num_tris, 3)\n    tri_normals = cross_product / torch.norm(cross_product, dim=-1, keepdim=True)  # (batch_size, num_tris, 3)\n\n    return tri_normals\n\n\n",
  "canonical_solution": "    \n    geometry = geometry.permute(0, 2, 1)\n    tri_1 = tris[:, 0]\n    tri_2 = tris[:, 1]\n    tri_3 = tris[:, 2]\n\n    vert_1 = torch.index_select(geometry, 2, tri_1)\n    vert_2 = torch.index_select(geometry, 2, tri_2)\n    vert_3 = torch.index_select(geometry, 2, tri_3)\n\n    nnorm = torch.cross(vert_2 - vert_1, vert_3 - vert_1, 1)\n    normal = nn.functional.normalize(nnorm).permute(0, 2, 1)\n    return normal\n\n\n",
  "unprocess_testcases": "geometry = torch.tensor([[[0., 0., 0.],\n                  [1., 0., 0.],\n                  [0., 1., 0.]]], dtype=torch.float32)\ntris = torch.tensor([[0, 1, 2]], dtype=torch.int64)\nassert((SOLUTION_SIGNATURE(geometry, tris) == compute_tri_normal(geometry, tris)).all())\n\ngeometry = torch.tensor([[[0., 0., 0.], [1., 0., 0.], [0., 1., 0.], [1., 1., 0.], [2., 0., 0.], [0., 2., 0.]],\n                 [[0., 0., 0.], [0., 1., 0.], [1., 0., 0.], [0., 0., 1.], [1., 1., 1.], [1., 0., 1.]]],\n                dtype=torch.float32)\ntris = torch.tensor([[0, 1, 2], [3, 4, 5]], dtype=torch.int64)\nassert((SOLUTION_SIGNATURE(geometry, tris) == compute_tri_normal(geometry, tris)).all())\n\ngeometry = torch.tensor([[[0., 0., 0.],\n                  [1., 1., 1.],\n                  [2., 2., 2.]]], dtype=torch.float32)\ntris = torch.tensor([[0, 1, 2]], dtype=torch.int64)\nassert((SOLUTION_SIGNATURE(geometry, tris) == compute_tri_normal(geometry, tris)).all())\n"
 },
 {
  "task_id": "RWPB/48",
  "prompt": "def select_best_resolution(original_size, possible_resolutions):\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        original_size (tuple): The original size of the image in the format (width, height).\n        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2), ...].\n\n    Returns:\n        tuple: The best fit resolution in the format (width, height).\n    \"\"\"\n",
  "solution": "\n\n\ndef select_best_resolution(original_size, possible_resolutions):\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        original_size (tuple): The original size of the image in the format (width, height).\n        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2),...].\n\n    Returns:\n        tuple: The best fit resolution in the format (width, height).\n    \"\"\"\n    best_resolution = None\n    best_fit_ratio = float('inf')\n    for resolution in possible_resolutions:\n        width_ratio = original_size[0] / resolution[0]\n        height_ratio = original_size[1] / resolution[1]\n        fit_ratio = max(width_ratio, height_ratio)\n        if fit_ratio < best_fit_ratio:\n            best_fit_ratio = fit_ratio\n            best_resolution = resolution\n\n    return best_resolution\n\n\n",
  "canonical_solution": "\n    original_width, original_height = original_size\n    best_fit = None\n    max_effective_resolution = 0\n    min_wasted_resolution = float('inf')\n\n    for width, height in possible_resolutions:\n        scale = min(width / original_width, height / original_height)\n        downscaled_width, downscaled_height = int(original_width * scale), int(original_height * scale)\n        effective_resolution = min(downscaled_width * downscaled_height, original_width * original_height)\n        wasted_resolution = (width * height) - effective_resolution\n\n        if effective_resolution > max_effective_resolution or (effective_resolution == max_effective_resolution and wasted_resolution < min_wasted_resolution):\n            max_effective_resolution = effective_resolution\n            min_wasted_resolution = wasted_resolution\n            best_fit = (width, height)\n\n    return best_fit\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE((1920, 1080), [(1280, 720), (1920, 1080), (2560, 1440)])[0] == select_best_resolution((1920, 1080), [(1280, 720), (1920, 1080), (2560, 1440)])[0])\nassert(SOLUTION_SIGNATURE((1920, 1080), [(1024, 768), (1280, 1024), (1600, 1200)])[0] == select_best_resolution((1920, 1080), [(1024, 768), (1280, 1024), (1600, 1200)])[0])\nassert(SOLUTION_SIGNATURE((800, 600), [(1024, 768), (1280, 960), (1920, 1440)])[0] == select_best_resolution((800, 600), [(1024, 768), (1280, 960), (1920, 1440)])[0])\nassert(SOLUTION_SIGNATURE((1920, 1080), [(1280, 720), (1920, 1080), (2560, 1440)])[1] == select_best_resolution((1920, 1080), [(1280, 720), (1920, 1080), (2560, 1440)])[1])\nassert(SOLUTION_SIGNATURE((1920, 1080), [(1024, 768), (1280, 1024), (1600, 1200)])[1] == select_best_resolution((1920, 1080), [(1024, 768), (1280, 1024), (1600, 1200)])[1])\nassert(SOLUTION_SIGNATURE((800, 600), [(1024, 768), (1280, 960), (1920, 1440)])[1] == select_best_resolution((800, 600), [(1024, 768), (1280, 960), (1920, 1440)])[1])\n"
 },
 {
  "task_id": "RWPB/19",
  "prompt": "def rank_accuracy(preds, references):\n    '''\n    Computes the accuracy of ranking predictions compared to reference rankings.\n\n    Parameters\n    ----------\n    preds : list\n        A list of numeric predictions, typically confidence scores, where higher\n        values represent stronger predictions.\n    references : list\n        A list of actual values or labels, with the same ordering principle as\n        'preds', used as a ground truth for accuracy calculation.\n\n    Return\n    ------\n    float\n        The proportion of correct ranking relations in 'preds' relative to 'references'.\n\n    This function first generates all possible pairs of different predictions and checks if\n    the ranking (greater or lesser) of the first element relative to the second in each pair\n    is correct based on the corresponding reference values.\n    '''\n",
  "solution": "\n\n\ndef rank_accuracy(preds, references):\n    '''\n    Computes the accuracy of ranking predictions compared to reference rankings.\n\n    Parameters\n    ----------\n    preds : list\n        A list of numeric predictions, typically confidence scores, where higher\n        values represent stronger predictions.\n    references : list\n        A list of actual values or labels, with the same ordering principle as\n        'preds', used as a ground truth for accuracy calculation.\n\n    Return\n    ------\n    float\n        The proportion of correct ranking relations in 'preds' relative to'references'.\n\n    This function first generates all possible pairs of different predictions and checks if\n    the ranking (greater or lesser) of the first element relative to the second in each pair\n    is correct based on the corresponding reference values.\n    '''\n    assert len(preds) == len(references), \"The length of 'preds' and'references' must be equal.\"\n    pairs = [(i, j) for i in range(len(preds)) for j in range(i + 1, len(preds))]\n    correct_pairs = 0\n    for i, j in pairs:\n        if (preds[i] > preds[j]) == (references[i] > references[j]):\n            correct_pairs += 1\n    return correct_pairs / len(pairs)\n\n\n",
  "canonical_solution": "\n    # Initialize an empty list to store the pairs and their comparison results\n    pairs = []\n\n    # Generate all pairwise comparisons from preds except self-comparisons\n    for i, pred in enumerate(preds):\n        for j, pred2 in enumerate(preds):\n            if i == j:\n                continue  # Skip comparing the same elements\n            pairs.append((i, j, pred > pred2))  # Store index and comparison result\n\n    # Count how many of these predicted comparisons are correct according to the references\n    correct = 0\n    for i, ref in enumerate(references):\n        for j, ref2 in enumerate(references):\n            if i == j:\n                continue  # Skip self-comparisons\n            if (i, j, ref > ref2) in pairs:\n                correct += 1  # Increment if the prediction comparison is correct\n\n    # Calculate the accuracy as the ratio of correct predictions to total comparisons\n    return correct / len(pairs) if pairs else 0  # Handle the case where pairs might be empty\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(preds = [0.9, 0.1, 0.5], references = [3, 1, 2]) == rank_accuracy(preds = [0.9, 0.1, 0.5], references = [3, 1, 2]))\nassert(SOLUTION_SIGNATURE(preds = [0.5, 0.9, 0.1], references = [3, 1, 2]) == rank_accuracy(preds = [0.5, 0.9, 0.1], references = [3, 1, 2]))\nassert(SOLUTION_SIGNATURE(preds = [0.5, 0.5, 0.1], references = [2, 2, 1]) == rank_accuracy(preds = [0.5, 0.5, 0.1], references = [2, 2, 1]))\n"
 },
 {
  "task_id": "RWPB/4",
  "prompt": "import numpy as np\nimport torch.nn.functional as F\nimport torch\nimport roma\n\ndef rot6d_to_rotmat(x):\n    \"\"\"\n    6D rotation representation to 3x3 rotation matrix. \n    If the size of matrix is not (B,6), please return ValueError\n\n    Args:\n        x: (B,6) Batch of 6-D rotation representations.\n    Returns:\n        torch.Tensor: Batch of corresponding rotation matrices with shape (B,3,3).\n    \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\nimport torch.nn.functional as F\nimport torch\nimport roma\n\ndef rot6d_to_rotmat(x):\n    \"\"\"\n    6D rotation representation to 3x3 rotation matrix. \n    If the size of matrix is not (B,6), please return ValueError\n\n    Args:\n        x: (B,6) Batch of 6-D rotation representations.\n    Returns:\n        torch.Tensor: Batch of corresponding rotation matrices with shape (B,3,3).\n    \"\"\"\n    if x.shape!= (B,6):\n        raise ValueError(\"The size of matrix is not (B,6)\")\n    x = x.view(-1, 2, 3)\n    x1 = x[:, 0]\n    x2 = x[:, 1]\n    x1_norm = torch.norm(x1, dim=1)\n    x2_norm = torch.norm(x2, dim=1)\n    x1_unit = x1 / x1_norm[:, None]\n    x2_unit = x2 / x2_norm[:, None]\n    z_axis = torch.cross(x1_unit, x2_unit, dim=1)\n    z_axis_norm = torch.norm(z_axis, dim=1)\n    z_axis_unit = z_axis / z_axis_norm[:, None]\n    y_axis = F.normalize(x1_unit, dim=1)\n    x_axis = torch.cross(y_axis, z_axis_unit, dim=1)\n    rotmat = torch.stack((x_axis, y_axis, z_axis_unit), dim=1)\n    return rotmat\n\n",
  "canonical_solution": "    \n    if x.size(1) != 6:\n        return ValueError\n    x = x.reshape(-1,2,3).permute(0, 2, 1).contiguous()\n    y = roma.special_gramschmidt(x)\n    return y\n\n\n",
  "unprocess_testcases": "a = torch.randn(3, 6)\nb = torch.randn(1, 6)\nc = torch.randn(3, 5)\n\nassert((SOLUTION_SIGNATURE(a) == rot6d_to_rotmat(a)).all())\nassert((SOLUTION_SIGNATURE(b) == rot6d_to_rotmat(b)).all())\nassert(SOLUTION_SIGNATURE(c) == rot6d_to_rotmat(c))"
 },
 {
  "task_id": "RWPB/39",
  "prompt": "import re\nfrom typing import List\n\n\ndef clean_html(\n        html_to_clean: str,\n        tags_to_remove: List[str] = [\"style\", \"svg\", \"script\"],\n        attributes_to_keep: List[str] = [\"id\", \"href\"],\n) -> str:\n    \"\"\"\n    Cleans HTML content by removing specified tags and their contents, and strips out all attributes except those specified.\n\n    Args:\n        html_to_clean (str): The HTML content to clean.\n        tags_to_remove (List[str]): List of HTML tags to remove from the content. Default tags are 'style', 'svg', and 'script'.\n        attributes_to_keep (List[str]): List of HTML attributes to retain in the remaining HTML tags. Default is ['id', 'href'].\n\n    Returns:\n        str: The cleaned HTML content, free of specified tags and with only desired attributes retained.\n\n    This function first removes all specified tags and their contents using regular expressions. It then eliminates all attributes\n    from the remaining tags except for those explicitly listed to be kept. This is useful for preparing HTML content for environments\n    where only basic styling or functionality is needed, or for security purposes to remove potentially malicious content.\n\n    Example:\n        >>> clean_html('<div id=\"main\" style=\"color:red\">Hello <script>alert(\"World\")</script></div>',\n                       tags_to_remove=['script'], attributes_to_keep=['id'])\n        '<div id=\"main\">Hello </div>'\n    \"\"\"\n",
  "solution": "\n\n\ndef clean_html(\n        html_to_clean: str,\n        tags_to_remove: List[str] = [\"style\", \"svg\", \"script\"],\n        attributes_to_keep: List[str] = [\"id\", \"href\"],\n) -> str:\n    \"\"\"\n    Cleans HTML content by removing specified tags and their contents, and strips out all attributes except those specified.\n\n    Args:\n        html_to_clean (str): The HTML content to clean.\n        tags_to_remove (List[str]): List of HTML tags to remove from the content. Default tags are'style','svg', and'script'.\n        attributes_to_keep (List[str]): List of HTML attributes to retain in the remaining HTML tags. Default is ['id', 'href'].\n\n    Returns:\n        str: The cleaned HTML content, free of specified tags and with only desired attributes retained.\n\n    This function first removes all specified tags and their contents using regular expressions. It then eliminates all attributes\n    from the remaining tags except for those explicitly listed to be kept. This is useful for preparing HTML content for environments\n    where only basic styling or functionality is needed, or for security purposes to remove potentially malicious content.\n\n    Example:\n        >>> clean_html('<div id=\"main\" style=\"color:red\">Hello <script>alert(\"World\")</script></div>',\n                       tags_to_remove=['script'], attributes_to_keep=['id'])\n        '<div id=\"main\">Hello </div>'\n    \"\"\"\n    for tag in tags_to_remove:\n        html_to_clean = re.sub(rf\"<{tag}[^>]*>.*?</{tag}>\", \"\", html_to_clean, flags=re.DOTALL)\n    html_to_clean = re.sub(rf\"<([^>]+)>\", lambda m: keep_attributes(m.group(1), attributes_to_keep), html_to_clean)\n\n    return html_to_clean\n\n\ndef keep_attributes(tag: str, attributes_to_keep: List[str]) -> str:\n    \"\"\"\n    Keeps only the specified attributes in an HTML tag.\n\n    Args:\n        tag (str): The HTML tag to process.\n        attributes_to_keep (List[str]): List of HTML attributes to retain in the tag.\n\n    Returns:\n        str: The modified HTML tag with only the desired attributes retained.\n\n    Example:\n        >>> keep_attributes('div id=\"main\" style=\"color:red\"', ['id'])\n        'div id=\"main\"'\n    \"\"\"\n    attributes = tag.split()\n    attributes = [attr for attr in attributes if attr.split(\"=\")[0] in attributes_to_keep]\n    return \" \".join(attributes)\n\n\n",
  "canonical_solution": "\n    # Remove specified tags and their contents completely\n    for tag in tags_to_remove:\n        html_to_clean = re.sub(\n            rf\"<{tag}[^>]*>.*?</{tag}>\", \"\", html_to_clean, flags=re.DOTALL\n        )\n\n    # Compile a pattern that matches any attribute not in the list of attributes to keep\n    attributes_to_keep = \"|\".join(attributes_to_keep)\n    pattern = rf'\\b(?!({attributes_to_keep})\\b)\\w+(?:-\\w+)?\\s*=\\s*[\"\\'][^\"\\']*[\"\\']'\n\n    # Remove all attributes that are not explicitly listed to be kept\n    cleaned_html = re.sub(pattern, \"\", html_to_clean)\n\n    cleaned_html = re.sub(rf'\\s*>', \">\", cleaned_html)\n\n    return cleaned_html\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE('<div><style> .text { font-size: 12px; } </style><p id=\"info\">Visit our <a href=\"http://example.com\" style=\"color: red;\">website</a></p></div>') == clean_html('<div><style> .text { font-size: 12px; } </style><p id=\"info\">Visit our <a href=\"http://example.com\" style=\"color: red;\">website</a></p></div>'))\nassert(SOLUTION_SIGNATURE('<div id=\"container\"><script>alert(\\'Hello\\');</script><svg><circle cx=\"50\" cy=\"50\" r=\"40\" stroke=\"black\" fill=\"red\"></circle></svg><a href=\"#nothing\" onclick=\"javascript:void(0);\" style=\"color: black;\">Click here</a></div>') == clean_html('<div id=\"container\"><script>alert(\\'Hello\\');</script><svg><circle cx=\"50\" cy=\"50\" r=\"40\" stroke=\"black\" fill=\"red\"></circle></svg><a href=\"#nothing\" onclick=\"javascript:void(0);\" style=\"color: black;\">Click here</a></div>'))\nassert(SOLUTION_SIGNATURE('<header style=\"background: black;\"><nav id=\"top-nav\" class=\"navigation\" data-toggle=\"collapse\"><a href=\"/home\" title=\"Home\">Home</a></nav></header>') == clean_html('<header style=\"background: black;\"><nav id=\"top-nav\" class=\"navigation\" data-toggle=\"collapse\"><a href=\"/home\" title=\"Home\">Home</a></nav></header>'))\n"
 },
 {
  "task_id": "RWPB/68",
  "prompt": "import numpy as np\n\ndef last_before(val, arr):\n  \"\"\"\n  Find the index of the last element in a sorted array that is less than or equal to a given value.Requires sorted (monotonic) arr.\n  Args:\n      - val: int, the value to search for\n      - arr: numpy.ndarray, the sorted array\n  Returns:\n      idx: int, the index of the last element in the array that is less than or equal to val\n  \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\n\ndef last_before(val, arr):\n  \"\"\"\n  Find the index of the last element in a sorted array that is less than or equal to a given value.\n  Args:\n      - val: int, the value to search for\n      - arr: numpy.ndarray, the sorted array\n  Returns:\n      idx: int, the index of the last element in the array that is less than or equal to val\n  \"\"\"\n  idx = np.searchsorted(arr, val, side='right') - 1\n  if idx < 0 or arr[idx] > val:\n      return -1\n  return idx\n\n",
  "canonical_solution": "  \n  assert np.all(np.diff(arr) >= 0)\n  return np.searchsorted(arr, val) - 1\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(val = 5, arr = np.array([1, 3, 5, 7])) == last_before(val = 5, arr = np.array([1, 3, 5, 7])))\nassert(SOLUTION_SIGNATURE(val = 4, arr = np.array([1, 3, 5, 7])) == last_before(val = 4, arr = np.array([1, 3, 5, 7])))\nassert(SOLUTION_SIGNATURE(val = 0, arr = np.array([1, 3, 5, 7])) == last_before(val = 0, arr = np.array([1, 3, 5, 7])))\n"
 },
 {
  "task_id": "RWPB/135",
  "prompt": "import torch\n\ndef mask_iou(mask1, mask2, eps=1e-7):\n    \"\"\"\n    Calculate masks IoU.\n\n    Args:\n        mask1 (torch.Tensor): A tensor of shape (N, n) where N is the number of ground truth objects and n is the\n                        product of image width and height.\n        mask2 (torch.Tensor): A tensor of shape (M, n) where M is the number of predicted objects and n is the\n                        product of image width and height.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n\n    Returns:\n        (torch.Tensor): A tensor of shape (N, M) representing masks IoU.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef mask_iou(mask1, mask2, eps=1e-7):\n    \"\"\"\n    Calculate masks IoU.\n\n    Args:\n        mask1 (torch.Tensor): A tensor of shape (N, n) where N is the number of ground truth objects and n is the\n                        product of image width and height.\n        mask2 (torch.Tensor): A tensor of shape (M, n) where M is the number of predicted objects and n is the\n                        product of image width and height.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n\n    Returns:\n        (torch.Tensor): A tensor of shape (N, M) representing masks IoU.\n    \"\"\"\n    intersection = (mask1 * mask2).sum(dim=1)\n    union = (mask1 + mask2).sum(dim=1) - intersection\n    iou = (intersection + eps) / (union + eps)\n    iou = iou.view(mask1.shape[0], mask2.shape[0])\n\n    return iou\n\n",
  "canonical_solution": "\n    intersection = torch.matmul(mask1, mask2.T).clamp_(0)\n    union = (mask1.sum(1)[:, None] + mask2.sum(1)[None]) - intersection  # (area1 + area2) - intersection\n    return intersection / (union + eps)\n\n\n",
  "unprocess_testcases": "mask1 = torch.tensor([\n    [1, 0, 1, 0],  # Object 1\n    [0, 1, 0, 1]   # Object 2\n])\nmask2 = torch.tensor([\n    [1, 0, 0, 1],  # Prediction 1\n    [0, 1, 1, 0],  # Prediction 2\n    [1, 1, 1, 1]   # Prediction 3\n])\nassert((SOLUTION_SIGNATURE(mask1, mask2) == mask_iou(mask1, mask2)).all())\n\nmask1 = torch.tensor([\n    [1, 0, 0, 0],  # Object 1\n    [0, 1, 0, 0]   # Object 2\n])\nmask2 = torch.tensor([\n    [0, 0, 1, 0],  # Prediction 1\n    [0, 0, 0, 1]   # Prediction 2\n])\nassert((SOLUTION_SIGNATURE(mask1, mask2) == mask_iou(mask1, mask2)).all())\n\n\nmask1 = torch.tensor([\n    [1, 1, 0, 0, 1, 1],  # Object 1\n    [0, 1, 1, 1, 0, 0],  # Object 2\n    [1, 0, 1, 0, 1, 0]   # Object 3\n])\nmask2 = torch.tensor([\n    [1, 0, 0, 1, 1, 1],  # Prediction 1\n    [0, 1, 1, 0, 0, 1]   # Prediction 2\n])\nassert((SOLUTION_SIGNATURE(mask1, mask2) == mask_iou(mask1, mask2)).all())\n"
 },
 {
  "task_id": "RWPB/101",
  "prompt": "import torch\n\ndef generate_grids(lower_limit, upper_limit, grid_size):\n    \"\"\"\n    Generate a grid of points within the specified limits.\n\n    Args:\n        lower_limit (torch.Tensor): Lower boundary of the grid.\n        upper_limit (torch.Tensor): Upper boundary of the grid.\n        grid_size (torch.Tensor): Number of grid points per dimension.\n\n    Returns:\n        tuple: Two tensors, each containing the lower and upper bounds of each grid cell.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef generate_grids(lower_limit, upper_limit, grid_size):\n    \"\"\"\n    Generate a grid of points within the specified limits.\n\n    Args:\n        lower_limit (torch.Tensor): Lower boundary of the grid.\n        upper_limit (torch.Tensor): Upper boundary of the grid.\n        grid_size (torch.Tensor): Number of grid points per dimension.\n\n    Returns:\n        tuple: Two tensors, each containing the lower and upper bounds of each grid cell.\n    \"\"\"\n    grid_points = torch.linspace(lower_limit, upper_limit, grid_size)\n    grid_cells = torch.meshgrid(grid_points, grid_points)\n    lower_bounds = torch.stack(grid_cells, dim=-1)\n    upper_bounds = torch.stack(grid_cells, dim=-1) + (upper_limit - lower_limit) / (grid_size - 1)\n\n    return lower_bounds, upper_bounds\n\n\nTo test the function, we can use the following code:\n\n\nimport torch\n\nlower_limit = torch.tensor([0.0, 0.0])\nupper_limit = torch.tensor([1.0, 1.0])\ngrid_size = torch.tensor(3)\n\nexpected_lower_bounds = torch.tensor([\n    [0.0, 0.0],\n    [0.0, 0.5],\n    [0.0, 1.0],\n    [0.5, 0.0],\n    [0.5, 0.5],\n    [0.5, 1.0],\n    [1.0, 0.0],\n    [1.0, 0.5],\n    [1.0, 1.0],\n])\n\nexpected_upper_bounds = torch.tensor([\n    [0.0, 0.5],\n    [0.0, 1.0],\n    [0.0, 1.5],\n    [0.5, 0.5],\n    [0.5, 1.0],\n    [0.5, 1.5],\n    [1.0, 0.5],\n    [1.0, 1.0],\n    [1.0, 1.5],\n])\n\nlower_bounds, upper_bounds = generate_grids(lower_limit, upper_limit, grid_size)\n\nassert torch.allclose(lower_bounds, expected_lower_bounds)\nassert torch.allclose(upper_bounds, expected_upper_bounds)\n\n\nThe function `generate_grids` takes three arguments: `lower_limit`, `upper_limit`, and `grid_size`. The `lower_limit` and `upper_limit` are the lower and upper boundaries of the grid, respectively. The `grid_size` is the number of grid points per dimension.\n\nThe function first calculates the grid points using `torch.linspace` with the `grid_size` argument. Then, it uses `torch.meshgrid` to create a grid of points. The `grid_cells` variable contains a list of tensors, each representing a dimension of the grid.\n\nNext, the function stacks the grid cells along the last dimension using `torch.stack`. This creates a tensor with shape `(grid_size, grid_size, 2)`, where the last dimension represents the coordinates of each grid point.\n\nFinally, the function calculates the lower and upper bounds of each grid cell by adding the cell coordinates to the lower and upper limits, respectively. The `lower_bounds` and `upper_bounds` variables contain the lower and upper bounds of each grid cell, respectively.\n\nThe function returns a tuple containing the `lower_bounds` and `upper_bounds` tensors.\n\nTo test the function, we can use the following code:\n\n\nimport torch\n\nlower_limit = torch.tensor([0.0, 0.0])\nupper_limit = torch.tensor([1.0, 1.0])\ngrid_size = torch.tensor(3)\n\nexpected_lower_bounds = torch.tensor([\n    [0.0, 0.0],\n    [0.0, 0.5],\n    [0.0, 1.0],\n    [0.5, 0.0],\n    [0.5, 0.5],\n    [0.5, 1.0],\n    [1.0, 0.0],\n    [1.0, 0.5],\n    [1.0, 1.0],\n])\n\nexpected_upper_bounds = torch.tensor([\n    [0.0, 0.5],\n    [0.0",
  "canonical_solution": "    \n    ndim = lower_limit.size(0)\n    assert ndim == upper_limit.size(0)\n    assert lower_limit.ndim == upper_limit.ndim == 1\n    grids = [None] * ndim\n    steps = (upper_limit - lower_limit) / grid_size\n    for d in range(ndim):\n        grids[d] = torch.linspace(\n            lower_limit[d], upper_limit[d], grid_size[d] + 1, device=lower_limit.device\n        )[: grid_size[d]]\n    lower = torch.cartesian_prod(*grids)\n    upper = lower + steps\n    return lower, upper\n\n",
  "unprocess_testcases": "\nlower_limit = torch.tensor([0.0])\nupper_limit = torch.tensor([1.0])\ngrid_size = torch.tensor([2])\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[0] == generate_grids(lower_limit, upper_limit, grid_size)[0]).all())\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[1] == generate_grids(lower_limit, upper_limit, grid_size)[1]).all())\n\nlower_limit = torch.tensor([0.0, 0.0])\nupper_limit = torch.tensor([1.0, 1.0])\ngrid_size = torch.tensor([2, 2])\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[0] == generate_grids(lower_limit, upper_limit, grid_size)[0]).all())\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[1] == generate_grids(lower_limit, upper_limit, grid_size)[1]).all())\n\nlower_limit = torch.tensor([-1.0, 0.0, 2.0])\nupper_limit = torch.tensor([1.0, 1.0, 5.0])\ngrid_size = torch.tensor([2, 3, 1])\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[0] == generate_grids(lower_limit, upper_limit, grid_size)[0]).all())\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[1] == generate_grids(lower_limit, upper_limit, grid_size)[1]).all())\n"
 },
 {
  "task_id": "RWPB/111",
  "prompt": "import torch\n\ndef mesh_grid(B, H, W):\n    \"\"\"Create a mesh grid.\n\n    Args:\n        B (int): Batch size.\n        H (int): Height of the grid.\n        W (int): Width of the grid.\n\n    Returns:\n        torch.Tensor: A tensor of shape (B, 2, H, W) containing the mesh grid coordinates.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef mesh_grid(B, H, W):\n    \"\"\"Create a mesh grid.\n\n    Args:\n        B (int): Batch size.\n        H (int): Height of the grid.\n        W (int): Width of the grid.\n\n    Returns:\n        torch.Tensor: A tensor of shape (B, 2, H, W) containing the mesh grid coordinates.\n    \"\"\"\n    x_coords = torch.arange(W).repeat(H, 1)\n    y_coords = torch.arange(H).repeat(W, 1).t()\n    grid = torch.stack([x_coords, y_coords], dim=0)\n    grid = grid.unsqueeze(0).repeat(B, 1, 1, 1)\n    return grid\n\n\n",
  "canonical_solution": "    \n    x_base = torch.arange(0, W).repeat(B, H, 1)  # BHW\n    y_base = torch.arange(0, H).repeat(B, W, 1).transpose(1, 2)  # BHW\n\n    base_grid = torch.stack([x_base, y_base], 1)  # B2HW\n    return base_grid\n\n",
  "unprocess_testcases": "B=1\nH=1\nW=1\nassert((SOLUTION_SIGNATURE(B, H, W) == mesh_grid(B, H, W)).all())\n\nB=1\nH=5\nW=10\nassert((SOLUTION_SIGNATURE(B, H, W) == mesh_grid(B, H, W)).all())\n\nB=100\nH=10\nW=10\nassert((SOLUTION_SIGNATURE(B, H, W) == mesh_grid(B, H, W)).all())\n\nB=3\nH=10\nW=5\nassert((SOLUTION_SIGNATURE(B, H, W) == mesh_grid(B, H, W)).all())\n"
 },
 {
  "task_id": "RWPB/140",
  "prompt": "from typing import Callable, Dict, Tuple, Any\nimport inspect\nfrom functools import partial\n\ndef get_args_for_func(func: Callable, params: Dict) -> Tuple[Callable, Dict[str, Any]]:\n    \"\"\"\n    Filters keyword arguments from a dictionary based on a function's parameter list and creates a partially\n    applied function that already includes the matched keyword arguments.\n\n    Parameters:\n        func (Callable): The function for which the keyword arguments need to be matched.\n        params (Dict): A dictionary containing potential keyword arguments to pass to the function.\n\n    Returns:\n        Tuple[Callable, Dict[str, Any]]:\n            A tuple containing:\n            - A partial function with pre-applied keyword arguments that are valid for the specified function.\n            - A dictionary of the keyword arguments that were applicable to the function.\n\n    Example:\n        input_dict = {\"a\": \"foo\", \"b\": \"bar\"}\n        def example_func(a: str):\n            return a\n        func, kwargs = get_args_for_func(example_func, input_dict)\n        # func can now be called as func() and it would execute example_func(a=\"foo\")\n        # kwargs will be {'a': 'foo'}\n    \"\"\"\n",
  "solution": "\n\n\nfrom typing import Callable, Dict, Tuple, Any\nimport inspect\nfrom functools import partial\n\ndef get_args_for_func(func: Callable, params: Dict) -> Tuple[Callable, Dict[str, Any]]:\n    \"\"\"\n    Filters keyword arguments from a dictionary based on a function's parameter list and creates a partially\n    applied function that already includes the matched keyword arguments.\n\n    Parameters:\n        func (Callable): The function for which the keyword arguments need to be matched.\n        params (Dict): A dictionary containing potential keyword arguments to pass to the function.\n\n    Returns:\n        Tuple[Callable, Dict[str, Any]]:\n            A tuple containing:\n            - A partial function with pre-applied keyword arguments that are valid for the specified function.\n            - A dictionary of the keyword arguments that were applicable to the function.\n\n    Example:\n        input_dict = {\"a\": \"foo\", \"b\": \"bar\"}\n        def example_func(a: str):\n            return a\n        func, kwargs = get_args_for_func(example_func, input_dict)\n        # func can now be called as func() and it would execute example_func(a=\"foo\")\n        # kwargs will be {'a': 'foo'}\n    \"\"\"\n    sig = inspect.signature(func)\n    kwargs = {k: v for k, v in params.items() if k in sig.parameters}\n    partial_func = partial(func, **kwargs)\n    return partial_func, kwargs\n\n",
  "canonical_solution": "    \n    # Extracts function argument names using inspect\n    _kwargs = {k: v for k, v in params.items() if k in inspect.getfullargspec(func).args}\n    # Creates a partial function that pre-applies these keyword arguments\n    return (\n        partial(func, **_kwargs),\n        _kwargs,\n    )\n\n\n",
  "unprocess_testcases": "def add(x, y):\n    return x + y\n\ndef multiply(x, y):\n    return x * y\n\ndef greet(name, msg=\"Hello\"):\n    return f\"{msg}, {name}\"\n\nfunc, kwargs = get_args_for_func(add, {'x': 10, 'y': 5})\n\nassert(SOLUTION_SIGNATURE(add, {'x': 10, 'y': 5})[0]() == 15)\nassert(SOLUTION_SIGNATURE(add, {'x': 10, 'y': 5})[1] == get_args_for_func(add, {'x': 10, 'y': 5})[1])\n\nassert(SOLUTION_SIGNATURE(multiply, {'x': 2, 'y': 8, 'z': 15})[0]() == 16)\nassert(SOLUTION_SIGNATURE(multiply, {'x': 2, 'y': 8, 'z': 15})[1] == get_args_for_func(multiply, {'x': 2, 'y': 8, 'z': 15})[1])\n\nassert(SOLUTION_SIGNATURE(greet, {'name': 'Alice'})[0]() == \"Hello, Alice\")\nassert(SOLUTION_SIGNATURE(greet, {'name': 'Alice'})[1] == get_args_for_func(greet, {'name': 'Alice'})[1])"
 },
 {
  "task_id": "RWPB/125",
  "prompt": "import numpy as np\n\ndef resample_segments(segments, n=1000):\n    \"\"\"\n    Resamples each segment in a list to have a specified number of equidistant points.\n\n    Args:\n        segments (list): A list of numpy arrays of shape (n, 2), where each array represents a segment with n points.\n        n (int): The number of points to resample each segment to, defaults to 1000.\n\n    Returns:\n        list: A list of numpy arrays, each reshaped to (n, 2), representing the resampled segments.\n    \"\"\"\n",
  "solution": "\nimport numpy as np\n\ndef resample_segments(segments, n=1000):\n    \"\"\"\n    Resamples each segment in a list to have a specified number of equidistant points.\n\n    Args:\n        segments (list): A list of numpy arrays of shape (n, 2), where each array represents a segment with n points.\n        n (int): The number of points to resample each segment to, defaults to 1000.\n\n    Returns:\n        list: A list of numpy arrays, each reshaped to (n, 2), representing the resampled segments.\n    \"\"\"\n    resampled_segments = []\n    for segment in segments:\n        x = segment[:, 0]\n        y = segment[:, 1]\n        t = np.linspace(0, 1, n)\n        x_resampled = np.interp(t, np.linspace(0, 1, len(x)), x)\n        y_resampled = np.interp(t, np.linspace(0, 1, len(y)), y)\n        resampled_segment = np.column_stack((x_resampled, y_resampled))\n        resampled_segments.append(resampled_segment)\n    return resampled_segments\n",
  "canonical_solution": "    \n    for i, s in enumerate(segments):\n        # Ensure the segment is closed by appending the first point to the end\n        s = np.concatenate((s, s[0:1, :]), axis=0)\n        # Create an array of the desired number of points spaced between 0 and the last index of the segment\n        x = np.linspace(0, len(s) - 1, n)\n        # Create an array of existing indices\n        xp = np.arange(len(s))\n        # Interpolate new points for both x and y coordinates\n        segments[i] = np.stack(\n            [np.interp(x, xp, s[:, j]) for j in range(2)], axis=-1\n        )  # Stack interpolated x and y coordinates\n    return segments\n\n",
  "unprocess_testcases": "segment1 = np.array([[0, 0], [2.5, 2.5], [5, 5], [7.5, 7.5], [10, 10]])\nsegment2 = np.array([[0, 0], [1, 5], [2, 10], [3, 15], [4, 20], [5, 25]])\nsegments = [segment1, segment2]\nassert((SOLUTION_SIGNATURE(segments, n=100)[i] == resample_segments(segments, n=100)[i]).all() for i in range(len(resample_segments(segments, n=100))))\n\nsegment = np.array([[0, 0], [1, 1]])\nsegments = [segment]\nassert((SOLUTION_SIGNATURE(segments, n=3)[i] == resample_segments(segments, n=3)[i]).all for i in range(len(resample_segments(segments, n=3))))\n\nsegment = np.array([[0, 0], [1, 1]])\nsegments = [segment]\nassert((SOLUTION_SIGNATURE(segments, n=1)[i] == resample_segments(segments, n=1)[i]).all() for i in range(len(resample_segments(segments, n=1))))\n\nrepeated_point_segment = np.array([[1, 1]] * 5)\ncircle = np.array([[np.cos(theta), np.sin(theta)] for theta in np.linspace(0, 2 * np.pi, 100, endpoint=False)])\nsegments = [repeated_point_segment, circle]\nassert((SOLUTION_SIGNATURE(segments, n=50)[i] == resample_segments(segments, n=50)[i]).all() for i in range(len(resample_segments(segments, n=50))))\n"
 },
 {
  "task_id": "RWPB/78",
  "prompt": "import pandas\n\ndef check_all_columns_numeric(df):\n    \"\"\"\n    Check if all columns in a DataFrame are numeric. Return True if so, False otherwise.\n    \"\"\"\n",
  "solution": "\n\n\nimport pandas as pd\n\ndef check_all_columns_numeric(df):\n    \"\"\"\n    Check if all columns in a DataFrame are numeric. Return True if so, False otherwise.\n    \"\"\"\n    return df.select_dtypes(include=pd.np.number).shape[1] == df.shape[1]\n\n\n",
  "canonical_solution": "    \n    return df.select_dtypes(include=[int, float]).shape[1] == df.shape[1]\n\n\n",
  "unprocess_testcases": "\nimport pandas as pd\ndf_numeric = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4.1, 5.2, 6.3]\n})\nassert(SOLUTION_SIGNATURE(df_numeric) == check_all_columns_numeric(df_numeric))\n\ndf_mixed = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': ['x', 'y', 'z']\n})\nassert(SOLUTION_SIGNATURE(df_mixed) == check_all_columns_numeric(df_mixed))\n\ndf_empty = pd.DataFrame()\nassert(SOLUTION_SIGNATURE(df_empty) == check_all_columns_numeric(df_empty))\n\ndf_varied_types = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [1.5, 2.5, 3.5],\n    'C': [True, False, True]\n})\nassert(SOLUTION_SIGNATURE(df_varied_types) == check_all_columns_numeric(df_varied_types))\n"
 },
 {
  "task_id": "RWPB/29",
  "prompt": "import torch\n\ndef iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    \"\"\"\n    Computes the Intersection over Union (IoU) for the foreground class in binary segmentation.\n\n    Args:\n        preds (Tensor): Predictions from the model, where 1 indicates foreground and 0 indicates background.\n        labels (Tensor): Ground truth binary masks corresponding to the predictions.\n        EMPTY (float, optional): The value to return if there is no union between predicted and ground truth pixels.\n                                 Default is 1.0.\n        ignore (int, optional): Label value to ignore during IoU calculation. This is useful for ignoring certain\n                                pixels in evaluation, such as boundary or undefined areas.\n        per_image (bool, optional): If True, calculates IoU independently for each image and then averages them.\n                                    If False, calculates IoU across the entire batch as a whole. Default is True.\n\n    Returns:\n        float: The IoU score multiplied by 100 to convert it to a percentage.\n\n    This function calculates the IoU score for the foreground class, accounting for any ignore labels\n    and handling both individual image evaluations and batch-wide evaluations. The IoU is calculated\n    by determining the intersection and union of the predicted and actual foreground areas, and handling\n    cases where the union might be zero.\n    \"\"\"\n",
  "solution": "\n\n\ndef iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    \"\"\"\n    Computes the Intersection over Union (IoU) for the foreground class in binary segmentation.\n\n    Args:\n        preds (Tensor): Predictions from the model, where 1 indicates foreground and 0 indicates background.\n        labels (Tensor): Ground truth binary masks corresponding to the predictions.\n        EMPTY (float, optional): The value to return if there is no union between predicted and ground truth pixels.\n                                 Default is 1.0.\n        ignore (int, optional): Label value to ignore during IoU calculation. This is useful for ignoring certain\n                                pixels in evaluation, such as boundary or undefined areas.\n        per_image (bool, optional): If True, calculates IoU independently for each image and then averages them.\n                                    If False, calculates IoU across the entire batch as a whole. Default is True.\n\n    Returns:\n        float: The IoU score multiplied by 100 to convert it to a percentage.\n\n    This function calculates the IoU score for the foreground class, accounting for any ignore labels\n    and handling both individual image evaluations and batch-wide evaluations. The IoU is calculated\n    by determining the intersection and union of the predicted and actual foreground areas, and handling\n    cases where the union might be zero.\n    \"\"\"\n    if per_image:\n        ious = []\n        for pred, label in zip(preds, labels):\n            iou = iou_binary_image(pred, label, EMPTY=EMPTY, ignore=ignore)\n            ious.append(iou)\n        iou = np.mean(ious)\n    else:\n        pred = preds.flatten()\n        label = labels.flatten()\n        if ignore is not None:\n            mask = label!= ignore\n            pred = pred[mask]\n            label = label[mask]\n        intersection = (pred * label).sum()\n        union = pred.sum() + label.sum() - intersection\n        if union == 0:\n            return EMPTY\n        iou = float(intersection) / union\n    return iou * 100\n\n\ndef iou_binary_image(pred, label, EMPTY=1., ignore=None):\n    \"\"\"\n    Computes the Intersection over Union (IoU) for the foreground class in binary segmentation for a single image.\n\n    Args:\n        pred (Tensor): Predictions from the model, where 1 indicates foreground and 0 indicates background.\n        label (Tensor): Ground truth binary masks corresponding to the predictions.\n        EMPTY (float, optional): The value to return if there is no union between predicted and ground truth pixels.\n                                 Default is 1.0.\n        ignore (int, optional): Label value to ignore during IoU calculation. This is useful for ignoring certain\n                                pixels in evaluation, such as boundary or undefined areas.\n\n    Returns:\n        float: The IoU score multiplied by 100 to convert it to a percentage.\n\n    This function calculates the IoU score for the foreground class, accounting for any ignore labels.\n    The IoU is calculated by determining the intersection and union of the predicted and actual foreground areas,\n    and handling cases where the union might be zero.\n    \"\"\"\n    if ignore is not None:\n        mask = label!= ignore\n        pred = pred[mask]\n        label = label[mask]\n    intersection = (pred * label).sum()\n    union = pred.sum() + label.sum() - intersection\n    if union == 0:\n        return EMPTY\n    iou = float(intersection) / union\n    return iou * 100\n\n\n",
  "canonical_solution": "\n    # Helper function to calculate mean\n    def mean(l):\n        return sum(l) / len(l) if l else EMPTY\n\n    # Handle batch-wide calculation by wrapping predictions and labels in tuples if per_image is False\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n\n    ious = []\n    for pred, label in zip(preds, labels):\n        # Calculate intersection and union for each image\n        intersection = ((label == 1) & (pred == 1)).sum()\n        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n\n        # Calculate IoU, handling cases with no union\n        if not union:\n            iou = EMPTY\n        else:\n            iou = float(intersection) / float(union)\n\n        ious.append(iou)\n\n    # Calculate the mean IoU across images if per_image, or for the batch\n    iou = mean(ious)\n    return 100 * iou  # Return IoU as a percentage\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(torch.tensor([[1,0,1,0],[0,1,0,1]]), torch.tensor([[1,0,1,0],[0,1,0,1]]), 1.0, None, True) == iou_binary(torch.tensor([[1,0,1,0],[0,1,0,1]]), torch.tensor([[1,0,1,0],[0,1,0,1]]), 1.0, None, True))\nassert(SOLUTION_SIGNATURE(torch.tensor([[0,0,0,0],[1,1,1,1]]), torch.tensor([[0,0,0,0],[0,0,0,0]]), 0.5, 1, True) == iou_binary(torch.tensor([[0,0,0,0],[1,1,1,1]]), torch.tensor([[0,0,0,0],[0,0,0,0]]), 0.5, 1, True))\nassert(SOLUTION_SIGNATURE(torch.tensor([[1,1,0,0],[0,0,1,1]]), torch.tensor([[1,0,0,1],[1,1,0,0]]), 1.0, 0, False) == iou_binary(torch.tensor([[1,1,0,0],[0,0,1,1]]), torch.tensor([[1,0,0,1],[1,1,0,0]]), 1.0, 0, False))\n"
 },
 {
  "task_id": "RWPB/5",
  "prompt": "def seconds_to_hh_mm_ss(seconds):\n    \"\"\"\n    Converts a given number of seconds to a string in hh:mm:ss format.\n\n    Args:\n        seconds: An integer representing the number of seconds.\n\n    Returns:\n        A string in hh:mm:ss format, or \"00:00:00\" if seconds is negative.\n    \"\"\"\n",
  "solution": "\n\n\ndef seconds_to_hh_mm_ss(seconds):\n    \"\"\"\n    Converts a given number of seconds to a string in hh:mm:ss format.\n\n    Args:\n        seconds: An integer representing the number of seconds.\n\n    Returns:\n        A string in hh:mm:ss format, or \"00:00:00\" if seconds is negative.\n    \"\"\"\n    if seconds < 0:\n        return \"00:00:00\"\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n\n\n",
  "canonical_solution": "    if seconds < 0:\n        return \"00:00:00\"\n\n    hours = seconds // 3600  # Get hours by dividing by total seconds in an hour\n    seconds %= 3600  # Get remaining seconds after hours calculation\n\n    minutes = seconds // 60  # Get minutes by dividing by total seconds in a minute\n    seconds %= 60  # Get remaining seconds after minutes calculation\n\n    # Format hour, minute, and second values with leading zeros\n    hours_str = f\"{hours:02d}\"\n    minutes_str = f\"{minutes:02d}\"\n    seconds_str = f\"{seconds:02d}\"\n\n    return f\"{hours_str}:{minutes_str}:{seconds_str}\"\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(-10) == seconds_to_hh_mm_ss(-10))\nassert(SOLUTION_SIGNATURE(3661) == seconds_to_hh_mm_ss(3661))\nassert(SOLUTION_SIGNATURE(7200) == seconds_to_hh_mm_ss(7200))"
 },
 {
  "task_id": "RWPB/38",
  "prompt": "from typing import List\n\ndef extract_imports_from_lines(lines: List[str]) -> str:\n    \"\"\"\n    Extracts import statements from a list of Python code lines.\n\n    Args:\n        lines (List[str]): A list of strings, each representing a line of Python code.\n\n    Returns:\n        str: A single string containing all extracted import and from-import lines, separated by new lines.\n\n    This function filters through a list of Python code lines, identifying and extracting lines that are import statements.\n    It supports both 'import' and 'from ... import ...' formats. This is useful for analyzing or consolidating imports\n    from a set of Python code lines, such as when preparing a script or checking for consistency and redundancy in imports.\n    \"\"\"\n",
  "solution": "\nfrom typing import List\n\ndef extract_imports_from_lines(lines: List[str]) -> str:\n    \"\"\"\n    Extracts import statements from a list of Python code lines.\n\n    Args:\n        lines (List[str]): A list of strings, each representing a line of Python code.\n\n    Returns:\n        str: A single string containing all extracted import and from-import lines, separated by new lines.\n\n    This function filters through a list of Python code lines, identifying and extracting lines that are import statements.\n    It supports both 'import' and 'from... import...' formats. This is useful for analyzing or consolidating imports\n    from a set of Python code lines, such as when preparing a script or checking for consistency and redundancy in imports.\n    \"\"\"\n    extracted_imports = []\n    for line in lines:\n        if line.startswith(\"import\") or line.startswith(\"from \"):\n            extracted_imports.append(line)\n    return \"\\n\".join(extracted_imports)\n",
  "canonical_solution": "\n    # Use list comprehension to filter out lines that start with 'import' or 'from', indicating import statements\n    return \"\\n\".join(\n        [line for line in lines if line.startswith(\"from\") or line.startswith(\"import\")]\n    )\n\n\n\n",
  "unprocess_testcases": "a = [\n    \"import os\",\n    \"x = 10\",                 # Non-import line\n    \"from sys import argv\",\n    \"print('Hello World!')\",  # Non-import line\n    \"import math\"\n]\nassert(SOLUTION_SIGNATURE(a) == extract_imports_from_lines(a))\n\nb = [\n    \"Import os\",                  # Incorrect case\n    \"# from datetime import datetime\",  # Commented import\n    \"from collections import deque\",    # Correct import\n    \"IMPORT sys\",                 # Incorrect case\n    \"from math import pi\"         # Correct import\n]\nassert(SOLUTION_SIGNATURE(b) == extract_imports_from_lines(b))\n\nc = [\n    \"from numpy import array, random\",\n    \"import scipy.stats as stats\",  # Aliased import\n    \"from os.path import join, exists\",  # Multiple imports from same module\n    \"x = 'import should not be detected'\",  # Import within a string\n    \"import pandas as pd\"  # Aliased import\n]\nassert(SOLUTION_SIGNATURE(c) == extract_imports_from_lines(c))\n"
 },
 {
  "task_id": "RWPB/69",
  "prompt": "import numpy as np\nimport math\n\ndef fovx_to_fovy(fovx, aspect):\n    \"\"\"\n    Converts the horizontal field of view (FoVx) to the vertical field of view (FoVy) based on the aspect ratio.\n\n    Parameters:\n    fovx (float): The horizontal field of view in radians.\n    aspect (float): The aspect ratio of the image (width/height).\n\n    Returns:\n    float: The vertical field of view in radians.\n\n    Process:\n    1. Calculate the tangent of half the horizontal field of view (FoVx/2).\n    2. Divide this value by the aspect ratio to get the tangent of half the vertical field of view.\n    3. Use the arctangent function to find half the vertical field of view from its tangent.\n    4. Multiply by 2 to obtain the full vertical field of view (FoVy).\n    \"\"\"\n",
  "solution": "\nimport numpy as np\nimport math\n\ndef fovx_to_fovy(fovx, aspect):\n    \"\"\"\n    Converts the horizontal field of view (FoVx) to the vertical field of view (FoVy) based on the aspect ratio.\n\n    Parameters:\n    fovx (float): The horizontal field of view in radians.\n    aspect (float): The aspect ratio of the image (width/height).\n\n    Returns:\n    float: The vertical field of view in radians.\n\n    Process:\n    1. Calculate the tangent of half the horizontal field of view (FoVx/2).\n    2. Divide this value by the aspect ratio to get the tangent of half the vertical field of view.\n    3. Use the arctangent function to find half the vertical field of view from its tangent.\n    4. Multiply by 2 to obtain the full vertical field of view (FoVy).\n    \"\"\"\n    return 2 * math.atan(math.tan(fovx / 2) / aspect)\n",
  "canonical_solution": "    \n    return np.arctan(np.tan(fovx / 2) / aspect) * 2.0\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(math.pi/4, 16/9) == fovx_to_fovy(math.pi/4, 16/9))\nassert(SOLUTION_SIGNATURE(math.pi/3, 0.5) == fovx_to_fovy(math.pi/3, 0.5))\nassert(SOLUTION_SIGNATURE(math.pi, 1) == fovx_to_fovy(math.pi, 1))\n"
 },
 {
  "task_id": "RWPB/134",
  "prompt": "def select_best_resolution(original_size, possible_resolutions):\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        original_size (tuple): The original size of the image in the format (width, height).\n        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2), ...].\n\n    Returns:\n        tuple: The best fit resolution in the format (width, height).\n    \"\"\"\n",
  "solution": "\n\n\ndef select_best_resolution(original_size, possible_resolutions):\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        original_size (tuple): The original size of the image in the format (width, height).\n        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2),...].\n\n    Returns:\n        tuple: The best fit resolution in the format (width, height).\n    \"\"\"\n    best_resolution = None\n    best_fit_ratio = float('inf')\n    for resolution in possible_resolutions:\n        width_ratio = original_size[0] / resolution[0]\n        height_ratio = original_size[1] / resolution[1]\n        fit_ratio = max(width_ratio, height_ratio)\n        if fit_ratio < best_fit_ratio:\n            best_fit_ratio = fit_ratio\n            best_resolution = resolution\n\n    return best_resolution\n\n\n",
  "canonical_solution": "    \n    original_width, original_height = original_size\n    best_fit = None\n    max_effective_resolution = 0\n    min_wasted_resolution = float('inf')\n\n    for width, height in possible_resolutions:\n        scale = min(width / original_width, height / original_height)\n        downscaled_width, downscaled_height = int(original_width * scale), int(original_height * scale)\n        effective_resolution = min(downscaled_width * downscaled_height, original_width * original_height)\n        wasted_resolution = (width * height) - effective_resolution\n\n        if effective_resolution > max_effective_resolution or (effective_resolution == max_effective_resolution and wasted_resolution < min_wasted_resolution):\n            max_effective_resolution = effective_resolution\n            min_wasted_resolution = wasted_resolution\n            best_fit = (width, height)\n\n    return best_fit\n\n\n",
  "unprocess_testcases": "original_size = (1920, 1080)\npossible_resolutions = [(800, 600), (1920, 1080), (2560, 1440)]\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[0] == select_best_resolution(original_size, possible_resolutions)[0])\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[1] == select_best_resolution(original_size, possible_resolutions)[1])\n\noriginal_size = (1024, 768)\npossible_resolutions = [(800, 600), (1600, 1200), (1280, 960)]\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[0] == select_best_resolution(original_size, possible_resolutions)[0])\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[1] == select_best_resolution(original_size, possible_resolutions)[1])\n\n\noriginal_size = (1280, 720)\npossible_resolutions = [(1024, 768), (1920, 1080), (1600, 900)]\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[0] == select_best_resolution(original_size, possible_resolutions)[0])\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[1] == select_best_resolution(original_size, possible_resolutions)[1])\n\n\n"
 },
 {
  "task_id": "RWPB/100",
  "prompt": "import torch\n\ndef project_onto_planes(planes: torch.Tensor, coordinates: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Projects 3D coordinates onto a batch of 2D planes, returning 2D plane coordinates.\n\n    Args:\n        planes (torch.Tensor): Tensor defining plane axes with shape (n_planes, 3, 3).\n        coordinates (torch.Tensor): Tensor of 3D coordinates with shape (N, M, 3).\n\n    Returns:\n        torch.Tensor: Projected 2D coordinates with shape (N*n_planes, M, 2).\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef project_onto_planes(planes: torch.Tensor, coordinates: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Projects 3D coordinates onto a batch of 2D planes, returning 2D plane coordinates.\n\n    Args:\n        planes (torch.Tensor): Tensor defining plane axes with shape (n_planes, 3, 3).\n        coordinates (torch.Tensor): Tensor of 3D coordinates with shape (N, M, 3).\n\n    Returns:\n        torch.Tensor: Projected 2D coordinates with shape (N*n_planes, M, 2).\n    \"\"\"\n    batch_size, num_points, _ = coordinates.shape\n    num_planes = planes.shape[0]\n    projected_coordinates = torch.empty(batch_size * num_planes, num_points, 2)\n    for i in range(batch_size):\n        for j in range(num_planes):\n            plane_axes = planes[j]\n            plane_normal = plane_axes[:, 2]\n            plane_origin = plane_axes[:, 0]\n            plane_u = plane_axes[:, 1] - plane_origin\n            plane_v = torch.cross(plane_normal, plane_u)\n            plane_u = plane_u / torch.norm(plane_u)\n            plane_v = plane_v / torch.norm(plane_v)\n            plane_coordinates = coordinates[i] - plane_origin\n            u_coordinates = torch.sum(plane_coordinates * plane_u, dim=1)\n            v_coordinates = torch.sum(plane_coordinates * plane_v, dim=1)\n            projected_coordinates[i * num_planes + j] = torch.stack([u_coordinates, v_coordinates], dim=1)\n\n    return projected_coordinates\n\n\n",
  "canonical_solution": "    \n    N, M, C = coordinates.shape\n    n_planes, _, _ = planes.shape\n    coordinates = coordinates.unsqueeze(1).expand(-1, n_planes, -1, -1).reshape(N*n_planes, M, 3)\n    inv_planes = torch.linalg.inv(planes).unsqueeze(0).expand(N, -1, -1, -1).reshape(N*n_planes, 3, 3)\n    projections = torch.bmm(coordinates, inv_planes)\n    return projections[..., :2]\n\n\n",
  "unprocess_testcases": "planes = torch.tensor([[[1, 0, 0], [0, 1, 0], [0, 0, 1]], \n                       [[0, 1, 0], [1, 0, 0], [0, 0, 1]]], dtype=torch.float32)\ncoordinates = torch.tensor([[[1, 2, 3], [4, 5, 6]]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(planes, coordinates) == project_onto_planes(planes, coordinates)).all())\n\nplanes = torch.tensor([[[1, 0, 0], [0, 1, 0], [0, 0, 1]], \n                       [[0, 1, 0], [1, 0, 0], [0, 0, 1]],\n                       [[1, 0, 0], [0, 0, 1], [0, 1, 0]]], dtype=torch.float32)\ncoordinates = torch.tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n                            [[9, 8, 7], [6, 5, 4], [3, 2, 1]]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(planes, coordinates) == project_onto_planes(planes, coordinates)).all())\n\nplanes = torch.tensor([[[1, 0, 0], [0, 1, 0], [0, 0, 1]]], dtype=torch.float32)\ncoordinates = torch.tensor([[[0, -1, -2]]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(planes, coordinates) == project_onto_planes(planes, coordinates)).all())\n"
 },
 {
  "task_id": "RWPB/110",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\n\ndef find_intersection(plane_normal,plane_point,ray_direction,ray_point):\n    \"\"\"\n    Finds the intersection point of a line and a plane.\n\n    Args:\n        - plane_normal: numpy array, representing a normal vector of the plane.\n        - plane_point: numpy array, representing a point on the plane.\n        - ray_direction: numpy array, representing a direction vector of the line.\n        - ray_point: numpy array, representing a point on the line.\n\n    Returns:\n        Psi: numpy array or None, the intersection point of the line and the plane.\n    \"\"\"\n",
  "solution": "\n\n\nimport math\nimport numpy as np\nfrom decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\ndef find_intersection(plane_normal, plane_point, ray_direction, ray_point):\n    \"\"\"\n    Finds the intersection point of a line and a plane.\n\n    Args:\n        plane_normal: numpy array, representing a normal vector of the plane.\n        plane_point: numpy array, representing a point on the plane.\n        ray_direction: numpy array, representing a direction vector of the line.\n        ray_point: numpy array, representing a point on the line.\n\n    Returns:\n        Psi: numpy array or None, the intersection point of the line and the plane.\n    \"\"\"\n    denominator = np.dot(plane_normal, ray_direction)\n    if denominator == 0:\n        return None\n    t = np.dot(plane_normal, plane_point - ray_point) / denominator\n    intersection_point = ray_point + t * ray_direction\n    return intersection_point\n\n",
  "canonical_solution": "    \n    epsilon=1e-6\n    ndotu = plane_normal.dot(ray_direction)\n    if abs(ndotu) < epsilon:\n        # no intersection or line is within plane\n        return None\n    w = ray_point - plane_point\n    si = -plane_normal.dot(w) / ndotu\n    Psi = w + si * ray_direction + plane_point #intersections\n    return Psi\n\n\n",
  "unprocess_testcases": "plane_normal = np.array([0, 0, 1])\nplane_point = np.array([0, 0, 5])\nray_direction = np.array([1, 1, -1])\nray_point = np.array([1, 1, 10])\nassert((SOLUTION_SIGNATURE(plane_point, plane_point, ray_direction, ray_point) == find_intersection(plane_point, plane_point, ray_direction, ray_point)).all())\n\nplane_normal = np.array([0, 0, 1])\nplane_point = np.array([0, 3, 5])\nray_direction = np.array([1, 1, -1])\nray_point = np.array([1, 1, 9])\nassert((SOLUTION_SIGNATURE(plane_point, plane_point, ray_direction, ray_point) == find_intersection(plane_point, plane_point, ray_direction, ray_point)).all())\n\nplane_normal = np.array([0, 1, 0])\nplane_point = np.array([0, 2, 0])\nray_direction = np.array([1, 0, 0])\nray_point = np.array([0, 0, 0])\nassert(SOLUTION_SIGNATURE(plane_point, plane_point, ray_direction, ray_point) == find_intersection(plane_point, plane_point, ray_direction, ray_point))\n\nplane_normal = np.array([0, 0, 1])\nplane_point = np.array([0, 0, 0])\nray_direction = np.array([1, 1, 0])\nray_point = np.array([1, 1, 0])\nassert(SOLUTION_SIGNATURE(plane_point, plane_point, ray_direction, ray_point) == find_intersection(plane_point, plane_point, ray_direction, ray_point))\n"
 },
 {
  "task_id": "RWPB/124",
  "prompt": "import math\nimport torch\n\n\ndef make_divisible(x, divisor):\n    \"\"\"\n    Adjusts x to the nearest value that is divisible by divisor.\n\n    Arguments:\n    x : int or float\n        The value to be made divisible.\n    divisor : int or torch.Tensor\n        The divisor to be used. If a torch.Tensor is provided, uses its maximum value.\n\n    Returns:\n    int\n        The nearest value to x that is divisible by the specified divisor.\n    \"\"\"\n",
  "solution": "\n\n\ndef make_divisible(x, divisor):\n    \"\"\"\n    Adjusts x to the nearest value that is divisible by divisor.\n\n    Arguments:\n    x : int or float\n        The value to be made divisible.\n    divisor : int or torch.Tensor\n        The divisor to be used. If a torch.Tensor is provided, uses its maximum value.\n\n    Returns:\n    int\n        The nearest value to x that is divisible by the specified divisor.\n    \"\"\"\n    if isinstance(divisor, torch.Tensor):\n        divisor = divisor.max().item()\n    return math.ceil(x / divisor) * divisor\n\n\n",
  "canonical_solution": "    \n    # Convert divisor to int if it is a torch.Tensor, taking the maximum value in the tensor\n    if isinstance(divisor, torch.Tensor):\n        divisor = int(divisor.max())  # Convert to int\n\n    # Compute the nearest value to x that is divisible by divisor\n    return math.ceil(x / divisor) * divisor  # Use ceil to ensure it rounds up to the nearest divisor\n\n",
  "unprocess_testcases": "x = 23\ndivisor = 5\nassert(SOLUTION_SIGNATURE(x, divisor) == make_divisible(x, divisor))\n\nx = 14.7\ndivisor = 3\nassert(SOLUTION_SIGNATURE(x, divisor) == make_divisible(x, divisor))\n\nx = 100\ndivisor = torch.Tensor([2,8,10])\nassert(SOLUTION_SIGNATURE(x, divisor) == make_divisible(x, divisor))\n"
 },
 {
  "task_id": "RWPB/79",
  "prompt": "import numpy as np\n\ndef pr_from_grav(grav):\n  \"\"\"\n  Compute pitch and roll angles from a matrix of gravity vectors in body frame.\n  Args:\n      - grav: numpy.ndarray, matrix of gravity vectors in body frame (shape: (n, 3))\n  Returns:\n      - pitches: numpy.ndarray, pitch angles (shape: (n,))\n      - rolls: numpy.ndarray, roll angles (shape: (n,))\n  \"\"\"\n",
  "solution": "\nimport numpy as np\n\ndef pr_from_grav(grav):\n  \"\"\"\n  Compute pitch and roll angles from a matrix of gravity vectors in body frame.\n  Args:\n      - grav: numpy.ndarray, matrix of gravity vectors in body frame (shape: (n, 3))\n  Returns:\n      - pitches: numpy.ndarray, pitch angles (shape: (n,))\n      - rolls: numpy.ndarray, roll angles (shape: (n,))\n  \"\"\"\n  pitches = np.arcsin(-grav[:, 0])\n  rolls = np.arcsin(grav[:, 1] / np.cos(pitches))\n\n  return pitches, rolls\n",
  "canonical_solution": "  \n  pitches = np.arcsin(-grav[:, 0])\n  rolls = np.arctan2(grav[:, 1], grav[:, 2])\n  return pitches, rolls\n\n\n",
  "unprocess_testcases": "a = np.array([[0, 0, -1],  # No tilt (pitch=roll=0)\n          [0, 0.7071, 0.7071],  # Tilted 45 degrees along Y-axis\n          [0.5, 0, 0.866],  # Tilted 30 degrees along X-axis\n          [0, -1, 0]])  # Upside down along Y-axis\n\nb = np.array([[-1, 0, 0],    # Pitch -90 degrees (pointing directly downward)\n          [1, 0, 0],     # Pitch 90 degrees (pointing directly upward)\n          [0, 1, 0],     # Roll 90 degrees\n          [0, 0, 1],     # No tilt, gravity perfectly aligned\n          [0, -1, 1e-6]]) # Near vertical Y-axis, small Z to avoid zero\n\nc = np.array([[0, 0, 0],  # Zero vector\n          [0, 0, 0],  # Repeated zero vector to test consistent handling\n          [0, 0, 0]])\n\nd = np.array([[1e-12, 0, -1],   # Extremely small x component\n          [0, 1e-12, -1],   # Extremely small y component\n          [1e-12, 1e-12, -1]])  # Small x and y components\n\ne = np.array([[1, 0, 0],   # Along X-axis\n          [0, 1, 0],   # Along Y-axis\n          [0, 0, 1],   # Along Z-axis\n          [0, 0, -1],  # Along -Z-axis\n          [-1, 0, 0],  # Along -X-axis\n          [0, -1, 0]]) # Along -Y-axis\n\nassert((SOLUTION_SIGNATURE(a)[0] == pr_from_grav(a)[0]).all())\nassert((SOLUTION_SIGNATURE(b)[0] == pr_from_grav(b)[0]).all())\nassert((SOLUTION_SIGNATURE(c)[0] == pr_from_grav(c)[0]).all())\nassert((SOLUTION_SIGNATURE(d)[0] == pr_from_grav(d)[0]).all())\nassert((SOLUTION_SIGNATURE(e)[0] == pr_from_grav(e)[0]).all())\nassert((SOLUTION_SIGNATURE(a)[1] == pr_from_grav(a)[1]).all())\nassert((SOLUTION_SIGNATURE(b)[1] == pr_from_grav(b)[1]).all())\nassert((SOLUTION_SIGNATURE(c)[1] == pr_from_grav(c)[1]).all())\nassert((SOLUTION_SIGNATURE(d)[1] == pr_from_grav(d)[1]).all())\nassert((SOLUTION_SIGNATURE(e)[1] == pr_from_grav(e)[1]).all())"
 },
 {
  "task_id": "RWPB/28",
  "prompt": "import numpy as np\nimport math\n\n\ndef mean(l, ignore_nan=False, empty=0):\n    \"\"\"\n    Calculates the mean of a collection of numbers, handling NaN values and empty collections.\n\n    Parameters\n    ----------\n    l : iterable\n        An iterable of numbers (e.g., list, tuple, generator).\n    ignore_nan : bool, optional\n        If True, NaN values are ignored in the calculation. Default is False.\n    empty : int, str, optional\n        Defines the behavior when the input iterable is empty:\n            - If 0, the function returns 0.\n            - If 'raise', the function return a ValueError.\n            - For other values, the function returns the value of `empty`.\n\n    Returns\n    -------\n    float\n        The mean of the input numbers, excluding NaN values if `ignore_nan` is True.\n\n    or\n    ------\n    ValueError\n        If the input iterable is empty and `empty` is set to 'raise'.\n\n    Notes\n    -----\n    This function is similar to the built-in `mean` function in numpy, but it is designed to work\n    with generators and other iterables that do not support indexing.\n    \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\nimport math\n\n\ndef mean(l, ignore_nan=False, empty=0):\n    \"\"\"\n    Calculates the mean of a collection of numbers, handling NaN values and empty collections.\n\n    Parameters\n    ----------\n    l : iterable\n        An iterable of numbers (e.g., list, tuple, generator).\n    ignore_nan : bool, optional\n        If True, NaN values are ignored in the calculation. Default is False.\n    empty : int, str, optional\n        Defines the behavior when the input iterable is empty:\n            - If 0, the function returns 0.\n            - If 'raise', the function return a ValueError.\n            - For other values, the function returns the value of `empty`.\n\n    Returns\n    -------\n    float\n        The mean of the input numbers, excluding NaN values if `ignore_nan` is True.\n\n    or\n    ------\n    ValueError\n        If the input iterable is empty and `empty` is set to 'raise'.\n\n    Notes\n    -----\n    This function is similar to the built-in `mean` function in numpy, but it is designed to work\n    with generators and other iterables that do not support indexing.\n    \"\"\"\n    if not l:\n        if empty == \"raise\":\n            raise ValueError(\"Empty input iterable\")\n        return empty\n    if ignore_nan:\n        l = [x for x in l if not math.isnan(x)]\n    return sum(l) / len(l)\n\n\n",
  "canonical_solution": "\n    # Create an iterator from the input iterable\n    l = iter(l)\n\n    # Initialize the accumulator and the count of elements\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        # If the iterable is empty, handle it based on the value of `empty`\n        if empty == 'raise':\n            return ValueError\n        return empty\n\n    # Sum the elements of the iterable and count them\n    for n, v in enumerate(l, 2):\n        acc += v\n\n    # Calculate the mean, taking care of the case when there's only one element\n    return acc / n if n > 1 else acc\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE([231, 2.5, 44, 5.0, 10.0], True, 0) == mean([231, 2.5, 44, 5.0, 10.0], True, 0))\nassert(SOLUTION_SIGNATURE([], False, 'raise') == mean([], False, 'raise'))\nassert(SOLUTION_SIGNATURE([1, 2.5, 3, 4.5], False, 0) == mean([1, 2.5, 3, 4.5], False, 0))\n"
 },
 {
  "task_id": "RWPB/1",
  "prompt": "from typing import Optional\n\nimport torch\nfrom matplotlib import cm\nfrom torchtyping import TensorType\n\ndef apply_colormap(\n    image: TensorType[\"bs\":..., 1],\n    cmap=\"viridis\",\n) -> TensorType[\"bs\":..., \"rgb\":3]:\n    \"\"\"Convert single channel to a color image. \n    Args:\n        image: Single channel image.\n        cmap: Colormap for image.\n    Returns:\n        TensorType: Colored image\n    \"\"\"\n",
  "solution": "\n\n\nfrom typing import Optional\n\nimport torch\nfrom matplotlib import cm\nfrom torchtyping import TensorType\n\ndef apply_colormap(\n    image: TensorType[\"bs\":..., 1],\n    cmap=\"viridis\",\n) -> TensorType[\"bs\":..., \"rgb\":3]:\n    \"\"\"Convert single channel to a color image. \n    Args:\n        image: Single channel image.\n        cmap: Colormap for image.\n    Returns:\n        TensorType: Colored image\n    \"\"\"\n    norm = cm.colors.Normalize(vmin=image.min(), vmax=image.max())\n    colormap = cm.get_cmap(cmap)\n    colored_image = colormap(norm(image.squeeze(-1)))\n    colored_image = torch.tensor(colored_image)\n    return colored_image\n\n\n",
  "canonical_solution": "\n    colormap = cm.get_cmap(cmap)\n    colormap = torch.tensor(colormap.colors).to(image.device)  # type: ignore\n    image_long = (image * 255).long()\n    image_long_min = torch.min(image_long)\n    image_long_max = torch.max(image_long)\n    assert image_long_min >= 0, f\"the min value is {image_long_min}\"\n    assert image_long_max <= 255, f\"the max value is {image_long_max}\"\n    return colormap[image_long[..., 0]]\n\n\n",
  "unprocess_testcases": "a = torch.rand(2, 10, 10, 1)\nb = torch.rand(1, 100, 100, 1)\nc = torch.tensor([[[[0.0]], [[1.0]]]])\n\nassert((SOLUTION_SIGNATURE(a) == apply_colormap(a)).all())\nassert((SOLUTION_SIGNATURE(b, 'plasma') == apply_colormap(b, 'plasma')).all())\nassert((SOLUTION_SIGNATURE(c) == apply_colormap(c)).all())"
 },
 {
  "task_id": "RWPB/59",
  "prompt": "import torch\n\ndef modify_logit_for_repetition_penalty(logits, prev_output_tokens, repetition_penalty=1.0):\n    \"\"\"\n    Adjusts logits to penalize or favor the repetition of tokens that have already appeared,\n    based on the provided repetition penalty.\n\n    Args:\n        logits (torch.Tensor): The logits from a language model, of shape (batch_size, vocab_size).\n        prev_output_tokens (torch.Tensor): The indices of previously generated tokens for each sequence\n                                           in the batch, of shape (batch_size, seq_len).\n        repetition_penalty (float): The penalty factor for repetition. Values >1 penalize, values <1 favor\n                                    repetition. The default value of 1.0 means no penalty.\n\n    Returns:\n        torch.Tensor: The adjusted logits.\n\n    This function applies a repetition penalty as described in the paper:\n    \"CTRL: A Conditional Transformer Language Model for Controllable Generation\".\n    It decreases the probability of tokens that have appeared previously if the penalty is >1,\n    and increases if <1 but not equal to zero.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef modify_logit_for_repetition_penalty(logits, prev_output_tokens, repetition_penalty=1.0):\n    \"\"\"\n    Adjusts logits to penalize or favor the repetition of tokens that have already appeared,\n    based on the provided repetition penalty.\n\n    Args:\n        logits (torch.Tensor): The logits from a language model, of shape (batch_size, vocab_size).\n        prev_output_tokens (torch.Tensor): The indices of previously generated tokens for each sequence\n                                           in the batch, of shape (batch_size, seq_len).\n        repetition_penalty (float): The penalty factor for repetition. Values >1 penalize, values <1 favor\n                                    repetition. The default value of 1.0 means no penalty.\n\n    Returns:\n        torch.Tensor: The adjusted logits.\n\n    This function applies a repetition penalty as described in the paper:\n    \"CTRL: A Conditional Transformer Language Model for Controllable Generation\".\n    It decreases the probability of tokens that have appeared previously if the penalty is >1,\n    and increases if <1 but not equal to zero.\n    \"\"\"\n    batch_size, seq_len = prev_output_tokens.shape\n    prev_output_tokens = prev_output_tokens.reshape(batch_size * seq_len)\n    logits = logits.reshape(batch_size * seq_len, -1)\n    logits[torch.arange(batch_size * seq_len), prev_output_tokens] *= repetition_penalty\n    logits = logits.reshape(batch_size, seq_len, -1)\n\n    return logits\n\n\n",
  "canonical_solution": "\n    if repetition_penalty == 1.0:\n        return logits  # No modification needed if penalty is 1\n\n    # Gather scores of previously generated tokens from logits\n    score = torch.gather(logits, 1, prev_output_tokens)\n\n    # Apply repetition penalty:\n    # If score < 0 (unlikely tokens), multiplying by a penalty > 1 will make them even less likely\n    # If score > 0 (likely tokens), dividing by a penalty > 1 will make them less likely\n    adjusted_score = torch.where(score < 0, score * repetition_penalty, score / repetition_penalty)\n\n    # Scatter the adjusted scores back into the logits tensor at the positions of the previous tokens\n    logits.scatter_(1, prev_output_tokens, adjusted_score)\n\n    return logits\n\n\n",
  "unprocess_testcases": "a = torch.rand(1, 5)\n\nassert((SOLUTION_SIGNATURE(a, torch.tensor([[0, 1]])) == modify_logit_for_repetition_penalty(a, torch.tensor([[0, 1]]))).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[10.0, -5.0, 0.0, 3.0, -1.0]]), torch.tensor([[0, 3]]), 2.0) == modify_logit_for_repetition_penalty(torch.tensor([[10.0, -5.0, 0.0, 3.0, -1.0]]), torch.tensor([[0, 3]]), 2.0)).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[2.0, -2.0, 4.0, -1.0, 0.0]]), torch.tensor([[0, 1, 3]]), 0.5) == modify_logit_for_repetition_penalty(torch.tensor([[2.0, -2.0, 4.0, -1.0, 0.0]]), torch.tensor([[0, 1, 3]]), 0.5)).all())\n"
 },
 {
  "task_id": "RWPB/104",
  "prompt": "MAX_VAL = (1 << 256) - 1\n\n\n# does not overflow\ndef mulMod(x, y, z):\n    \"\"\"\n    Compute (x * y) % z without overflow.\n    Args:\n        - x: int, multiplicand\n        - y: int, multiplier\n        - z: int, modulus\n    Returns:\n        result: int, the result of (x * y) % z\n    \"\"\"\n",
  "solution": "\n\n\ndef mulMod(x, y, z):\n    \"\"\"\n    Compute (x * y) % z without overflow.\n    Args:\n        - x: int, multiplicand\n        - y: int, multiplier\n        - z: int, modulus\n    Returns:\n        result: int, the result of (x * y) % z\n    \"\"\"\n    result = 0\n    while x > 0:\n        if x & 1:\n            result = (result + y) % z\n        x >>= 1\n        y = (y << 1) % z\n    return result\n\n\n",
  "canonical_solution": "    \n    return (x * y) % z\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(10, 20, 7) == mulMod(10, 20, 7))\nassert(SOLUTION_SIGNATURE(3214, 34, 423) == mulMod(3214, 34, 423))\n\nlarge_x = (1 << 255)\nlarge_y = (1 << 255)\nlarge_z = (1 << 256) - 1\nassert(SOLUTION_SIGNATURE(large_x, large_y, large_z) == mulMod(large_x, large_y, large_z))\n\nassert(SOLUTION_SIGNATURE(0, 10, 3) == mulMod(0, 10, 3))\nassert(SOLUTION_SIGNATURE(10, 0, 3) == mulMod(10, 0, 3))\nassert(SOLUTION_SIGNATURE(10, 20, 1) == mulMod(10, 20, 1))\n"
 },
 {
  "task_id": "RWPB/130",
  "prompt": "import numpy as np\n\ndef bbox_ioa(box1, box2, iou=False, eps=1e-7):\n    \"\"\"\n    Calculate the intersection over box2 area given box1 and box2. Boxes are in x1y1x2y2 format.\n\n    Args:\n        box1 (np.ndarray): A numpy array of shape (n, 4) representing n bounding boxes.\n        box2 (np.ndarray): A numpy array of shape (m, 4) representing m bounding boxes.\n        iou (bool): Calculate the standard IoU if True else return inter_area/box2_area.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n\n    Returns:\n        (np.ndarray): A numpy array of shape (n, m) representing the intersection over box2 area.\n    \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\n\ndef bbox_ioa(box1, box2, iou=False, eps=1e-7):\n    \"\"\"\n    Calculate the intersection over box2 area given box1 and box2. Boxes are in x1y1x2y2 format.\n\n    Args:\n        box1 (np.ndarray): A numpy array of shape (n, 4) representing n bounding boxes.\n        box2 (np.ndarray): A numpy array of shape (m, 4) representing m bounding boxes.\n        iou (bool): Calculate the standard IoU if True else return inter_area/box2_area.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n\n    Returns:\n        (np.ndarray): A numpy array of shape (n, m) representing the intersection over box2 area.\n    \"\"\"\n    assert box1.shape[1] == 4, \"box1 is not in x1y1x2y2 format\"\n    assert box2.shape[1] == 4, \"box2 is not in x1y1x2y2 format\"\n\n    box1 = box1.reshape(-1, 1, 4)\n    box2 = box2.reshape(1, -1, 4)\n\n    inter_x1 = np.maximum(box1[:, :, 0], box2[:, :, 0])\n    inter_y1 = np.maximum(box1[:, :, 1], box2[:, :, 1])\n    inter_x2 = np.minimum(box1[:, :, 2], box2[:, :, 2])\n    inter_y2 = np.minimum(box1[:, :, 3], box2[:, :, 3])\n\n    inter_area = np.maximum(inter_x2 - inter_x1, 0) * np.maximum(inter_y2 - inter_y1, 0)\n    box2_area = (box2[:, :, 2] - box2[:, :, 0]) * (box2[:, :, 3] - box2[:, :, 1])\n\n    if iou:\n        return inter_area / (box2_area + eps)\n    else:\n        return inter_area / (box2_area - inter_area + eps)\n\n",
  "canonical_solution": "\n    # Get the coordinates of bounding boxes\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1.T\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2.T\n\n    # Intersection area\n    inter_area = (np.minimum(b1_x2[:, None], b2_x2) - np.maximum(b1_x1[:, None], b2_x1)).clip(0) * (\n        np.minimum(b1_y2[:, None], b2_y2) - np.maximum(b1_y1[:, None], b2_y1)\n    ).clip(0)\n\n    # Box2 area\n    area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n    if iou:\n        box1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n        area = area + box1_area[:, None] - inter_area\n\n    # Intersection over box2 area\n    return inter_area / (area + eps)\n\n\n",
  "unprocess_testcases": "box1 = np.array([[0, 0, 2, 2]])\nbox2 = np.array([[1, 1, 3, 3]])\niou = False\nassert((SOLUTION_SIGNATURE(box1, box2, iou) == bbox_ioa(box1, box2, iou)).all())\n\nbox1 = np.array([[0, 0, 1, 1]])\nbox2 = np.array([[2, 2, 3, 3]])\niou = False\nassert((SOLUTION_SIGNATURE(box1, box2, iou) == bbox_ioa(box1, box2, iou)).all())\n\nbox1 = np.array([[0, 0, 3, 3], [4, 4, 5, 5]])\nbox2 = np.array([[1, 1, 4, 4], [0, 0, 2, 2]])\niou = True\nassert((SOLUTION_SIGNATURE(box1, box2, iou) == bbox_ioa(box1, box2, iou)).all())\n\nbox1 = np.array([[0, 0, 3, 3], [4, 4, 5, 5]])\nbox2 = np.array([[1, 1, 4, 4], [0, 0, 2, 2]])\niou = True\nassert((SOLUTION_SIGNATURE(box1, box2, iou, 2e-5) == bbox_ioa(box1, box2, iou, 2e-5)).all())\n\nbox1 = np.array([[1, 3, 3, 6], [7, 7, 2, 6]])\nbox2 = np.array([[9, 3, 5, 4], [0, 1, 3, 8]])\niou = True\nassert((SOLUTION_SIGNATURE(box1, box2, iou) == bbox_ioa(box1, box2, iou)).all())\n"
 },
 {
  "task_id": "RWPB/120",
  "prompt": "def parity(cycles):\n    \"\"\"\n    Calculate the parity (even or odd) of a permutation given its disjoint cycles.\n\n    Args:\n        cycles (list): A list of disjoint cycles, where each cycle is represented as a list.\n\n    Returns:\n        int: 0 if the permutation is even, 1 if it is odd.\n    \"\"\"\n",
  "solution": "\n\n\ndef parity(cycles):\n    \"\"\"\n    Calculate the parity (even or odd) of a permutation given its disjoint cycles.\n\n    Args:\n        cycles (list): A list of disjoint cycles, where each cycle is represented as a list.\n\n    Returns:\n        int: 0 if the permutation is even, 1 if it is odd.\n    \"\"\"\n    sign = 1\n    for cycle in cycles:\n        if len(cycle) % 2 == 0:\n            sign *= -1\n    return (sign + 1) // 2\n\n\n",
  "canonical_solution": "    \n    res = 0\n    for i in cycles:\n        if len(i) > 1:\n            res += len(i) - 1\n    return res % 2\n\n",
  "unprocess_testcases": "cycles = [[1,2,3,4]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1, 2], [3, 4, 5], [6, 7, 8, 9]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1], [2, 3], [4, 5, 6], [7]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = []\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1], [2], [3], [4]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1, 2, 3], [4, 5, 6], [7, 8, 9, 10, 11]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n"
 },
 {
  "task_id": "RWPB/114",
  "prompt": "import torch\n\ndef reshape_weight_task_tensors(task_tensors, weights):\n    \"\"\"\n    Reshapes `weights` to match the shape of `task_tensors` by unsqeezing in the remaining dimensions.\n\n    Args:\n        task_tensors (torch.Tensor): The tensors that will be used to reshape `weights`.\n        weights (torch.Tensor): The tensor to be reshaped.\n\n    Returns:\n        torch.Tensor: The reshaped tensor.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef reshape_weight_task_tensors(task_tensors, weights):\n    \"\"\"\n    Reshapes `weights` to match the shape of `task_tensors` by unsqeezing in the remaining dimensions.\n\n    Args:\n        task_tensors (torch.Tensor): The tensors that will be used to reshape `weights`.\n        weights (torch.Tensor): The tensor to be reshaped.\n\n    Returns:\n        torch.Tensor: The reshaped tensor.\n    \"\"\"\n    return weights.unsqueeze(1).unsqueeze(1)\n\n",
  "canonical_solution": "    \n    new_shape = weights.shape + (1,) * (task_tensors.dim() - weights.dim())\n    weights = weights.view(new_shape)\n    return weights\n\n\n",
  "unprocess_testcases": "task_tensors = torch.rand((2, 3, 4))\nweights = torch.rand((2))\nassert((SOLUTION_SIGNATURE(task_tensors, weights) == reshape_weight_task_tensors(task_tensors, weights)).all())\n\ntask_tensors = torch.rand((5, 6, 7, 8))\nweights = torch.rand((5, 6))\nassert((SOLUTION_SIGNATURE(task_tensors, weights) == reshape_weight_task_tensors(task_tensors, weights)).all())\n\ntask_tensors = torch.rand((10, 20))\nweights = torch.rand((10))\nassert((SOLUTION_SIGNATURE(task_tensors, weights) == reshape_weight_task_tensors(task_tensors, weights)).all())\n"
 },
 {
  "task_id": "RWPB/49",
  "prompt": "import torch\n\ndef make_even_first_dim(tensor):\n    \"\"\"\n    Ensures that the first dimension of the tensor is even. If it's odd, reduces it by one.\n\n    Args:\n    tensor (torch.Tensor): Input tensor.\n\n    Returns:\n    torch.Tensor: Tensor with an even first dimension.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef make_even_first_dim(tensor):\n    \"\"\"\n    Ensures that the first dimension of the tensor is even. If it's odd, reduces it by one.\n\n    Args:\n    tensor (torch.Tensor): Input tensor.\n\n    Returns:\n    torch.Tensor: Tensor with an even first dimension.\n    \"\"\"\n    if tensor.shape[0] % 2 == 1:\n        tensor = tensor[:-1]\n    return tensor\n\n\n",
  "canonical_solution": "\n    size = list(tensor.size())\n    if size[0] % 2 == 1:\n        size[0] -= 1\n        return tensor[:size[0]]\n    return tensor\n\n",
  "unprocess_testcases": "a = torch.randn(4, 3, 2)\nb = torch.randn(5, 3, 2)\nc = torch.randn(1, 4, 4)\n\nassert((SOLUTION_SIGNATURE(a) == make_even_first_dim(a)).all())\nassert((SOLUTION_SIGNATURE(b) == make_even_first_dim(b)).all())\nassert((SOLUTION_SIGNATURE(c) == make_even_first_dim(c)).all())\n"
 },
 {
  "task_id": "RWPB/18",
  "prompt": "import numpy as np\n\ndef last_before(val, arr):\n  \"\"\"\n  Find the index of the last element in a sorted array that is less than or equal to a given value.Requires sorted (monotonic) arr.\n  Args:\n      - val: int, the value to search for\n      - arr: numpy.ndarray, the sorted array\n  Returns:\n      idx: int, the index of the last element in the array that is less than or equal to val\n  \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\n\ndef last_before(val, arr):\n  \"\"\"\n  Find the index of the last element in a sorted array that is less than or equal to a given value.\n  Args:\n      - val: int, the value to search for\n      - arr: numpy.ndarray, the sorted array\n  Returns:\n      idx: int, the index of the last element in the array that is less than or equal to val\n  \"\"\"\n  idx = np.searchsorted(arr, val, side='right') - 1\n  if idx < 0 or arr[idx] > val:\n      return -1\n  return idx\n\n",
  "canonical_solution": "\n  assert np.all(np.diff(arr) >= 0)\n  return np.searchsorted(arr, val) - 1\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(val = 5, arr = np.array([1, 2, 3, 4, 5])) == last_before(val = 5, arr = np.array([1, 2, 3, 4, 5])))\nassert(SOLUTION_SIGNATURE(val = 3.5, arr = np.array([1, 2, 3, 4, 5])) == last_before(val = 3.5, arr = np.array([1, 2, 3, 4, 5])))\nassert(SOLUTION_SIGNATURE(val = 0, arr = np.array([1, 2, 3, 4, 5])) == last_before(val = 0, arr = np.array([1, 2, 3, 4, 5])))\n"
 },
 {
  "task_id": "RWPB/66",
  "prompt": "import numpy as np\n\ndef focal_length_to_fovy(focal_length, sensor_height):\n    \"\"\"\n    Calculates the vertical field of view (FoVy) from the focal length of the lens and the sensor height.\n\n    Parameters:\n    focal_length (float): The focal length of the lens.\n    sensor_height (float): The height of the camera sensor.\n\n    Returns:\n    float: The vertical field of view in radians.\n\n    Process:\n    1. Compute half the sensor height.\n    2. Divide half the sensor height by the focal length to get the tangent of half the vertical field of view.\n    3. Use the arctangent function to determine half the vertical field of view.\n    4. Multiply by 2 to get the full vertical field of view (FoVy).\n    \"\"\"\n",
  "solution": "\nimport numpy as np\n\ndef focal_length_to_fovy(focal_length, sensor_height):\n    \"\"\"\n    Calculates the vertical field of view (FoVy) from the focal length of the lens and the sensor height.\n\n    Parameters:\n    focal_length (float): The focal length of the lens.\n    sensor_height (float): The height of the camera sensor.\n\n    Returns:\n    float: The vertical field of view in radians.\n\n    Process:\n    1. Compute half the sensor height.\n    2. Divide half the sensor height by the focal length to get the tangent of half the vertical field of view.\n    3. Use the arctangent function to determine half the vertical field of view.\n    4. Multiply by 2 to get the full vertical field of view (FoVy).\n    \"\"\"\n    half_sensor_height = sensor_height / 2\n    half_fovy = np.arctan(half_sensor_height / focal_length)\n    fovy = 2 * half_fovy\n\n    return fovy\n",
  "canonical_solution": "    \n    return 2 * np.arctan(0.5 * sensor_height / focal_length)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(focal_length=50, sensor_height=24) == focal_length_to_fovy(focal_length=50, sensor_height=24))\nassert(SOLUTION_SIGNATURE(focal_length=1, sensor_height=24) == focal_length_to_fovy(focal_length=1, sensor_height=24))\nassert(SOLUTION_SIGNATURE(focal_length=1000, sensor_height=24) == focal_length_to_fovy(focal_length=1000, sensor_height=24))\n"
 },
 {
  "task_id": "RWPB/37",
  "prompt": "import math\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\n\ndef euler_and_translation_to_matrix(euler_angles, translation_vector):\n    \"\"\"\n    Converts Euler angles and a translation vector into a 4x4 transformation matrix.\n\n    Args:\n        euler_angles (list or np.array): Euler angles in degrees for rotation around the x, y, and z axes.\n        translation_vector (list or np.array): Translation vector for x, y, and z coordinates.\n\n    Returns:\n        np.array: A 4x4 transformation matrix combining both rotation and translation.\n\n    This function constructs a homogeneous transformation matrix that combines both rotation and translation.\n    The rotation is specified by Euler angles (roll, pitch, yaw), which are converted into a rotation matrix.\n    The translation is represented as a vector. This transformation matrix is useful in applications such as\n    robotics, 3D graphics, and computer vision, where transformations in 3D space are common.\n    \"\"\"\n",
  "solution": "\n\n\ndef euler_and_translation_to_matrix(euler_angles, translation_vector):\n    \"\"\"\n    Converts Euler angles and a translation vector into a 4x4 transformation matrix.\n\n    Args:\n        euler_angles (list or np.array): Euler angles in degrees for rotation around the x, y, and z axes.\n        translation_vector (list or np.array): Translation vector for x, y, and z coordinates.\n\n    Returns:\n        np.array: A 4x4 transformation matrix combining both rotation and translation.\n\n    This function constructs a homogeneous transformation matrix that combines both rotation and translation.\n    The rotation is specified by Euler angles (roll, pitch, yaw), which are converted into a rotation matrix.\n    The translation is represented as a vector. This transformation matrix is useful in applications such as\n    robotics, 3D graphics, and computer vision, where transformations in 3D space are common.\n    \"\"\"\n    roll, pitch, yaw = euler_angles\n    roll = np.deg2rad(roll)\n    pitch = np.deg2rad(pitch)\n    yaw = np.deg2rad(yaw)\n    rotation_matrix = R.from_euler('xyz', [roll, pitch, yaw]).as_matrix()\n    translation_matrix = np.eye(4)\n    translation_matrix[:3, 3] = translation_vector\n    transformation_matrix = np.dot(translation_matrix, rotation_matrix)\n\n    return transformation_matrix\n\n\n",
  "canonical_solution": "\n    # Create a rotation object from the Euler angles\n    rotation = R.from_euler('xyz', euler_angles, degrees=True)\n    # Convert the rotation object to a 3x3 rotation matrix\n    rotation_matrix = rotation.as_matrix()\n\n    # Initialize a 4x4 identity matrix\n    matrix = np.eye(4)\n    # Insert the rotation matrix into the top-left 3x3 part of the identity matrix\n    matrix[:3, :3] = rotation_matrix\n    # Insert the translation vector into the first three elements of the fourth column\n    matrix[:3, 3] = translation_vector\n\n    return matrix\n\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([45, 45, 45]), np.array([10, 0, 5])) == euler_and_translation_to_matrix(np.array([45, 45, 45]), np.array([10, 0, 5]))).all())\nassert((SOLUTION_SIGNATURE([0, 0, 0], [5, 5, 5]) == euler_and_translation_to_matrix([0, 0, 0], [5, 5, 5])).all())\nassert((SOLUTION_SIGNATURE([-90, 360, 270], [-1, -1, -1]) == euler_and_translation_to_matrix([-90, 360, 270], [-1, -1, -1])).all())\n"
 },
 {
  "task_id": "RWPB/95",
  "prompt": "import torch\n\ndef lines_focal(o, d):\n    \"\"\"\n    Estimates the focal point of a set of lines in 3D space given their origins and directions.\n\n    Parameters:\n    o (torch.Tensor): A tensor of shape (N, 3) representing the origins of N lines in 3D space.\n    d (torch.Tensor): A tensor of shape (N, 3) representing the directions of N lines in 3D space.\n\n    Returns:\n    torch.Tensor: A tensor of shape (3,) representing the estimated focal point in 3D space.\n\n    Process:\n    1. Normalize the direction vectors `d` to unit length along the last dimension.\n    2. Create a 3x3 identity matrix `I` with the same dtype and device as the input tensors.\n    3. Compute the sum `S` of the outer products of each direction vector with itself, subtracting the identity matrix `I` from each outer product. This results in a tensor of shape (3, 3).\n    4. Compute the sum `C` of the matrix products of each adjusted direction outer product with the corresponding origin vector. This results in a tensor of shape (3,).\n    5. Compute the pseudo-inverse of `S` using `torch.linalg.pinv`.\n    6. Multiply the pseudo-inverse of `S` with `C` to obtain the estimated focal point.\n\n    Returns the estimated focal point as a tensor of shape (3,).\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef lines_focal(o, d):\n    \"\"\"\n    Estimates the focal point of a set of lines in 3D space given their origins and directions.\n\n    Parameters:\n    o (torch.Tensor): A tensor of shape (N, 3) representing the origins of N lines in 3D space.\n    d (torch.Tensor): A tensor of shape (N, 3) representing the directions of N lines in 3D space.\n\n    Returns:\n    torch.Tensor: A tensor of shape (3,) representing the estimated focal point in 3D space.\n    \"\"\"\n    d_norm = torch.nn.functional.normalize(d, dim=-1)\n    I = torch.eye(3, dtype=o.dtype, device=o.device)\n    S = torch.sum(torch.einsum(\"bi,bj->bij\", d_norm, d_norm) - I, dim=0)\n    C = torch.sum(torch.einsum(\"bi,bj->bij\", d_norm, o), dim=0)\n    S_pinv = torch.linalg.pinv(S)\n    focal_point = torch.matmul(S_pinv, C)\n\n    return focal_point\n\n\n",
  "canonical_solution": "    \n    d = d / torch.norm(d, dim=-1, keepdim=True)\n    I = torch.eye(3, dtype=o.dtype, device=o.device)\n    S = torch.sum(d[..., None] @ torch.transpose(d[..., None], 1, 2) - I[None, ...], dim=0)\n    C = torch.sum((d[..., None] @ torch.transpose(d[..., None], 1, 2) - I[None, ...]) @ o[..., None], dim=0).squeeze(1)\n    return torch.linalg.pinv(S) @ C\n\n\n",
  "unprocess_testcases": "o = torch.tensor([[0, 0, 0], [1, 1, 1]], dtype=torch.float32)\nd = torch.tensor([[1, 0, 0], [0, 1, 0]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(o, d) == lines_focal(o, d)).all())\n\no = torch.tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=torch.float32)\nd = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(o, d) == lines_focal(o, d)).all())\n\no = torch.tensor([[0, 0, 0], [1, 1, 1], [2, 2, 2]], dtype=torch.float32)\nd = torch.tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(o, d) == lines_focal(o, d)).all())\n"
 },
 {
  "task_id": "RWPB/52",
  "prompt": "import torch\n\ndef reshape_tensor(x, heads):\n    \"\"\"\n    Reshape a tensor to fit a multi-head attention mechanism structure.\n    If the shape of x is invalid, please return ValueError.\n\n    Arguments:\n    x : torch.Tensor\n        The input tensor with shape (batch_size, length, width), where 'width'\n        is typically the embedding dimension or a multiple of 'heads'.\n    heads : int\n        The number of heads in the multi-head attention mechanism.\n\n    Returns:\n    torch.Tensor\n        A reshaped tensor with dimensions corresponding to multi-head structures,\n        specifically (batch_size * n_heads, length, dim_per_head).\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef reshape_tensor(x, heads):\n    \"\"\"\n    Reshape a tensor to fit a multi-head attention mechanism structure.\n    If the shape of x is invalid, please return ValueError.\n\n    Arguments:\n    x : torch.Tensor\n        The input tensor with shape (batch_size, length, width), where 'width'\n        is typically the embedding dimension or a multiple of 'heads'.\n    heads : int\n        The number of heads in the multi-head attention mechanism.\n\n    Returns:\n    torch.Tensor\n        A reshaped tensor with dimensions corresponding to multi-head structures,\n        specifically (batch_size * n_heads, length, dim_per_head).\n    \"\"\"\n    if x.shape[-1] % heads!= 0:\n        raise ValueError(\"Invalid shape: 'width' must be divisible by 'heads'.\")\n    dim_per_head = x.shape[-1] // heads\n    x = x.reshape(x.shape[0], x.shape[1], heads, dim_per_head)\n    x = x.transpose(1, 2)\n    x = x.reshape(x.shape[0] * x.shape[1], x.shape[2], x.shape[3])\n\n    return x\n\n\n",
  "canonical_solution": "\n    bs, length, width = x.shape\n    # Reshape to introduce a new dimension for heads, assuming that width can be evenly divided by heads.\n    # width is divided into 'heads' and the dimension per head is calculated as width // heads\n    try:\n        x = x.view(bs, length, heads, -1)\n    except:\n        return ValueError\n    # (bs, length, heads, dim_per_head) --> (bs, heads, length, dim_per_head)\n    # Transpose to swap the 'length' and 'heads' dimensions to align heads for parallel processing\n    x = x.transpose(1, 2)\n    # (bs, heads, length, dim_per_head) --> (bs * heads, length, dim_per_head)\n    # Reshape to merge batch size and heads for processing each head independently in subsequent operations\n    x = x.reshape(bs * heads, length, -1)\n    return x\n\n\n",
  "unprocess_testcases": "a = torch.rand(2, 5, 10)\nb = torch.rand(3, 8, 15)\nc = torch.rand(1, 1, 4)\n\nassert((SOLUTION_SIGNATURE(a, heads=2) == reshape_tensor(a, heads=2)).all())\nassert(SOLUTION_SIGNATURE(b, heads=4) == reshape_tensor(b, heads=4))\nassert((SOLUTION_SIGNATURE(c, heads=2) == reshape_tensor(c, heads=2)).all())\n"
 },
 {
  "task_id": "RWPB/42",
  "prompt": "from PIL import Image\nimport numpy as np\n\ndef resize_and_center_crop(image, target_width, target_height):\n    \"\"\"\n    Resizes an input image and then crops it to the target dimensions, centered on the resized image.\n\n    Args:\n        image (np.array): The input image as a NumPy array.\n        target_width (int): The desired width of the output image.\n        target_height (int): The desired height of the output image.\n\n    Returns:\n        np.array: The processed image as a NumPy array, resized and cropped to the target dimensions.\n\n    This function first converts the input image to a PIL Image object, then calculates the scale factor needed\n    to ensure that the resized image will cover the target dimensions. It resizes the image using the LANCZOS filter\n    for high-quality downsampling. After resizing, it calculates the coordinates necessary to crop the image to the\n    target dimensions centered on the resized image, and finally converts the cropped PIL image back to a NumPy array.\n    \"\"\"\n",
  "solution": "\nfrom PIL import Image\nimport numpy as np\n\ndef resize_and_center_crop(image, target_width, target_height):\n    \"\"\"\n    Resizes an input image and then crops it to the target dimensions, centered on the resized image.\n\n    Args:\n        image (np.array): The input image as a NumPy array.\n        target_width (int): The desired width of the output image.\n        target_height (int): The desired height of the output image.\n\n    Returns:\n        np.array: The processed image as a NumPy array, resized and cropped to the target dimensions.\n\n    This function first converts the input image to a PIL Image object, then calculates the scale factor needed\n    to ensure that the resized image will cover the target dimensions. It resizes the image using the LANCZOS filter\n    for high-quality downsampling. After resizing, it calculates the coordinates necessary to crop the image to the\n    target dimensions centered on the resized image, and finally converts the cropped PIL image back to a NumPy array.\n    \"\"\"\n    pil_image = Image.fromarray(image)\n    scale_factor = max(target_width / pil_image.width, target_height / pil_image.height)\n    resized_width = int(pil_image.width * scale_factor)\n    resized_height = int(pil_image.height * scale_factor)\n    resized_image = pil_image.resize((resized_width, resized_height), Image.LANCZOS)\n    x_offset = (resized_width - target_width) // 2\n    y_offset = (resized_height - target_height) // 2\n    cropped_image = resized_image.crop((x_offset, y_offset, x_offset + target_width, y_offset + target_height))\n    return np.array(cropped_image)\n",
  "canonical_solution": "\n    # Convert the input NumPy array to a PIL Image object\n    pil_image = Image.fromarray(image)\n    original_width, original_height = pil_image.size\n\n    # Calculate the scale factor to resize the image\n    scale_factor = max(target_width / original_width, target_height / original_height)\n\n    # Calculate the new dimensions\n    resized_width = int(round(original_width * scale_factor))\n    resized_height = int(round(original_height * scale_factor))\n\n    # Resize the image using the LANCZOS filter for high quality\n    resized_image = pil_image.resize((resized_width, resized_height), Image.LANCZOS)\n\n    # Calculate the coordinates for cropping the image to center it\n    left = (resized_width - target_width) / 2\n    top = (resized_height - target_height) / 2\n    right = (resized_width + target_width) / 2\n    bottom = (resized_height + target_height) / 2\n\n    # Crop the image using the calculated coordinates\n    cropped_image = resized_image.crop((left, top, right, bottom))\n\n    # Convert the cropped PIL image back to a NumPy array and return\n    return np.array(cropped_image)\n\n\n",
  "unprocess_testcases": "a = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\nb = np.random.randint(0, 256, (100, 200, 3), dtype=np.uint8)\nc = np.random.randint(0, 256, (1000, 800, 3), dtype=np.uint8)\n\nassert((SOLUTION_SIGNATURE(a, 100, 100) == resize_and_center_crop(a, 100, 100)).all())\nassert((SOLUTION_SIGNATURE(b, 150, 100) == resize_and_center_crop(b, 150, 100)).all())\nassert((SOLUTION_SIGNATURE(c, 200, 150) == resize_and_center_crop(c, 200, 150)).all())\n"
 },
 {
  "task_id": "RWPB/13",
  "prompt": "import numpy as np\n\n\ndef remove_scale_from_camera_pose(pose_matrix):\n    '''pose_matrix is a 4x4 numpy array\n\n    Args:\n        - pose_matrix: numpy.array\n    Return:\n        - pose_matrix: numpy.array\n    '''\n",
  "solution": "\n\n\nimport numpy as np\n\n\ndef remove_scale_from_camera_pose(pose_matrix):\n    '''pose_matrix is a 4x4 numpy array\n\n    Args:\n        - pose_matrix: numpy.array\n    Return:\n        - pose_matrix: numpy.array\n    '''\n    pose_matrix[:3, :3] = pose_matrix[:3, :3] / np.linalg.norm(pose_matrix[:3, :3], axis=0)\n    return pose_matrix\n\n\n",
  "canonical_solution": "\n    # Ensure the matrix is in floating point format to avoid type casting issues in division\n    pose_matrix = pose_matrix.astype(np.float64)\n\n    # For each of the first three columns, normalize to remove scale\n    for i in range(3):\n        column = pose_matrix[:, i]\n        scale_factor = np.linalg.norm(column[:3])  # Compute the norm of the column, excluding the bottom element\n        if scale_factor > 0:  # Avoid division by zero\n            pose_matrix[:, i] /= scale_factor  # Normalize column to remove scale\n\n    # The last column (translation) and the last row are not modified, as they do not contribute to scale\n    return pose_matrix\n\n",
  "unprocess_testcases": "a = np.array([[3, 0, 0, 0],\n          [0, 4, 0, 0],\n          [0, 0, 2, 0],\n          [0, 0, 0, 1]])\nb = np.array([[0, 0, 0, 0],\n          [0, 0, 0, 0],\n          [0, 0, 0, 0],\n          [0, 0, 0, 1]])\nc = np.array([[1, 2, 3, 0],\n          [4, 5, 6, 0],\n          [7, 8, 9, 0],\n          [0, 0, 0, 1]])\n\nassert((SOLUTION_SIGNATURE(a) == remove_scale_from_camera_pose(a)).all())\nassert((SOLUTION_SIGNATURE(b) == remove_scale_from_camera_pose(b)).all())\nassert((SOLUTION_SIGNATURE(c) == remove_scale_from_camera_pose(c)).all())"
 },
 {
  "task_id": "RWPB/85",
  "prompt": "from typing import Optional, Sequence\n\nimport torch\n\ndef window_partition(x: torch.Tensor, window_size: Sequence[int]):\n    \"\"\"\n    Partition an image feature map into smaller windows in an ANE (Apple Neural Engine) friendly manner,\n    avoiding the use of 6D tensors.\n\n    :param x: A 4D tensor representing the image feature map with shape (batch_size, H, W, C), where:\n              - batch_size (B) is the number of images in the batch,\n              - H is the height of the feature map,\n              - W is the width of the feature map,\n              - C is the number of channels.\n    :param window_size: A sequence of two integers (win_h, win_w) specifying the height and width of the windows.\n    :returns: A 4D tensor of partitioned feature map windows with shape (batch_size * num_windows, win_h, win_w, C), where:\n              - num_windows is the total number of windows per image (calculated as (H / win_h) * (W / win_w)),\n              - win_h is the window height,\n              - win_w is the window width,\n              - C is the number of channels.\n    :rtype: torch.Tensor\n\n    The process of partitioning is as follows:\n    1. Reshape the input tensor to split the height dimension (H) into smaller chunks of size win_h,\n       resulting in shape (B, H // win_h, win_h, W, C).\n    2. Flatten the batch and height dimensions together to shape (B * H // win_h, win_h, W, C).\n    3. Reshape again to split the width dimension (W) into smaller chunks of size win_w,\n       resulting in shape (B * H // win_h, win_h, W // win_w, win_w, C).\n    4. Permute the dimensions to arrange the tensor into shape (B * H // win_h, W // win_w, win_h, win_w, C).\n    5. Flatten the batch and width dimensions together to produce the final shape (batch_size * num_windows, win_h, win_w, C).\n    \"\"\"\n",
  "solution": "\nfrom typing import Optional, Sequence\n\nimport torch\n\ndef window_partition(x: torch.Tensor, window_size: Sequence[int]):\n    \"\"\"\n    Partition an image feature map into smaller windows in an ANE (Apple Neural Engine) friendly manner,\n    avoiding the use of 6D tensors.\n\n    :param x: A 4D tensor representing the image feature map with shape (batch_size, H, W, C), where:\n              - batch_size (B) is the number of images in the batch,\n              - H is the height of the feature map,\n              - W is the width of the feature map,\n              - C is the number of channels.\n    :param window_size: A sequence of two integers (win_h, win_w) specifying the height and width of the windows.\n    :returns: A 4D tensor of partitioned feature map windows with shape (batch_size * num_windows, win_h, win_w, C), where:\n              - num_windows is the total number of windows per image (calculated as (H // win_h) * (W // win_w)),\n              - win_h is the window height,\n              - win_w is the window width,\n              - C is the number of channels.\n    :rtype: torch.Tensor\n\n    The process of partitioning is as follows:\n    1. Reshape the input tensor to split the height dimension (H) into smaller chunks of size win_h,\n       resulting in shape (B, H // win_h, win_h, W, C).\n    2. Flatten the batch and height dimensions together to shape (B * H // win_h, win_h, W, C).\n    3. Reshape again to split the width dimension (W) into smaller chunks of size win_w,\n       resulting in shape (B * H // win_h, win_h, W // win_w, win_w, C).\n    4. Permute the dimensions to arrange the tensor into shape (B * H // win_h, W // win_w, win_h, win_w, C).\n    5. Flatten the batch and width dimensions together to produce the final shape (batch_size * num_windows, win_h, win_w, C).\n    \"\"\"\n    B, H, W, C = x.shape\n    win_h, win_w = window_size\n    x = x.reshape(B, H // win_h, win_h, W // win_w, win_w, C)\n    x = x.flatten(1, 2)\n    x = x.reshape(B * (H // win_h), W // win_w, win_h, win_w, C)\n    x = x.permute(0, 1, 3, 2, 4).contiguous()\n    x = x.flatten(1, 2)\n    return x\n",
  "canonical_solution": "    \n    B, H, W, C = x.shape\n    # example partition process: 1, 12, 16, 160 -> 1, 2, 6, 16, 160 -> 2, 6, 16, 160 -> 2, 6, 2, 8, 160 -> ...\n    x = x.reshape(\n        (B, H // window_size[0], window_size[0], W, C)\n    )  # B, H//w_size, w_size, W, C\n    x = x.reshape(\n        (B * H // window_size[0], window_size[0], W, C)\n    )  # B * H // w_size, w_size, W, C\n    x = x.reshape(\n        (\n            B * H // window_size[0],\n            window_size[0],\n            W // window_size[1],\n            window_size[1],\n            -1,\n        )\n    )\n    x = x.permute((0, 2, 1, 3, 4))\n    windows = x.reshape((-1, window_size[0], window_size[1], C))\n    return windows\n\n\n",
  "unprocess_testcases": "a = torch.rand((2, 8, 8, 3))\nws = (4, 4)\nassert((SOLUTION_SIGNATURE(a, ws) == window_partition(a, ws)).all())\n\na = torch.rand((1, 10, 10, 5))\nws = (5, 2)\nassert((SOLUTION_SIGNATURE(a, ws) == window_partition(a, ws)).all())\n\na = torch.rand((1, 6, 8, 3))\nws = (3, 2)\nassert((SOLUTION_SIGNATURE(a, ws) == window_partition(a, ws)).all())\n"
 },
 {
  "task_id": "RWPB/27",
  "prompt": "import torch\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens the scores and labels from a batch of predictions, removing entries with a specified ignore label.\n\n    Parameters\n    ----------\n    scores : torch.Tensor\n        The tensor containing the prediction scores for each item in the batch.\n    labels : torch.Tensor\n        The tensor containing the true labels for each item in the batch.\n    ignore : int or float, optional\n        The label to ignore during the flattening process. If None, all labels are kept.\n\n    Returns\n    -------\n    vscores : torch.Tensor\n        A flattened tensor containing the prediction scores without the ignored labels.\n    vlabels : torch.Tensor\n        A flattened tensor containing the true labels without the ignored labels.\n\n    Notes\n    -----\n    This function is typically used in the binary classification case where 'scores' and 'labels'\n    are expected to be tensors of shape (batch_size, ...) and the function flattens them to a 1D\n    tensor (-1,). If the 'ignore' parameter is provided, it filters out the corresponding labels\n    and scores from the result.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens the scores and labels from a batch of predictions, removing entries with a specified ignore label.\n\n    Parameters\n    ----------\n    scores : torch.Tensor\n        The tensor containing the prediction scores for each item in the batch.\n    labels : torch.Tensor\n        The tensor containing the true labels for each item in the batch.\n    ignore : int or float, optional\n        The label to ignore during the flattening process. If None, all labels are kept.\n\n    Returns\n    -------\n    vscores : torch.Tensor\n        A flattened tensor containing the prediction scores without the ignored labels.\n    vlabels : torch.Tensor\n        A flattened tensor containing the true labels without the ignored labels.\n\n    Notes\n    -----\n    This function is typically used in the binary classification case where'scores' and 'labels'\n    are expected to be tensors of shape (batch_size,...) and the function flattens them to a 1D\n    tensor (-1,). If the 'ignore' parameter is provided, it filters out the corresponding labels\n    and scores from the result.\n    \"\"\"\n    assert scores.shape == labels.shape, \"Scores and labels must have the same shape.\"\n    if ignore is not None:\n        mask = labels!= ignore\n        scores = scores[mask]\n        labels = labels[mask]\n    vscores = scores.view(-1)\n    vlabels = labels.view(-1)\n    return vscores, vlabels\n\n",
  "canonical_solution": "\n    # Flatten the scores and labels to 1D tensors\n    scores = scores.view(-1)\n    labels = labels.view(-1)\n\n    # If there's no ignore label, return the flattened scores and labels\n    if ignore is None:\n        return scores, labels\n\n    # Create a boolean mask for entries that do not have the ignore label\n    valid = (labels != ignore)\n\n    # Apply the mask to filter out the ignored labels and return the valid scores and labels\n    vscores = scores[valid]\n    vlabels = labels[valid]\n\n    return vscores, vlabels\n\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([[0.5, 0.2], [0.7, 0.4]]), torch.tensor([[1, 1], [0, 0]]), 2)[0] == flatten_binary_scores(torch.tensor([[0.5, 0.2], [0.7, 0.4]]), torch.tensor([[1, 1], [0, 0]]), 2)[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[0.5, 0.2], [0.7, 0.4]]), torch.tensor([[1, 1], [0, 0]]), 2)[1] == flatten_binary_scores(torch.tensor([[0.5, 0.2], [0.7, 0.4]]), torch.tensor([[1, 1], [0, 0]]), 2)[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[0.1, 0.3], [0.5, 0.7]]), torch.tensor([[1, 2], [1, 2]]), 2)[0] == flatten_binary_scores(torch.tensor([[0.1, 0.3], [0.5, 0.7]]), torch.tensor([[1, 2], [1, 2]]), 2)[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[0.1, 0.3], [0.5, 0.7]]), torch.tensor([[1, 2], [1, 2]]), 2)[1] == flatten_binary_scores(torch.tensor([[0.1, 0.3], [0.5, 0.7]]), torch.tensor([[1, 2], [1, 2]]), 2)[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[0.2, 0.4], [0.6, 0.8]]), torch.tensor([[0, 1], [0, 1]]), None)[0] == flatten_binary_scores(torch.tensor([[0.2, 0.4], [0.6, 0.8]]), torch.tensor([[0, 1], [0, 1]]), None)[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[0.2, 0.4], [0.6, 0.8]]), torch.tensor([[0, 1], [0, 1]]), None)[1] == flatten_binary_scores(torch.tensor([[0.2, 0.4], [0.6, 0.8]]), torch.tensor([[0, 1], [0, 1]]), None)[1]).all())\n"
 },
 {
  "task_id": "RWPB/76",
  "prompt": "import torch\n\ndef quat_to_rotmat(quat: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Convert quaternion representation to rotation matrix.\n    Args:\n        quat (torch.Tensor) of shape (B, 4); 4 <===> (w, x, y, z).\n    Returns:\n        torch.Tensor: Corresponding rotation matrices with shape (B, 3, 3).\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef quat_to_rotmat(quat: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Convert quaternion representation to rotation matrix.\n    Args:\n        quat (torch.Tensor) of shape (B, 4); 4 <===> (w, x, y, z).\n    Returns:\n        torch.Tensor: Corresponding rotation matrices with shape (B, 3, 3).\n    \"\"\"\n    w, x, y, z = quat.unbind(dim=-1)\n    rot_mat = torch.zeros((quat.shape[0], 3, 3), device=quat.device)\n    rot_mat[:, 0, 0] = 1 - 2 * (y ** 2 + z ** 2)\n    rot_mat[:, 0, 1] = 2 * (x * y - w * z)\n    rot_mat[:, 0, 2] = 2 * (x * z + w * y)\n    rot_mat[:, 1, 0] = 2 * (x * y + w * z)\n    rot_mat[:, 1, 1] = 1 - 2 * (x ** 2 + z ** 2)\n    rot_mat[:, 1, 2] = 2 * (y * z - w * x)\n    rot_mat[:, 2, 0] = 2 * (x * z - w * y)\n    rot_mat[:, 2, 1] = 2 * (y * z + w * x)\n    rot_mat[:, 2, 2] = 1 - 2 * (x ** 2 + y ** 2)\n\n    return rot_mat\n\n\n",
  "canonical_solution": "    \n    norm_quat = quat\n    norm_quat = norm_quat / norm_quat.norm(p=2, dim=1, keepdim=True)\n    w, x, y, z = norm_quat[:, 0], norm_quat[:, 1], norm_quat[:, 2], norm_quat[:, 3]\n\n    B = quat.size(0)\n\n    w2, x2, y2, z2 = w.pow(2), x.pow(2), y.pow(2), z.pow(2)\n    wx, wy, wz = w * x, w * y, w * z\n    xy, xz, yz = x * y, x * z, y * z\n\n    rotMat = torch.stack(\n        [\n            w2 + x2 - y2 - z2,\n            2 * xy - 2 * wz,\n            2 * wy + 2 * xz,\n            2 * wz + 2 * xy,\n            w2 - x2 + y2 - z2,\n            2 * yz - 2 * wx,\n            2 * xz - 2 * wy,\n            2 * wx + 2 * yz,\n            w2 - x2 - y2 + z2,\n        ],\n        dim=1,\n    ).view(B, 3, 3)\n    return rotMat\n\n\n\n\n",
  "unprocess_testcases": "a = torch.tensor([[1.0, 0.0, 0.0, 0.0]])\nb = torch.tensor([\n    [0.7071, 0.7071, 0.0, 0.0],\n    [0.7071, 0.0, 0.7071, 0.0],\n    [0.7071, 0.0, 0.0, 0.7071]\n])\nc = torch.tensor([\n    [2.0, 0.0, 0.0, 0.0],\n    [1.4142, 1.4142, 0.0, 0.0],\n    [10.0, 0.0, 0.0, 10.0]\n])\n\nassert((SOLUTION_SIGNATURE(a) == quat_to_rotmat(a)).all())\nassert((SOLUTION_SIGNATURE(b) == quat_to_rotmat(b)).all())\nassert((SOLUTION_SIGNATURE(c) == quat_to_rotmat(c)).all())"
 },
 {
  "task_id": "RWPB/56",
  "prompt": "import torch\nimport numpy as np\nfrom torch.nn import functional as F\n\n\ndef grid_distortion(images, strength=0.5):\n    \"\"\"\n    Applies a grid-based distortion to a batch of images by perturbing the grid points.\n\n    Arguments:\n    images : torch.Tensor\n        A tensor of shape [B, C, H, W] representing the batch of images.\n    strength : float, optional\n        The strength of the distortion, must be a value between 0 and 1. Default is 0.5.\n\n    Returns:\n    torch.Tensor\n        A tensor of distorted images with the same shape as the input.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\nimport numpy as np\nfrom torch.nn import functional as F\n\n\ndef grid_distortion(images, strength=0.5):\n    \"\"\"\n    Applies a grid-based distortion to a batch of images by perturbing the grid points.\n\n    Arguments:\n    images : torch.Tensor\n        A tensor of shape [B, C, H, W] representing the batch of images.\n    strength : float, optional\n        The strength of the distortion, must be a value between 0 and 1. Default is 0.5.\n\n    Returns:\n    torch.Tensor\n        A tensor of distorted images with the same shape as the input.\n    \"\"\"\n    assert 0 <= strength <= 1, \"Strength must be a value between 0 and 1.\"\n    B, C, H, W = images.shape\n    grid = F.affine_grid(\n        torch.eye(2).unsqueeze(0).repeat(B, 1, 3),\n        (B, H, W),\n        align_corners=False,\n    )\n    grid += torch.randn(B, H, W, 2) * strength\n    distorted_images = F.grid_sample(images, grid, align_corners=False)\n\n    return distorted_images\n\n\n",
  "canonical_solution": "\n    B, C, H, W = images.shape\n\n    # Determine the resolution of the grid\n    num_steps = np.random.randint(8, 17)  # Random grid size between 8 and 16\n    grid_steps = torch.linspace(-1, 1, num_steps)  # Steps from -1 to 1 for normalized coordinates\n\n    grids = []  # To store grids for each image in the batch\n    for b in range(B):\n        # Generate distorted x-coordinates\n        x_steps = torch.linspace(0, 1, num_steps)  # Equally spaced steps between 0 and 1\n        # Add random perturbation scaled by the distortion strength\n        x_steps = (x_steps + strength * (torch.rand_like(x_steps) - 0.5) / (num_steps - 1)).clamp(0, 1)\n        x_steps = (x_steps * W).long()  # Scale steps to image width\n        x_steps[0] = 0\n        x_steps[-1] = W  # Ensure edges remain at the borders\n\n        xs = [torch.linspace(grid_steps[i], grid_steps[i + 1], x_steps[i + 1] - x_steps[i]) for i in\n              range(num_steps - 1)]\n        xs = torch.cat(xs, dim=0)  # Concatenate to form a single tensor\n\n        # Generate distorted y-coordinates using similar steps as x-coordinates\n        y_steps = torch.linspace(0, 1, num_steps)\n        y_steps = (y_steps + strength * (torch.rand_like(y_steps) - 0.5) / (num_steps - 1)).clamp(0, 1)\n        y_steps = (y_steps * H).long()\n        y_steps[0] = 0\n        y_steps[-1] = H\n\n        ys = [torch.linspace(grid_steps[i], grid_steps[i + 1], y_steps[i + 1] - y_steps[i]) for i in\n              range(num_steps - 1)]\n        ys = torch.cat(ys, dim=0)\n\n        # Construct the distortion grid using meshgrid\n        grid_x, grid_y = torch.meshgrid(xs, ys, indexing='xy')\n        grid = torch.stack([grid_x, grid_y], dim=-1)  # Stack to form [H, W, 2] grid\n\n        grids.append(grid)\n\n    grids = torch.stack(grids, dim=0).to(images.device)  # Convert list to tensor and move to the same device as images\n\n    # Apply grid sample to distort the images according to the generated grids\n    images = F.grid_sample(images, grids, align_corners=False)\n\n    return images\n\n",
  "unprocess_testcases": "a = torch.rand([2, 3, 32, 32])\nb = torch.rand([1, 1, 64, 64])\nc = torch.rand([1, 1, 64, 64])\nd = torch.rand([10, 3, 256, 256])\n\nassert(SOLUTION_SIGNATURE(a).size() == grid_distortion(a).size())\nassert(SOLUTION_SIGNATURE(b, 0).size()== grid_distortion(b, 0).size())\nassert(SOLUTION_SIGNATURE(c, 1).size() == grid_distortion(c, 1).size())\nassert(SOLUTION_SIGNATURE(d, 0.3).size() == grid_distortion(d, 0.3).size())\n"
 },
 {
  "task_id": "RWPB/33",
  "prompt": "import torch\n\n\ndef top_p_sample(prob_dist: torch.Tensor, top_p: float) -> torch.Tensor:\n    \"\"\"\n    Applies top-p filtering to a probability distribution to focus sampling on a subset of likely outputs.\n\n    Args:\n        prob_dist (torch.Tensor): The original probability distribution from which to sample, shape (batch_size, vocab_size).\n        top_p (float): The cumulative probability threshold used for filtering. Probabilities that add up to `top_p` are kept.\n\n    Returns:\n        torch.Tensor: The modified probability distribution after applying top-p filtering.\n\n    This method is used to reduce the sample space to the most probable tokens whose cumulative probability\n    exceeds the threshold `top_p`. It improves sampling outcomes by eliminating highly improbable options.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\n\ndef top_p_sample(prob_dist: torch.Tensor, top_p: float) -> torch.Tensor:\n    \"\"\"\n    Applies top-p filtering to a probability distribution to focus sampling on a subset of likely outputs.\n\n    Args:\n        prob_dist (torch.Tensor): The original probability distribution from which to sample, shape (batch_size, vocab_size).\n        top_p (float): The cumulative probability threshold used for filtering. Probabilities that add up to `top_p` are kept.\n\n    Returns:\n        torch.Tensor: The modified probability distribution after applying top-p filtering.\n\n    This method is used to reduce the sample space to the most probable tokens whose cumulative probability\n    exceeds the threshold `top_p`. It improves sampling outcomes by eliminating highly improbable options.\n    \"\"\"\n    sorted_indices = torch.argsort(prob_dist, dim=-1, descending=True)\n    cumulative_probabilities = torch.cumsum(prob_dist.gather(dim=-1, index=sorted_indices), dim=-1)\n    filtered_indices = sorted_indices.gather(dim=-1, index=torch.lt(cumulative_probabilities, top_p))\n    filtered_prob_dist = torch.zeros_like(prob_dist).scatter(dim=-1, index=filtered_indices, src=torch.ones_like(filtered_indices))\n\n    return filtered_prob_dist\n\n\n",
  "canonical_solution": "\n    # Sort the probabilities and their corresponding indices in descending order\n    sorted_probs, sorted_indices = torch.sort(prob_dist, descending=True, dim=-1)\n\n    # Compute the cumulative sum of the sorted probabilities\n    cum_sum_probs = torch.cumsum(sorted_probs, dim=-1)  # Shape: (batch_size, vocab_size)\n\n    # Create a boolean mask where probabilities that exceed the threshold are set to True\n    sorted_indices_to_remove = cum_sum_probs > top_p\n\n    # Shift the indices to the right to include the first token above the threshold\n    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n    sorted_indices_to_remove[:, 0] = 0  # Ensure the first token is always included\n\n    # Convert the mask to boolean and apply it to sorted_probs by setting removed probs to 0\n    sorted_probs[sorted_indices_to_remove] = 0\n\n    # Reverse the sorting process to reconstruct the original order of probabilities\n    reversed_indices = torch.argsort(sorted_indices)\n    prob_dist = torch.gather(sorted_probs, -1, reversed_indices)\n\n    # Normalize the probabilities to ensure they sum to 1\n    prob_dist = prob_dist / prob_dist.sum(dim=-1, keepdim=True)\n\n    return prob_dist\n\n\n",
  "unprocess_testcases": "a = torch.tensor([[0.5, 0.2, 0.1, 0.1, 0.05, 0.05]])\nb = torch.tensor([[0.05, 0.15, 0.2, 0.3, 0.25, 0.05],\n                          [0.4, 0.1, 0.1, 0.1, 0.1, 0.2],\n                          [0.25, 0.25, 0.25, 0.15, 0.05, 0.05]])\nc = torch.tensor([[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]])\n\nassert((SOLUTION_SIGNATURE(a, 0.3) == top_p_sample(a, 0.3)).all())\nassert((SOLUTION_SIGNATURE(b, 0.3) == top_p_sample(b, 0.3)).all())\nassert((SOLUTION_SIGNATURE(c, 0.5) == top_p_sample(c, 0.5)).all())\n"
 },
 {
  "task_id": "RWPB/62",
  "prompt": "import torch\n\n\ndef broadcat(tensors, dim=-1):\n    \"\"\"\n    Broadcasts a list of tensors to a common shape and concatenates them along a specified dimension.\n    The function should return ValueError when the first dimensions of the tensors are incompatible for broadcasting.\n    \n    Arguments:\n    tensors : list of torch.Tensor\n        List of tensors to broadcast and concatenate.\n    dim : int, optional\n        Dimension along which to concatenate the tensors.\n\n    Returns:\n    torch.Tensor\n        A tensor resulting from broadcasting and concatenating the input tensors along the specified dimension.\n    \"\"\"\n",
  "solution": "\n\n\ndef broadcat(tensors, dim=-1):\n    \"\"\"\n    Broadcasts a list of tensors to a common shape and concatenates them along a specified dimension.\n    The function should return ValueError when the first dimensions of the tensors are incompatible for broadcasting.\n    \n    Arguments:\n    tensors : list of torch.Tensor\n        List of tensors to broadcast and concatenate.\n    dim : int, optional\n        Dimension along which to concatenate the tensors.\n\n    Returns:\n    torch.Tensor\n        A tensor resulting from broadcasting and concatenating the input tensors along the specified dimension.\n    \"\"\"\n    if len(tensors) == 0:\n        raise ValueError(\"No tensors provided.\")\n    shape = tensors[0].shape\n    for tensor in tensors[1:]:\n        if tensor.shape[:dim]!= shape[:dim]:\n            raise ValueError(\"Incompatible shapes for broadcasting.\")\n        shape = tuple(max(s1, s2) for s1, s2 in zip(shape, tensor.shape))\n    broadcasted_tensors = [tensor.expand(shape) for tensor in tensors]\n    return torch.cat(broadcasted_tensors, dim=dim)\n\n\n",
  "canonical_solution": "    \n\n    num_tensors = len(tensors)\n    if num_tensors == 0:\n        return ValueError\n\n    # Check that all tensors have the same number of dimensions\n    shape_lens = set(map(lambda t: len(t.shape), tensors))\n    if len(shape_lens) != 1:\n        return ValueError\n    shape_len = list(shape_lens)[0]\n\n    # Handle negative dimension indices\n    dim = (dim + shape_len) if dim < 0 else dim\n    if dim >= shape_len:\n        return ValueError\n\n    # Collect dimensions of all tensors and find maximum dimension sizes for broadcasting\n    dimensions = list(zip(*[t.shape for t in tensors]))\n    max_dims = [max(dim_sizes) for dim_sizes in dimensions]\n\n    # Ensure all dimensions are compatible for broadcasting\n    for i, dim_sizes in enumerate(dimensions):\n        if i != dim and any(size != max_dims[i] and size != 1 for size in dim_sizes):\n            return ValueError\n\n    # Broadcast each tensor to the maximum dimension size\n    broadcasted_tensors = [t.expand(*max_dims) for t in tensors]\n\n    # Concatenate the broadcasted tensors along the specified dimension\n    return torch.cat(broadcasted_tensors, dim=dim)\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE([torch.tensor([[1], [2]]), torch.tensor([[3, 3, 3], [4, 4, 4]])]) == broadcat([torch.tensor([[1], [2]]), torch.tensor([[3, 3, 3], [4, 4, 4]])])).all())\nassert((SOLUTION_SIGNATURE([torch.tensor([[1], [2], [3]]), torch.tensor([[4, 4], [5, 5], [6, 6]])], dim=-1) == broadcat([torch.tensor([[1], [2], [3]]), torch.tensor([[4, 4], [5, 5], [6, 6]])], dim=-1)).all())\nassert(SOLUTION_SIGNATURE([torch.tensor([[1, 1, 1], [2, 2, 2]]), torch.tensor([[3, 3, 3], [4, 4, 4], [5, 5, 5]])]) == broadcat([torch.tensor([[1, 1, 1], [2, 2, 2]]), torch.tensor([[3, 3, 3], [4, 4, 4], [5, 5, 5]])]))\n"
 },
 {
  "task_id": "RWPB/91",
  "prompt": "import math\nimport numpy as np\n\n\ndef r_sqrd(x, y):\n    \"\"\"\n    Calculate R^2 of best fit simple least squares regression line.\n    If denominator is zero, the code should return KeyError\n    Args:\n        - x: numpy array, shape (n,), x coordinates of (x,y) points.\n        - y: numpy array, shape (n,), y coordinates of (x,y) points.\n\n    Return:\n        r_squared: float, R^2 of the best fit simple least squares regression line.\n\n    \"\"\"\n",
  "solution": "\n\n\nimport math\nimport numpy as np\n\n\ndef r_sqrd(x, y):\n    \"\"\"\n    Calculate R^2 of best fit simple least squares regression line.\n    If denominator is zero, the code should return KeyError\n    Args:\n        - x: numpy array, shape (n,), x coordinates of (x,y) points.\n        - y: numpy array, shape (n,), y coordinates of (x,y) points.\n\n    Return:\n        r_squared: float, R^2 of the best fit simple least squares regression line.\n\n    \"\"\"\n    if x.shape!= y.shape:\n        raise ValueError(\"x and y must have same shape\")\n    n = len(x)\n    mean_x = np.mean(x)\n    mean_y = np.mean(y)\n    numerator = 0\n    denominator = 0\n    for i in range(n):\n        numerator += (x[i] - mean_x) * (y[i] - mean_y)\n        denominator += (x[i] - mean_x) ** 2\n    if denominator == 0:\n        raise KeyError(\"Denominator is zero\")\n    r = numerator / denominator\n    r_squared = r ** 2\n\n    return r_squared\n\n\n",
  "canonical_solution": "    \n    x_dot_x = x.dot(x)\n    x_dot_y = x.dot(y)\n    x_mean = x.mean()\n    x_sum = x.sum()\n    y_mean = y.mean()\n    denominator = x_dot_x - x_mean * x_sum\n    if denominator == 0:\n        return KeyError\n    \n    m = (x_dot_y - y_mean * x_sum) / denominator\n    b = (y_mean * x_dot_x - x_mean * x_dot_y) / denominator\n    y_pred = m * x + b\n    residuals = y - y_pred\n    tot = y - y_mean\n\n    if int(tot.dot(tot)) == 0:\n        return KeyError\n\n    return 1.0 - residuals.dot(residuals) / tot.dot(tot)\n\n    \n\n",
  "unprocess_testcases": "x = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 4, 6, 8, 10])\nassert(SOLUTION_SIGNATURE(x, y) == r_sqrd(x, y))\n\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 2, 2, 2, 2])\nassert(SOLUTION_SIGNATURE(x, y) == r_sqrd(x, y))\n\nx = np.array([1, 1, 1, 1, 1])\ny = np.array([1, 2, 3, 4, 5])\nassert(SOLUTION_SIGNATURE(x, y) == r_sqrd(x, y))\n\nx = np.array([1, 1, 3, 1, 5])\ny = np.array([1, 0, 3, 1, 5])\nassert(SOLUTION_SIGNATURE(x, y) == r_sqrd(x, y))\n\nx = np.array([6, 1, 5, 7, 5])\ny = np.array([0, -1, 3, 2, 3])\nassert(SOLUTION_SIGNATURE(x, y) == r_sqrd(x, y))\n"
 },
 {
  "task_id": "RWPB/81",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\ndef remove_outliers(arr, num_std_devs=2):\n    \"\"\"\n    Remove outliers from an array based on a specified number of standard deviations from the mean.\n    Args:\n        - arr: numpy array, the input array\n        - num_std_devs: int, number of standard deviations from the mean to consider as outliers (default is 2)\n    Returns:\n        filtered_arr: numpy array, the array with outliers removed\n    \"\"\"\n",
  "solution": "\n\n\nimport math\nimport numpy as np\nfrom decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\ndef remove_outliers(arr, num_std_devs=2):\n    \"\"\"\n    Remove outliers from an array based on a specified number of standard deviations from the mean.\n    Args:\n        - arr: numpy array, the input array\n        - num_std_devs: int, number of standard deviations from the mean to consider as outliers (default is 2)\n    Returns:\n        filtered_arr: numpy array, the array with outliers removed\n    \"\"\"\n    mean = np.mean(arr)\n    std_dev = np.std(arr)\n    threshold = num_std_devs * std_dev\n    filtered_arr = [x for x in arr if abs(x - mean) <= threshold]\n    return np.array(filtered_arr)\n\n\n",
  "canonical_solution": "    \n    if len(arr) < 2:\n        return arr\n    mean = np.mean(arr)\n    std_dev = np.std(arr)\n    return arr[np.abs(arr - mean) <= num_std_devs * std_dev]\n\n\n",
  "unprocess_testcases": "a = np.random.normal(50, 10, size=100)\nb = np.linspace(0, 100, 100)\nc = np.append(np.random.normal(50, 10, size=95), [150, 200, -50, -100])\n\nassert((SOLUTION_SIGNATURE(a) == remove_outliers(a)).all())\nassert((SOLUTION_SIGNATURE(b) == remove_outliers(b)).all())\nassert((SOLUTION_SIGNATURE(c) == remove_outliers(c)).all())\n"
 },
 {
  "task_id": "RWPB/72",
  "prompt": "import numpy as np\n\ndef deriv_fitting_matrix(degree, t_end=1.0):\n  \"\"\"\n    Returns A s.t. that the vector x that satisfies\n    If degree % 2 != 0, return AssertionError\n\n    Ax = b\n\n    contains polynomial coefficients\n\n      p(t) = x_1 + x_2 t + x_3 t^2 ...\n\n    so that\n\n    p(0) = b_1\n    p^(i)(0) = b_{i+1}\n    p(t_end) = b_{degree / 2}\n    p^(i)(t_end) = b_{degree / 2 + i + 1}\n\n    i.e. the first degree / 2 derivatives of p at 0 match the first degree / 2\n         entries of b and the first degree / 2 derivatives of p at 1, match the last\n         degree / 2 entries of b\n    Args:\n        - degree: int, degree of the polynomial\n        - t_end: float, end point for the polynomial (default is 1.0)\n\n    Returns:\n        - A: numpy.ndarray, fitting matrix A\n  \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\n\ndef deriv_fitting_matrix(degree, t_end=1.0):\n  \"\"\"\n    Returns A s.t. that the vector x that satisfies\n    If degree % 2!= 0, return AssertionError\n\n    Ax = b\n\n    contains polynomial coefficients\n\n      p(t) = x_1 + x_2 t + x_3 t^2...\n\n    so that\n\n    p(0) = b_1\n    p^(i)(0) = b_{i+1}\n    p(t_end) = b_{degree / 2}\n    p^(i)(t_end) = b_{degree / 2 + i + 1}\n\n    i.e. the first degree / 2 derivatives of p at 0 match the first degree / 2\n         entries of b and the first degree / 2 derivatives of p at 1, match the last\n         degree / 2 entries of b\n    Args:\n        - degree: int, degree of the polynomial\n        - t_end: float, end point for the polynomial (default is 1.0)\n\n    Returns:\n        - A: numpy.ndarray, fitting matrix A\n  \"\"\"\n  assert degree % 2 == 0, \"degree must be even\"\n  A = np.zeros((degree, degree))\n  for i in range(degree):\n      A[i, :] = np.power(t_end, i) * np.power(np.linspace(0, 1, degree), i)\n  return A\n",
  "canonical_solution": "\n  if degree % 2 == 0:\n    return AssertionError\n\n  A = np.zeros((degree, degree))\n\n  ts = t_end ** np.array(range(degree))\n\n  constant_term = 1\n  poly = np.ones(degree)\n  for i in range(degree // 2):\n    A[i, i] = constant_term\n    A[degree // 2 + i, :] = np.hstack((np.zeros(i), poly * ts[:degree - i]))\n    poly = np.polynomial.polynomial.polyder(poly)\n    constant_term *= (i + 1)\n\n  return A\n\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(2, 1.0) == deriv_fitting_matrix(2, 1.0))\nassert(SOLUTION_SIGNATURE(6, 1.0) == deriv_fitting_matrix(6, 1.0))\nassert((SOLUTION_SIGNATURE(5, 0.5) == deriv_fitting_matrix(5, 0.5)).all())\nassert((SOLUTION_SIGNATURE(31, 0.9) == deriv_fitting_matrix(31, 0.9)).all())\nassert((SOLUTION_SIGNATURE(73, 0.1) == deriv_fitting_matrix(73, 0.1)).all())\nassert((SOLUTION_SIGNATURE(9, 3) == deriv_fitting_matrix(9, 3)).all())\n"
 },
 {
  "task_id": "RWPB/23",
  "prompt": "def merge(ids, pair, idx):\n    \"\"\"\n    Merges consecutive occurrences of a pair of integers in a list with a new integer.\n\n    Parameters\n    ----------\n    ids : list[int]\n        The list of integers where the merging will occur.\n    pair : tuple[int, int]\n        The pair of integers to be merged.\n    idx : int\n        The new integer that will replace the consecutive occurrences of 'pair'.\n\n    Returns\n    -------\n    list[int]\n        A new list of integers with all occurrences of 'pair' merged into 'idx'.\n\n    Examples\n    --------\n    >>> merge([1, 2, 3, 1, 2], (1, 2), 4)\n    [4, 3, 4]\n\n    The function iterates through the list 'ids' and checks for consecutive occurrences\n    of the integers in 'pair'. When such a pair is found, it is replaced by 'idx' in the\n    new list 'newids'. If the pair is not found, the current integer is simply appended to\n    'newids'. The function returns the modified list with the pairs merged.\n    \"\"\"\n",
  "solution": "\n\n\ndef merge(ids, pair, idx):\n    \"\"\"\n    Merges consecutive occurrences of a pair of integers in a list with a new integer.\n\n    Parameters\n    ----------\n    ids : list[int]\n        The list of integers where the merging will occur.\n    pair : tuple[int, int]\n        The pair of integers to be merged.\n    idx : int\n        The new integer that will replace the consecutive occurrences of 'pair'.\n\n    Returns\n    -------\n    list[int]\n        A new list of integers with all occurrences of 'pair' merged into 'idx'.\n\n    Examples\n    --------\n    >>> merge([1, 2, 3, 1, 2], (1, 2), 4)\n    [4, 3, 4]\n\n    The function iterates through the list 'ids' and checks for consecutive occurrences\n    of the integers in 'pair'. When such a pair is found, it is replaced by 'idx' in the\n    new list 'newids'. If the pair is not found, the current integer is simply appended to\n    'newids'. The function returns the modified list with the pairs merged.\n    \"\"\"\n    newids = []\n    i = 0\n    while i < len(ids):\n        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n            newids.append(idx)\n            i += 2\n        else:\n            newids.append(ids[i])\n            i += 1\n    return newids\n\n\n",
  "canonical_solution": "\n    newids = []\n    i = 0\n    while i < len(ids):\n        # Check if the current and next item form the pair and are not the last element\n        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n            # Append the idx to the new list and skip the next item\n            newids.append(idx)\n            i += 2\n        else:\n            # Append the current item to the new list\n            newids.append(ids[i])\n            i += 1\n    return newids\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(ids = [1, 2, 1, 2, 1, 2, 3], pair = (1, 2), idx = 4) == merge(ids = [1, 2, 1, 2, 1, 2, 3], pair = (1, 2), idx = 4))\nassert(SOLUTION_SIGNATURE(ids = [1, 3, 4, 5, 6], pair = (1, 2), idx = 4) == merge(ids = [1, 3, 4, 5, 6], pair = (1, 2), idx = 4))\nassert(SOLUTION_SIGNATURE(ids = [1, 2, 3, 4, 1, 2], pair = (1, 2), idx = 4) == merge(ids = [1, 2, 3, 4, 1, 2], pair = (1, 2), idx = 4))\n"
 },
 {
  "task_id": "RWPB/17",
  "prompt": "import numpy as np\n\ndef generate_linear_parameters(start, stop, num):\n    \"\"\"\n    Generates a list of linearly spaced parameters between start and stop.\n\n    Parameters\n    ----------\n    start : float\n        The starting value of the sequence.\n    stop : float\n        The ending value of the sequence.\n    num : int\n        The number of values to generate.\n\n    Returns\n    -------\n    List[float]\n        A list containing 'num' values, linearly spaced between 'start' and 'stop'.\n\n    Notes\n    -----\n    This function uses numpy's linspace function to create an array of 'num' values evenly\n    distributed over the interval [start, stop]. The resulting array is then converted to a list.\n    \"\"\"\n",
  "solution": "\nimport numpy as np\n\ndef generate_linear_parameters(start, stop, num):\n    \"\"\"\n    Generates a list of linearly spaced parameters between start and stop.\n\n    Parameters\n    ----------\n    start : float\n        The starting value of the sequence.\n    stop : float\n        The ending value of the sequence.\n    num : int\n        The number of values to generate.\n\n    Returns\n    -------\n    List[float]\n        A list containing 'num' values, linearly spaced between'start' and'stop'.\n\n    Notes\n    -----\n    This function uses numpy's linspace function to create an array of 'num' values evenly\n    distributed over the interval [start, stop]. The resulting array is then converted to a list.\n    \"\"\"\n    return np.linspace(start, stop, num).tolist()\n",
  "canonical_solution": "\n    # Generate linearly spaced values between 'start' and 'stop'\n    parames = list(\n        np.linspace(\n            start=start,\n            stop=stop,\n            num=num,\n        )\n    )\n    return parames\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(start=0, stop=10, num=5) == generate_linear_parameters(start=0, stop=10, num=5))\nassert(SOLUTION_SIGNATURE(start=0, stop=10, num=1) == generate_linear_parameters(start=0, stop=10, num=1))\nassert(SOLUTION_SIGNATURE(start=0, stop=10, num=0) == generate_linear_parameters(start=0, stop=10, num=0))\n"
 },
 {
  "task_id": "RWPB/46",
  "prompt": "from typing import List, Union\n\nimport numpy as np\ndef cosine_similarity(X: Union[List[List[float]], List[np.ndarray], np.ndarray], Y: Union[List[List[float]], List[np.ndarray], np.ndarray]) -> np.ndarray:\n    \"\"\"\n    Compute the row-wise cosine similarity between two matrices of equal width.\n    If the length of x is not equal the length of Y, return ValueError\n    \n    Parameters:\n    X (Union[List[List[float]], List[np.ndarray], np.ndarray]): A matrix (or a list of lists) where each row represents a vector.\n    Y (Union[List[List[float]], List[np.ndarray], np.ndarray]): Another matrix (or a list of lists) where each row represents a vector. Must have the same number of columns as X.\n\n    Returns:\n    np.ndarray: A matrix of cosine similarities. Each element at position (i, j) represents the cosine similarity between the i-th row of X and the j-th row of Y.\n\n    The process involves:\n    1. Converting the inputs X and Y to numpy arrays if they are not already in that format.\n    2. Checking that X and Y have the same number of columns to ensure they are comparable.\n    3. Calculating the L2 norm (Euclidean norm) of each row in X and Y.\n    4. Computing the cosine similarity using the dot product of X and Y, normalized by the outer product of the row norms.\n    5. Handling cases where division by zero or invalid operations might occur by setting such results to zero.\n    \"\"\"\n",
  "solution": "\nfrom typing import List, Union\n\nimport numpy as np\ndef cosine_similarity(X: Union[List[List[float]], List[np.ndarray], np.ndarray], Y: Union[List[List[float]], List[np.ndarray], np.ndarray]) -> np.ndarray:\n    \"\"\"\n    Compute the row-wise cosine similarity between two matrices of equal width.\n    If the length of x is not equal the length of Y, return ValueError\n    \n    Parameters:\n    X (Union[List[List[float]], List[np.ndarray], np.ndarray]): A matrix (or a list of lists) where each row represents a vector.\n    Y (Union[List[List[float]], List[np.ndarray], np.ndarray]): Another matrix (or a list of lists) where each row represents a vector. Must have the same number of columns as X.\n\n    Returns:\n    np.ndarray: A matrix of cosine similarities. Each element at position (i, j) represents the cosine similarity between the i-th row of X and the j-th row of Y.\n\n    The process involves:\n    1. Converting the inputs X and Y to numpy arrays if they are not already in that format.\n    2. Checking that X and Y have the same number of columns to ensure they are comparable.\n    3. Calculating the L2 norm (Euclidean norm) of each row in X and Y.\n    4. Computing the cosine similarity using the dot product of X and Y, normalized by the outer product of the row norms.\n    5. Handling cases where division by zero or invalid operations might occur by setting such results to zero.\n    \"\"\"\n    if isinstance(X, list) and isinstance(Y, list):\n        X = np.array(X)\n        Y = np.array(Y)\n    if X.shape[1]!= Y.shape[1]:\n        raise ValueError(\"X and Y must have the same number of columns.\")\n    x_norm = np.linalg.norm(X, axis=1)\n    y_norm = np.linalg.norm(Y, axis=1)\n    cosine_similarity_matrix = np.dot(X, Y.T) / np.outer(x_norm, y_norm)\n    cosine_similarity_matrix[np.isnan(cosine_similarity_matrix)] = 0\n    return cosine_similarity_matrix\n",
  "canonical_solution": "\n    if len(X) == 0 or len(Y) == 0:\n        return np.array([])\n\n    X = np.array(X)\n    Y = np.array(Y)\n    if X.shape[1] != Y.shape[1]:\n        return ValueError\n    \n    X_norm = np.linalg.norm(X, axis=1)\n    Y_norm = np.linalg.norm(Y, axis=1)\n    # Ignore divide by zero errors run time warnings as those are handled below.\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        similarity = np.dot(X, Y.T) / np.outer(X_norm, Y_norm)\n    similarity[np.isnan(similarity) | np.isinf(similarity)] = 0.0\n    return similarity\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE([[1, 2], [3, 4]], [[5, 6], [7, 8]]) == cosine_similarity([[1, 2], [3, 4]], [[5, 6], [7, 8]])).all())\nassert((SOLUTION_SIGNATURE([[0, 0], [1, 1]], [[1, 1], [0, 0]]) == cosine_similarity([[0, 0], [1, 1]], [[1, 1], [0, 0]])).all())\nassert(SOLUTION_SIGNATURE([[1, 2, 3], [4, 5, 6]], [[1, 2], [3, 4]]) == cosine_similarity([[1, 2, 3], [4, 5, 6]], [[1, 2], [3, 4]]))\nassert((SOLUTION_SIGNATURE(np.array([[1, 2], [3, 4]]), [[5, 6], [7, 8]]) == cosine_similarity(np.array([[1, 2], [3, 4]]), [[5, 6], [7, 8]])).all())\nassert((SOLUTION_SIGNATURE(np.array([[0, 0], [1, 1]]), np.array([[1, 1], [0, 0]])) == cosine_similarity(np.array([[0, 0], [1, 1]]), np.array([[1, 1], [0, 0]]))).all())\n"
 },
 {
  "task_id": "RWPB/57",
  "prompt": "def clean_folder_name(folder_name: str) -> str:\n    \"\"\"\n    Sanitizes a folder name by replacing characters that are invalid in file paths with underscores.\n\n    Arguments:\n    folder_name : str\n        The original folder name that may contain invalid characters.\n\n    Returns:\n    str\n        A sanitized version of the folder name where all invalid characters have been replaced with underscores.\n    \"\"\"\n",
  "solution": "\n\n\ndef clean_folder_name(folder_name: str) -> str:\n    \"\"\"\n    Sanitizes a folder name by replacing characters that are invalid in file paths with underscores.\n\n    Arguments:\n    folder_name : str\n        The original folder name that may contain invalid characters.\n\n    Returns:\n    str\n        A sanitized version of the folder name where all invalid characters have been replaced with underscores.\n    \"\"\"\n    invalid_chars = ['\\\\', '/', ':', '*', '?', '\"', '<', '>', '|']\n    sanitized_name = folder_name.replace(' ', '_')\n    for char in invalid_chars:\n        sanitized_name = sanitized_name.replace(char, '_')\n\n    return sanitized_name\n\n\n",
  "canonical_solution": "\n    cleaned_name = folder_name  # Start with the original folder name\n    invalid_chars = '<>:\"/\\\\|?*.'  # Define a string of characters that are invalid in file names\n\n    # Replace each invalid character with an underscore\n    for char in invalid_chars:\n        cleaned_name = cleaned_name.replace(char, \"_\")\n\n    return cleaned_name\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(\"Project<Name>:Version/1.2\\3|4*5?6\") == clean_folder_name(\"Project<Name>:Version/1.2\\3|4*5?6\"))\nassert(SOLUTION_SIGNATURE(\"Regular_Folder_Name\") == clean_folder_name(\"Regular_Folder_Name\"))\nassert(SOLUTION_SIGNATURE(\"<>>:|?**....\") == clean_folder_name(\"<>>:|?**....\"))\n"
 },
 {
  "task_id": "RWPB/32",
  "prompt": "import math\nimport torch\n\ndef repeat_to_batch_size(tensor, batch_size):\n    \"\"\"\n    Repeats the elements of the tensor to match the specified batch size or truncates it if larger.\n\n    Parameters\n    ----------\n    tensor : torch.Tensor\n        The input tensor which may need to be repeated or truncated.\n    batch_size : int\n        The target batch size to match the tensor's size.\n\n    Returns\n    -------\n    torch.Tensor\n        The tensor with its first dimension adjusted to match the batch size, either by\n        repeating elements or truncating the tensor.\n\n    Notes\n    -----\n    This function checks the size of the input tensor along its first dimension and performs\n    one of the following operations:\n    - If the tensor's size is greater than the batch size, it truncates the tensor to the batch size.\n    - If the tensor's size is smaller than the batch size, it repeats the elements of the tensor\n      to match the batch size, rounding up the number of repetitions if necessary.\n    - If the tensor's size is equal to the batch size, it returns the tensor unchanged.\n\n    The function uses PyTorch's repeat method to duplicate the tensor elements along the first\n    dimension and math.ceil to ensure that the tensor is repeated enough times to reach at least\n    the batch size.\n    \"\"\"\n",
  "solution": "\n\n\nimport math\nimport torch\n\ndef repeat_to_batch_size(tensor, batch_size):\n    \"\"\"\n    Repeats the elements of the tensor to match the specified batch size or truncates it if larger.\n\n    Parameters\n    ----------\n    tensor : torch.Tensor\n        The input tensor which may need to be repeated or truncated.\n    batch_size : int\n        The target batch size to match the tensor's size.\n\n    Returns\n    -------\n    torch.Tensor\n        The tensor with its first dimension adjusted to match the batch size, either by\n        repeating elements or truncating the tensor.\n\n    Notes\n    -----\n    This function checks the size of the input tensor along its first dimension and performs\n    one of the following operations:\n    - If the tensor's size is greater than the batch size, it truncates the tensor to the batch size.\n    - If the tensor's size is smaller than the batch size, it repeats the elements of the tensor\n      to match the batch size, rounding up the number of repetitions if necessary.\n    - If the tensor's size is equal to the batch size, it returns the tensor unchanged.\n\n    The function uses PyTorch's repeat method to duplicate the tensor elements along the first\n    dimension and math.ceil to ensure that the tensor is repeated enough times to reach at least\n    the batch size.\n    \"\"\"\n    tensor_size = tensor.size(0)\n    if tensor_size > batch_size:\n        return tensor[:batch_size]\n    elif tensor_size < batch_size:\n        num_repetitions = math.ceil(batch_size / tensor_size)\n        return tensor.repeat(num_repetitions, 1)\n    else:\n        return tensor\n\n",
  "canonical_solution": "\n    if tensor.shape[0] > batch_size:\n        # If the tensor is larger than the batch size, truncate it\n        return tensor[:batch_size]\n    elif tensor.shape[0] < batch_size:\n        # If the tensor is smaller than the batch size, calculate the number of repetitions\n        repeat_times = math.ceil(batch_size / tensor.shape[0])\n        # Repeat the tensor along the first dimension and then truncate to batch size\n        return tensor.repeat([repeat_times] + [1] * (len(tensor.shape) - 1))[:batch_size]\n    # If the tensor's size is equal to the batch size, return it as is\n    return tensor\n\n\n",
  "unprocess_testcases": "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\nb = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15], [16, 17, 18]])\nc = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n\nassert((SOLUTION_SIGNATURE(a, 4) == repeat_to_batch_size(a, 4)).all())\nassert((SOLUTION_SIGNATURE(b, 4) == repeat_to_batch_size(b, 4)).all())\nassert((SOLUTION_SIGNATURE(c, 4) == repeat_to_batch_size(c, 4)).all())\n"
 },
 {
  "task_id": "RWPB/63",
  "prompt": "import torch\nfrom einops import rearrange\n\n\n\ndef rotate_half(x):\n    \"\"\"\n    Rotates the last dimension of a tensor by 90 degrees counterclockwise,\n    assuming the last dimension represents 2D coordinates or complex numbers.\n\n    Arguments:\n    x : torch.Tensor\n        Input tensor with its last dimension size as a multiple of 2, representing 2D vectors.\n\n    Returns:\n    torch.Tensor\n        A tensor with the same shape as the input, where each 2D vector has been rotated.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\nfrom einops import rearrange\n\ndef rotate_half(x):\n    \"\"\"\n    Rotates the last dimension of a tensor by 90 degrees counterclockwise,\n    assuming the last dimension represents 2D coordinates or complex numbers.\n\n    Arguments:\n    x : torch.Tensor\n        Input tensor with its last dimension size as a multiple of 2, representing 2D vectors.\n\n    Returns:\n    torch.Tensor\n        A tensor with the same shape as the input, where each 2D vector has been rotated.\n    \"\"\"\n    x = rearrange(x, '... (i2 j2) ->... i2 j2', j2=2)\n    x = torch.flip(x, dims=[-1])\n    x = rearrange(x, '... i2 j2 ->... (i2 j2)')\n\n    return x\n\nx = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\nexpected_output = torch.tensor([[2.0, 4.0], [1.0, 3.0]])\nassert torch.allclose(rotate_half(x), expected_output)\n\nx = torch.tensor([[1.0+2.0j, 3.0+4.0j], [5.0+6.0j, 7.0+8.0j]])\nexpected_output = torch.tensor([[3.0+4.0j, 7.0+8.0j], [1.0+2.0j, 5.0+6.0j]])\nassert torch.allclose(rotate_half(x), expected_output)\n\nx = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nexpected_output = torch.tensor([[[2.0, 4.0], [1.0, 3.0]], [[6.0, 8.0], [5.0, 7.0]]])\nassert torch.allclose(rotate_half(x), expected_output)\n\nx = torch.tensor([[[1.0+2.0j, 3.0+4.0j], [5.0+6.0j, 7.0+8.0j]], [[9.0+10.0j, 11.0+12.0j], [13.0+14.0j, 15.0+16.0j]]])\nexpected_output = torch.tensor([[[3.0+4.0j, 7.0+8.0j], [1.0+2.0j, 5.0+6.0j]], [[11.0+12.0j, 15.0+16.0j], [9.0+10.0j, 13.0+14.0j]]])\nassert torch.allclose(rotate_half(x), expected_output)\n\n",
  "canonical_solution": "    \n    # Reshape x assuming the last dimension can be split into 2 parts\n    x = rearrange(x, '... (d r) -> ... d r', r=2)\n\n    # Unbind the last dimension into two separate tensors\n    x1, x2 = x.unbind(dim=-1)\n\n    # Rotate each vector by 90 degrees counterclockwise\n    x = torch.stack((-x2, x1), dim=-1)\n\n    # Rearrange back to the original dimension structure\n    return rearrange(x, '... d r -> ... (d r)')\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([[1, 0], [0, 1]])) == rotate_half(torch.tensor([[1, 0], [0, 1]]))).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[[1, 0, 0, 1], [0, 1, -1, 0]]])) == rotate_half(torch.tensor([[[1, 0, 0, 1], [0, 1, -1, 0]]]))).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[1, 0]])) == rotate_half(torch.tensor([[1, 0]]))).all())\n"
 },
 {
  "task_id": "RWPB/90",
  "prompt": "import torch\n\ndef scale(s, device=None):\n    \"\"\"\n    Creates a 4x4 homogeneous transformation matrix for uniform scaling in 3D space.\n\n    Parameters:\n    s (float): Scaling factor applied equally to the x, y, and z axes.\n    device (torch.device, optional): The device on which to create the tensor (e.g., 'cpu' or 'cuda'). Default is None, which means the tensor will be created on the default device.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the scaling transformation matrix.\n\n    Process:\n    1. Construct a 4x4 identity matrix.\n    2. Replace the diagonal elements with the scaling factor for the x, y, and z coordinates.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef scale(s, device=None):\n    \"\"\"\n    Creates a 4x4 homogeneous transformation matrix for uniform scaling in 3D space.\n\n    Parameters:\n    s (float): Scaling factor applied equally to the x, y, and z axes.\n    device (torch.device, optional): The device on which to create the tensor (e.g., 'cpu' or 'cuda'). Default is None, which means the tensor will be created on the default device.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the scaling transformation matrix.\n\n    Process:\n    1. Construct a 4x4 identity matrix.\n    2. Replace the diagonal elements with the scaling factor for the x, y, and z coordinates.\n    \"\"\"\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    identity_matrix = torch.eye(4, device=device)\n    identity_matrix[0, 0] = s\n    identity_matrix[1, 1] = s\n    identity_matrix[2, 2] = s\n    return identity_matrix\n\n",
  "canonical_solution": "    \n    return torch.tensor([[ s, 0, 0, 0], \n                         [ 0, s, 0, 0], \n                         [ 0, 0, s, 0], \n                         [ 0, 0, 0, 1]], dtype=torch.float32, device=device)\n\n\n",
  "unprocess_testcases": "a = torch.pi / 4\nassert((SOLUTION_SIGNATURE(a) == scale(a)).all())\n\na = 0\nassert((SOLUTION_SIGNATURE(a) == scale(a)).all())\n\na = 2 * torch.pi\nassert((SOLUTION_SIGNATURE(a) == scale(a)).all())\n"
 },
 {
  "task_id": "RWPB/80",
  "prompt": "import pandas as pd\n\ndef non_numeric_columns_and_head(df, num_rows=20):\n    \"\"\"\n    Identify non-numeric columns in a DataFrame and return their names and head.\n\n    :param df: Pandas DataFrame to be examined.\n    :param num_rows: Number of rows to include in the head (default is 20).\n    :return: A tuple with two elements:\n             1. List of column names that are not numeric (integer or float).\n             2. DataFrame containing the head of the non-numeric columns.\n    \"\"\"\n",
  "solution": "\n\n\nimport pandas as pd\n\ndef non_numeric_columns_and_head(df, num_rows=20):\n    \"\"\"\n    Identify non-numeric columns in a DataFrame and return their names and head.\n\n    :param df: Pandas DataFrame to be examined.\n    :param num_rows: Number of rows to include in the head (default is 20).\n    :return: A tuple with two elements:\n             1. List of column names that are not numeric (integer or float).\n             2. DataFrame containing the head of the non-numeric columns.\n    \"\"\"\n    non_numeric_columns = [col for col in df.columns if df[col].dtype not in ['int64', 'float64']]\n    non_numeric_head = df[non_numeric_columns].head(num_rows)\n    return non_numeric_columns, non_numeric_head\n\n\n",
  "canonical_solution": "    \n    # Identify columns that are not of numeric data type\n    non_numeric_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]\n    \n    # Get the head of the non-numeric columns\n    non_numeric_head = df[non_numeric_cols].head(num_rows).to_csv()\n    \n    return non_numeric_cols, non_numeric_head\n\n",
  "unprocess_testcases": "data = {\n  'id': [1, 2, 3, 4, 5],\n  'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n  'date_of_birth': pd.to_datetime(['1990-01-01', '1992-05-15', '1988-08-21', '1990-07-30', '1985-01-01']),\n  'salary': [50000.00, 60000.00, 55000.00, 45000.00, 70000.00],\n  'is_active': [True, False, True, False, True]\n}\ndf = pd.DataFrame(data)\nassert(SOLUTION_SIGNATURE(df)[0] == non_numeric_columns_and_head(df)[0])\nassert(SOLUTION_SIGNATURE(df)[1] == non_numeric_columns_and_head(df)[1])\n\ndata = {\n  'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n  'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'],\n  'email': ['alice@example.com', 'bob@example.com', 'charlie@example.com', 'david@example.com', 'eva@example.com']\n}\ndf = pd.DataFrame(data)\nassert(SOLUTION_SIGNATURE(df)[0] == non_numeric_columns_and_head(df)[0])\nassert(SOLUTION_SIGNATURE(df)[1] == non_numeric_columns_and_head(df)[1])\n\ndata = {\n  'id': [1, 2, 3],\n  'name': ['Alice', 'Bob', 'Charlie'],\n  'temperature': [98.6, 99.1, 98.7]\n}\ndf = pd.DataFrame(data)\nnum_rows = 10\nassert(SOLUTION_SIGNATURE(df)[0] == non_numeric_columns_and_head(df)[0])\nassert(SOLUTION_SIGNATURE(df)[1] == non_numeric_columns_and_head(df)[1])\n"
 },
 {
  "task_id": "RWPB/73",
  "prompt": "import numpy as np\n\n\ndef mse_to_psnr(mse):\n  \"\"\"\n  Compute the Peak Signal-to-Noise Ratio (PSNR) from the Mean Squared Error (MSE).\n  If mse is equal zero, the code need to return ValueError\n\n  Parameters:\n  mse (float): Mean Squared Error value. This value should be a non-negative float, \n               representing the average squared difference between the reference \n               and the distorted image.\n\n  Returns:\n  float: The PSNR value in decibels (dB).\n\n  Process:\n  - The function assumes that the maximum possible pixel value (I_max) in the image \n    is 1.\n  - The formula to convert MSE to PSNR is given by:\n      PSNR = -10 * log10(MSE)\n  - Use np.log to calculate.\n  \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\n\n\ndef mse_to_psnr(mse):\n  \"\"\"\n  Compute the Peak Signal-to-Noise Ratio (PSNR) from the Mean Squared Error (MSE).\n  If mse is equal zero, the code need to return ValueError\n\n  Parameters:\n  mse (float): Mean Squared Error value. This value should be a non-negative float, \n               representing the average squared difference between the reference \n               and the distorted image.\n\n  Returns:\n  float: The PSNR value in decibels (dB).\n\n  Process:\n  - The function assumes that the maximum possible pixel value (I_max) in the image \n    is 1.\n  - The formula to convert MSE to PSNR is given by:\n      PSNR = -10 * log10(MSE)\n  - Use np.log to calculate.\n  \"\"\"\n  if mse <= 0:\n      raise ValueError(\"mse must be a positive float\")\n  psnr = -10 * np.log10(mse)\n  return psnr\n\n\n",
  "canonical_solution": "  \n  if mse == 0:\n    return ValueError\n  \n  return -10. / np.log(10.) * np.log(mse)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(0.01) == mse_to_psnr(0.01))\nassert(SOLUTION_SIGNATURE(1e-10) == mse_to_psnr(1e-10))\nassert(SOLUTION_SIGNATURE(0) == mse_to_psnr(0))\nassert(SOLUTION_SIGNATURE(1) == mse_to_psnr(1))\nassert(SOLUTION_SIGNATURE(0.2312) == mse_to_psnr(0.2312))\n"
 },
 {
  "task_id": "RWPB/22",
  "prompt": "def get_stats(ids, counts=None):\n    \"\"\"\n    Computes the frequency of consecutive integer pairs in a list.\n\n    Args:\n        ids (list of int): List of integers whose consecutive pairs are to be counted.\n        counts (dict, optional): Existing dictionary to update with new counts. If None, a new dictionary is created.\n\n    Returns:\n        dict: A dictionary where each key is a tuple representing a pair of consecutive integers,\n              and the value is the count of how often each pair appears.\n\n    Example:\n        >>> get_stats([1, 2, 3, 1, 2])\n        {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n\n    This function iterates over consecutive elements in the provided list, creating pairs,\n    and either updates an existing dictionary or creates a new one to record the frequency of each pair.\n    \"\"\"\n",
  "solution": "\ndef get_stats(ids, counts=None):\n    if counts is None:\n        counts = {}\n    for i in range(len(ids) - 1):\n        pair = (ids[i], ids[i + 1])\n        counts[pair] = counts.get(pair, 0) + 1\n    return counts\n",
  "canonical_solution": "\n    counts = {} if counts is None else counts  # Initialize the dictionary if not provided\n    for pair in zip(ids, ids[1:]):  # Create pairs from consecutive elements\n        counts[pair] = counts.get(pair, 0) + 1  # Increment the count for each pair in the dictionary\n    return counts\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE([]) == get_stats([]))\nassert(SOLUTION_SIGNATURE([1, 2, 1, 2, 1, 2, 3, 4, 3, 4]) == get_stats([1, 2, 1, 2, 1, 2, 3, 4, 3, 4]))\nassert(SOLUTION_SIGNATURE([5, 6, 7, 5, 6], counts={(5, 6): 1, (6, 7): 2}) == get_stats([5, 6, 7, 5, 6], counts={(5, 6): 1, (6, 7): 2}))\n"
 },
 {
  "task_id": "RWPB/16",
  "prompt": "import numpy as np\n\ndef his_match(src, dst):\n    \"\"\"\n    Performs histogram matching between two color images src and dst.\n\n    Parameters\n    ----------\n    src : numpy.ndarray\n        Source image array of shape (H, W, 3) and type float64.\n    dst : numpy.ndarray\n        Destination image array of shape (H, W, 3) and type float64.\n\n    Returns\n    -------\n    numpy.ndarray\n        The matched image array of the same shape and type as src and dst.\n\n    Notes\n    -----\n    This function converts the images to uint8 type, calculates the cumulative\n    distribution functions (CDF) for each channel of the source and destination\n    images, and then finds the best match for each pixel in the destination image\n    in the source image's CDF. The matched values are then used to create a new\n    image that has a similar color distribution to the source image.\n    \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\n\ndef his_match(src, dst):\n    \"\"\"\n    Performs histogram matching between two color images src and dst.\n\n    Parameters\n    ----------\n    src : numpy.ndarray\n        Source image array of shape (H, W, 3) and type float64.\n    dst : numpy.ndarray\n        Destination image array of shape (H, W, 3) and type float64.\n\n    Returns\n    -------\n    numpy.ndarray\n        The matched image array of the same shape and type as src and dst.\n\n    Notes\n    -----\n    This function converts the images to uint8 type, calculates the cumulative\n    distribution functions (CDF) for each channel of the source and destination\n    images, and then finds the best match for each pixel in the destination image\n    in the source image's CDF. The matched values are then used to create a new\n    image that has a similar color distribution to the source image.\n    \"\"\"\n    src = np.clip(src, 0, 1)\n    dst = np.clip(dst, 0, 1)\n    src = (src * 255).astype(np.uint8)\n    dst = (dst * 255).astype(np.uint8)\n    src_cdf = np.zeros_like(src)\n    dst_cdf = np.zeros_like(dst)\n    for channel in range(3):\n        src_hist, _ = np.histogram(src[:, :, channel], bins=256, range=(0, 255))\n        src_cdf[:, :, channel] = np.cumsum(src_hist)\n        src_cdf[:, :, channel] = src_cdf[:, :, channel] / src_cdf[:, :, channel].max()\n        dst_hist, _ = np.histogram(dst[:, :, channel], bins=256, range=(0, 255))\n        dst_cdf[:, :, channel] = np.cumsum(dst_hist)\n        dst_cdf[:, :, channel] = dst_cdf[:, :, channel] / dst_cdf[:, :, channel].max()\n    matched = np.zeros_like(dst)\n    for channel in range(3):\n        matched_values = np.interp(dst[:, :, channel], src_cdf[:, :, channel], dst_cdf[:, :, channel])\n        matched[:, :, channel] = matched_values\n    matched = matched.astype(np.uint8)\n    matched = matched.astype(np.float64) / 255\n\n    return matched\n\n\n",
  "canonical_solution": "\n    # Scale images to [0, 255] range and convert to uint8 type\n    src = src * 255.0\n    dst = dst * 255.0\n    src = src.astype(np.uint8)\n    dst = dst.astype(np.uint8)\n    res = np.zeros_like(dst)\n\n    # Initialize CDF arrays for each channel\n    cdf_src = np.zeros((3, 256))\n    cdf_dst = np.zeros((3, 256))\n    cdf_res = np.zeros((3, 256))\n\n    # Keyword arguments for histogram calculation\n    kw = dict(bins=256, range=(0, 256), density=True)\n\n    # Calculate histograms and CDFs for each channel\n    for ch in range(3):\n        his_src, _ = np.histogram(src[:, :, ch], **kw)\n        hist_dst, _ = np.histogram(dst[:, :, ch], **kw)\n        cdf_src[ch] = np.cumsum(his_src)\n        cdf_dst[ch] = np.cumsum(hist_dst)\n\n        # Find the best match index for each pixel in the destination image\n        index = np.searchsorted(cdf_src[ch], cdf_dst[ch], side=\"left\")\n        np.clip(index, 0, 255, out=index)\n\n        # Assign the matched values to the result image\n        res[:, :, ch] = index[dst[:, :, ch]]\n\n        # Update histogram and CDF for the result image\n        his_res, _ = np.histogram(res[:, :, ch], **kw)\n        cdf_res[ch] = np.cumsum(his_res)\n\n    # Scale the result back to [0, 1] range and return\n    return res / 255.0\n\n\n\n",
  "unprocess_testcases": "a = np.random.rand(100, 100, 3)\nb = np.full((50, 50, 3), 0.5)\nc = np.full((50, 50, 3), 0.7)\nd = np.random.rand(64, 64, 3)\ne = np.random.rand(128, 128, 3)\n\nassert((SOLUTION_SIGNATURE(src = a, dst = a) == his_match(src = a, dst = a)).all())\nassert((SOLUTION_SIGNATURE(src = b, dst = c) == his_match(src = b, dst = c)).all())\nassert((SOLUTION_SIGNATURE(src = d, dst = e) == his_match(src = d, dst = e)).all())\n\n"
 },
 {
  "task_id": "RWPB/47",
  "prompt": "def disjoint_cycles(perm):\n    \"\"\"\n    Find the disjoint cycles of a permutation.\n\n    Args:\n        perm (list): The permutation as a list.\n\n    Returns:\n        list: A list of lists, where each inner list represents a disjoint cycle.\n    \"\"\"\n",
  "solution": "\n\n\ndef disjoint_cycles(perm):\n    \"\"\"\n    Find the disjoint cycles of a permutation.\n\n    Args:\n        perm (list): The permutation as a list.\n\n    Returns:\n        list: A list of lists, where each inner list represents a disjoint cycle.\n    \"\"\"\n    cycles = []\n    visited = set()\n\n    for i in range(len(perm)):\n        if i not in visited:\n            cycle = [i]\n            visited.add(i)\n            j = perm[i] - 1\n            while j!= i:\n                cycle.append(j)\n                visited.add(j)\n                j = perm[j] - 1\n            cycles.append(cycle)\n\n    return cycles\n\n\n",
  "canonical_solution": "\n    used = set()\n    cycles = []\n    minimum = 1\n    while len(used) < len(perm):\n        cycle = [minimum]\n        used.add(minimum)\n        elt = perm[minimum - 1]\n        while elt not in cycle:\n            cycle.append(elt)\n            used.add(elt)\n            elt = perm[elt - 1]\n        if len(set(perm).symmetric_difference(used)) != 0:\n            minimum = min(set(perm).symmetric_difference(used))\n        cycles.append(cycle)\n    return cycles\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE([2, 3, 4, 5, 1]) == disjoint_cycles([2, 3, 4, 5, 1]))\nassert(SOLUTION_SIGNATURE([2, 1, 4, 3, 6, 5]) == disjoint_cycles([2, 1, 4, 3, 6, 5]))\nassert(SOLUTION_SIGNATURE([1, 3, 2, 5, 4]) == disjoint_cycles([1, 3, 2, 5, 4]))\n"
 },
 {
  "task_id": "RWPB/67",
  "prompt": "MAX_VAL = (1 << 256) - 1\n\n# does not revert on overflow\ndef unsafeMul(x, y):\n    \"\"\"\n    Multiply two integers, ignoring overflow.\n    Args:\n        - x: int, first operand\n        - y: int, second operand\n    Returns:\n        result: int, the product of x and y, with overflow ignored\n    \"\"\"\n",
  "solution": "\n\n\nMAX_VAL = (1 << 256) - 1\n\ndef unsafeMul(x, y):\n    \"\"\"\n    Multiply two integers, ignoring overflow.\n    Args:\n        - x: int, first operand\n        - y: int, second operand\n    Returns:\n        result: int, the product of x and y, with overflow ignored\n    \"\"\"\n    result = x * y\n    if result > MAX_VAL:\n        result = MAX_VAL\n    return result\n\n\n",
  "canonical_solution": "    \n    return (x * y) & MAX_VAL\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(15, 10) == unsafeMul(15, 10))\nassert(SOLUTION_SIGNATURE((1 << 256) - 1, 1) == unsafeMul((1 << 256) - 1, 1))\nassert(SOLUTION_SIGNATURE(pow(2, 128), pow(2, 129)) == unsafeMul(pow(2, 128), pow(2, 129)))\n"
 },
 {
  "task_id": "RWPB/36",
  "prompt": "import numpy as np\n\ndef smooth_pose_seq(pose_seq, window_size=5):\n    \"\"\"\n    Smooths a sequence of poses using a moving average filter.\n\n    Args:\n        pose_seq (np.array): Array of pose data, where each pose is expected to be an array of coordinates.\n        window_size (int, optional): The number of frames to include in the moving average window. Default is 5.\n\n    Returns:\n        np.array: An array of smoothed poses with the same shape as the input.\n\n    This function applies a simple moving average smoothing technique to a sequence of poses, which is often useful\n    in motion analysis or animation to reduce noise or fluctuations between frames. The function handles edge cases by\n    adjusting the window size at the start and end of the sequence, ensuring all poses are smoothed.\n    \"\"\"\n",
  "solution": "\n\n\nimport numpy as np\n\ndef smooth_pose_seq(pose_seq, window_size=5):\n    \"\"\"\n    Smooths a sequence of poses using a moving average filter.\n\n    Args:\n        pose_seq (np.array): Array of pose data, where each pose is expected to be an array of coordinates.\n        window_size (int, optional): The number of frames to include in the moving average window. Default is 5.\n\n    Returns:\n        np.array: An array of smoothed poses with the same shape as the input.\n\n    This function applies a simple moving average smoothing technique to a sequence of poses, which is often useful\n    in motion analysis or animation to reduce noise or fluctuations between frames. The function handles edge cases by\n    adjusting the window size at the start and end of the sequence, ensuring all poses are smoothed.\n    \"\"\"\n    if window_size % 2 == 0:\n        window_size += 1\n    half_window = window_size // 2\n    smoothed_seq = np.zeros_like(pose_seq)\n    for i in range(len(pose_seq)):\n        start_idx = max(0, i - half_window)\n        end_idx = min(len(pose_seq), i + half_window + 1)\n        window_size = end_idx - start_idx\n        smoothed_seq[i] = np.mean(pose_seq[start_idx:end_idx], axis=0)\n\n    return smoothed_seq\n\n\n",
  "canonical_solution": "\n    # Initialize an array of zeros with the same shape as the input sequence to store the smoothed poses\n    smoothed_pose_seq = np.zeros_like(pose_seq)\n\n    # Iterate over each pose in the sequence\n    for i in range(len(pose_seq)):\n        # Calculate the start and end indices of the window, ensuring they are within the bounds of the sequence\n        start = max(0, i - window_size // 2)\n        end = min(len(pose_seq), i + window_size // 2 + 1)\n\n        # Compute the mean of the poses within the window and assign it to the smoothed sequence\n        smoothed_pose_seq[i] = np.mean(pose_seq[start:end], axis=0)\n\n    return smoothed_pose_seq\n\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([[1, 2], [2, 3], [3, 4]])) == smooth_pose_seq(np.array([[1, 2], [2, 3], [3, 4]]))).all())\nassert((SOLUTION_SIGNATURE(np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6], [7, 7], [8, 8]])) == smooth_pose_seq(np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6], [7, 7], [8, 8]]))).all())\nassert((SOLUTION_SIGNATURE(np.array([[1e10, 1e10], [1e-10, 1e-10], [1.5, 2.5], [0.001, 0.002], [5000, 5000]])) == smooth_pose_seq(np.array([[1e10, 1e10], [1e-10, 1e-10], [1.5, 2.5], [0.001, 0.002], [5000, 5000]]))).all())\n"
 },
 {
  "task_id": "RWPB/94",
  "prompt": "import torch\n\ndef lookAt(eye, at, up):\n    \"\"\"\n    Creates a view transformation matrix using the \"look-at\" method, which is commonly used in 3D graphics to create a camera view matrix.\n\n    Parameters:\n    eye (torch.Tensor): A 1D tensor with 3 elements representing the position of the camera in world coordinates.\n    at (torch.Tensor): A 1D tensor with 3 elements representing the point in world coordinates that the camera is looking at.\n    up (torch.Tensor): A 1D tensor with 3 elements representing the up direction of the camera.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the view transformation matrix.\n\n    Process:\n    1. Compute the forward vector `w` as the normalized vector pointing from `at` to `eye`.\n    2. Compute the right vector `u` as the normalized cross product of `up` and `w`.\n    3. Compute the true up vector `v` as the cross product of `w` and `u`.\n    4. Construct the translation matrix to move the scene so that the camera is at the origin.\n    5. Construct the rotation matrix to align the camera's orientation with the computed `u`, `v`, and `w` vectors.\n    6. Combine the translation and rotation matrices by matrix multiplication to get the final view transformation matrix.\n    \"\"\"\n",
  "solution": "\nimport torch\n\ndef lookAt(eye, at, up):\n    \"\"\"\n    Creates a view transformation matrix using the \"look-at\" method, which is commonly used in 3D graphics to create a camera view matrix.\n\n    Parameters:\n    eye (torch.Tensor): A 1D tensor with 3 elements representing the position of the camera in world coordinates.\n    at (torch.Tensor): A 1D tensor with 3 elements representing the point in world coordinates that the camera is looking at.\n    up (torch.Tensor): A 1D tensor with 3 elements representing the up direction of the camera.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the view transformation matrix.\n\n    Process:\n    1. Compute the forward vector `w` as the normalized vector pointing from `at` to `eye`.\n    2. Compute the right vector `u` as the normalized cross product of `up` and `w`.\n    3. Compute the true up vector `v` as the cross product of `w` and `u`.\n    4. Construct the translation matrix to move the scene so that the camera is at the origin.\n    5. Construct the rotation matrix to align the camera's orientation with the computed `u`, `v`, and `w` vectors.\n    6. Combine the translation and rotation matrices by matrix multiplication to get the final view transformation matrix.\n    \"\"\"\n    w = (at - eye).normalize()\n    u = torch.cross(up, w).normalize()\n    v = torch.cross(w, u)\n    translation = torch.eye(4)\n    translation[:3, 3] = -eye\n    rotation = torch.eye(4)\n    rotation[:3, :3] = torch.stack([u, v, w], dim=1)\n    view_matrix = torch.matmul(rotation, translation)\n\n    return view_matrix\n",
  "canonical_solution": "    \n    a = eye - at\n    w = a / torch.linalg.norm(a)\n    u = torch.cross(up, w)\n    u = u / torch.linalg.norm(u)\n    v = torch.cross(w, u)\n    translate = torch.tensor([[1, 0, 0, -eye[0]], \n                              [0, 1, 0, -eye[1]], \n                              [0, 0, 1, -eye[2]], \n                              [0, 0, 0, 1]], dtype=eye.dtype, device=eye.device)\n    rotate = torch.tensor([[u[0], u[1], u[2], 0], \n                           [v[0], v[1], v[2], 0], \n                           [w[0], w[1], w[2], 0], \n                           [0, 0, 0, 1]], dtype=eye.dtype, device=eye.device)\n    return rotate @ translate\n\n\n",
  "unprocess_testcases": "\neye = torch.tensor([1.0, 1, 1])\nat = torch.tensor([0.0, 0, 0])\nup = torch.tensor([0.0, 1, 0])\nassert((SOLUTION_SIGNATURE(eye, at, up) == lookAt(eye, at, up)).all())\n\neye = torch.tensor([0.0, 0, 1])\nat = torch.tensor([0.0, 0, 0])\nup = torch.tensor([0.0, 1, 0])\nassert((SOLUTION_SIGNATURE(eye, at, up) == lookAt(eye, at, up)).all())\n\neye = torch.tensor([-3.0, 2, 5])\nat = torch.tensor([4.0, -1, 0])\nup = torch.tensor([1.0, 1, 1])\nassert((SOLUTION_SIGNATURE(eye, at, up) == lookAt(eye, at, up)).all())\n"
 },
 {
  "task_id": "RWPB/53",
  "prompt": "import re\n\ndef general_detokenize(string):\n    \"\"\"\n    Detokenizes a string by fixing spacing around punctuation and contractions.\n\n    Arguments:\n    string : str\n        The input string that needs to be detokenized.\n\n    Returns:\n    str\n        The detokenized string with corrected spacing.\n    \"\"\"\n",
  "solution": "\n\n\nimport re\n\ndef general_detokenize(string):\n    \"\"\"\n    Detokenizes a string by fixing spacing around punctuation and contractions.\n\n    Arguments:\n    string : str\n        The input string that needs to be detokenized.\n\n    Returns:\n    str\n        The detokenized string with corrected spacing.\n    \"\"\"\n    string = re.sub(r\" ([,.!?:;])\", r\"\\1\", string)\n    string = re.sub(r\" ([`'\\\"]) \", r\"\\1\", string)\n    string = re.sub(r\" ([`'\\\"])$\", r\"\\1\", string)\n    string = re.sub(r\"^([`'\\\"]) \", r\"\\1\", string)\n    string = re.sub(r\"^([`'\\\"])$\", r\"\\1\", string)\n    string = re.sub(r\"^' \", r\"'\", string)\n    string = re.sub(r\"'$\", r\"'\", string)\n    string = re.sub(r\"^'$\", r\"'\", string)\n    string = re.sub(r\"^\\\" \", r\"\\\"\", string)\n    string = re.sub(r\"\\\"$\", r\"\\\"\", string)\n    string = re.sub(r\"^\\\"$\", r\"\\\"\", string)\n    string = re.sub(r\"^` \", r\"`\", string)\n    string = re.sub(r\"`$\", r\"`\", string)\n    string = re.sub(r\"^`$\", r\"`\", string)\n    string = re.sub(r\"^, \", r\",\", string)\n    string = re.sub(r\"^; \", r\";\", string)\n    string = re.sub(r\"^: \", r\":\", string)\n    string = re.sub(r\"^! \", r\"!\", string)\n    string = re.sub(r\"^\\\\? \", r\"?\", string)\n    string = re.sub(r\"^\\\\. \", r\".\", string)\n    string = re.sub(r\"^- \", r\"-\", string)\n    string = re.sub(r\"^-- \", r\"--\", string)\n    string = re.sub(r\"^--$\", r\"--\", string)\n    string = re.sub(r\"^-- $\", r\"--\", string)\n    string = re.sub(r\"^--, \", r\"--,\", string)\n    string = re.sub(r\"^--; \", r\"--;\", string)\n    string = re.sub(r\"^--: \", r\"--:\", string)\n    string = re.sub(r\"^--! \", r\"--!\", string)\n    string = re.sub(r\"^--\\\\? \", r\"--?\", string)\n    string = re.sub(r\"^--\\\\. \", r\"--.\", string)\n    string = re.sub(r\"^-- -\", r\"--\", string)\n    string = re.sub(r\"^--$\", r\"--\", string)\n    string = re.sub(r\"^-- $\", r\"--\", string)\n    string = re.sub(r\"^--, \", r\"--,\", string)\n    string = re.sub(r\"^--; \", r\"--;\", string)\n    string = re.sub(r\"^--: \", r\"--:\", string)\n    string = re.sub(r\"^--! \", r\"--!\", string)\n    string = re.sub(r\"^--\\\\? \", r\"--?\", string)\n    string = re.sub(r\"^--\\\\. \", r\"--.\", string)\n    string = re.sub(r\"^-- -\", r\"--\", string)\n    string = re.sub(r\"^--$\", r\"--\", string)\n    string = re.sub(r\"^-- $\", r\"--\", string)\n    string = re.sub(r\"^--, \", r\"--,\", string)\n    string = re.sub(r\"^--; \", r\"--;\", string)\n    string = re.sub(r\"^--: \", r\"--:\", string)\n    string = re.sub(r\"^--! \", r\"--!\", string)\n    string = re.sub(r\"^--\\\\? \", r\"--?\", string)\n    string = re.sub(r\"^--\\\\. \", r\"--.\", string)\n    string = re.sub(r\"^-- -\", r\"--\", string)\n    string = re.sub(r\"^--$\", r\"--\", string)\n    string = re.sub(r\"^-- $\", r\"--\", string)\n    string = re.sub(r\"^--, \", r\"--,\",",
  "canonical_solution": "\n    # Fix the space before contractions like \"n't\" (e.g., \" ca n't\" -> \"can't\")\n    string = string.replace(\" n't\", \"n't\")\n\n    # Remove space before closing parentheses and after opening parentheses\n    string = string.replace(\" )\", \")\")\n    string = string.replace(\"( \", \"(\")\n\n    # Correct spacing issues with quotation marks\n    string = string.replace('\" ', '\"')\n    string = string.replace(' \"', '\"')\n\n    # Use regular expressions to remove space before commas and periods, and other single character punctuations\n    string = re.sub(r\" (['.,])\", r\"\\1\", string)\n\n    return string\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE('She said , \" Hello world ! \" I can not believe it ; can you ? Yes , I ca n\\'t .') == general_detokenize('She said , \" Hello world ! \" I can not believe it ; can you ? Yes , I ca n\\'t .'))\nassert(SOLUTION_SIGNATURE('We need ( in an ideal world ) a kind of setup that works ( efficiently and effectively ).') == general_detokenize('We need ( in an ideal world ) a kind of setup that works ( efficiently and effectively ).'))\nassert(SOLUTION_SIGNATURE('This is a test . What do you think ? Isn\\'t it interesting ?') == general_detokenize('This is a test . What do you think ? Isn\\'t it interesting ?'))\n"
 },
 {
  "task_id": "RWPB/43",
  "prompt": "import torch\nimport numpy as np\n\n\ndef inv(mat):\n    \"\"\"\n    Inverts a matrix that can be either a torch.Tensor or a numpy.ndarray.\n\n    Parameters\n    ----------\n    mat : torch.Tensor or np.ndarray\n        A square matrix to be inverted.\n\n    Returns\n    -------\n    torch.Tensor or np.ndarray\n        The inverse of the input matrix, with the same type as the input.\n\n    Raises\n    ------\n    ValueError\n        If the input is not a torch.Tensor or np.ndarray.\n\n    Notes\n    -----\n    This function checks the type of the input matrix and applies the appropriate inverse\n    operation using either PyTorch's torch.linalg.inv for tensor objects or NumPy's\n    np.linalg.inv for ndarray objects. It is important that the input matrix is square\n    and non-singular (i.e., it has an inverse), otherwise the inversion will fail.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\nimport numpy as np\n\n\ndef inv(mat):\n    \"\"\"\n    Inverts a matrix that can be either a torch.Tensor or a numpy.ndarray.\n\n    Parameters\n    ----------\n    mat : torch.Tensor or np.ndarray\n        A square matrix to be inverted.\n\n    Returns\n    -------\n    torch.Tensor or np.ndarray\n        The inverse of the input matrix, with the same type as the input.\n\n    Raises\n    ------\n    ValueError\n        If the input is not a torch.Tensor or np.ndarray.\n\n    Notes\n    -----\n    This function checks the type of the input matrix and applies the appropriate inverse\n    operation using either PyTorch's torch.linalg.inv for tensor objects or NumPy's\n    np.linalg.inv for ndarray objects. It is important that the input matrix is square\n    and non-singular (i.e., it has an inverse), otherwise the inversion will fail.\n    \"\"\"\n    if isinstance(mat, torch.Tensor):\n        return torch.linalg.inv(mat)\n    elif isinstance(mat, np.ndarray):\n        return np.linalg.inv(mat)\n    else:\n        raise ValueError(\"Input must be a torch.Tensor or np.ndarray.\")\n\n\n",
  "canonical_solution": "    \n    # Check if the input matrix is a torch.Tensor and invert it using torch.linalg.inv\n    if isinstance(mat, torch.Tensor):\n        return torch.linalg.inv(mat)\n\n    # Check if the input matrix is a np.ndarray and invert it using np.linalg.inv\n    if isinstance(mat, np.ndarray):\n        return np.linalg.inv(mat)\n\n    # If the input is neither a torch.Tensor nor a np.ndarray, raise a ValueError\n    return ValueError\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([[4.0, 7.0], [2.0, 6.0]])) == inv(torch.tensor([[4.0, 7.0], [2.0, 6.0]]))).all())\nassert((SOLUTION_SIGNATURE(np.array([[1, 2, 3], [0, 1, 4], [5, 6, 0]])) == inv(np.array([[1, 2, 3], [0, 1, 4], [5, 6, 0]]))).all())\nassert(SOLUTION_SIGNATURE([[1, 2], [3, 4]]) == inv([[1, 2], [3, 4]]))\n"
 },
 {
  "task_id": "RWPB/12",
  "prompt": "import torch\n\n\ndef normalize_activation(x, eps=1e-10):\n    \"\"\"\n    Normalize activation values along the channel dimension.\n    Args:\n        - x: Tensor, the input activation tensor.\n        - eps: float, optional, small value to avoid division by zero.\n    Return:\n        normalized_activation: Tensor, the normalized activation tensor.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\n\ndef normalize_activation(x, eps=1e-10):\n    \"\"\"\n    Normalize activation values along the channel dimension.\n    Args:\n        - x: Tensor, the input activation tensor.\n        - eps: float, optional, small value to avoid division by zero.\n    Return:\n        normalized_activation: Tensor, the normalized activation tensor.\n    \"\"\"\n    mean = torch.mean(x, dim=1, keepdim=True)\n    std = torch.std(x, dim=1, keepdim=True)\n    normalized_activation = (x - mean) / (std + eps)\n    return normalized_activation\n\n\n",
  "canonical_solution": "    \n    norm_factor = torch.sqrt(torch.sum(x ** 2, dim=1, keepdim=True))\n    return x / (norm_factor + eps)\n\n\n\n",
  "unprocess_testcases": "a = torch.rand([1, 3, 5, 5])\nb = torch.rand([2, 3, 4, 4])\nc = torch.rand([1, 1, 1, 1])\n\nassert((SOLUTION_SIGNATURE(a) == normalize_activation(a)).all())\nassert((SOLUTION_SIGNATURE(b) == normalize_activation(b)).all())\nassert((SOLUTION_SIGNATURE(c) == normalize_activation(c)).all())\n"
 },
 {
  "task_id": "RWPB/84",
  "prompt": "import torch\n\ndef build_homog_matrix(R, t=None):\n    \"\"\"\n    Create a batch of homogeneous transformation matrices from given rotation matrices and translation vectors.\n    If the shape of R do not match the shape of t, please return ValueError\n\n    Args:\n    R (torch.Tensor): Rotation matrices of shape (B, Nj, 3, 3), where B is the batch size, and Nj is the number of joints.\n    t (torch.Tensor, optional): Translation vectors of shape (B, Nj, 3, 1). If None, a zero translation vector is used.\n    \n    Returns:\n    torch.Tensor: Homogeneous transformation matrices of shape (B, Nj, 4, 4).\n    \n    Process:\n    1. If the translation vector `t` is not provided, initialize it to zero vectors of the appropriate shape.\n    2. If the rotation matrix `R` is not provided, initialize it to identity matrices of the appropriate shape.\n    3. Ensure that the shapes of `R` and `t` match the expected dimensions.\n    4. Concatenate the rotation matrix `R` and translation vector `t` along the last dimension to form a (B, Nj, 3, 4) local transformation matrix.\n    5. Create a padding row [0, 0, 0, 1] and expand it to match the batch and joint dimensions, forming a (B, Nj, 1, 4) tensor.\n    6. Concatenate the padded row to the local transformation matrix to create a final homogeneous matrix of shape (B, Nj, 4, 4).\n    7. Return the resulting homogeneous transformation matrix.\n    \"\"\"\n",
  "solution": "\nimport torch\n\ndef build_homog_matrix(R, t=None):\n    \"\"\"\n    Create a batch of homogeneous transformation matrices from given rotation matrices and translation vectors.\n    If the shape of R do not match the shape of t, please return ValueError\n\n    Args:\n    R (torch.Tensor): Rotation matrices of shape (B, Nj, 3, 3), where B is the batch size, and Nj is the number of joints.\n    t (torch.Tensor, optional): Translation vectors of shape (B, Nj, 3, 1). If None, a zero translation vector is used.\n    \n    Returns:\n    torch.Tensor: Homogeneous transformation matrices of shape (B, Nj, 4, 4).\n    \n    Process:\n    1. If the translation vector `t` is not provided, initialize it to zero vectors of the appropriate shape.\n    2. If the rotation matrix `R` is not provided, initialize it to identity matrices of the appropriate shape.\n    3. Ensure that the shapes of `R` and `t` match the expected dimensions.\n    4. Concatenate the rotation matrix `R` and translation vector `t` along the last dimension to form a (B, Nj, 3, 4) local transformation matrix.\n    5. Create a padding row [0, 0, 0, 1] and expand it to match the batch and joint dimensions, forming a (B, Nj, 1, 4) tensor.\n    6. Concatenate the padded row to the local transformation matrix to create a final homogeneous matrix of shape (B, Nj, 4, 4).\n    7. Return the resulting homogeneous transformation matrix.\n    \"\"\"\n    if t is None:\n        t = torch.zeros_like(R[:, :, :, :1])\n    if R is None:\n        R = torch.eye(3).unsqueeze(0).unsqueeze(1).expand_as(t)\n    if R.shape!= t.shape:\n        raise ValueError(\"The shape of R and t must match.\")\n    local_transform = torch.cat([R, t], dim=-1)\n    padding_row = torch.zeros_like(local_transform[:, :, :1, :])\n    padding_row[:, :, :, -1] = 1\n    homog_matrix = torch.cat([local_transform, padding_row], dim=-2)\n\n    return homog_matrix\n",
  "canonical_solution": "    \n    \n    if t is None:\n        B = R.shape[0]\n        Nj = R.shape[1]\n        t = torch.zeros(B, Nj, 3, 1).to(R.device)\n    \n    if R is None:\n        B = t.shape[0]\n        Nj = t.shape[1]\n        R = torch.eye(3).unsqueeze(0).unsqueeze(0).repeat(B, Nj, 1, 1).to(t.device)\n    \n    B = t.shape[0]\n    Nj = t.shape[1]\n        \n    # import ipdb; ipdb.set_trace()\n    if R.shape != (B, Nj, 3, 3) or t.shape != (B, Nj, 3, 1):\n        return KeyError\n    \n    G = torch.cat([R, t], dim=-1) # BxJx3x4 local transformation matrix\n    pad_row = torch.FloatTensor([0, 0, 0, 1]).to(R.device).view(1, 1, 1, 4).expand(B, Nj, -1, -1) # BxJx1x4\n    G = torch.cat([G, pad_row], dim=2) # BxJx4x4 padded to be 4x4 matrix an enable multiplication for the kinematic chain\n\n    return G\n\n\n",
  "unprocess_testcases": "R = torch.eye(3).unsqueeze(0).unsqueeze(0).repeat(2, 3, 1, 1)\nt = torch.tensor([[[[1.0], [2.0], [3.0]], [[4.0], [5.0], [6.0]], [[7.0], [8.0], [9.0]]],\n                  [[[1.0], [0.0], [1.0]], [[2.0], [2.0], [2.0]], [[3.0], [3.0], [3.0]]]])\nassert(((SOLUTION_SIGNATURE(R=R, t=t) == build_homog_matrix(R=R, t=t))).all())\n\nR = torch.tensor([[[[0.0, -1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],\n                   [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]]])\nassert(SOLUTION_SIGNATURE(R=R, t=t) == build_homog_matrix(R=R, t=t))\n\nR = torch.eye(3).unsqueeze(0).unsqueeze(0).repeat(3, 2, 1, 1)\nt = torch.zeros(2, 3, 3, 1)  # Intentional shape mismatch to trigger an assertion error\nassert(SOLUTION_SIGNATURE(R=R, t=t) == build_homog_matrix(R=R, t=t))\n"
 },
 {
  "task_id": "RWPB/26",
  "prompt": "import torch\n\ndef to_one_hot(tensor, num_cls, dim=1, ignore_index=255):\n    \"\"\"\n    Converts a given tensor to a one-hot encoded tensor.\n\n    Args:\n        tensor (torch.Tensor): A tensor containing class indices.\n        num_cls (int): The total number of classes.\n        dim (int, optional): The dimension along which to apply one-hot encoding. Default is 1.\n        ignore_index (int, optional): A specific index that should be ignored during the conversion. Default is 255.\n\n    Returns:\n        torch.Tensor: A one-hot encoded tensor.\n\n    This function takes a tensor with class indices (usually integers) and converts it into a one-hot\n    encoded format. Entries with the 'ignore_index' are treated as background and set to zero before encoding.\n    The function uses the scatter_ method to expand the tensor along the specified 'dim' dimension\n    and sets the corresponding index to 1, indicating class presence.\n    \"\"\"\n",
  "solution": "\n\n\nimport torch\n\ndef to_one_hot(tensor, num_cls, dim=1, ignore_index=255):\n    \"\"\"\n    Converts a given tensor to a one-hot encoded tensor.\n\n    Args:\n        tensor (torch.Tensor): A tensor containing class indices.\n        num_cls (int): The total number of classes.\n        dim (int, optional): The dimension along which to apply one-hot encoding. Default is 1.\n        ignore_index (int, optional): A specific index that should be ignored during the conversion. Default is 255.\n\n    Returns:\n        torch.Tensor: A one-hot encoded tensor.\n\n    This function takes a tensor with class indices (usually integers) and converts it into a one-hot\n    encoded format. Entries with the 'ignore_index' are treated as background and set to zero before encoding.\n    The function uses the scatter_ method to expand the tensor along the specified 'dim' dimension\n    and sets the corresponding index to 1, indicating class presence.\n    \"\"\"\n    tensor = tensor.clone()\n    tensor[tensor == ignore_index] = 0\n    one_hot = torch.zeros(tensor.size()[0], num_cls, *tensor.size()[2:], dtype=tensor.dtype, device=tensor.device)\n    one_hot.scatter_(dim, tensor.unsqueeze(dim), 1)\n    return one_hot\n\n",
  "canonical_solution": "\n    # Get dimensions of the input tensor\n    b, h, w = tensor.shape\n\n    # Set ignore_index to 0 to treat it as background\n    tensor[tensor == ignore_index] = 0\n\n    # Initialize a new tensor of zeros with dimensions for batch, classes, height, and width\n    onehot_tensor = torch.zeros(b, num_cls, h, w)\n\n    # Perform the one-hot encoding using scatter_\n    onehot_tensor.scatter_(dim, tensor.unsqueeze(dim), 1)\n\n    return onehot_tensor\n\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([[[1, 2], [0, 1]]]), num_cls = 3) == to_one_hot(torch.tensor([[[1, 2], [0, 1]]]), num_cls = 3)).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[[255, 1], [2, 255]]]), num_cls = 4, ignore_index = 255) == to_one_hot(torch.tensor([[[255, 1], [2, 255]]]), num_cls = 4, ignore_index = 255)).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[[0, 1, 2, 3, 4, 5]]]), num_cls = 1000) == to_one_hot(torch.tensor([[[0, 1, 2, 3, 4, 5]]]), num_cls = 1000)).all())\n"
 },
 {
  "task_id": "RWPB/77",
  "prompt": "def is_power_of_two(n):\n    \"\"\"\n    chat.openai.com/chat\n    Return True if n is a power of 2, otherwise return False.\n\n    The function is_power_of_two takes an integer n as input and returns True if n is a power of 2, otherwise it returns False.\n    The function works by first checking if n is less than or equal to 0. If n is less than or equal to 0, it can't be a power of 2, so the function returns False.\n    If n is greater than 0, the function checks whether n is a power of 2 by using a bitwise AND operation between n and n-1. If n is a power of 2, then it will have only one bit set to 1 in its binary representation. When we subtract 1 from a power of 2, all the bits to the right of that bit become 1, and the bit itself becomes 0. So, when we perform a bitwise AND between n and n-1, we get 0 if n is a power of 2, and a non-zero value otherwise.\n    Thus, if the result of the bitwise AND operation is 0, then n is a power of 2 and the function returns True. Otherwise, the function returns False.\n\n    \"\"\"\n",
  "solution": "\n\n\ndef is_power_of_two(n):\n    return n > 0 and (n & (n - 1)) == 0\n\n\n",
  "canonical_solution": "    \n    if n <= 0:\n        return False\n    return (n & (n - 1)) == 0\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(0) == is_power_of_two(0))\nassert(SOLUTION_SIGNATURE(-16) == is_power_of_two(-16))\nassert(SOLUTION_SIGNATURE(16) == is_power_of_two(16))\nassert(SOLUTION_SIGNATURE(1024) == is_power_of_two(1024))\nassert(SOLUTION_SIGNATURE(18) == is_power_of_two(18))\nassert(SOLUTION_SIGNATURE(999) == is_power_of_two(999))\n"
 }
]