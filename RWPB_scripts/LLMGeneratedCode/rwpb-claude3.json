[
 {
  "task_id": "RWPB/109",
  "prompt": "import torch\n\ndef rot_trans_geo(geometry, rot, trans):\n    \"\"\"\n    Applies rotation and translation to geometry.\n\n    Args:\n    geometry (torch.Tensor): Tensor of shape [batch_size, num_points, 3] representing the geometry points.\n    rot (torch.Tensor): Rotation matrix of shape [batch_size, 3, 3].\n    trans (torch.Tensor): Translation vector of shape [batch_size, 3].\n\n    Returns:\n    torch.Tensor: Transformed geometry of shape [batch_size, num_points, 3].\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport torch\n\ndef rot_trans_geo(geometry, rot, trans):\n    \"\"\"\n    Applies rotation and translation to geometry.\n\n    Args:\n    geometry (torch.Tensor): Tensor of shape [batch_size, num_points, 3] representing the geometry points.\n    rot (torch.Tensor): Rotation matrix of shape [batch_size, 3, 3].\n    trans (torch.Tensor): Translation vector of shape [batch_size, 3].\n\n    Returns:\n    torch.Tensor: Transformed geometry of shape [batch_size, num_points, 3].\n    \"\"\"\n    # Ensure the input tensors have the correct shape\n    assert geometry.dim() == 3 and geometry.size(-1) == 3, \"geometry should have shape [batch_size, num_points, 3]\"\n    assert rot.dim() == 3 and rot.size(1) == 3 and rot.size(2) == 3, \"rot should have shape [batch_size, 3, 3]\"\n    assert trans.dim() == 2 and trans.size(-1) == 3, \"trans should have shape [batch_size, 3]\"\n\n    # Apply rotation to the geometry\n    rotated_geometry = torch.matmul(geometry, rot.transpose(1, 2))\n\n    # Apply translation to the rotated geometry\n    transformed_geometry = rotated_geometry + trans.unsqueeze(1)\n\n    return transformed_geometry\n```\n\nExplanation:\n1. The function `rot_trans_geo` takes three arguments: `geometry`, `rot`, and `trans`.\n2. We first ensure that the input tensors have the correct shape using `assert` statements. This helps catch any potential errors early in the function.\n3. To apply rotation to the geometry, we use `torch.matmul` to perform matrix multiplication between the `geometry` tensor and the transpose of the `rot` tensor. The transpose is necessary because the rotation matrix should be multiplied from the right side of the geometry points.\n4. To apply translation to the rotated geometry, we add the `trans` tensor to the `rotated_geometry` tensor. Since `trans` has shape `[batch_size, 3]`, we use `unsqueeze(1)` to add a new dimension at index 1, making it compatible with the shape of `rotated_geometry`.\n5. Finally, we return the `transformed_geometry` tensor, which has the same shape as the input `geometry` tensor.\n\nThis function applies the given rotation and translation to the input geometry points and returns the transformed geometry.",
  "canonical_solution": "    \n    rott_geo = torch.bmm(rot, geometry.permute(0, 2, 1)) + trans.view(-1, 3, 1)\n    return rott_geo.permute(0, 2, 1)\n\n\n",
  "unprocess_testcases": "geometry = torch.tensor([[[1., 2., 3.],\n                          [4., 5., 6.]]])\nrot = torch.eye(3).unsqueeze(0)  # Identity rotation for 1 batch\ntrans = torch.zeros(1, 3)        # Zero translation for 1 batch\nassert((SOLUTION_SIGNATURE(geometry, rot, trans) == rot_trans_geo(geometry, rot, trans)).all())\n\ngeometry = torch.tensor([[[1., 0., 0.],\n                          [0., 1., 0.]],\n                         [[0., 0., 1.],\n                          [1., 1., 1.]]])\nrot = torch.tensor([[[0., -1., 0.], [1., 0., 0.], [0., 0., 1.]],  # 90 degrees on Z-axis\n                    [[-1., 0., 0.], [0., 1., 0.], [0., 0., -1.]]]) # 180 degrees on Y-axis\ntrans = torch.tensor([[1., 2., 3.],\n                      [0., -1., 2.]])\nassert((SOLUTION_SIGNATURE(geometry, rot, trans) == rot_trans_geo(geometry, rot, trans)).all())\n\ngeometry = torch.tensor([[[0., 0., 0.],\n                          [999., 999., 999.]]])\nrot = torch.tensor([[[0., 1., 0.], [-1., 0., 0.], [0., 0., 1.]]])  # 90 degrees on X-axis\ntrans = torch.tensor([[1000., 2000., 3000.]])\nassert((SOLUTION_SIGNATURE(geometry, rot, trans) == rot_trans_geo(geometry, rot, trans)).all())\n"
 },
 {
  "task_id": "RWPB/54",
  "prompt": "def get_crop_box(box, expand):\n    \"\"\"\n    Expands a given bounding box by a specified factor, keeping the center of the box the same.\n\n    Arguments:\n    box : list or tuple\n        The original bounding box specified as (x, y, x1, y1) where (x, y) is the top-left corner and\n        (x1, y1) is the bottom-right corner.\n    expand : float\n        The factor by which to expand the bounding box. For example, a factor of 1.5 will increase\n        the size of the box by 50%.\n\n    Returns:\n    tuple\n        A tuple containing the new expanded bounding box as a list [new_x, new_y, new_x1, new_y1] and\n        the half-size of the new box `s`.\n    \"\"\"\n",
  "solution": "Sure! Here's the completed code to solve the problem without providing unit test cases:\n\n```python\ndef get_crop_box(box, expand):\n    \"\"\"\n    Expands a given bounding box by a specified factor, keeping the center of the box the same.\n\n    Arguments:\n    box : list or tuple\n        The original bounding box specified as (x, y, x1, y1) where (x, y) is the top-left corner and\n        (x1, y1) is the bottom-right corner.\n    expand : float\n        The factor by which to expand the bounding box. For example, a factor of 1.5 will increase\n        the size of the box by 50%.\n\n    Returns:\n    tuple\n        A tuple containing the new expanded bounding box as a list [new_x, new_y, new_x1, new_y1] and\n        the half-size of the new box `s`.\n    \"\"\"\n    x, y, x1, y1 = box\n    w, h = x1 - x, y1 - y\n    center_x, center_y = x + w // 2, y + h // 2\n\n    # Calculate the new half-size of the box\n    s = max(w, h) * expand // 2\n\n    # Calculate the new coordinates of the expanded box\n    new_x = center_x - s\n    new_y = center_y - s\n    new_x1 = center_x + s\n    new_y1 = center_y + s\n\n    return [new_x, new_y, new_x1, new_y1], s\n```\n\nExplanation:\n1. We unpack the `box` tuple into separate variables `x`, `y`, `x1`, and `y1` representing the coordinates of the top-left and bottom-right corners of the original box.\n\n2. We calculate the width `w` and height `h` of the original box by subtracting the corresponding coordinates.\n\n3. We calculate the center coordinates `center_x` and `center_y` of the original box by adding half of the width and height to the top-left coordinates.\n\n4. We calculate the new half-size `s` of the expanded box by taking the maximum of the width and height of the original box, multiplying it by the `expand` factor, and dividing the result by 2.\n\n5. We calculate the new coordinates of the expanded box:\n   - `new_x` is obtained by subtracting the half-size `s` from the center x-coordinate.\n   - `new_y` is obtained by subtracting the half-size `s` from the center y-coordinate.\n   - `new_x1` is obtained by adding the half-size `s` to the center x-coordinate.\n   - `new_y1` is obtained by adding the half-size `s` to the center y-coordinate.\n\n6. Finally, we return a tuple containing the new expanded bounding box as a list `[new_x, new_y, new_x1, new_y1]` and the half-size `s` of the new box.\n\nThis code expands the given bounding box by the specified `expand` factor while keeping the center of the box the same. The resulting expanded box coordinates and half-size are returned as a tuple.",
  "canonical_solution": "    \n    # Extract the coordinates from the original bounding box\n    x, y, x1, y1 = box\n    # Calculate the center of the original box\n    x_c, y_c = (x + x1) // 2, (y + y1) // 2\n    # Calculate width and height of the original box\n    w, h = x1 - x, y1 - y\n    # Calculate half of the size of the expanded box\n    s = int(max(w, h) // 2 * expand)\n    # Define the new expanded bounding box based on the center and size\n    crop_box = [x_c - s, y_c - s, x_c + s, y_c + s]\n\n    return crop_box, s\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(box = (100, 100, 200, 200), expand = 1.5)[0] == get_crop_box(box = (100, 100, 200, 200), expand = 1.5)[0])\nassert(SOLUTION_SIGNATURE(box = (50, 50, 150, 150), expand = 1.0)[0] == get_crop_box(box = (50, 50, 150, 150), expand = 1.0)[0])\nassert(SOLUTION_SIGNATURE(box = (-100, -100, 100, 100), expand = 2.0)[0] == get_crop_box(box = (-100, -100, 100, 100), expand = 2.0)[0])\nassert(SOLUTION_SIGNATURE(box = (100, 100, 200, 200), expand = 1.5)[1] == get_crop_box(box = (100, 100, 200, 200), expand = 1.5)[1])\nassert(SOLUTION_SIGNATURE(box = (50, 50, 150, 150), expand = 1.0)[1] == get_crop_box(box = (50, 50, 150, 150), expand = 1.0)[1])\nassert(SOLUTION_SIGNATURE(box = (-100, -100, 100, 100), expand = 2.0)[1] == get_crop_box(box = (-100, -100, 100, 100), expand = 2.0)[1])\n"
 },
 {
  "task_id": "RWPB/93",
  "prompt": "MAX_VAL = (1 << 256) - 1\n\n# does not revert on overflow\ndef unsafeSub(x, y):\n    \"\"\"\n    Subtract two integers, ignoring overflow.\n    Args:\n        - x: int, the minuend\n        - y: int, the subtrahend\n    Returns:\n        result: int, the difference of x and y, with overflow ignored\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nMAX_VAL = (1 << 256) - 1\n\n# does not revert on overflow\ndef unsafeSub(x, y):\n    \"\"\"\n    Subtract two integers, ignoring overflow.\n    Args:\n        - x: int, the minuend\n        - y: int, the subtrahend\n    Returns:\n        result: int, the difference of x and y, with overflow ignored\n    \"\"\"\n    result = (x - y) & MAX_VAL\n    return result\n```\n\nExplanation:\n1. The function `unsafeSub` takes two integers `x` and `y` as input.\n2. The subtraction `x - y` is performed, which may result in an overflow if the result is outside the range of valid integer values.\n3. The result of the subtraction is then bitwise AND-ed (`&`) with `MAX_VAL`, which is defined as `(1 << 256) - 1`. This operation effectively masks the result to keep only the least significant 256 bits, ignoring any overflow.\n4. The masked result is returned as the final result of the subtraction.\n\nBy performing the bitwise AND operation with `MAX_VAL`, the function ignores any overflow that may occur during the subtraction and returns the result as if the overflow did not happen. This is an unsafe operation and should be used with caution, as it may lead to unexpected behavior if the result of the subtraction is outside the valid range of integer values.",
  "canonical_solution": "    \n    return (x - y) & MAX_VAL\n\n\n",
  "unprocess_testcases": "\nx = 100\ny = 50\nassert(SOLUTION_SIGNATURE(x, y) == unsafeSub(x, y))\n\nx = 0\ny = 100\nassert(SOLUTION_SIGNATURE(x, y) == unsafeSub(x, y))\n\nx = (1 << 256) - 1\ny = 1\nassert(SOLUTION_SIGNATURE(x, y) == unsafeSub(x, y))\n"
 },
 {
  "task_id": "RWPB/60",
  "prompt": "import torch\n\n\ndef rms_norm_ref(x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False):\n    \"\"\"\n    Applies Residual Multi-scale Normalization (RMSNorm) to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        The input tensor.\n    weight : torch.Tensor\n        The weight tensor for scaling the normalized input.\n    bias : torch.Tensor or None\n        The bias tensor for shifting the normalized input. If None, no bias is added.\n    residual : torch.Tensor or None\n        The residual tensor to be added to the input. If None, no residual is added.\n    eps : float, optional\n        A small constant added for numerical stability.\n    prenorm : bool, optional\n        If True, the function returns both the normalized output and the original input.\n    upcast : bool, optional\n        If True, upcasts all tensors to float type for computation.\n\n    Returns\n    -------\n    torch.Tensor or tuple of torch.Tensor\n        The RMS normalized output tensor. If `prenorm` is True, returns a tuple\n        containing the output tensor and the original input tensor.\n\n    Notes\n    -----\n    RMSNorm is a normalization technique that scales the input by the inverse square root\n    of the mean of squares. This function performs the normalization, applies the weight\n    and bias, and optionally adds a residual connection. If `upcast` is True, the\n    function upcasts all tensors to float type before performing the operations to\n    maintain precision.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `rms_norm_ref` function:\n\n```python\nimport torch\n\n\ndef rms_norm_ref(x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False):\n    \"\"\"\n    Applies Residual Multi-scale Normalization (RMSNorm) to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        The input tensor.\n    weight : torch.Tensor\n        The weight tensor for scaling the normalized input.\n    bias : torch.Tensor or None\n        The bias tensor for shifting the normalized input. If None, no bias is added.\n    residual : torch.Tensor or None\n        The residual tensor to be added to the input. If None, no residual is added.\n    eps : float, optional\n        A small constant added for numerical stability.\n    prenorm : bool, optional\n        If True, the function returns both the normalized output and the original input.\n    upcast : bool, optional\n        If True, upcasts all tensors to float type for computation.\n\n    Returns\n    -------\n    torch.Tensor or tuple of torch.Tensor\n        The RMS normalized output tensor. If `prenorm` is True, returns a tuple\n        containing the output tensor and the original input tensor.\n\n    Notes\n    -----\n    RMSNorm is a normalization technique that scales the input by the inverse square root\n    of the mean of squares. This function performs the normalization, applies the weight\n    and bias, and optionally adds a residual connection. If `upcast` is True, the\n    function upcasts all tensors to float type before performing the operations to\n    maintain precision.\n    \"\"\"\n    if upcast:\n        x = x.float()\n        weight = weight.float()\n        if bias is not None:\n            bias = bias.float()\n        if residual is not None:\n            residual = residual.float()\n\n    # Compute the mean of squares along the last dimension\n    var = torch.mean(x * x, dim=-1, keepdim=True)\n\n    # Compute the inverse square root of the mean of squares\n    inv_rms = torch.rsqrt(var + eps)\n\n    # Normalize the input tensor\n    x_norm = x * inv_rms\n\n    # Apply weight and bias\n    output = x_norm * weight\n    if bias is not None:\n        output = output + bias\n\n    # Add residual connection if provided\n    if residual is not None:\n        output = output + residual\n\n    if prenorm:\n        return output, x\n    else:\n        return output\n```\n\nThis implementation follows the description of the RMSNorm function. Here's a breakdown of the code:\n\n1. If `upcast` is True, the function upcasts all input tensors to float type to maintain precision.\n\n2. It computes the mean of squares along the last dimension of the input tensor `x` using `torch.mean(x * x, dim=-1, keepdim=True)`. This gives the variance of each feature across the last dimension.\n\n3. It computes the inverse square root of the mean of squares using `torch.rsqrt(var + eps)`. The `eps` term is added for numerical stability.\n\n4. The input tensor `x` is normalized by multiplying it with the inverse square root of the mean of squares (`x_norm = x * inv_rms`).\n\n5. The normalized tensor is scaled by the weight tensor (`x_norm * weight`).\n\n6. If a bias tensor is provided, it is added to the scaled normalized tensor.\n\n7. If a residual tensor is provided, it is added to the output tensor.\n\n8. If `prenorm` is True, the function returns a tuple containing the output tensor and the original input tensor. Otherwise, it returns only the output tensor.\n\nThis implementation should correctly apply Residual Multi-scale Normalization to the input tensor based on the provided parameters.",
  "canonical_solution": "\n    # Preserve the original dtype\n    dtype = x.dtype\n\n    # Upcast weight, bias, and x to float if necessary\n    if upcast:\n        weight = weight.float()\n        bias = bias.float() if bias is not None else None\n        x = x.float()\n        residual = residual.float() if residual is not None else residual\n\n    # Add residual to input if provided\n    if residual is not None:\n        x = (x + residual).to(x.dtype)\n\n    # Compute the reciprocal of the standard deviation of the input\n    rstd = 1 / torch.sqrt((x.square()).mean(dim=-1, keepdim=True) + eps)\n\n    # Apply normalization, weight, and bias\n    out = (x * rstd * weight) + bias if bias is not None else (x * rstd * weight)\n\n    # Cast the output back to the original dtype\n    out = out.to(dtype)\n\n    # Return the output only or the output with the original input if prenorm is True\n    return out if not prenorm else (out, x)\n\n\n",
  "unprocess_testcases": "a = torch.rand(10, 5)\nb = torch.rand(5,)\n\nassert((SOLUTION_SIGNATURE(a, b, b, None) == rms_norm_ref(a, b, b, None)).all())\nassert((SOLUTION_SIGNATURE(a, b, b, None, 1e-5, False, True) == rms_norm_ref(a, b, b, None, 1e-5, False, True)).all())\nassert((SOLUTION_SIGNATURE(a, b, b, a) == rms_norm_ref(a, b, b, a)).all())\nassert((SOLUTION_SIGNATURE(a, b, None, b, 1e-6, True, False)[0] == rms_norm_ref(a, b, None, b, 1e-6, True, False)[0]).all())\nassert((SOLUTION_SIGNATURE(a, b, None, b, 1e-6, True, False)[1] == rms_norm_ref(a, b, None, b, 1e-6, True, False)[1]).all())\nassert((SOLUTION_SIGNATURE(a, b, None) == rms_norm_ref(a, b, None)).all())\n"
 },
 {
  "task_id": "RWPB/31",
  "prompt": "import torch\nimport math\n\ndef resize_to_batch_size(tensor, batch_size):\n    \"\"\"\n    Adjusts the batch size of a tensor to the specified size, interpolating if necessary.\n\n    Args:\n        tensor (torch.Tensor): Input tensor whose batch size needs to be adjusted.\n        batch_size (int): The desired batch size after adjustment.\n\n    Returns:\n        torch.Tensor: The tensor with its batch size adjusted to the specified size.\n\n    This function handles resizing of the batch dimension of a tensor. If the current batch size matches\n    the desired batch size, the tensor is returned unchanged. For reducing the batch size, it uses linear\n    interpolation to select indices, ensuring that the resized tensor covers the whole original batch as\n    evenly as possible. For increasing the batch size, it applies a different interpolation method to avoid\n    extrapolation and to ensure that every element from the original batch is represented.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport torch\nimport math\n\ndef resize_to_batch_size(tensor, batch_size):\n    \"\"\"\n    Adjusts the batch size of a tensor to the specified size, interpolating if necessary.\n\n    Args:\n        tensor (torch.Tensor): Input tensor whose batch size needs to be adjusted.\n        batch_size (int): The desired batch size after adjustment.\n\n    Returns:\n        torch.Tensor: The tensor with its batch size adjusted to the specified size.\n\n    This function handles resizing of the batch dimension of a tensor. If the current batch size matches\n    the desired batch size, the tensor is returned unchanged. For reducing the batch size, it uses linear\n    interpolation to select indices, ensuring that the resized tensor covers the whole original batch as\n    evenly as possible. For increasing the batch size, it applies a different interpolation method to avoid\n    extrapolation and to ensure that every element from the original batch is represented.\n    \"\"\"\n    current_batch_size = tensor.shape[0]\n\n    if current_batch_size == batch_size:\n        return tensor\n\n    if current_batch_size > batch_size:\n        # Reduce batch size\n        indices = torch.linspace(0, current_batch_size - 1, batch_size).round().long()\n        return tensor[indices]\n\n    # Increase batch size\n    repeat_factor = math.ceil(batch_size / current_batch_size)\n    repeated_tensor = tensor.repeat(repeat_factor, *[1] * (tensor.dim() - 1))\n    indices = torch.linspace(0, repeated_tensor.shape[0] - 1, batch_size).long()\n    return repeated_tensor[indices]\n```\n\nExplanation:\n\n1. We first check if the current batch size (`current_batch_size`) matches the desired `batch_size`. If they are the same, we simply return the original tensor without any modifications.\n\n2. If the current batch size is larger than the desired batch size, we need to reduce the batch size:\n   - We use `torch.linspace` to generate `batch_size` evenly spaced indices between 0 and `current_batch_size - 1`.\n   - We round the indices to the nearest integer using `.round()` and convert them to long integers using `.long()`.\n   - We use these indices to select the corresponding elements from the tensor along the batch dimension.\n\n3. If the current batch size is smaller than the desired batch size, we need to increase the batch size:\n   - We calculate the repeat factor (`repeat_factor`) by dividing the desired `batch_size` by the `current_batch_size` and rounding up to the nearest integer using `math.ceil()`.\n   - We repeat the tensor along the batch dimension using `.repeat()`, where the first argument is the `repeat_factor` and the remaining dimensions are kept unchanged (`*[1] * (tensor.dim() - 1)`).\n   - We generate `batch_size` evenly spaced indices between 0 and `repeated_tensor.shape[0] - 1` using `torch.linspace`.\n   - We convert the indices to long integers using `.long()`.\n   - We use these indices to select the corresponding elements from the repeated tensor along the batch dimension.\n\n4. Finally, we return the resized tensor with the desired batch size.\n\nThis code provides a flexible way to adjust the batch size of a tensor, handling both reducing and increasing the batch size while ensuring that the resized tensor covers the original batch as evenly as possible.",
  "canonical_solution": "    \n    # Retrieve the initial batch size from the tensor\n    in_batch_size = tensor.shape[0]\n\n    # Return the tensor unchanged if the current batch size is already the desired size\n    if in_batch_size == batch_size:\n        return tensor\n\n    # Return only the required portion of the tensor if the desired batch size is less than or equal to 1\n    if batch_size <= 1:\n        return tensor[:batch_size]\n\n    # Create an empty tensor to store the output, maintaining the original data type and device\n    output = torch.empty([batch_size] + list(tensor.shape[1:]), dtype=tensor.dtype, device=tensor.device)\n\n    # Handle reduction of batch size\n    if batch_size < in_batch_size:\n        scale = (in_batch_size - 1) / (batch_size - 1)\n        for i in range(batch_size):\n            output[i] = tensor[min(round(i * scale), in_batch_size - 1)]\n\n    # Handle increase in batch size\n    else:\n        scale = in_batch_size / batch_size\n        for i in range(batch_size):\n            index = min(math.floor((i + 0.5) * scale), in_batch_size - 1)\n            output[i] = tensor[index]\n\n    return output\n\n\n",
  "unprocess_testcases": "a = torch.randn(5, 3, 3)\nb = torch.randn(10, 4, 4)\nc = torch.randn(3, 2, 2)\n\nassert((SOLUTION_SIGNATURE(a, 5) == resize_to_batch_size(a, 5)).all())\nassert((SOLUTION_SIGNATURE(b, 3) == resize_to_batch_size(b, 3)).all())\nassert((SOLUTION_SIGNATURE(c, 8) == resize_to_batch_size(c, 8)).all())\n"
 },
 {
  "task_id": "RWPB/21",
  "prompt": "import re\n\ndef validate_user_id(user_id):\n    \"\"\"\n    Validates a user ID against a specific pattern.\n\n    Parameters\n    ----------\n    user_id : str\n        The user ID to validate.\n\n    Returns\n    -------\n    bool\n        True if the user ID is valid, False otherwise.\n\n    Notes\n    -----\n    The function checks if the user ID is a non-empty string that:\n    - Starts with a letter (uppercase or lowercase).\n    - Followed by any number of alphanumeric characters or underscores.\n\n    The validation is performed using a regular expression (regex) pattern.\n    \"\"\"\n",
  "solution": "Certainly! Here's the completed code to validate the user ID based on the given requirements:\n\n```python\nimport re\n\ndef validate_user_id(user_id):\n    \"\"\"\n    Validates a user ID against a specific pattern.\n\n    Parameters\n    ----------\n    user_id : str\n        The user ID to validate.\n\n    Returns\n    -------\n    bool\n        True if the user ID is valid, False otherwise.\n\n    Notes\n    -----\n    The function checks if the user ID is a non-empty string that:\n    - Starts with a letter (uppercase or lowercase).\n    - Followed by any number of alphanumeric characters or underscores.\n\n    The validation is performed using a regular expression (regex) pattern.\n    \"\"\"\n    pattern = r'^[a-zA-Z][a-zA-Z0-9_]*$'\n    if user_id and re.match(pattern, user_id):\n        return True\n    else:\n        return False\n```\n\nExplanation:\n- The `pattern` variable defines the regular expression pattern for validating the user ID.\n  - `^` asserts the start of the string.\n  - `[a-zA-Z]` matches any single letter (uppercase or lowercase).\n  - `[a-zA-Z0-9_]*` matches any number of alphanumeric characters or underscores.\n  - `$` asserts the end of the string.\n- The `if` statement checks two conditions:\n  - `user_id`: Ensures that the `user_id` is not an empty string or `None`.\n  - `re.match(pattern, user_id)`: Uses the `re.match()` function to check if the `user_id` matches the specified `pattern`.\n- If both conditions are true, the function returns `True`, indicating that the user ID is valid.\n- If either condition is false, the function returns `False`, indicating that the user ID is invalid.\n\nThis code implements the validation logic based on the given requirements. It checks if the user ID is a non-empty string that starts with a letter and is followed by any number of alphanumeric characters or underscores.",
  "canonical_solution": "    \n    # Define the regex pattern for a valid user ID\n    pattern = r'^[A-Za-z][A-Za-z0-9_]*$'\n\n    # Check if the user_id matches the pattern and is an instance of str\n    if isinstance(user_id, str) and re.match(pattern, user_id):\n        return True\n    else:\n        return False\n    \n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(\"A1_bcd\") == validate_user_id(\"A1_bcd\"))\nassert(SOLUTION_SIGNATURE(\"Z\") == validate_user_id(\"Z\"))\nassert(SOLUTION_SIGNATURE(\"123abc\") == validate_user_id(\"123abc\"))\n"
 },
 {
  "task_id": "RWPB/70",
  "prompt": "import numpy as np\n\ndef psnr_to_mse(psnr):\n  \"\"\"\n  Compute the Mean Squared Error (MSE) given the Peak Signal-to-Noise Ratio (PSNR).\n\n  Parameters:\n  psnr (float): The Peak Signal-to-Noise Ratio, typically in decibels (dB).\n\n  Returns:\n  float: The computed Mean Squared Error (MSE).\n\n  Process:\n  - The function assumes the maximum pixel value is 1.\n  - PSNR is converted to a linear scale using the formula:\n    MSE = exp(-0.1 * ln(10) * PSNR)\n  - This conversion is based on the logarithmic relationship between PSNR and MSE.\n  - The constant -0.1 * ln(10) transforms the PSNR value into a scale where \n    the exponentiation gives the MSE.\n  \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of computing the Mean Squared Error (MSE) given the Peak Signal-to-Noise Ratio (PSNR):\n\n```python\nimport numpy as np\n\ndef psnr_to_mse(psnr):\n  \"\"\"\n  Compute the Mean Squared Error (MSE) given the Peak Signal-to-Noise Ratio (PSNR).\n\n  Parameters:\n  psnr (float): The Peak Signal-to-Noise Ratio, typically in decibels (dB).\n\n  Returns:\n  float: The computed Mean Squared Error (MSE).\n\n  Process:\n  - The function assumes the maximum pixel value is 1.\n  - PSNR is converted to a linear scale using the formula:\n    MSE = exp(-0.1 * ln(10) * PSNR)\n  - This conversion is based on the logarithmic relationship between PSNR and MSE.\n  - The constant -0.1 * ln(10) transforms the PSNR value into a scale where \n    the exponentiation gives the MSE.\n  \"\"\"\n  mse = np.exp(-0.1 * np.log(10) * psnr)\n  return mse\n```\n\nThe completed function `psnr_to_mse` takes the `psnr` value as input and computes the corresponding Mean Squared Error (MSE) using the formula mentioned in the process description.\n\nThe key steps in the function are:\n\n1. The constant `-0.1 * np.log(10)` is used to transform the PSNR value into a scale suitable for exponentiation.\n2. The transformed PSNR value is then exponentiated using `np.exp()` to obtain the MSE value.\n3. The computed MSE is returned as the output of the function.\n\nThis function assumes that the maximum pixel value is 1, as mentioned in the process description. If the maximum pixel value is different, you would need to adjust the formula accordingly.\n\nPlease note that this function only computes the MSE given the PSNR. If you need to compute the PSNR given the MSE, you would need to use the inverse formula.",
  "canonical_solution": "  \n  return np.exp(-0.1 * np.log(10.) * psnr)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(1) == psnr_to_mse(1))\nassert(SOLUTION_SIGNATURE(0) == psnr_to_mse(0))\nassert(SOLUTION_SIGNATURE(0.2) == psnr_to_mse(0.2))\nassert(SOLUTION_SIGNATURE(0.31423) == psnr_to_mse(0.31423))\nassert(SOLUTION_SIGNATURE(1657546223) == psnr_to_mse(1657546223))\n"
 },
 {
  "task_id": "RWPB/83",
  "prompt": "import math\nimport cv2\nimport numpy as np\n\ndef apply_min_size(sample, size, image_interpolation_method=cv2.INTER_AREA):\n    \"\"\"\n    Resize the images in the sample to ensure they meet the minimum specified size while maintaining the aspect ratio.\n\n    Args:\n        sample (dict): A dictionary containing the images to be resized. The keys are:\n            - \"image\": The main image to be resized.\n            - \"disparity\": The disparity map associated with the image.\n            - \"mask\": A binary mask associated with the image.\n        size (tuple): The minimum desired size (height, width) for the images.\n        image_interpolation_method (int, optional): Interpolation method to be used for resizing the main image. \n                                                    Defaults to cv2.INTER_AREA.\n\n    Returns:\n        tuple: The new size (height, width) of the resized images.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport math\nimport cv2\nimport numpy as np\n\ndef apply_min_size(sample, size, image_interpolation_method=cv2.INTER_AREA):\n    \"\"\"\n    Resize the images in the sample to ensure they meet the minimum specified size while maintaining the aspect ratio.\n\n    Args:\n        sample (dict): A dictionary containing the images to be resized. The keys are:\n            - \"image\": The main image to be resized.\n            - \"disparity\": The disparity map associated with the image.\n            - \"mask\": A binary mask associated with the image.\n        size (tuple): The minimum desired size (height, width) for the images.\n        image_interpolation_method (int, optional): Interpolation method to be used for resizing the main image. \n                                                    Defaults to cv2.INTER_AREA.\n\n    Returns:\n        tuple: The new size (height, width) of the resized images.\n    \"\"\"\n    # Get the current size of the image\n    h, w = sample[\"image\"].shape[:2]\n    \n    # Calculate the scaling factors to meet the minimum size\n    scale_h = size[0] / h\n    scale_w = size[1] / w\n    scale = max(scale_h, scale_w)\n    \n    # Calculate the new size while maintaining the aspect ratio\n    new_size = (int(math.ceil(w * scale)), int(math.ceil(h * scale)))\n    \n    # Resize the main image\n    sample[\"image\"] = cv2.resize(sample[\"image\"], new_size, interpolation=image_interpolation_method)\n    \n    # Resize the disparity map\n    if \"disparity\" in sample:\n        sample[\"disparity\"] = cv2.resize(sample[\"disparity\"], new_size, interpolation=cv2.INTER_NEAREST)\n    \n    # Resize the binary mask\n    if \"mask\" in sample:\n        sample[\"mask\"] = cv2.resize(sample[\"mask\"], new_size, interpolation=cv2.INTER_NEAREST)\n    \n    return new_size\n```\n\nExplanation:\n1. We start by getting the current height (`h`) and width (`w`) of the main image in the `sample` dictionary.\n\n2. We calculate the scaling factors (`scale_h` and `scale_w`) needed to meet the minimum size specified by the `size` parameter. We take the maximum of these scaling factors to ensure that both the height and width meet the minimum size.\n\n3. Using the maximum scaling factor (`scale`), we calculate the new size (`new_size`) for the images while maintaining the aspect ratio. We use `math.ceil()` to round up the new size to the nearest integer.\n\n4. We resize the main image using `cv2.resize()` with the `new_size` and the specified `image_interpolation_method`.\n\n5. If the `sample` dictionary contains a disparity map (key: `\"disparity\"`), we resize it using `cv2.resize()` with the `new_size` and `cv2.INTER_NEAREST` interpolation method to preserve the integer values.\n\n6. If the `sample` dictionary contains a binary mask (key: `\"mask\"`), we resize it using `cv2.resize()` with the `new_size` and `cv2.INTER_NEAREST` interpolation method to preserve the binary values.\n\n7. Finally, we return the `new_size` tuple representing the new height and width of the resized images.\n\nThis code ensures that the images in the `sample` dictionary are resized to meet the minimum specified size while maintaining their aspect ratio. The main image is resized using the specified interpolation method, while the disparity map and binary mask (if present) are resized using the nearest-neighbor interpolation method to preserve their original values.",
  "canonical_solution": "    \n    shape = list(sample[\"disparity\"].shape)\n\n    if shape[0] >= size[0] and shape[1] >= size[1]:\n        return sample\n\n    scale = [0, 0]\n    scale[0] = size[0] / shape[0]\n    scale[1] = size[1] / shape[1]\n\n    scale = max(scale)\n\n    shape[0] = math.ceil(scale * shape[0])\n    shape[1] = math.ceil(scale * shape[1])\n\n    # resize\n    sample[\"image\"] = cv2.resize(\n        sample[\"image\"], tuple(shape[::-1]), interpolation=image_interpolation_method\n    )\n\n    sample[\"disparity\"] = cv2.resize(\n        sample[\"disparity\"], tuple(shape[::-1]), interpolation=cv2.INTER_NEAREST\n    )\n    sample[\"mask\"] = cv2.resize(\n        sample[\"mask\"].astype(np.float32),\n        tuple(shape[::-1]),\n        interpolation=cv2.INTER_NEAREST,\n    )\n    sample[\"mask\"] = sample[\"mask\"].astype(bool)\n\n    return tuple(shape)\n\n\n",
  "unprocess_testcases": "sample = {\n    \"image\": np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8),\n    \"disparity\": np.random.randint(0, 256, (100, 100), dtype=np.uint8),\n    \"mask\": np.random.randint(0, 2, (100, 100), dtype=np.bool_)\n}\nsize = (200, 300)\nassert((SOLUTION_SIGNATURE(sample, size) == apply_min_size(sample, size)).all())\n\nsample = {\n    \"image\": np.random.randint(0, 256, (300, 300, 3), dtype=np.uint8),\n    \"disparity\": np.random.randint(0, 256, (300, 300), dtype=np.uint8),\n    \"mask\": np.random.randint(0, 2, (300, 300), dtype=np.bool_)\n}\nsize = (200, 200)\nassert((SOLUTION_SIGNATURE(sample, size) == apply_min_size(sample, size)).all())\n\nsample = {\n    \"image\": np.random.randint(0, 256, (150, 100, 3), dtype=np.uint8),\n    \"disparity\": np.random.randint(0, 256, (150, 100), dtype=np.uint8),\n    \"mask\": np.random.randint(0, 2, (150, 100), dtype=np.bool_)\n}\nsize = (300, 450)\nassert((SOLUTION_SIGNATURE(sample, size) == apply_min_size(sample, size)).all())\n"
 },
 {
  "task_id": "RWPB/44",
  "prompt": "import torch\n\ndef interleave(tensor1, tensor2):\n    \"\"\"\n    Interleaves two tensors along a new dimension, creating two interleaved sequences.\n    If the \n\n    Args:\n        tensor1 (torch.Tensor): The first tensor to interleave.\n        tensor2 (torch.Tensor): The second tensor to interleave.\n\n    Returns:\n        tuple: A tuple containing two tensors. The first tensor is interleaved starting with tensor1,\n               and the second tensor is interleaved starting with tensor2.\n\n    This function interleaves two input tensors element-wise along a newly created dimension and then flattens\n    the interleaved tensor back into the original dimension. It produces two outputs: one starting with the first tensor\n    and the other starting with the second tensor. This can be particularly useful for tasks where data from two different\n    sources need to be alternately processed or analyzed.\n\n    Example Usage:\n        tensor1 = torch.tensor([1, 3, 5])\n        tensor2 = torch.tensor([2, 4, 6])\n        res1, res2 = interleave(tensor1, tensor2)\n        print(res1)  # Outputs: tensor([1, 2, 3, 4, 5, 6])\n        print(res2)  # Outputs: tensor([2, 1, 4, 3, 6, 5])\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport torch\n\ndef interleave(tensor1, tensor2):\n    \"\"\"\n    Interleaves two tensors along a new dimension, creating two interleaved sequences.\n\n    Args:\n        tensor1 (torch.Tensor): The first tensor to interleave.\n        tensor2 (torch.Tensor): The second tensor to interleave.\n\n    Returns:\n        tuple: A tuple containing two tensors. The first tensor is interleaved starting with tensor1,\n               and the second tensor is interleaved starting with tensor2.\n\n    This function interleaves two input tensors element-wise along a newly created dimension and then flattens\n    the interleaved tensor back into the original dimension. It produces two outputs: one starting with the first tensor\n    and the other starting with the second tensor. This can be particularly useful for tasks where data from two different\n    sources need to be alternately processed or analyzed.\n\n    Example Usage:\n        tensor1 = torch.tensor([1, 3, 5])\n        tensor2 = torch.tensor([2, 4, 6])\n        res1, res2 = interleave(tensor1, tensor2)\n        print(res1)  # Outputs: tensor([1, 2, 3, 4, 5, 6])\n        print(res2)  # Outputs: tensor([2, 1, 4, 3, 6, 5])\n    \"\"\"\n    # Check if the input tensors have the same shape\n    if tensor1.shape != tensor2.shape:\n        raise ValueError(\"Input tensors must have the same shape.\")\n\n    # Stack the tensors along a new dimension\n    stacked = torch.stack((tensor1, tensor2), dim=0)\n\n    # Reshape the stacked tensor to interleave the elements\n    interleaved = stacked.view(-1)\n\n    # Split the interleaved tensor into two sequences\n    res1 = interleaved[0::2]\n    res2 = interleaved[1::2]\n\n    return res1, res2\n```\n\nExplanation:\n1. We first check if the input tensors `tensor1` and `tensor2` have the same shape using the `shape` attribute. If they have different shapes, we raise a `ValueError` with an appropriate error message.\n\n2. We stack the two input tensors along a new dimension using `torch.stack()`. The `dim=0` argument specifies that the new dimension should be added as the first dimension.\n\n3. We reshape the stacked tensor using `view(-1)` to flatten it into a 1-dimensional tensor. This effectively interleaves the elements of `tensor1` and `tensor2`.\n\n4. We split the interleaved tensor into two sequences using slicing. `interleaved[0::2]` selects every even-indexed element starting from index 0, which corresponds to the elements of `tensor1`. `interleaved[1::2]` selects every odd-indexed element starting from index 1, which corresponds to the elements of `tensor2`.\n\n5. Finally, we return a tuple containing the two interleaved sequences `res1` and `res2`.\n\nThe completed code should solve the problem of interleaving two tensors along a new dimension and creating two interleaved sequences.",
  "canonical_solution": "\n    # Stack the tensors alternately along a new dimension (dim=1)\n    res1 = torch.stack((tensor1, tensor2), dim=1).flatten(0, 1)  # Flatten across original and new dimension\n    res2 = torch.stack((tensor2, tensor1), dim=1).flatten(0, 1)  # Start with the second tensor\n\n    return res1, res2\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([1, 3, 5]), torch.tensor([2, 4, 6]))[0] == interleave(torch.tensor([1, 3, 5]), torch.tensor([2, 4, 6]))[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 3, 5]), torch.tensor([2, 4, 6]))[1] == interleave(torch.tensor([1, 3, 5]), torch.tensor([2, 4, 6]))[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 3, 5, 7]), torch.tensor([2, 4, 5, 0]))[0] == interleave(torch.tensor([1, 3, 5, 7]), torch.tensor([2, 4, 5, 0]))[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 3, 5, 7]), torch.tensor([2, 4, 5, 0]))[1] == interleave(torch.tensor([1, 3, 5, 7]), torch.tensor([2, 4, 5, 0]))[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))[0] == interleave(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))[1] == interleave(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))[1]).all())\n"
 },
 {
  "task_id": "RWPB/15",
  "prompt": "import numpy as np\n\n\ndef cosine_distance(a, b):\n    \"\"\"\n    Compute the consine distance between vector a and vector b. Only support `a` is an embedding vector, `b` is a vector or matrix.\n    if the length of a is not equal the length of b, please return KeyError\n    Input: \n    a: numpy vector\n    b: numpy vector\n    return:\n    the cosine distance between vector a and vector b: int.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport numpy as np\n\n\ndef cosine_distance(a, b):\n    \"\"\"\n    Compute the cosine distance between vector a and vector b. Only support `a` is an embedding vector, `b` is a vector or matrix.\n    if the length of a is not equal the length of b, please return KeyError\n    Input: \n    a: numpy vector\n    b: numpy vector or matrix\n    return:\n    the cosine distance between vector a and vector b: float or numpy array.\n    \"\"\"\n    if a.ndim != 1:\n        raise ValueError(\"Vector a must be a one-dimensional numpy array.\")\n    \n    if b.ndim not in [1, 2]:\n        raise ValueError(\"Vector b must be a one-dimensional or two-dimensional numpy array.\")\n    \n    if a.shape[0] != b.shape[0]:\n        raise KeyError(\"The length of vector a must be equal to the length of vector b.\")\n    \n    a_norm = np.linalg.norm(a)\n    b_norm = np.linalg.norm(b, axis=0)\n    \n    if b.ndim == 1:\n        dot_product = np.dot(a, b)\n        cosine_dist = 1 - dot_product / (a_norm * b_norm)\n    else:\n        dot_product = np.dot(a, b.T)\n        cosine_dist = 1 - dot_product / (a_norm * b_norm)\n    \n    return cosine_dist\n```\n\nExplanation:\n\n1. We first check if `a` is a one-dimensional numpy array using `a.ndim != 1`. If it's not, we raise a `ValueError` with an appropriate message.\n\n2. We then check if `b` is either a one-dimensional or two-dimensional numpy array using `b.ndim not in [1, 2]`. If it's not, we raise a `ValueError` with an appropriate message.\n\n3. We compare the length of `a` with the length of `b` using `a.shape[0] != b.shape[0]`. If they are not equal, we raise a `KeyError` with an appropriate message.\n\n4. We calculate the Euclidean norm of vector `a` using `np.linalg.norm(a)`.\n\n5. We calculate the Euclidean norm of vector `b` using `np.linalg.norm(b, axis=0)`. If `b` is a matrix, the `axis=0` parameter ensures that the norm is calculated along the columns.\n\n6. If `b` is a one-dimensional vector (`b.ndim == 1`), we calculate the dot product between `a` and `b` using `np.dot(a, b)`. We then calculate the cosine distance using the formula `1 - dot_product / (a_norm * b_norm)`.\n\n7. If `b` is a two-dimensional matrix (`b.ndim == 2`), we calculate the dot product between `a` and the transpose of `b` using `np.dot(a, b.T)`. We then calculate the cosine distance using the formula `1 - dot_product / (a_norm * b_norm)`.\n\n8. Finally, we return the cosine distance as a float if `b` is a vector, or as a numpy array if `b` is a matrix.\n\nThis code calculates the cosine distance between vector `a` and vector `b`, handling the cases where `b` can be either a vector or a matrix. It raises appropriate errors if the dimensions or lengths of the vectors are not compatible.",
  "canonical_solution": "\n    if a.shape[-1] != b.shape[-1]:\n        return ValueError\n    dist = np.dot(a, b.T) / (np.linalg.norm(a, axis=-1) * np.linalg.norm(b, axis=-1))\n    return dist\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(a = np.array([1, 1]), b = np.array([0, 1])) == cosine_distance(a = np.array([1, 1]), b = np.array([0, 1]))).all())\nassert(SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([0, 1])) == cosine_distance(a = np.array([1, 0, 0]), b = np.array([0, 1])))\nassert((SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])) == cosine_distance(a = np.array([1, 0, 0]), b = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))).all())\n"
 },
 {
  "task_id": "RWPB/119",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\ndef rotate_about_x_axis(vector, theta):\n    \"\"\"\n    Rotate a 3D vector about the x-axis by a specified angle.\n    Args:\n        - vector: numpy array of shape (3,), the 3D vector to be rotated\n        - theta: float, angle of rotation in radians\n    Returns:\n        rotated_vector: numpy array of shape (3,), the rotated 3D vector\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of rotating a 3D vector about the x-axis by a specified angle:\n\n```python\nimport math\nimport numpy as np\n\ndef rotate_about_x_axis(vector, theta):\n    \"\"\"\n    Rotate a 3D vector about the x-axis by a specified angle.\n    Args:\n        - vector: numpy array of shape (3,), the 3D vector to be rotated\n        - theta: float, angle of rotation in radians\n    Returns:\n        rotated_vector: numpy array of shape (3,), the rotated 3D vector\n    \"\"\"\n    # Create the rotation matrix\n    rotation_matrix = np.array([\n        [1, 0, 0],\n        [0, math.cos(theta), -math.sin(theta)],\n        [0, math.sin(theta), math.cos(theta)]\n    ])\n    \n    # Apply the rotation matrix to the vector\n    rotated_vector = np.dot(rotation_matrix, vector)\n    \n    return rotated_vector\n```\n\nExplanation:\n\n1. We define the `rotate_about_x_axis` function that takes two arguments: `vector` (a 3D vector represented as a numpy array) and `theta` (the angle of rotation in radians).\n\n2. Inside the function, we create the rotation matrix for rotation about the x-axis. The rotation matrix is a 3x3 matrix that represents the transformation applied to the vector. For rotation about the x-axis, the matrix is:\n   ```\n   [1         0         0]\n   [0  cos(theta) -sin(theta)]\n   [0  sin(theta)  cos(theta)]\n   ```\n\n3. We apply the rotation matrix to the vector using matrix multiplication. This is done using the `np.dot` function from NumPy, which performs matrix multiplication between the rotation matrix and the vector.\n\n4. Finally, we return the rotated vector.\n\nThis code provides a simple and efficient way to rotate a 3D vector about the x-axis by a specified angle using matrix multiplication. The resulting rotated vector will have the same magnitude as the original vector but will be oriented differently in 3D space based on the angle of rotation.",
  "canonical_solution": "    \n    return np.dot(np.array([\n[1, 0, 0],\n[0, np.cos(theta), -np.sin(theta)],\n[0, np.sin(theta), np.cos(theta)]        \n]), vector)\n\n\n",
  "unprocess_testcases": "vector = np.array([0, 1, 0])\ntheta = np.pi / 2\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_x_axis(vector, theta)).all())\n\nvector = np.array([1, 1, 1])\ntheta = 0\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_x_axis(vector, theta)).all())\n\nvector = np.array([1, 0, 0])\ntheta = -np.pi / 4\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_x_axis(vector, theta)).all())\n"
 },
 {
  "task_id": "RWPB/139",
  "prompt": "import torch\nimport numpy as np\n\ndef rotate_normalmap_by_angle_torch(normal_map, angle):\n    \"\"\"\n    Rotates the normals in a normal map along the y-axis by a specified angle using PyTorch.\n\n    Args:\n        normal_map (torch.Tensor): A tensor of shape (H, W, 3) representing the normal map,\n                                   where H is the height, W is the width, and each normal is a 3D vector with components in [-1, 1].\n                                   This tensor should be on a CUDA device for GPU acceleration.\n        angle (float): The rotation angle in degrees.\n\n    Returns:\n        torch.Tensor: The rotated normal map with the same shape as the input.\n    \"\"\"\n",
  "solution": "Here's the completed code to rotate a normal map by a specified angle using PyTorch:\n\n```python\nimport torch\nimport numpy as np\n\ndef rotate_normalmap_by_angle_torch(normal_map, angle):\n    \"\"\"\n    Rotates the normals in a normal map along the y-axis by a specified angle using PyTorch.\n\n    Args:\n        normal_map (torch.Tensor): A tensor of shape (H, W, 3) representing the normal map,\n                                   where H is the height, W is the width, and each normal is a 3D vector with components in [-1, 1].\n                                   This tensor should be on a CUDA device for GPU acceleration.\n        angle (float): The rotation angle in degrees.\n\n    Returns:\n        torch.Tensor: The rotated normal map with the same shape as the input.\n    \"\"\"\n    # Convert angle from degrees to radians\n    angle_rad = np.deg2rad(angle)\n\n    # Create a rotation matrix around the y-axis\n    cos_angle = torch.cos(angle_rad)\n    sin_angle = torch.sin(angle_rad)\n    rotation_matrix = torch.tensor([[cos_angle, 0, -sin_angle],\n                                    [0, 1, 0],\n                                    [sin_angle, 0, cos_angle]], dtype=torch.float32).to(normal_map.device)\n\n    # Reshape the normal map to a 2D tensor of shape (H*W, 3)\n    H, W, _ = normal_map.shape\n    flattened_normals = normal_map.view(-1, 3)\n\n    # Apply the rotation matrix to each normal vector\n    rotated_normals = torch.matmul(flattened_normals, rotation_matrix)\n\n    # Reshape the rotated normals back to the original shape (H, W, 3)\n    rotated_normal_map = rotated_normals.view(H, W, 3)\n\n    return rotated_normal_map\n```\n\nExplanation:\n\n1. The function takes a normal map tensor (`normal_map`) and a rotation angle in degrees (`angle`) as input.\n\n2. The rotation angle is converted from degrees to radians using `np.deg2rad()`.\n\n3. A rotation matrix around the y-axis is created using the cosine and sine of the angle. The rotation matrix is a 3x3 tensor of dtype `torch.float32` and is moved to the same device as the normal map tensor.\n\n4. The normal map tensor is reshaped from (H, W, 3) to (H*W, 3) using `view()` to facilitate matrix multiplication with the rotation matrix.\n\n5. The rotation matrix is applied to each normal vector using `torch.matmul()`, which performs matrix multiplication between the flattened normals and the rotation matrix.\n\n6. The rotated normals are reshaped back to the original shape (H, W, 3) using `view()`.\n\n7. The rotated normal map is returned as a tensor with the same shape as the input.\n\nNote: This implementation assumes that the input normal map tensor is already on a CUDA device for GPU acceleration. If the tensor is on the CPU, you can move it to the GPU using `normal_map = normal_map.to('cuda')` before calling the function.",
  "canonical_solution": "    \n    # Convert the angle from degrees to radians and move it to the same device as the normal_map\n    angle = torch.tensor(angle / 180 * np.pi).to(normal_map.device)\n\n    # Define the rotation matrix for rotating around the y-axis\n    R = torch.tensor([\n        [torch.cos(angle), 0, torch.sin(angle)],  # X' = X*cos(theta) + Z*sin(theta)\n        [0, 1, 0],                                # Y' = Y (no change)\n        [-torch.sin(angle), 0, torch.cos(angle)]  # Z' = -X*sin(theta) + Z*cos(theta)\n    ]).to(normal_map.device)\n\n    # Apply the rotation matrix to each normal vector in the normal map\n    # Reshape the normal map from (H, W, 3) to (H*W, 3) for matrix multiplication, then reshape back to (H, W, 3)\n    return torch.matmul(normal_map.view(-1, 3), R.T).view(normal_map.shape)\n\n\n",
  "unprocess_testcases": "normal_map = torch.rand(10, 10, 3) * 2 - 1\nangle = 0\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle_torch(normal_map, angle)).all())\n\nnormal_map = torch.tensor([[[0, 0, 1]]], dtype=torch.float32).repeat(5, 5, 1)\nangle = 90\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle_torch(normal_map, angle)).all())\n\nnormal_map = torch.rand(50, 50, 3) * 2 - 1\nangle = 360\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle_torch(normal_map, angle)).all())\n"
 },
 {
  "task_id": "RWPB/97",
  "prompt": "import numpy as np\nimport torch\n\ndef numpy_image_to_torch(image: np.ndarray) -> torch.Tensor:\n    \"\"\"\n    Converts a NumPy image array to a normalized PyTorch tensor and reorders the dimensions.\n    If the img is invalid, please return ValueError\n    \n    Parameters:\n    image (np.ndarray): The input image in NumPy array format. \n                        - For color images, the shape should be (H, W, C) where H is height, W is width, and C is the number of channels (typically 3 for RGB).\n                        - For grayscale images, the shape should be (H, W).\n\n    Returns:\n    torch.Tensor: The converted image as a PyTorch tensor with normalized values in the range [0, 1].\n                  - For color images, the shape will be (C, H, W).\n                  - For grayscale images, the shape will be (1, H, W).\n\n    Process:\n    1. Check the number of dimensions of the input image:\n       - If the image is a 3D array (color image), transpose the dimensions from (H, W, C) to (C, H, W).\n       - If the image is a 2D array (grayscale image), add a new axis to create a channel dimension, resulting in a shape of (1, H, W).\n       - If the image does not match these conditions, raise a ValueError indicating it is not a valid image.\n    2. Normalize the pixel values by dividing by 255.0 to convert the range from [0, 255] to [0, 1].\n    3. Convert the resulting NumPy array to a PyTorch tensor with dtype=torch.float.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport numpy as np\nimport torch\n\ndef numpy_image_to_torch(image: np.ndarray) -> torch.Tensor:\n    \"\"\"\n    Converts a NumPy image array to a normalized PyTorch tensor and reorders the dimensions.\n    If the img is invalid, please return ValueError\n    \n    Parameters:\n    image (np.ndarray): The input image in NumPy array format. \n                        - For color images, the shape should be (H, W, C) where H is height, W is width, and C is the number of channels (typically 3 for RGB).\n                        - For grayscale images, the shape should be (H, W).\n\n    Returns:\n    torch.Tensor: The converted image as a PyTorch tensor with normalized values in the range [0, 1].\n                  - For color images, the shape will be (C, H, W).\n                  - For grayscale images, the shape will be (1, H, W).\n\n    Process:\n    1. Check the number of dimensions of the input image:\n       - If the image is a 3D array (color image), transpose the dimensions from (H, W, C) to (C, H, W).\n       - If the image is a 2D array (grayscale image), add a new axis to create a channel dimension, resulting in a shape of (1, H, W).\n       - If the image does not match these conditions, raise a ValueError indicating it is not a valid image.\n    2. Normalize the pixel values by dividing by 255.0 to convert the range from [0, 255] to [0, 1].\n    3. Convert the resulting NumPy array to a PyTorch tensor with dtype=torch.float.\n    \"\"\"\n    if image.ndim == 3:\n        # Color image (H, W, C)\n        image = image.transpose(2, 0, 1)\n    elif image.ndim == 2:\n        # Grayscale image (H, W)\n        image = image[np.newaxis, ...]\n    else:\n        raise ValueError(\"Invalid image dimensions. Expected 2D or 3D array.\")\n\n    # Normalize pixel values to [0, 1]\n    image = image / 255.0\n\n    # Convert NumPy array to PyTorch tensor\n    tensor = torch.from_numpy(image).float()\n\n    return tensor\n```\n\nExplanation:\n1. We first check the number of dimensions of the input `image` using `image.ndim`:\n   - If `image.ndim` is 3, it means the image is a color image with shape (H, W, C). In this case, we transpose the dimensions using `image.transpose(2, 0, 1)` to reorder them to (C, H, W).\n   - If `image.ndim` is 2, it means the image is a grayscale image with shape (H, W). In this case, we add a new axis using `image[np.newaxis, ...]` to create a channel dimension, resulting in a shape of (1, H, W).\n   - If `image.ndim` is neither 2 nor 3, we raise a `ValueError` indicating that the image dimensions are invalid.\n\n2. We normalize the pixel values by dividing the `image` array by 255.0. This converts the pixel values from the range [0, 255] to [0, 1].\n\n3. Finally, we convert the resulting NumPy array to a PyTorch tensor using `torch.from_numpy(image).float()`. The `float()` method ensures that the tensor has a floating-point data type.\n\nThe function returns the converted PyTorch tensor with the normalized pixel values and the desired shape:\n- For color images, the shape will be (C, H, W).\n- For grayscale images, the shape will be (1, H, W).\n\nIf the input `image` has invalid dimensions, a `ValueError` will be raised.",
  "canonical_solution": "    \n    if image.ndim == 3:\n        image = image.transpose((2, 0, 1))  # HxWxC to CxHxW\n    elif image.ndim == 2:\n        image = image[None]  # add channel axis\n    else:\n        return ValueError\n    return torch.tensor(image / 255.0, dtype=torch.float)\n\n\n",
  "unprocess_testcases": "\n\ncolor_image = np.random.randint(0, 256, size=(100, 200, 3), dtype=np.uint8)\nassert((SOLUTION_SIGNATURE(color_image) == numpy_image_to_torch(color_image)).all())\n\ngrayscale_image = np.random.randint(0, 256, size=(50, 50), dtype=np.uint8)\nassert((SOLUTION_SIGNATURE(grayscale_image) == numpy_image_to_torch(grayscale_image)).all())\n\ninvalid_image = np.random.randint(0, 256, size=(100,), dtype=np.uint8)\nassert(SOLUTION_SIGNATURE(invalid_image) == numpy_image_to_torch(invalid_image))\n"
 },
 {
  "task_id": "RWPB/8",
  "prompt": "import numpy as np\nimport torch\n\ndef cvimg2torch(img):\n    '''Convert a img to tensor\n    input:\n        im -> ndarray uint8 HxWxC \n    return\n        tensor -> torch.tensor BxCxHxW \n    '''\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport numpy as np\nimport torch\n\ndef cvimg2torch(img):\n    '''Convert a img to tensor\n    input:\n        im -> ndarray uint8 HxWxC \n    return\n        tensor -> torch.tensor BxCxHxW \n    '''\n    # Convert the image from uint8 to float32\n    img = img.astype(np.float32)\n\n    # Normalize the image values from [0, 255] to [0, 1]\n    img /= 255.0\n\n    # Transpose the image dimensions from HxWxC to CxHxW\n    img = img.transpose((2, 0, 1))\n\n    # Add a batch dimension to the image\n    img = np.expand_dims(img, axis=0)\n\n    # Convert the NumPy array to a PyTorch tensor\n    tensor = torch.from_numpy(img)\n\n    return tensor\n```\n\nExplanation:\n\n1. First, we convert the input image from `uint8` to `float32` using `img.astype(np.float32)`. This is necessary because PyTorch tensors typically work with floating-point values.\n\n2. Next, we normalize the image values from the range [0, 255] to [0, 1] by dividing each pixel value by 255.0. This is a common preprocessing step in computer vision tasks.\n\n3. We transpose the image dimensions from HxWxC (Height x Width x Channels) to CxHxW (Channels x Height x Width) using `img.transpose((2, 0, 1))`. This is because PyTorch expects the channel dimension to be the first dimension.\n\n4. We add a batch dimension to the image using `np.expand_dims(img, axis=0)`. This is necessary because PyTorch models typically expect a batch dimension as the first dimension, even if there is only one image in the batch.\n\n5. Finally, we convert the NumPy array to a PyTorch tensor using `torch.from_numpy(img)`.\n\nThe function takes an input image as a NumPy array with dimensions HxWxC and returns a PyTorch tensor with dimensions BxCxHxW, where B represents the batch dimension (which is 1 in this case).",
  "canonical_solution": "\n    img = img.astype(float) / 255.0\n    img = img.transpose(2, 0, 1) # NHWC -> NCHW\n    img = np.expand_dims(img, 0)\n    img = torch.from_numpy(img).float()\n    return img\n\n\n\n",
  "unprocess_testcases": "a = (np.random.rand(256, 256, 3) * 255).astype(np.uint8)\nb = np.repeat((np.random.rand(10, 10) * 255).astype(np.uint8)[:, :, np.newaxis], 3, axis=2)\nc = np.array([[[300, -10, 500], [256, 255, 0]]], dtype=np.int32)\n\nassert((SOLUTION_SIGNATURE(a) == cvimg2torch(a)).all())\nassert((SOLUTION_SIGNATURE(b) == cvimg2torch(b)).all())\nassert((SOLUTION_SIGNATURE(c) == cvimg2torch(c)).all())"
 },
 {
  "task_id": "RWPB/35",
  "prompt": "def flatten_dict(d: dict, parent_key: str = \"\", sep: str = \"/\") -> dict:\n    \"\"\"\n    Flattens a nested dictionary by collapsing its nested structure into a single layer dictionary with keys\n    representing paths, separated by a specified separator.\n\n    Args:\n        d (dict): The dictionary to flatten.\n        parent_key (str): The base path for keys in the nested dictionary. Initially left empty.\n        sep (str): The separator to use when concatenating nested keys.\n\n    Returns:\n        dict: A flattened version of the dictionary where all nested keys are transformed into single-layered keys.\n\n    Example:\n        >>> dct = {\"a\": {\"b\": 1, \"c\": {\"d\": 2}}, \"e\": 3}\n        >>> print(flatten_dict(dct))\n        {\"a/b\": 1, \"a/c/d\": 2, \"e\": 3}\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of flattening a nested dictionary:\n\n```python\ndef flatten_dict(d: dict, parent_key: str = \"\", sep: str = \"/\") -> dict:\n    \"\"\"\n    Flattens a nested dictionary by collapsing its nested structure into a single layer dictionary with keys\n    representing paths, separated by a specified separator.\n\n    Args:\n        d (dict): The dictionary to flatten.\n        parent_key (str): The base path for keys in the nested dictionary. Initially left empty.\n        sep (str): The separator to use when concatenating nested keys.\n\n    Returns:\n        dict: A flattened version of the dictionary where all nested keys are transformed into single-layered keys.\n\n    Example:\n        >>> dct = {\"a\": {\"b\": 1, \"c\": {\"d\": 2}}, \"e\": 3}\n        >>> print(flatten_dict(dct))\n        {\"a/b\": 1, \"a/c/d\": 2, \"e\": 3}\n    \"\"\"\n    flattened = {}\n    for key, value in d.items():\n        new_key = parent_key + sep + key if parent_key else key\n        if isinstance(value, dict):\n            flattened.update(flatten_dict(value, new_key, sep))\n        else:\n            flattened[new_key] = value\n    return flattened\n```\n\nExplanation:\n\n1. The `flatten_dict` function takes three parameters:\n   - `d`: The dictionary to be flattened.\n   - `parent_key`: The base path for keys in the nested dictionary. It is initially an empty string.\n   - `sep`: The separator to use when concatenating nested keys. It defaults to \"/\".\n\n2. We initialize an empty dictionary called `flattened` to store the flattened key-value pairs.\n\n3. We iterate over each key-value pair in the input dictionary `d`:\n   - If `parent_key` is not empty, we create a new key by concatenating `parent_key`, `sep`, and the current `key`. Otherwise, we use the current `key` as is.\n   - If the current `value` is a dictionary, we recursively call `flatten_dict` on the nested dictionary, passing the `new_key` as the `parent_key`. The returned flattened dictionary is then updated into the `flattened` dictionary using the `update` method.\n   - If the current `value` is not a dictionary, we directly assign the `value` to the `new_key` in the `flattened` dictionary.\n\n4. Finally, we return the `flattened` dictionary, which contains the flattened key-value pairs.\n\nThe function uses recursion to handle nested dictionaries. When a nested dictionary is encountered, it recursively flattens the nested dictionary and updates the flattened dictionary with the resulting key-value pairs.\n\nThe `parent_key` parameter keeps track of the base path for the keys in the nested dictionary, allowing the function to generate the flattened keys by concatenating the parent key with the current key using the specified separator.\n\nThis implementation assumes that the values in the dictionary are either dictionaries or non-dictionary types. If there are other nested data structures like lists or tuples, additional handling may be required.",
  "canonical_solution": "\n    items = []\n    for k, v in d.items():\n        # Generate the new key by concatenating the current key with the parent key using the separator\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            # Recursively flatten the dictionary and extend the list items\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            # Add the key-value pair to the items list\n            items.append((new_key, v))\n    return dict(items)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE({\"a\": {\"b\": {\"c\": {\"d\": 1}}}, \"e\": {\"f\": 2, \"g\": {\"h\": 3}}, \"i\": 4}) == flatten_dict({\"a\": {\"b\": {\"c\": {\"d\": 1}}}, \"e\": {\"f\": 2, \"g\": {\"h\": 3}}, \"i\": 4}))\nassert(SOLUTION_SIGNATURE({\"a\": {\"b\": 1, \"c\": {\"d\": 2}}, \"e\": 3}, sep='-') == flatten_dict({\"a\": {\"b\": 1, \"c\": {\"d\": 2}}, \"e\": 3}, sep='-'))\nassert(SOLUTION_SIGNATURE({\"a\": {}, \"b\": {\"c\": {}}, \"d\": 5}) == flatten_dict({\"a\": {}, \"b\": {\"c\": {}}, \"d\": 5}))\n"
 },
 {
  "task_id": "RWPB/64",
  "prompt": "import numpy as np\n\ndef moving_average(arr, window_size):\n    \"\"\"\n    Calculates the moving average of an array with a specified window size and returns both\n    the smoothed values and the residuals.\n\n    Arguments:\n    arr : numpy.ndarray\n        The input array over which the moving average is computed.\n    window_size : int\n        The number of elements in the moving average window.\n\n    Returns:\n    list of numpy.ndarray\n        The first element is an array of the moving averages, and the second element is an array of residuals.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport numpy as np\n\ndef moving_average(arr, window_size):\n    \"\"\"\n    Calculates the moving average of an array with a specified window size and returns both\n    the smoothed values and the residuals.\n\n    Arguments:\n    arr : numpy.ndarray\n        The input array over which the moving average is computed.\n    window_size : int\n        The number of elements in the moving average window.\n\n    Returns:\n    list of numpy.ndarray\n        The first element is an array of the moving averages, and the second element is an array of residuals.\n    \"\"\"\n    # Calculate the moving average\n    smoothed = np.convolve(arr, np.ones(window_size), 'valid') / window_size\n    \n    # Calculate the residuals\n    residuals = arr[window_size-1:] - smoothed\n    \n    return [smoothed, residuals]\n```\n\nExplanation:\n\n1. The `moving_average` function takes two arguments: `arr` (the input array) and `window_size` (the number of elements in the moving average window).\n\n2. We use `np.convolve()` to calculate the moving average. The function convolves the input array `arr` with a window of ones of size `window_size`. The 'valid' mode ensures that the output array has only those elements that do not rely on the zero-padding. Finally, we divide the result by `window_size` to get the average.\n\n3. To calculate the residuals, we subtract the smoothed values from the corresponding elements of the input array. Since the smoothed array has a smaller size than the input array (due to the 'valid' mode), we slice the input array starting from `window_size-1` to align it with the smoothed array.\n\n4. The function returns a list containing two elements: the smoothed values (moving averages) and the residuals.\n\nThis implementation efficiently calculates the moving average and residuals using NumPy functions, avoiding explicit loops for better performance.",
  "canonical_solution": "\n    # Pad the input array with zeros at the beginning to handle the boundary effect\n    arr_padded = np.pad(arr, (window_size - 1, 0), \"constant\")\n\n    # Calculate the moving average using a convolution with a uniform kernel (i.e., np.ones(window_size))\n    smoothed_arr = np.convolve(arr_padded, np.ones(window_size), \"valid\") / window_size\n\n    # Calculate residuals by subtracting the smoothed array from the original array\n    residuals = arr - smoothed_arr\n\n    return [smoothed_arr, residuals]\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5, 6]), 3)[0] == moving_average(np.array([1, 2, 3, 4, 5, 6]), 3)[0]).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5, 6]), 3)[1] == moving_average(np.array([1, 2, 3, 4, 5, 6]), 3)[1]).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5, 6]), 6)[0] == moving_average(np.array([1, 2, 3, 4, 5, 6]), 6)[0]).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5, 6]), 6)[1] == moving_average(np.array([1, 2, 3, 4, 5, 6]), 6)[1]).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3]), 5)[0] == moving_average(np.array([1, 2, 3]), 5)[0]).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3]), 5)[1] == moving_average(np.array([1, 2, 3]), 5)[1]).all())\n"
 },
 {
  "task_id": "RWPB/50",
  "prompt": "import torch\n\ndef unpad_image(tensor: torch.Tensor, original_size: tuple) -> torch.Tensor:\n    \"\"\"\n    Removes padding from an image tensor to restore it to its original aspect ratio.\n    The function assumes the tensor is in the CxHxW format and has been padded symmetrically.\n\n    Args:\n        tensor (torch.Tensor): The image tensor, assumed to be in CxHxW format.\n        original_size (tuple): The original dimensions of the image as (height, width).\n\n    Returns:\n        torch.Tensor: The unpadded image tensor resized back to the original aspect ratio.\n\n    The function calculates the aspect ratio of the original image and the current tensor,\n    determining the necessary adjustments to either height or width. It then crops the tensor\n    to match the original aspect ratio, ensuring that the central part of the image is retained.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport torch\n\ndef unpad_image(tensor: torch.Tensor, original_size: tuple) -> torch.Tensor:\n    \"\"\"\n    Removes padding from an image tensor to restore it to its original aspect ratio.\n    The function assumes the tensor is in the CxHxW format and has been padded symmetrically.\n\n    Args:\n        tensor (torch.Tensor): The image tensor, assumed to be in CxHxW format.\n        original_size (tuple): The original dimensions of the image as (height, width).\n\n    Returns:\n        torch.Tensor: The unpadded image tensor resized back to the original aspect ratio.\n\n    The function calculates the aspect ratio of the original image and the current tensor,\n    determining the necessary adjustments to either height or width. It then crops the tensor\n    to match the original aspect ratio, ensuring that the central part of the image is retained.\n    \"\"\"\n    original_height, original_width = original_size\n    tensor_height, tensor_width = tensor.shape[1], tensor.shape[2]\n\n    # Calculate the aspect ratios\n    original_aspect_ratio = original_width / original_height\n    tensor_aspect_ratio = tensor_width / tensor_height\n\n    if original_aspect_ratio > tensor_aspect_ratio:\n        # Crop the height\n        new_height = int(tensor_width / original_aspect_ratio)\n        margin = (tensor_height - new_height) // 2\n        unpadded_tensor = tensor[:, margin:margin+new_height, :]\n    else:\n        # Crop the width\n        new_width = int(tensor_height * original_aspect_ratio)\n        margin = (tensor_width - new_width) // 2\n        unpadded_tensor = tensor[:, :, margin:margin+new_width]\n\n    return unpadded_tensor\n```\n\nExplanation:\n\n1. The function takes two arguments: `tensor`, which is the image tensor assumed to be in CxHxW format, and `original_size`, which is a tuple representing the original dimensions of the image as (height, width).\n\n2. We extract the original height and width from `original_size` and the tensor height and width from the tensor's shape.\n\n3. We calculate the aspect ratios of the original image and the current tensor by dividing the width by the height.\n\n4. We compare the original aspect ratio with the tensor aspect ratio to determine whether to crop the height or width.\n\n5. If the original aspect ratio is greater than the tensor aspect ratio, it means we need to crop the height:\n   - We calculate the new height based on the tensor width and the original aspect ratio.\n   - We calculate the margin (the amount to crop from the top and bottom) by subtracting the new height from the tensor height and dividing by 2 to ensure equal padding on both sides.\n   - We crop the tensor along the height dimension using the calculated margin.\n\n6. If the original aspect ratio is less than or equal to the tensor aspect ratio, it means we need to crop the width:\n   - We calculate the new width based on the tensor height and the original aspect ratio.\n   - We calculate the margin (the amount to crop from the left and right) by subtracting the new width from the tensor width and dividing by 2 to ensure equal padding on both sides.\n   - We crop the tensor along the width dimension using the calculated margin.\n\n7. Finally, we return the unpadded tensor, which has been resized back to the original aspect ratio.\n\nThis function removes the padding from an image tensor and restores it to its original aspect ratio, ensuring that the central part of the image is retained.",
  "canonical_solution": "\n    # Extract original and current dimensions\n    original_height, original_width = original_size\n    _, current_height, current_width = tensor.shape\n\n    # Calculate aspect ratios\n    original_aspect_ratio = original_width / original_height\n    current_aspect_ratio = current_width / current_height\n\n    # Determine how to adjust tensor to match original aspect ratio\n    if original_aspect_ratio > current_aspect_ratio:\n        # Original is wider than current: adjust height\n        scale_factor = current_width / original_width\n        new_height = int(original_height * scale_factor)\n        vertical_padding = (current_height - new_height) // 2\n        # Crop vertically\n        unpadded_tensor = tensor[:, vertical_padding:vertical_padding + new_height, :]\n    else:\n        # Original is taller than current: adjust width\n        scale_factor = current_height / original_height\n        new_width = int(original_width * scale_factor)\n        horizontal_padding = (current_width - new_width) // 2\n        # Crop horizontally\n        unpadded_tensor = tensor[:, :, horizontal_padding:horizontal_padding + new_width]\n\n    return unpadded_tensor\n\n\n",
  "unprocess_testcases": "a = torch.rand(3, 10, 20)\nb = torch.rand(3, 20, 10)\nc = torch.rand(3, 15, 30)\n\nassert((SOLUTION_SIGNATURE(a, (5, 10)) == unpad_image(a, (5, 10))).all())\nassert((SOLUTION_SIGNATURE(b, (10, 5)) == unpad_image(b, (10, 5))).all())\nassert((SOLUTION_SIGNATURE(c, (10, 20)) == unpad_image(c, (10, 20))).all())\n"
 },
 {
  "task_id": "RWPB/11",
  "prompt": "from typing import List\n\ndef pad_to_max_seq_length(ls: List[int], max_seq_length: int, pad_idx: int=0, pad_right: bool=True, check: bool=True):\n    \"\"\"Apply padding to an input sequence.\n\n    Args:\n        ls: List[int]: sequence to pad.\n        max_seq_length: int: max length up to which to apply padding.\n        pad_idx :int: element to use for padding.\n        pad_right: bool: True if padding is applied to right side of sequence, False to pad on left side.\n        check: bool: True if result length should be checked as under the max sequence length.\n        if result length is not equal the max sequence length, please return ValueError\n\n    Returns:\n        Sequence with specified padding applied.\n\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nfrom typing import List\n\ndef pad_to_max_seq_length(ls: List[int], max_seq_length: int, pad_idx: int=0, pad_right: bool=True, check: bool=True):\n    \"\"\"Apply padding to an input sequence.\n\n    Args:\n        ls: List[int]: sequence to pad.\n        max_seq_length: int: max length up to which to apply padding.\n        pad_idx :int: element to use for padding.\n        pad_right: bool: True if padding is applied to right side of sequence, False to pad on left side.\n        check: bool: True if result length should be checked as under the max sequence length.\n        if result length is not equal the max sequence length, please return ValueError\n\n    Returns:\n        Sequence with specified padding applied.\n\n    \"\"\"\n    seq_length = len(ls)\n    \n    if seq_length > max_seq_length:\n        raise ValueError(\"Input sequence length exceeds the maximum sequence length.\")\n    \n    padding_length = max_seq_length - seq_length\n    padding = [pad_idx] * padding_length\n    \n    if pad_right:\n        padded_sequence = ls + padding\n    else:\n        padded_sequence = padding + ls\n    \n    if check and len(padded_sequence) != max_seq_length:\n        raise ValueError(\"Padded sequence length does not match the maximum sequence length.\")\n    \n    return padded_sequence\n```\n\nExplanation:\n1. We first check if the length of the input sequence `ls` exceeds the `max_seq_length`. If it does, we raise a `ValueError` indicating that the input sequence length is too long.\n\n2. We calculate the `padding_length` by subtracting the length of the input sequence `seq_length` from the `max_seq_length`.\n\n3. We create a list `padding` containing `padding_length` number of `pad_idx` elements. This list will be used for padding the input sequence.\n\n4. We apply the padding based on the `pad_right` parameter. If `pad_right` is `True`, we concatenate the input sequence `ls` with the `padding` list to pad on the right side. If `pad_right` is `False`, we concatenate the `padding` list with the input sequence `ls` to pad on the left side.\n\n5. If the `check` parameter is `True`, we verify if the length of the `padded_sequence` is equal to the `max_seq_length`. If it doesn't match, we raise a `ValueError` indicating that the padded sequence length is not equal to the maximum sequence length.\n\n6. Finally, we return the `padded_sequence`.\n\nThis code ensures that the input sequence is padded to the specified maximum sequence length, with the option to pad on the right or left side, and performs length checks if requested.",
  "canonical_solution": "\n    padding = [pad_idx] * (max_seq_length - len(ls))\n    if pad_right:\n        result = ls + padding\n    else:\n        result = padding + ls\n\n    if check:\n        if len(result) != max_seq_length:\n            return ValueError\n    return result\n\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE([1, 2, 3], 5) == pad_to_max_seq_length([1, 2, 3], 5))\nassert(SOLUTION_SIGNATURE([1, 2, 3], 5, pad_idx=-1) == pad_to_max_seq_length([1, 2, 3], 5, pad_idx=-1))\nassert(SOLUTION_SIGNATURE([1, 2, 3], 5, pad_right=False) == pad_to_max_seq_length([1, 2, 3], 5, pad_right=False))\nassert(SOLUTION_SIGNATURE([1, 2, 3], 3) == pad_to_max_seq_length([1, 2, 3], 3))\nassert(SOLUTION_SIGNATURE([1, 2, 3], 2) == pad_to_max_seq_length([1, 2, 3], 2))\n"
 },
 {
  "task_id": "RWPB/40",
  "prompt": "import inspect\nfrom typing import Callable, List\n\ndef extract_code_from_funct(funct: Callable) -> List[str]:\n    \"\"\"\n    Extracts the body of a Python function, removing the function definition and return statement, and adjusting indentation.\n\n    Args:\n        funct (Callable): The function from which code is extracted.\n\n    Returns:\n        List[str]: A list of strings, where each string is a line of code from the function's body, adjusted for indentation.\n\n    This function utilizes Python's `inspect` module to retrieve the source code of a given function. It then processes\n    this code to remove the function's signature and any return statement at the end, adjusting the indentation of the\n    remaining lines to align with the leftmost edge.\n\n    Example:\n        def example_function(x):\n            result = x + 1\n            return result\n\n        >>> extract_code_from_funct(example_function)\n        ['result = x + 1']\n    \"\"\"\n",
  "solution": "Here's the completed code for the `extract_code_from_funct` function:\n\n```python\nimport inspect\nfrom typing import Callable, List\n\ndef extract_code_from_funct(funct: Callable) -> List[str]:\n    \"\"\"\n    Extracts the body of a Python function, removing the function definition and return statement, and adjusting indentation.\n\n    Args:\n        funct (Callable): The function from which code is extracted.\n\n    Returns:\n        List[str]: A list of strings, where each string is a line of code from the function's body, adjusted for indentation.\n\n    This function utilizes Python's `inspect` module to retrieve the source code of a given function. It then processes\n    this code to remove the function's signature and any return statement at the end, adjusting the indentation of the\n    remaining lines to align with the leftmost edge.\n\n    Example:\n        def example_function(x):\n            result = x + 1\n            return result\n\n        >>> extract_code_from_funct(example_function)\n        ['result = x + 1']\n    \"\"\"\n    # Get the source code of the function\n    source_lines = inspect.getsourcelines(funct)[0]\n\n    # Find the index of the first line of the function's body\n    start_index = 0\n    for i, line in enumerate(source_lines):\n        if line.strip().startswith('def'):\n            start_index = i + 1\n            break\n\n    # Find the index of the last line of the function's body\n    end_index = len(source_lines)\n    for i in range(len(source_lines) - 1, start_index - 1, -1):\n        if source_lines[i].strip().startswith('return'):\n            end_index = i\n            break\n\n    # Extract the lines of the function's body\n    body_lines = source_lines[start_index:end_index]\n\n    # Adjust the indentation of the body lines\n    if body_lines:\n        # Find the minimum indentation level\n        min_indent = min(len(line) - len(line.lstrip()) for line in body_lines)\n\n        # Remove the common indentation from each line\n        body_lines = [line[min_indent:] for line in body_lines]\n\n    return body_lines\n```\n\nThis implementation does the following:\n\n1. It uses `inspect.getsourcelines(funct)` to retrieve the source code of the given function `funct` as a list of lines.\n\n2. It iterates over the lines to find the index of the first line of the function's body (the line after the function definition).\n\n3. It iterates over the lines in reverse order to find the index of the last line of the function's body (the line before the return statement, if any).\n\n4. It extracts the lines of the function's body using the start and end indices.\n\n5. If there are body lines, it calculates the minimum indentation level among all the lines and removes that common indentation from each line to align them with the leftmost edge.\n\n6. Finally, it returns the list of adjusted body lines.\n\nThis function extracts the code from the body of a given function, removing the function definition and return statement, and adjusting the indentation of the remaining lines.",
  "canonical_solution": "\n    # Retrieve the source code of the function\n    source_code = inspect.getsource(funct)\n\n    # Split the source code into individual lines and remove the first line (function definition)\n    source_code_lines = source_code.splitlines()[1:]\n\n    # Calculate the number of characters of indentation in the first line of the function body\n    nident = len(source_code_lines[0]) - len(source_code_lines[0].lstrip())\n\n    # Adjust each line to remove the initial indentation and exclude the last line (usually a return statement)\n    return [line[nident:] for line in source_code_lines[:-1]]\n\n\n",
  "unprocess_testcases": "def sample_function(x):\n    y = x + 10\n    z = y * 2\n    return z\n\nassert(SOLUTION_SIGNATURE(sample_function) == extract_code_from_funct(sample_function))\n\ndef empty_function():\n    pass\nassert(SOLUTION_SIGNATURE(empty_function) == extract_code_from_funct(empty_function))\n\ndef complex_function(x):\n    if x > 0:\n        for i in range(x):\n            print(i)\n    else:\n        while x < 0:\n            x += 1\n            print(x)\n    return x\nassert(SOLUTION_SIGNATURE(complex_function) == extract_code_from_funct(complex_function))\n"
 },
 {
  "task_id": "RWPB/74",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\ndef arccos2(vector, value):\n    \"\"\"\n    Calculate the inverse cosine of a value with consideration of the vector direction.\n    Args:\n        - vector: numpy array of shape (2,), the 2D vector to consider for direction\n        - value: float, the value for which to calculate the inverse cosine\n    Returns:\n        angle: float, the inverse cosine with consideration of vector direction\n    \"\"\"\n",
  "solution": "Here's the completed code for the `arccos2` function:\n\n```python\nimport math\nimport numpy as np\n\ndef arccos2(vector, value):\n    \"\"\"\n    Calculate the inverse cosine of a value with consideration of the vector direction.\n    Args:\n        - vector: numpy array of shape (2,), the 2D vector to consider for direction\n        - value: float, the value for which to calculate the inverse cosine\n    Returns:\n        angle: float, the inverse cosine with consideration of vector direction\n    \"\"\"\n    # Normalize the vector\n    vector_norm = np.linalg.norm(vector)\n    if vector_norm == 0:\n        raise ValueError(\"Vector must have non-zero length.\")\n    vector = vector / vector_norm\n    \n    # Calculate the angle using inverse cosine\n    angle = math.acos(value)\n    \n    # Determine the sign of the angle based on the vector direction\n    cross_product = vector[0] * math.sin(angle)\n    if cross_product < 0:\n        angle = -angle\n    \n    return angle\n```\n\nExplanation:\n\n1. We first normalize the input vector by dividing it by its Euclidean norm. This ensures that the vector has unit length. If the vector has zero length, we raise a `ValueError` since we cannot determine the direction.\n\n2. We calculate the angle using the `math.acos()` function, which returns the inverse cosine (arccos) of the input value. This gives us the absolute value of the angle.\n\n3. To determine the sign of the angle based on the vector direction, we calculate the cross product of the normalized vector and the unit vector in the positive y-direction (represented by `math.sin(angle)`). The cross product is calculated using the first component of the normalized vector and the sine of the angle.\n\n4. If the cross product is negative, it means the vector is pointing in the negative direction (clockwise), so we negate the angle to indicate the opposite direction.\n\n5. Finally, we return the angle, which represents the inverse cosine with consideration of the vector direction.\n\nThis function takes a 2D vector and a value as input, and it returns the inverse cosine of the value considering the direction of the vector. The returned angle will be positive if the vector is pointing in the counterclockwise direction and negative if the vector is pointing in the clockwise direction.",
  "canonical_solution": "    \n    if vector[0] == 0:\n        if vector[1] >= 0:\n            return 0\n        else:\n            return np.pi\n    return -np.sign(vector[0]) * np.arccos(value)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(np.array([-1, 0]), value = 0.5) == arccos2(np.array([-1, 0]), value = 0.5))\nassert(SOLUTION_SIGNATURE(np.array([1, 0]), value = 0.5) == arccos2(np.array([1, 0]), value = 0.5))\nassert(SOLUTION_SIGNATURE(np.array([0, 1]), value = -1) == arccos2(np.array([0, 1]), value = -1))\nassert(SOLUTION_SIGNATURE(np.array([0, -1]), value = -1) == arccos2(np.array([0, -1]), value = -1))\n"
 },
 {
  "task_id": "RWPB/25",
  "prompt": "import numpy as np\nimport cv2\n\ndef hole_fill(img):\n    \"\"\"\n    Performs hole filling on an image using morphological operations.\n\n    Args:\n        img (np.array): A grayscale image array where holes are to be filled.\n\n    Returns:\n        np.array: An image array with holes filled.\n\n    This function applies the flood fill algorithm to identify and fill holes in the input image.\n    The process involves padding the image, copying it for result calculation, and applying\n    bitwise operations to fill the holes based on the flood filled mask.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `hole_fill` function:\n\n```python\nimport numpy as np\nimport cv2\n\ndef hole_fill(img):\n    \"\"\"\n    Performs hole filling on an image using morphological operations.\n\n    Args:\n        img (np.array): A grayscale image array where holes are to be filled.\n\n    Returns:\n        np.array: An image array with holes filled.\n\n    This function applies the flood fill algorithm to identify and fill holes in the input image.\n    The process involves padding the image, copying it for result calculation, and applying\n    bitwise operations to fill the holes based on the flood filled mask.\n    \"\"\"\n    # Pad the image with a border of 1 pixel\n    padded_img = cv2.copyMakeBorder(img, 1, 1, 1, 1, cv2.BORDER_CONSTANT, value=0)\n\n    # Create a mask for flood filling\n    mask = np.zeros((padded_img.shape[0] + 2, padded_img.shape[1] + 2), dtype=np.uint8)\n\n    # Perform flood fill starting from the top-left corner\n    cv2.floodFill(padded_img, mask, (0, 0), 255)\n\n    # Invert the padded image\n    inverted_img = cv2.bitwise_not(padded_img)\n\n    # Combine the inverted image with the original image\n    result = img | inverted_img[1:-1, 1:-1]\n\n    return result\n```\n\nHere's how the code works:\n\n1. The function takes a grayscale image array (`img`) as input.\n\n2. The image is padded with a border of 1 pixel using `cv2.copyMakeBorder()`. The border is filled with zeros. This padding is necessary to handle holes that touch the image boundaries.\n\n3. A mask is created with the same shape as the padded image, plus an additional border of 2 pixels. The mask is initialized with zeros.\n\n4. The `cv2.floodFill()` function is called to perform flood filling starting from the top-left corner (0, 0) of the padded image. The flood fill operation identifies all connected pixels with the same intensity as the starting pixel and sets them to the specified fill value (255 in this case). The mask is updated to keep track of the filled pixels.\n\n5. The padded image is inverted using `cv2.bitwise_not()`. This step converts the background pixels to foreground pixels and vice versa.\n\n6. The inverted image is combined with the original image using the bitwise OR operation (`|`). The inverted image is sliced to remove the padding, so it matches the size of the original image. This step effectively fills the holes in the original image.\n\n7. Finally, the resulting image with filled holes is returned.\n\nNote: Make sure to have the OpenCV library (`cv2`) installed to run this code.\n\nThis implementation assumes that the input image is a grayscale image where the holes are represented by black pixels (intensity value of 0) and the object pixels are represented by white pixels (intensity value greater than 0). If your input image has a different representation, you may need to adjust the code accordingly.",
  "canonical_solution": "\n    # Pad the image to avoid boundary issues during flood fill\n    img = np.pad(img[1:-1, 1:-1], pad_width=1, mode='constant', constant_values=0)\n    img_copy = img.copy()  # Create a copy of the image for later use\n\n    # Create a mask for flood fill operation, size needs to be 2 pixels bigger than the image\n    mask = np.zeros((img.shape[0] + 2, img.shape[1] + 2), dtype=np.uint8)\n\n    # Perform flood fill from the top-left corner (0,0) with the new value 255\n    cv2.floodFill(img, mask, (0, 0), 255)\n\n    # Create inverse of the flood filled image\n    img_inverse = cv2.bitwise_not(img)\n\n    # Combine the original copy and the inverse to fill the holes\n    dst = cv2.bitwise_or(img_copy, img_inverse)\n\n    return dst\n\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(np.zeros((10, 10), dtype=np.uint8)) == hole_fill(np.zeros((10, 10), dtype=np.uint8)))\n\nimg_large_hole = np.ones((10, 10), dtype=np.uint8) * 255\nimg_large_hole[3:7, 3:7] = 0\nassert(SOLUTION_SIGNATURE(img_large_hole) == hole_fill(img_large_hole))\n\n# Create an image with both small holes on the edges and small holes scattered in the middle of the image\nimg_multiple_holes = np.ones((10, 10), dtype=np.uint8) * 255\nimg_multiple_holes[1, 1] = 0 # small holes in corners\nimg_multiple_holes[1, 8] = 0 # small holes in the other corner\nimg_multiple_holes[5, 5] = 0 # small holes in the centre\nimg_multiple_holes[8, 1] = 0 # edge holes\nimg_multiple_holes[8, 8] = 0 # holes at the edges\n\nassert(SOLUTION_SIGNATURE(img_multiple_holes) == hole_fill(img_multiple_holes))\n"
 },
 {
  "task_id": "RWPB/87",
  "prompt": "import torch\nimport numpy as np\n\n\ndef rotate_x(a, device=None):\n    \"\"\"\n    Creates a 4x4 homogeneous transformation matrix for rotation around the x-axis in 3D space.\n\n    Parameters:\n    a (float): Rotation angle in radians.\n    device (torch.device, optional): The device on which to create the tensor (e.g., 'cpu' or 'cuda'). Default is None, which means the tensor will be created on the default device.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the rotation transformation matrix around the x-axis.\n\n    Process:\n    1. Calculate the sine and cosine of the rotation angle.\n    2. Construct a 4x4 matrix where the rotation is applied to the y and z coordinates, leaving the x coordinate unchanged.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of creating a 4x4 homogeneous transformation matrix for rotation around the x-axis in 3D space:\n\n```python\nimport torch\nimport numpy as np\n\n\ndef rotate_x(a, device=None):\n    \"\"\"\n    Creates a 4x4 homogeneous transformation matrix for rotation around the x-axis in 3D space.\n\n    Parameters:\n    a (float): Rotation angle in radians.\n    device (torch.device, optional): The device on which to create the tensor (e.g., 'cpu' or 'cuda'). Default is None, which means the tensor will be created on the default device.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the rotation transformation matrix around the x-axis.\n\n    Process:\n    1. Calculate the sine and cosine of the rotation angle.\n    2. Construct a 4x4 matrix where the rotation is applied to the y and z coordinates, leaving the x coordinate unchanged.\n    \"\"\"\n    cos_a = torch.cos(torch.tensor(a, dtype=torch.float32))\n    sin_a = torch.sin(torch.tensor(a, dtype=torch.float32))\n\n    rot_x = torch.tensor([\n        [1, 0, 0, 0],\n        [0, cos_a, -sin_a, 0],\n        [0, sin_a, cos_a, 0],\n        [0, 0, 0, 1]\n    ], dtype=torch.float32)\n\n    if device is not None:\n        rot_x = rot_x.to(device)\n\n    return rot_x\n```\n\nExplanation:\n1. We calculate the sine and cosine of the rotation angle `a` using `torch.sin()` and `torch.cos()`. We convert the angle to a PyTorch tensor with `torch.tensor()` and specify the data type as `torch.float32`.\n\n2. We construct a 4x4 matrix `rot_x` using `torch.tensor()`. The matrix represents the rotation around the x-axis in homogeneous coordinates. The rotation is applied to the y and z coordinates, while the x coordinate remains unchanged. The matrix is defined as follows:\n   ```\n   [1    0       0    0]\n   [0  cos(a) -sin(a) 0]\n   [0  sin(a)  cos(a) 0]\n   [0    0       0    1]\n   ```\n\n3. If a `device` is specified (e.g., 'cpu' or 'cuda'), we move the `rot_x` tensor to the specified device using `rot_x.to(device)`. This ensures that the tensor is created on the desired device.\n\n4. Finally, we return the `rot_x` tensor representing the rotation transformation matrix around the x-axis.\n\nThis function can be used to create a rotation matrix around the x-axis for a given angle `a`. The resulting matrix can be used to transform 3D points or vectors by multiplying them with the matrix.",
  "canonical_solution": "    \n    s, c = np.sin(a), np.cos(a)\n    return torch.tensor([[1,  0, 0, 0], \n                         [0,  c, s, 0], \n                         [0, -s, c, 0], \n                         [0,  0, 0, 1]], dtype=torch.float32, device=device)\n\n\n",
  "unprocess_testcases": "a = torch.pi / 4\nassert((SOLUTION_SIGNATURE(a) == rotate_x(a)).all())\n\na = 0\nassert((SOLUTION_SIGNATURE(a) == rotate_x(a)).all())\n\na = 2 * torch.pi\nassert((SOLUTION_SIGNATURE(a) == rotate_x(a)).all())\n"
 },
 {
  "task_id": "RWPB/129",
  "prompt": "import torch\nimport torch.nn.functional as F\n\n\ndef scale_masks(masks, shape, padding=True):\n    \"\"\"\n    Rescale segment masks to shape.\n\n    Args:\n        masks (torch.Tensor): (N, C, H, W).\n        shape (tuple): Height and width.\n        padding (bool): If True, assuming the boxes is based on image augmented by yolo style. If False then do regular\n            rescaling.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport torch\nimport torch.nn.functional as F\n\n\ndef scale_masks(masks, shape, padding=True):\n    \"\"\"\n    Rescale segment masks to shape.\n\n    Args:\n        masks (torch.Tensor): (N, C, H, W).\n        shape (tuple): Height and width.\n        padding (bool): If True, assuming the boxes is based on image augmented by yolo style. If False then do regular\n            rescaling.\n    \"\"\"\n    height, width = shape\n    \n    if padding:\n        # Assuming masks are based on image augmented by YOLO style\n        # Determine the scale factor based on the original mask size and the desired shape\n        scale_factor = min(height / masks.shape[2], width / masks.shape[3])\n        \n        # Resize the masks using bilinear interpolation\n        scaled_masks = F.interpolate(masks, scale_factor=scale_factor, mode='bilinear', align_corners=False)\n        \n        # Pad the masks to match the desired shape\n        pad_height = height - scaled_masks.shape[2]\n        pad_width = width - scaled_masks.shape[3]\n        pad_top = pad_height // 2\n        pad_bottom = pad_height - pad_top\n        pad_left = pad_width // 2\n        pad_right = pad_width - pad_left\n        \n        scaled_masks = F.pad(scaled_masks, (pad_left, pad_right, pad_top, pad_bottom), mode='constant', value=0)\n    else:\n        # Regular rescaling without padding\n        scaled_masks = F.interpolate(masks, size=(height, width), mode='bilinear', align_corners=False)\n    \n    return scaled_masks\n```\n\nExplanation:\n\n1. The function takes in the `masks` tensor of shape (N, C, H, W), the desired `shape` (height and width), and a boolean `padding` flag.\n\n2. If `padding` is True, it assumes that the masks are based on an image augmented by YOLO style.\n   - It calculates the scale factor based on the original mask size and the desired shape, taking the minimum of the height and width ratios.\n   - It resizes the masks using bilinear interpolation with the calculated scale factor.\n   - It determines the padding dimensions required to match the desired shape.\n   - It pads the masks symmetrically on all sides using the calculated padding dimensions.\n\n3. If `padding` is False, it performs regular rescaling without padding.\n   - It directly resizes the masks to the desired shape using bilinear interpolation.\n\n4. Finally, it returns the scaled masks.\n\nThis code uses PyTorch's `F.interpolate()` function to resize the masks and `F.pad()` function to pad the masks if necessary. The bilinear interpolation mode is used for smooth resizing, and the `align_corners` parameter is set to False to avoid artifacts at the corners.",
  "canonical_solution": "    \n    mh, mw = masks.shape[2:]\n    gain = min(mh / shape[0], mw / shape[1])  # gain  = old / new\n    pad = [mw - shape[1] * gain, mh - shape[0] * gain]  # wh padding\n    if padding:\n        pad[0] /= 2\n        pad[1] /= 2\n    top, left = (int(pad[1]), int(pad[0])) if padding else (0, 0)  # y, x\n    bottom, right = (int(mh - pad[1]), int(mw - pad[0]))\n    masks = masks[..., top:bottom, left:right]\n\n    masks = F.interpolate(masks, shape, mode=\"bilinear\", align_corners=False)  # NCHW\n    return masks\n\n\n",
  "unprocess_testcases": "masks = torch.rand(2, 3, 100, 200)\ntarget_shape = (50, 100)\nassert((SOLUTION_SIGNATURE(masks, target_shape) == scale_masks(masks, target_shape)).all())\n\nmasks = torch.rand(1, 1, 150, 300)\ntarget_shape = (100, 100)\nassert((SOLUTION_SIGNATURE(masks, target_shape) == scale_masks(masks, target_shape)).all())\n\nmasks = torch.rand(3, 3, 200, 200)\ntarget_shape = (210, 210)\nassert((SOLUTION_SIGNATURE(masks, target_shape) == scale_masks(masks, target_shape)).all())\n"
 },
 {
  "task_id": "RWPB/138",
  "prompt": "import numpy as np\n\ndef rotate_normalmap_by_angle(normal_map: np.ndarray, angle: float):\n    \"\"\"\n    Rotates the normals in a normal map along the y-axis by a specified angle.\n\n    Args:\n        normal_map (np.ndarray): An array of shape (H, W, 3) representing the normal map,\n                                 where H is the height, W is the width, and each normal is a 3D vector with components in [-1, 1].\n        angle (float): The rotation angle in degrees.\n\n    Returns:\n        np.ndarray: The rotated normal map with the same shape as the input.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of rotating a normal map by a specified angle along the y-axis:\n\n```python\nimport numpy as np\n\ndef rotate_normalmap_by_angle(normal_map: np.ndarray, angle: float):\n    \"\"\"\n    Rotates the normals in a normal map along the y-axis by a specified angle.\n\n    Args:\n        normal_map (np.ndarray): An array of shape (H, W, 3) representing the normal map,\n                                 where H is the height, W is the width, and each normal is a 3D vector with components in [-1, 1].\n        angle (float): The rotation angle in degrees.\n\n    Returns:\n        np.ndarray: The rotated normal map with the same shape as the input.\n    \"\"\"\n    # Convert the angle from degrees to radians\n    angle_rad = np.deg2rad(angle)\n\n    # Create the rotation matrix along the y-axis\n    rotation_matrix = np.array([\n        [np.cos(angle_rad), 0, np.sin(angle_rad)],\n        [0, 1, 0],\n        [-np.sin(angle_rad), 0, np.cos(angle_rad)]\n    ])\n\n    # Reshape the normal map to a 2D array of shape (H*W, 3)\n    h, w, _ = normal_map.shape\n    normals_2d = normal_map.reshape(-1, 3)\n\n    # Apply the rotation matrix to each normal vector\n    rotated_normals_2d = np.dot(normals_2d, rotation_matrix)\n\n    # Reshape the rotated normals back to the original shape (H, W, 3)\n    rotated_normal_map = rotated_normals_2d.reshape(h, w, 3)\n\n    return rotated_normal_map\n```\n\nExplanation:\n\n1. The function takes a normal map (`normal_map`) and a rotation angle in degrees (`angle`) as input.\n\n2. It converts the angle from degrees to radians using `np.deg2rad()`.\n\n3. It creates the rotation matrix along the y-axis using the cosine and sine of the angle. The rotation matrix is a 3x3 matrix that represents the rotation transformation.\n\n4. The normal map is reshaped from a 3D array of shape (H, W, 3) to a 2D array of shape (H*W, 3) using `normal_map.reshape(-1, 3)`. This step flattens the spatial dimensions (height and width) into a single dimension, allowing us to apply the rotation matrix to each normal vector.\n\n5. The rotation matrix is applied to each normal vector using matrix multiplication (`np.dot()`). This step rotates each normal vector by the specified angle along the y-axis.\n\n6. The rotated normals are reshaped back to the original shape (H, W, 3) using `rotated_normals_2d.reshape(h, w, 3)`.\n\n7. Finally, the function returns the rotated normal map.\n\nThis code rotates the normals in the normal map by the specified angle along the y-axis. The rotation is performed by creating a rotation matrix and applying it to each normal vector using matrix multiplication. The resulting rotated normal map has the same shape as the input normal map.",
  "canonical_solution": "    \n    # Convert the angle from degrees to radians\n    angle = angle / 180 * np.pi\n    # Define the rotation matrix for rotating around the y-axis\n    R = np.array([\n        [np.cos(angle), 0, np.sin(angle)],  # X' = X*cos(theta) + Z*sin(theta)\n        [0, 1, 0],                          # Y' = Y (no change)\n        [-np.sin(angle), 0, np.cos(angle)]  # Z' = -X*sin(theta) + Z*cos(theta)\n    ])\n    # Apply the rotation matrix to each normal vector in the normal map\n    # Reshape the normal map from (H, W, 3) to (H*W, 3) for matrix multiplication, then reshape back to (H, W, 3)\n    return np.dot(normal_map.reshape(-1, 3), R.T).reshape(normal_map.shape)\n\n\n",
  "unprocess_testcases": "normal_map = np.array([[[0, 0, 1]]])\nangle = 90\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle(normal_map, angle)).all())\n\nnormal_map = np.array([[[1, 0, 0], [0, 1, 0], [0, 0, 1]]])\nangle = 0\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle(normal_map, angle)).all())\n\nnormal_map = np.array([[[1, 0, 0], [0, 1, 0], [0, 0, 1]]])\nangle = 720  # Two full rotations\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle(normal_map, angle)).all())\n"
 },
 {
  "task_id": "RWPB/96",
  "prompt": "import numpy as np\n\ndef movmean(arr, n_avg):\n  \"\"\"\n  Compute the moving average of an array.\n  Args:\n      - arr: numpy.ndarray, the input array\n      - n_avg: int, the number of elements to include in the moving average\n  Returns:\n      mov_avg: numpy.ndarray, the moving average of the input array\n\n  If the windows size is bigger than the length of array, the code should return a empty array.\n  \"\"\"\n",
  "solution": "Here's the completed code to compute the moving average of an array using NumPy:\n\n```python\nimport numpy as np\n\ndef movmean(arr, n_avg):\n    \"\"\"\n    Compute the moving average of an array.\n    Args:\n        - arr: numpy.ndarray, the input array\n        - n_avg: int, the number of elements to include in the moving average\n    Returns:\n        mov_avg: numpy.ndarray, the moving average of the input array\n\n    If the windows size is bigger than the length of array, the code should return an empty array.\n    \"\"\"\n    if n_avg > len(arr):\n        return np.array([])\n\n    mov_avg = np.zeros(len(arr) - n_avg + 1)\n    for i in range(len(mov_avg)):\n        window = arr[i:i+n_avg]\n        mov_avg[i] = np.mean(window)\n\n    return mov_avg\n```\n\nExplanation:\n\n1. We first check if the window size (`n_avg`) is bigger than the length of the input array (`arr`). If it is, we return an empty array using `np.array([])`.\n\n2. If the window size is valid, we initialize a new array called `mov_avg` with zeros. The length of `mov_avg` is calculated as `len(arr) - n_avg + 1`, which represents the number of moving average values that can be computed.\n\n3. We use a loop to iterate over the indices of `mov_avg`. For each index `i`:\n   - We extract a window of size `n_avg` from the input array `arr`, starting from index `i` and ending at index `i+n_avg`.\n   - We compute the mean of the window using `np.mean(window)` and assign it to the corresponding element of `mov_avg` at index `i`.\n\n4. Finally, we return the `mov_avg` array containing the moving average values.\n\nThis implementation computes the moving average by sliding a window of size `n_avg` over the input array and calculating the mean of each window. The resulting `mov_avg` array contains the moving average values for each position of the sliding window.",
  "canonical_solution": "  \n  sums = np.cumsum(arr, dtype=float)\n  sums[n_avg:] -= sums[:-n_avg]\n  return sums[n_avg - 1 :] / n_avg\n\n\n",
  "unprocess_testcases": "arr = [1, 2, 3, 4, 5]\nn_avg = 3\nassert((SOLUTION_SIGNATURE(arr, n_avg) == movmean(arr, n_avg)).all())\n\narr = [1, 2, 3, 4, 5]\nn_avg = 5\nassert((SOLUTION_SIGNATURE(arr, n_avg) == movmean(arr, n_avg)).all())\n\narr = [1, 2, 3]\nn_avg = 4\nassert((SOLUTION_SIGNATURE(arr, n_avg) == movmean(arr, n_avg)).all())\n"
 },
 {
  "task_id": "RWPB/9",
  "prompt": "def is_byte_token(s: str) -> bool:\n  \"\"\"Returns True if s is a byte string like \"<0xAB>\".\n    input:\n        s: string\n    return\n        bool: Ture or False\n  \"\"\"\n  # Bytes look like \"<0xAB>\".\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\ndef is_byte_token(s: str) -> bool:\n    \"\"\"Returns True if s is a byte string like \"<0xAB>\".\n    input:\n        s: string\n    return\n        bool: True or False\n    \"\"\"\n    # Bytes look like \"<0xAB>\".\n    if len(s) != 6:\n        return False\n    if s[0] != '<' or s[5] != '>':\n        return False\n    if s[1:3] != '0x':\n        return False\n    if not s[3].isdigit() and not ('A' <= s[3] <= 'F'):\n        return False\n    if not s[4].isdigit() and not ('A' <= s[4] <= 'F'):\n        return False\n    return True\n```\n\nExplanation:\n\n1. We first check if the length of the string `s` is exactly 6 characters. If not, it cannot be a valid byte token, so we return `False`.\n\n2. We then check if the first character of `s` is `'<'` and the last character is `'>'`. If either of these conditions is not met, we return `False`.\n\n3. Next, we check if the second and third characters of `s` are `'0x'`. If not, it is not a valid byte token, so we return `False`.\n\n4. We then check if the fourth character of `s` is either a digit or an uppercase letter between `'A'` and `'F'`. If not, it is not a valid hexadecimal digit, so we return `False`.\n\n5. Similarly, we check if the fifth character of `s` is either a digit or an uppercase letter between `'A'` and `'F'`. If not, it is not a valid hexadecimal digit, so we return `False`.\n\n6. If all the above conditions are satisfied, the string `s` is a valid byte token, so we return `True`.\n\nThis implementation checks if the given string `s` follows the format of a byte token, which is `\"<0xAB>\"`, where `AB` represents a two-digit hexadecimal value. The function returns `True` if `s` is a valid byte token and `False` otherwise.",
  "canonical_solution": "  \n  if len(s) != 6 or s[0:3] != \"<0x\" or s[-1] != \">\":\n    return False\n  return True\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(\"<0xAB>\") == is_byte_token(\"<0xAB>\"))\nassert(SOLUTION_SIGNATURE(\"<0xG1>\") == is_byte_token(\"<0xG1>\"))\nassert(SOLUTION_SIGNATURE(\"0xAB>\") == is_byte_token(\"0xAB>\"))\nassert(SOLUTION_SIGNATURE(\"<0xAB\") == is_byte_token(\"<0xAB\"))\n"
 },
 {
  "task_id": "RWPB/34",
  "prompt": "import numpy as np\n\ndef pad_tokens(tokens: np.ndarray, context_window: int, pad_token: int) -> np.ndarray:\n    \"\"\"\n    Pads or truncates a sequence of tokens to a specified length determined by the context window.\n\n    Args:\n        tokens (np.ndarray): An array of tokenized data, possibly with multiple sequences.\n                             The shape should typically be (batch_size, sequence_length) or (sequence_length,).\n        context_window (int): The target number of tokens in a sequence after padding or truncation.\n                              The final sequence length will be `context_window + 1`.\n        pad_token (int): The token used to pad shorter sequences.\n\n    Returns:\n        np.ndarray: An array where each sequence is exactly `context_window + 1` tokens long.\n\n    This function modifies the length of sequences in a batch of tokenized data:\n    - If a sequence is longer than `context_window + 1`, it gets truncated to this length.\n    - If a sequence is shorter, it gets padded with `pad_token` until it reaches the length of `context_window + 1`.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `pad_tokens` function:\n\n```python\nimport numpy as np\n\ndef pad_tokens(tokens: np.ndarray, context_window: int, pad_token: int) -> np.ndarray:\n    \"\"\"\n    Pads or truncates a sequence of tokens to a specified length determined by the context window.\n\n    Args:\n        tokens (np.ndarray): An array of tokenized data, possibly with multiple sequences.\n                             The shape should typically be (batch_size, sequence_length) or (sequence_length,).\n        context_window (int): The target number of tokens in a sequence after padding or truncation.\n                              The final sequence length will be `context_window + 1`.\n        pad_token (int): The token used to pad shorter sequences.\n\n    Returns:\n        np.ndarray: An array where each sequence is exactly `context_window + 1` tokens long.\n\n    This function modifies the length of sequences in a batch of tokenized data:\n    - If a sequence is longer than `context_window + 1`, it gets truncated to this length.\n    - If a sequence is shorter, it gets padded with `pad_token` until it reaches the length of `context_window + 1`.\n    \"\"\"\n    # Get the original shape of the tokens array\n    original_shape = tokens.shape\n\n    # Reshape the tokens array to a 2D array if it is 1D\n    if len(original_shape) == 1:\n        tokens = tokens.reshape(1, -1)\n\n    # Get the current number of sequences and their lengths\n    num_sequences, sequence_length = tokens.shape\n\n    # Create a new array with the desired shape (num_sequences, context_window + 1)\n    padded_tokens = np.full((num_sequences, context_window + 1), pad_token, dtype=tokens.dtype)\n\n    # Truncate or pad each sequence to the desired length\n    for i in range(num_sequences):\n        if sequence_length <= context_window + 1:\n            padded_tokens[i, :sequence_length] = tokens[i, :sequence_length]\n        else:\n            padded_tokens[i, :] = tokens[i, :context_window + 1]\n\n    # Reshape the padded tokens array back to the original shape if it was 1D\n    if len(original_shape) == 1:\n        padded_tokens = padded_tokens.reshape(-1)\n\n    return padded_tokens\n```\n\nThis implementation does the following:\n\n1. It gets the original shape of the `tokens` array to handle both 1D and 2D arrays.\n\n2. If the `tokens` array is 1D, it reshapes it to a 2D array with a single sequence.\n\n3. It determines the current number of sequences and their lengths from the reshaped `tokens` array.\n\n4. It creates a new array called `padded_tokens` with the desired shape `(num_sequences, context_window + 1)` and fills it with the `pad_token` value.\n\n5. It iterates over each sequence in the `tokens` array:\n   - If the sequence length is less than or equal to `context_window + 1`, it copies the sequence to the corresponding position in `padded_tokens`.\n   - If the sequence length is greater than `context_window + 1`, it truncates the sequence to the desired length and copies it to `padded_tokens`.\n\n6. If the original `tokens` array was 1D, it reshapes the `padded_tokens` array back to a 1D array.\n\n7. Finally, it returns the `padded_tokens` array with each sequence having a length of `context_window + 1`.\n\nThis function handles both single sequences (1D arrays) and batches of sequences (2D arrays) and ensures that each sequence in the output has a consistent length of `context_window + 1` tokens.",
  "canonical_solution": "\n    # Determine the target length for each sequence\n    target_length = context_window + 1\n    current_length = tokens.shape[-1]\n\n    # Truncate if the current length is greater than the target length\n    if current_length > target_length:\n        tokens = tokens[..., :target_length]\n    # Pad if the current length is less than the target length\n    elif current_length < target_length:\n        # Calculate the required number of padding elements\n        padding_size = target_length - current_length\n        # Create a padding array filled with the pad_token\n        padding = np.full(tokens.shape[:-1] + (padding_size,), pad_token)\n        # Concatenate the original tokens with the padding array\n        tokens = np.concatenate([tokens, padding], axis=-1)\n\n    # Ensure the final tokens array is of the correct shape\n    assert tokens.shape[-1] == target_length, \"Final token sequence length does not match the target length.\"\n\n    return tokens\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5, 6, 7]), 4, 0) == pad_tokens(np.array([1, 2, 3, 4, 5, 6, 7]), 4, 0)).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3]), 5, 0) == pad_tokens(np.array([1, 2, 3]), 5, 0)).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5]), 4, 0) == pad_tokens(np.array([1, 2, 3, 4, 5]), 4, 0)).all())\n"
 },
 {
  "task_id": "RWPB/65",
  "prompt": "import itertools as it\nimport numpy as np\n\n\ndef merge_dicts_recursively(*dicts):\n    \"\"\"\n    Create a dictionary whose keys are the union of all input dictionaries. \n    The value for each key is based on the first dictionary in the list with that key.\n    Dictionaries later in the list have higher priority.\n    When values are dictionaries, it is applied recursively.\n    Args:\n        - *dicts: tuple of dicts, the dictionaries to merge\n    Returns:\n        result: dict, the merged dictionary\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of merging dictionaries recursively:\n\n```python\nimport itertools as it\nimport numpy as np\n\n\ndef merge_dicts_recursively(*dicts):\n    \"\"\"\n    Create a dictionary whose keys are the union of all input dictionaries. \n    The value for each key is based on the first dictionary in the list with that key.\n    Dictionaries later in the list have higher priority.\n    When values are dictionaries, it is applied recursively.\n    Args:\n        - *dicts: tuple of dicts, the dictionaries to merge\n    Returns:\n        result: dict, the merged dictionary\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            if k in result and isinstance(result[k], dict) and isinstance(v, dict):\n                result[k] = merge_dicts_recursively(result[k], v)\n            else:\n                result[k] = v\n    return result\n```\n\nExplanation:\n\n1. We define the `merge_dicts_recursively` function that takes a variable number of dictionaries as arguments using `*dicts`.\n\n2. We initialize an empty dictionary called `result` to store the merged dictionary.\n\n3. We iterate over each dictionary `d` in the `dicts` tuple using a `for` loop.\n\n4. For each key-value pair `(k, v)` in the current dictionary `d`:\n   - If the key `k` already exists in the `result` dictionary and both the existing value `result[k]` and the new value `v` are dictionaries, we recursively call `merge_dicts_recursively` with `result[k]` and `v` as arguments to merge the nested dictionaries.\n   - Otherwise, we assign the value `v` to the key `k` in the `result` dictionary, overwriting any existing value.\n\n5. Finally, we return the `result` dictionary, which contains the merged key-value pairs from all the input dictionaries.\n\nThis implementation recursively merges the dictionaries, giving priority to the dictionaries later in the list. If a key exists in multiple dictionaries and the corresponding values are dictionaries, it recursively applies the merging process to those nested dictionaries.\n\nNote: The `itertools` and `numpy` imports are not used in this specific implementation, but they are kept in the code as per your request.",
  "canonical_solution": "    \n    result = dict()\n    all_items = it.chain(*[d.items() for d in dicts])\n    for key, value in all_items:\n        if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n            result[key] = merge_dicts_recursively(result[key], value)\n        else:\n            result[key] = value\n    return result\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE({'a': 1, 'b': 2}, {'b': 3, 'c': 4}) == merge_dicts_recursively({'a': 1, 'b': 2}, {'b': 3, 'c': 4}))\nassert(SOLUTION_SIGNATURE({'a': 1, 'nested': {'x': 10, 'y': 20}}, {'b': 2, 'nested': {'y': 30, 'z': 40}}) == merge_dicts_recursively({'a': 1, 'nested': {'x': 10, 'y': 20}}, {'b': 2, 'nested': {'y': 30, 'z': 40}}))\nassert(SOLUTION_SIGNATURE({'a': [1, 2, 3], 'b': 'hello'}, {'a': 'not a list', 'b': {'nested': 'dict'}}) == merge_dicts_recursively({'a': [1, 2, 3], 'b': 'hello'}, {'a': 'not a list', 'b': {'nested': 'dict'}}))\n"
 },
 {
  "task_id": "RWPB/51",
  "prompt": "import torch\nimport torch.nn.functional as F\n\n\ndef _ssim(img1, img2, window, window_size, channel, size_average=True):\n    \"\"\"\n    Calculate the Structural Similarity Index (SSIM) between two images.\n\n    Arguments:\n    img1, img2 : torch.Tensor\n        The two images between which SSIM is to be computed. They should have the same dimensions.\n    window : torch.Tensor\n        The window tensor used for mean and variance calculation, typically a Gaussian window.\n    window_size : int\n        The size of the Gaussian window, used here for padding calculations.\n    channel : int\n        The number of channels in img1 and img2 (e.g., 3 for RGB, 1 for grayscale).\n    size_average : bool, optional\n        If True (default), returns the mean SSIM over all elements. If False, returns a tensor of SSIM values for each channel.\n\n    Returns:\n    torch.Tensor\n        SSIM index as a float or as a tensor based on 'size_average' parameter.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `_ssim` function:\n\n```python\nimport torch\nimport torch.nn.functional as F\n\n\ndef _ssim(img1, img2, window, window_size, channel, size_average=True):\n    \"\"\"\n    Calculate the Structural Similarity Index (SSIM) between two images.\n\n    Arguments:\n    img1, img2 : torch.Tensor\n        The two images between which SSIM is to be computed. They should have the same dimensions.\n    window : torch.Tensor\n        The window tensor used for mean and variance calculation, typically a Gaussian window.\n    window_size : int\n        The size of the Gaussian window, used here for padding calculations.\n    channel : int\n        The number of channels in img1 and img2 (e.g., 3 for RGB, 1 for grayscale).\n    size_average : bool, optional\n        If True (default), returns the mean SSIM over all elements. If False, returns a tensor of SSIM values for each channel.\n\n    Returns:\n    torch.Tensor\n        SSIM index as a float or as a tensor based on 'size_average' parameter.\n    \"\"\"\n    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1 * mu2\n\n    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n\n    C1 = 0.01 ** 2\n    C2 = 0.03 ** 2\n\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n\n    if size_average:\n        return ssim_map.mean()\n    else:\n        return ssim_map.mean(1).mean(1).mean(1)\n```\n\nExplanation:\n1. We calculate the means `mu1` and `mu2` of `img1` and `img2` respectively using a 2D convolution with the provided `window` tensor. The `padding` ensures that the output has the same size as the input, and `groups=channel` applies the convolution independently for each channel.\n\n2. We calculate the squares of the means `mu1_sq` and `mu2_sq`, as well as the product of the means `mu1_mu2`.\n\n3. We calculate the variances `sigma1_sq` and `sigma2_sq` of `img1` and `img2` respectively, by applying the convolution on the squared images and subtracting the squared means.\n\n4. We calculate the covariance `sigma12` between `img1` and `img2` by applying the convolution on the product of the images and subtracting the product of the means.\n\n5. We define constants `C1` and `C2` to avoid division by zero and provide numerical stability.\n\n6. We calculate the SSIM map using the formula: `SSIM = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))`.\n\n7. If `size_average` is True, we return the mean of the SSIM map. Otherwise, we return the mean SSIM value for each channel by averaging over the spatial dimensions.\n\nThis implementation assumes that `img1` and `img2` have the same dimensions and number of channels, and that the `window` tensor is properly defined based on the desired window size and channel count.",
  "canonical_solution": "\n    # Apply convolution to calculate the mean of img1 and img2 using the specified window\n    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n\n    # Compute the squares of means\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    # Compute the mean of products\n    mu1_mu2 = mu1 * mu2\n\n    # Calculate variance of img1 and img2, and the covariance between them\n    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n\n    # Constants for stability (avoid division by zero)\n    C1 = 0.01 ** 2\n    C2 = 0.03 ** 2\n\n    # Compute the SSIM index\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n\n    # Return the average SSIM over all pixels if size_average is True, else return SSIM map\n    if size_average:\n        return ssim_map.mean()  # average over all channels and pixels\n    else:\n        return ssim_map.mean(1).mean(1).mean(1)  # average over each channel separately\n\n\n",
  "unprocess_testcases": "def gaussian_window(channel, window_size):\n    \"\"\" Creates a Gaussian window tensor for given channel and window size. \"\"\"\n    # Ensure the operation is done with tensors\n    _1D_window = torch.exp(-torch.arange(window_size).float()**2 / (2 * window_size**2))\n    _1D_window = _1D_window / _1D_window.sum()  # Normalize the window\n    _2D_window = _1D_window[:, None] * _1D_window[None, :]  # Create 2D Gaussian window\n    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n    return window\n\n\nimg = torch.rand(1, 3, 256, 256)\nassert((SOLUTION_SIGNATURE(img, img.clone(), gaussian_window(3, 11), 11, 3) == _ssim(img, img.clone(), gaussian_window(3, 11), 11, 3)).all())\n\nimg1 = torch.zeros(1, 1, 512, 512)  # Black image\nimg2 = torch.ones(1, 1, 512, 512)   # White image\nwindow = gaussian_window(1, 25)\nassert((SOLUTION_SIGNATURE(img1, img2, window, 25, 1, False) == _ssim(img1, img2, window, 25, 1, False)).all())\n\nimg1 = torch.rand(1, 1, 300, 300)\nimg2 = torch.rand(1, 1, 300, 300)\nwindow = gaussian_window(1, 25)\nassert((SOLUTION_SIGNATURE(img1, img2, window, 11, 1) == _ssim(img1, img2, window, 11, 1)).all())\n"
 },
 {
  "task_id": "RWPB/10",
  "prompt": "import torch\n\ndef euler2rot(euler_angle):\n    \"\"\"\n    Converts Euler angles to a rotation matrix.\n\n    Args:\n    euler_angle (torch.Tensor): Tensor of shape [batch_size, 3] containing Euler angles.\n\n    Returns:\n    torch.Tensor: Rotation matrix of shape [batch_size, 3, 3].\n    \"\"\"\n",
  "solution": "Here's the completed code to convert Euler angles to a rotation matrix using PyTorch:\n\n```python\nimport torch\nimport math\n\ndef euler2rot(euler_angle):\n    \"\"\"\n    Converts Euler angles to a rotation matrix.\n\n    Args:\n    euler_angle (torch.Tensor): Tensor of shape [batch_size, 3] containing Euler angles.\n\n    Returns:\n    torch.Tensor: Rotation matrix of shape [batch_size, 3, 3].\n    \"\"\"\n    batch_size = euler_angle.shape[0]\n    \n    # Extract Euler angles\n    alpha = euler_angle[:, 0]\n    beta = euler_angle[:, 1]\n    gamma = euler_angle[:, 2]\n    \n    # Compute sine and cosine values\n    cos_alpha = torch.cos(alpha)\n    sin_alpha = torch.sin(alpha)\n    cos_beta = torch.cos(beta)\n    sin_beta = torch.sin(beta)\n    cos_gamma = torch.cos(gamma)\n    sin_gamma = torch.sin(gamma)\n    \n    # Compute rotation matrix elements\n    r11 = cos_alpha * cos_beta\n    r12 = cos_alpha * sin_beta * sin_gamma - sin_alpha * cos_gamma\n    r13 = cos_alpha * sin_beta * cos_gamma + sin_alpha * sin_gamma\n    r21 = sin_alpha * cos_beta\n    r22 = sin_alpha * sin_beta * sin_gamma + cos_alpha * cos_gamma\n    r23 = sin_alpha * sin_beta * cos_gamma - cos_alpha * sin_gamma\n    r31 = -sin_beta\n    r32 = cos_beta * sin_gamma\n    r33 = cos_beta * cos_gamma\n    \n    # Stack rotation matrix elements\n    rot_matrix = torch.stack([\n        r11, r12, r13,\n        r21, r22, r23,\n        r31, r32, r33\n    ], dim=-1).view(batch_size, 3, 3)\n    \n    return rot_matrix\n```\n\nExplanation:\n\n1. We extract the Euler angles (alpha, beta, gamma) from the input tensor `euler_angle` using indexing.\n\n2. We compute the sine and cosine values of each Euler angle using `torch.sin()` and `torch.cos()` functions.\n\n3. We calculate the elements of the rotation matrix using the trigonometric formulas for Euler angles to rotation matrix conversion.\n\n4. We stack the rotation matrix elements into a tensor using `torch.stack()` and reshape it to the desired shape of [batch_size, 3, 3] using `view()`.\n\n5. Finally, we return the computed rotation matrix.\n\nThis function takes a tensor of Euler angles with shape [batch_size, 3] and returns the corresponding rotation matrix with shape [batch_size, 3, 3].\n\nNote: The Euler angles are assumed to be in radians.",
  "canonical_solution": "\n    batch_size = euler_angle.shape[0]\n    theta = euler_angle[:, 0].reshape(-1, 1, 1)\n    phi = euler_angle[:, 1].reshape(-1, 1, 1)\n    psi = euler_angle[:, 2].reshape(-1, 1, 1)\n    one = torch.ones((batch_size, 1, 1), dtype=torch.float32, device=euler_angle.device)\n    zero = torch.zeros((batch_size, 1, 1), dtype=torch.float32, device=euler_angle.device)\n    rot_x = torch.cat((\n        torch.cat((one, zero, zero), 1),\n        torch.cat((zero, theta.cos(), theta.sin()), 1),\n        torch.cat((zero, -theta.sin(), theta.cos()), 1),\n    ), 2)\n    rot_y = torch.cat((\n        torch.cat((phi.cos(), zero, -phi.sin()), 1),\n        torch.cat((zero, one, zero), 1),\n        torch.cat((phi.sin(), zero, phi.cos()), 1),\n    ), 2)\n    rot_z = torch.cat((\n        torch.cat((psi.cos(), -psi.sin(), zero), 1),\n        torch.cat((psi.sin(), psi.cos(), zero), 1),\n        torch.cat((zero, zero, one), 1)\n    ), 2)\n    return torch.bmm(rot_x, torch.bmm(rot_y, rot_z))\n\n\n",
  "unprocess_testcases": "a = torch.tensor([[0.0, 0.0, 0.0]])\nb = torch.tensor([[0.0, 0.0, torch.pi / 4]])\nc = torch.tensor([[torch.pi / 2, torch.pi / 4, torch.pi / 6]])\n\nassert((SOLUTION_SIGNATURE(a) == euler2rot(a)).all())\nassert((SOLUTION_SIGNATURE(b) == euler2rot(b)).all())\nassert((SOLUTION_SIGNATURE(c) == euler2rot(c)).all())\n"
 },
 {
  "task_id": "RWPB/41",
  "prompt": "def merge_short_sentences_en(sens):\n    \"\"\"Avoid short sentences by merging them with the following sentence.\n\n    Args:\n        List[str]: list of input sentences.\n\n    Returns:\n        List[str]: list of output sentences.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of merging short sentences in English:\n\n```python\ndef merge_short_sentences_en(sens):\n    \"\"\"Avoid short sentences by merging them with the following sentence.\n\n    Args:\n        List[str]: list of input sentences.\n\n    Returns:\n        List[str]: list of output sentences.\n    \"\"\"\n    min_length = 10  # Minimum length threshold for a sentence\n    merged_sens = []\n\n    i = 0\n    while i < len(sens):\n        current_sen = sens[i]\n\n        if len(current_sen) < min_length:\n            if i + 1 < len(sens):\n                next_sen = sens[i + 1]\n                merged_sen = current_sen + \" \" + next_sen\n                merged_sens.append(merged_sen)\n                i += 2  # Skip the next sentence since it's already merged\n            else:\n                merged_sens.append(current_sen)\n                i += 1\n        else:\n            merged_sens.append(current_sen)\n            i += 1\n\n    return merged_sens\n```\n\nExplanation:\n\n1. The function `merge_short_sentences_en` takes a list of sentences (`sens`) as input.\n\n2. We define a `min_length` variable to set the minimum length threshold for a sentence. In this case, it's set to 10 characters.\n\n3. We initialize an empty list `merged_sens` to store the merged sentences.\n\n4. We start a loop that iterates over the sentences in `sens` using the index `i`.\n\n5. For each sentence (`current_sen`), we check if its length is less than `min_length`.\n\n6. If the length of `current_sen` is less than `min_length`, we check if there is a next sentence available (i.e., `i + 1` is less than the length of `sens`).\n   - If there is a next sentence, we concatenate `current_sen` with the next sentence (`next_sen`) using a space separator and append the merged sentence to `merged_sens`. We then increment `i` by 2 to skip the next sentence since it's already merged.\n   - If there is no next sentence, we simply append `current_sen` to `merged_sens` and increment `i` by 1.\n\n7. If the length of `current_sen` is greater than or equal to `min_length`, we append it to `merged_sens` without merging and increment `i` by 1.\n\n8. Finally, we return the list of merged sentences (`merged_sens`).\n\nThis code merges short sentences (less than `min_length` characters) with the following sentence to avoid having very short sentences in the output. If a short sentence is the last sentence in the list, it remains unchanged.",
  "canonical_solution": "\n    sens_out = []\n    for s in sens:\n        # If the previous sentense is too short, merge them with\n        # the current sentence.\n        if len(sens_out) > 0 and len(sens_out[-1].split(\" \")) <= 2:\n            sens_out[-1] = sens_out[-1] + \" \" + s\n        else:\n            sens_out.append(s)\n    try:\n        if len(sens_out[-1].split(\" \")) <= 2:\n            sens_out[-2] = sens_out[-2] + \" \" + sens_out[-1]\n            sens_out.pop(-1)\n    except:\n        pass\n    return sens_out\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE([\"I am.\", \"Here now.\", \"John is going to the store.\", \"Oh.\", \"Really?\", \"That's interesting.\"]) == merge_short_sentences_en([\"I am.\", \"Here now.\", \"John is going to the store.\", \"Oh.\", \"Really?\", \"That's interesting.\"]))\nassert(SOLUTION_SIGNATURE([\"This is a proper sentence.\", \"Here is another example.\", \"Each of these sentences is long enough.\"]) == merge_short_sentences_en([\"This is a proper sentence.\", \"Here is another example.\", \"Each of these sentences is long enough.\"]))\nassert(SOLUTION_SIGNATURE([\"Weather is bad.\", \"Indeed.\", \"Very cold.\"]) == merge_short_sentences_en([\"Weather is bad.\", \"Indeed.\", \"Very cold.\"]))\n"
 },
 {
  "task_id": "RWPB/75",
  "prompt": "import numpy as np\n\ndef align_vector_to_another(a=np.array([0, 0, 1]), b=np.array([1, 0, 0])):\n    \"\"\"\n    Aligns vector 'a' to vector 'b' using an axis-angle rotation.\n\n    Parameters:\n    a (np.array): The initial vector to be aligned. Default is np.array([0, 0, 1]).\n    b (np.array): The target vector to align with. Default is np.array([1, 0, 0]).\n\n    Returns:\n    tuple: A tuple containing:\n        - axis_ (np.array): The normalized axis of rotation.\n        - angle (float): The angle of rotation in radians.\n\n    Process:\n    1. Check if vectors 'a' and 'b' are already aligned. If they are, return (None, None).\n    2. Compute the cross product of 'a' and 'b' to find the rotation axis.\n    3. Normalize the rotation axis to get a unit vector.\n    4. Calculate the angle between 'a' and 'b' using the arccosine of their dot product.\n\n    If 'a' and 'b' are not aligned, the function returns the necessary rotation axis and angle to align 'a' with 'b'.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of aligning one vector to another using an axis-angle rotation:\n\n```python\nimport numpy as np\n\ndef align_vector_to_another(a=np.array([0, 0, 1]), b=np.array([1, 0, 0])):\n    \"\"\"\n    Aligns vector 'a' to vector 'b' using an axis-angle rotation.\n\n    Parameters:\n    a (np.array): The initial vector to be aligned. Default is np.array([0, 0, 1]).\n    b (np.array): The target vector to align with. Default is np.array([1, 0, 0]).\n\n    Returns:\n    tuple: A tuple containing:\n        - axis_ (np.array): The normalized axis of rotation.\n        - angle (float): The angle of rotation in radians.\n\n    Process:\n    1. Check if vectors 'a' and 'b' are already aligned. If they are, return (None, None).\n    2. Compute the cross product of 'a' and 'b' to find the rotation axis.\n    3. Normalize the rotation axis to get a unit vector.\n    4. Calculate the angle between 'a' and 'b' using the arccosine of their dot product.\n\n    If 'a' and 'b' are not aligned, the function returns the necessary rotation axis and angle to align 'a' with 'b'.\n    \"\"\"\n    # Step 1: Check if vectors 'a' and 'b' are already aligned\n    if np.allclose(a, b) or np.allclose(a, -b):\n        return None, None\n\n    # Step 2: Compute the cross product of 'a' and 'b' to find the rotation axis\n    axis = np.cross(a, b)\n\n    # Step 3: Normalize the rotation axis to get a unit vector\n    axis_ = axis / np.linalg.norm(axis)\n\n    # Step 4: Calculate the angle between 'a' and 'b' using the arccosine of their dot product\n    angle = np.arccos(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n\n    return axis_, angle\n```\n\nExplanation:\n\n1. We first check if vectors 'a' and 'b' are already aligned by using `np.allclose()`. If they are aligned (either in the same direction or opposite directions), we return `(None, None)` since no rotation is needed.\n\n2. If the vectors are not aligned, we compute the cross product of 'a' and 'b' using `np.cross()` to find the rotation axis. The cross product gives us a vector perpendicular to both 'a' and 'b'.\n\n3. We normalize the rotation axis using `np.linalg.norm()` to get a unit vector. This ensures that the rotation axis has a magnitude of 1.\n\n4. To calculate the angle between 'a' and 'b', we use the arccosine of their dot product divided by the product of their magnitudes. The dot product is computed using `np.dot()`, and the magnitudes are calculated using `np.linalg.norm()`. The resulting angle is in radians.\n\n5. Finally, we return the normalized rotation axis (`axis_`) and the angle of rotation (`angle`).\n\nThis function takes two vectors 'a' and 'b' as input and returns the necessary rotation axis and angle to align 'a' with 'b'. If the vectors are already aligned, it returns `(None, None)`.",
  "canonical_solution": "    \n    if np.array_equal(a, b):\n        return None, None\n    axis_ = np.cross(a, b)\n    axis_ = axis_ / np.linalg.norm(axis_)\n    angle = np.arccos(np.dot(a, b))\n\n    return axis_, angle\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([1, 0, 0]))[0] == align_vector_to_another(a = np.array([1, 0, 0]), b = np.array([1, 0, 0]))[0]))\nassert((SOLUTION_SIGNATURE(a = np.array([0, 0, 1]), b = np.array([1, 0, 0]))[0] == align_vector_to_another(a = np.array([0, 0, 1]), b = np.array([1, 0, 0]))[0]).all())\nassert((SOLUTION_SIGNATURE(a = np.array([1, 0, 1]), b = np.array([-1, 0, 0]))[0] == align_vector_to_another(a = np.array([1, 0, 1]), b = np.array([-1, 0, 0]))[0]).all())\nassert((SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([1, 0, 1]))[0] == align_vector_to_another(a = np.array([1, 0, 0]), b = np.array([1, 0, 1]))[0]).all())\nassert((SOLUTION_SIGNATURE(a = np.array([1, 1, 1]), b = np.array([-1, 1, -1]))[0] == align_vector_to_another(a = np.array([1, 1, 1]), b = np.array([-1, 1, -1]))[0]).all())\nassert(SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([1, 0, 0]))[1] == align_vector_to_another(a = np.array([1, 0, 0]), b = np.array([1, 0, 0]))[1])\nassert(SOLUTION_SIGNATURE(a = np.array([0, 0, 1]), b = np.array([1, 0, 0]))[1] == align_vector_to_another(a = np.array([0, 0, 1]), b = np.array([1, 0, 0]))[1])\nassert(SOLUTION_SIGNATURE(a = np.array([1, 0, 1]), b = np.array([-1, 0, 0]))[1] == align_vector_to_another(a = np.array([1, 0, 1]), b = np.array([-1, 0, 0]))[1])\nassert(SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([1, 0, 1]))[1] == align_vector_to_another(a = np.array([1, 0, 0]), b = np.array([1, 0, 1]))[1])\nassert(SOLUTION_SIGNATURE(a = np.array([1, 1, 1]), b = np.array([-1, 1, -1]))[1] == align_vector_to_another(a = np.array([1, 1, 1]), b = np.array([-1, 1, -1]))[1])\n"
 },
 {
  "task_id": "RWPB/24",
  "prompt": "import re\nimport hashlib\n\n\ndef check(x, checksum_token):\n    \"\"\"\n    Verifies if the provided checksum in a given string matches the SHA-256 hash of the string with a replaced checksum.\n\n    Parameters\n    ----------\n    x : str\n        The input string that contains a checksum to be verified.\n    checksum_token : str\n        A placeholder token that will replace the current checksum in the string for hashing.\n\n    Returns\n    -------\n    bool\n        True if the checksum is correct, False otherwise.\n\n    Notes\n    -----\n    This function uses a regular expression to find the checksum in the input string `x`.\n    It then replaces the checksum with `checksum_token`, hashes the resulting string using SHA-256,\n    and compares the hash with the original checksum. If they match, the function returns True,\n    indicating that the checksum is valid. If they do not match or if no checksum is found,\n    the function returns False.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport re\nimport hashlib\n\n\ndef check(x, checksum_token):\n    \"\"\"\n    Verifies if the provided checksum in a given string matches the SHA-256 hash of the string with a replaced checksum.\n\n    Parameters\n    ----------\n    x : str\n        The input string that contains a checksum to be verified.\n    checksum_token : str\n        A placeholder token that will replace the current checksum in the string for hashing.\n\n    Returns\n    -------\n    bool\n        True if the checksum is correct, False otherwise.\n\n    Notes\n    -----\n    This function uses a regular expression to find the checksum in the input string `x`.\n    It then replaces the checksum with `checksum_token`, hashes the resulting string using SHA-256,\n    and compares the hash with the original checksum. If they match, the function returns True,\n    indicating that the checksum is valid. If they do not match or if no checksum is found,\n    the function returns False.\n    \"\"\"\n    # Use a regular expression to find the checksum in the input string\n    match = re.search(r'\\*([a-fA-F0-9]{64})\\*', x)\n    \n    if match:\n        # Extract the checksum from the matched group\n        checksum = match.group(1)\n        \n        # Replace the checksum with the provided checksum_token\n        modified_string = x.replace(f'*{checksum}*', checksum_token)\n        \n        # Calculate the SHA-256 hash of the modified string\n        sha256_hash = hashlib.sha256(modified_string.encode('utf-8')).hexdigest()\n        \n        # Compare the calculated hash with the original checksum\n        if sha256_hash == checksum:\n            return True\n    \n    return False\n```\n\nExplanation:\n\n1. We use a regular expression `r'\\*([a-fA-F0-9]{64})\\*'` to find the checksum in the input string `x`. The regular expression looks for a substring that starts and ends with an asterisk (`*`) and contains 64 hexadecimal characters (`[a-fA-F0-9]{64}`) in between.\n\n2. If a match is found (`if match:`), we extract the checksum from the matched group using `match.group(1)`. The captured group corresponds to the 64 hexadecimal characters.\n\n3. We replace the checksum in the original string with the provided `checksum_token` using `x.replace(f'*{checksum}*', checksum_token)`. This step is necessary to calculate the hash of the modified string.\n\n4. We calculate the SHA-256 hash of the modified string using `hashlib.sha256(modified_string.encode('utf-8')).hexdigest()`. We first encode the string as UTF-8 bytes and then compute the SHA-256 hash. The `hexdigest()` method returns the hash as a hexadecimal string.\n\n5. We compare the calculated hash with the original checksum using `if sha256_hash == checksum:`. If they match, it means the checksum is correct, and we return `True`.\n\n6. If no match is found or if the calculated hash does not match the original checksum, we return `False`.\n\nThis function can be used to verify the integrity of a string that contains a checksum. It ensures that the string has not been tampered with by recalculating the hash and comparing it with the provided checksum.",
  "canonical_solution": "\n\n    # Pre-compiled regular expression for matching and replacing checksums\n    re_checksum = re.compile(r'\"Checksum\": \"([0-9a-fA-F]{64})\"')\n\n    # Search for the checksum in the input string\n    m = re.search(re_checksum, x)\n    if not m:\n        return False\n\n    # Replace the checksum with the provided token\n    replaced = re.sub(re_checksum, f'\"Checksum\": \"{checksum_token}\"', x)\n\n    # Calculate the SHA-256 hash of the modified string\n    h = hashlib.sha256(replaced.encode(\"utf8\"))\n\n    # Compare the calculated hash with the original checksum\n    return h.hexdigest() == m.group(1)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE('{\"Data\": \"Example data\", \"Checksum\": \"0b3ad42a156a99abcce760eee811fcfcbb806126697b77c5eb9638e20c455439\"}', checksum_token = \"TOKEN\") == check('{\"Data\": \"Example data\", \"Checksum\": \"0b3ad42a156a99abcce760eee811fcfcbb806126697b77c5eb9638e20c455439\"}', checksum_token = \"TOKEN\"))\nassert(SOLUTION_SIGNATURE('{\"Data\": \"Example data\", \"Checksum\": \"f03a9dcfea7e161acf569d267c75034617de5779b416ea8e4aea18d52318dd4b\"}', checksum_token = \"Apple\") == check('{\"Data\": \"Example data\", \"Checksum\": \"f03a9dcfea7e161acf569d267c75034617de5779b416ea8e4aea18d52318dd4b\"}', checksum_token = \"Apple\"))\nassert(SOLUTION_SIGNATURE('{\"Data\": \"Example data\", \"Checksum\": \"f03a9dcfea7e161acf569d267c75034617de5779b416ea8e4aea18d52318dd4b\"}', checksum_token = \"LLMs\") == check('{\"Data\": \"Example data\", \"Checksum\": \"f03a9dcfea7e161acf569d267c75034617de5779b416ea8e4aea18d52318dd4b\"}', checksum_token = \"LLMs\"))\n"
 },
 {
  "task_id": "RWPB/86",
  "prompt": "import torch\n\ndef inverse_sigmoid(x):\n    \"\"\"\n    Computes the inverse sigmoid of a tensor.\n\n    Args:\n    x (torch.Tensor): Input tensor.\n\n    Returns:\n    torch.Tensor: Tensor with the inverse sigmoid applied.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport torch\n\ndef inverse_sigmoid(x):\n    \"\"\"\n    Computes the inverse sigmoid of a tensor.\n\n    Args:\n    x (torch.Tensor): Input tensor.\n\n    Returns:\n    torch.Tensor: Tensor with the inverse sigmoid applied.\n    \"\"\"\n    return torch.log(x / (1 - x))\n```\n\nExplanation:\n\nThe inverse sigmoid function is defined as:\n\n```\ninverse_sigmoid(x) = log(x / (1 - x))\n```\n\nwhere `x` is the input tensor.\n\nIn the code above, we use the `torch.log()` function to compute the natural logarithm of the element-wise division of `x` by `(1 - x)`. This applies the inverse sigmoid function to each element of the input tensor `x`.\n\nThe function takes a `torch.Tensor` as input and returns a new `torch.Tensor` with the inverse sigmoid applied to each element.\n\nNote: Make sure that the input tensor `x` contains values between 0 and 1 (exclusive) to avoid division by zero and to ensure the inverse sigmoid is defined for all elements.",
  "canonical_solution": "    \n    return torch.log(x / (1 - x))\n\n",
  "unprocess_testcases": "x = torch.tensor([0.2, 0.5, 0.8])\nassert((SOLUTION_SIGNATURE(x) == inverse_sigmoid(x)).all())\n\nx = torch.tensor([0.001, 0.999])\nassert((SOLUTION_SIGNATURE(x) == inverse_sigmoid(x)).all())\n\nx = torch.tensor([0.0, 1.0])\nassert((SOLUTION_SIGNATURE(x) == inverse_sigmoid(x)).all())\n"
 },
 {
  "task_id": "RWPB/128",
  "prompt": "import numpy as np\nimport torch\n\n\ndef xywh2xyxy(x):\n    \"\"\"\n    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n    top-left corner and (x2, y2) is the bottom-right corner.\n\n    Args:\n        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.\n\n    Returns:\n        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport numpy as np\nimport torch\n\n\ndef xywh2xyxy(x):\n    \"\"\"\n    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n    top-left corner and (x2, y2) is the bottom-right corner.\n\n    Args:\n        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.\n\n    Returns:\n        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.\n    \"\"\"\n    if isinstance(x, np.ndarray):\n        y = np.zeros_like(x)\n        y[..., 0] = x[..., 0] - x[..., 2] / 2  # x1 = x - width / 2\n        y[..., 1] = x[..., 1] - x[..., 3] / 2  # y1 = y - height / 2\n        y[..., 2] = x[..., 0] + x[..., 2] / 2  # x2 = x + width / 2\n        y[..., 3] = x[..., 1] + x[..., 3] / 2  # y2 = y + height / 2\n    elif isinstance(x, torch.Tensor):\n        y = torch.zeros_like(x)\n        y[..., 0] = x[..., 0] - x[..., 2] / 2  # x1 = x - width / 2\n        y[..., 1] = x[..., 1] - x[..., 3] / 2  # y1 = y - height / 2\n        y[..., 2] = x[..., 0] + x[..., 2] / 2  # x2 = x + width / 2\n        y[..., 3] = x[..., 1] + x[..., 3] / 2  # y2 = y + height / 2\n    else:\n        raise TypeError(f\"Unsupported input type: {type(x)}\")\n    \n    return y\n```\n\nThe `xywh2xyxy` function takes an input `x` which can be either a NumPy array (`np.ndarray`) or a PyTorch tensor (`torch.Tensor`) representing bounding box coordinates in the format (x, y, width, height). The function converts these coordinates to the format (x1, y1, x2, y2) where (x1, y1) represents the top-left corner and (x2, y2) represents the bottom-right corner of the bounding box.\n\nThe conversion is done using the following formulas:\n- x1 = x - width / 2\n- y1 = y - height / 2\n- x2 = x + width / 2\n- y2 = y + height / 2\n\nThe function checks the type of the input `x` using `isinstance` and performs the appropriate conversion based on whether it is a NumPy array or a PyTorch tensor. If the input type is not supported, a `TypeError` is raised.\n\nFinally, the converted bounding box coordinates in the format (x1, y1, x2, y2) are returned as the same type as the input (either `np.ndarray` or `torch.Tensor`).\n\nNote: The `...` in the indexing notation is used to handle broadcasting and support multi-dimensional arrays/tensors.",
  "canonical_solution": "    \n    assert x.shape[-1] == 4, f\"input shape last dimension expected 4 but input shape is {x.shape}\"\n    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\n    dw = x[..., 2] / 2  # half-width\n    dh = x[..., 3] / 2  # half-height\n    y[..., 0] = x[..., 0] - dw  # top left x\n    y[..., 1] = x[..., 1] - dh  # top left y\n    y[..., 2] = x[..., 0] + dw  # bottom right x\n    y[..., 3] = x[..., 1] + dh  # bottom right y\n    return y\n\n",
  "unprocess_testcases": "x = np.array([[10, 20, 30, 40]])\nassert((SOLUTION_SIGNATURE(x) == xywh2xyxy(x)).all())\n\nx = np.array([[10, 10, 20, 20], [30, 40, 10, 10], [50, 60, 30, 40]])\nassert((SOLUTION_SIGNATURE(x) == xywh2xyxy(x)).all())\n\nx = torch.tensor([[15, 25, 0, 35]])\nassert((SOLUTION_SIGNATURE(x) == xywh2xyxy(x)).all())\n"
 },
 {
  "task_id": "RWPB/108",
  "prompt": "import torch\n\ndef norm_grid(v_grid):\n    \"\"\"Normalize the grid values to the range [-1, 1].\n\n    Args:\n        v_grid (torch.Tensor): The input grid of shape (B, 2, H, W).\n\n    Returns:\n        torch.Tensor: The normalized grid of shape (B, H, W, 2).\n    \"\"\"\n",
  "solution": "Certainly! Here's the completed code for the `norm_grid` function:\n\n```python\nimport torch\n\ndef norm_grid(v_grid):\n    \"\"\"Normalize the grid values to the range [-1, 1].\n\n    Args:\n        v_grid (torch.Tensor): The input grid of shape (B, 2, H, W).\n\n    Returns:\n        torch.Tensor: The normalized grid of shape (B, H, W, 2).\n    \"\"\"\n    # Get the height and width of the grid\n    _, _, H, W = v_grid.shape\n\n    # Create a grid of coordinates in the range [-1, 1]\n    x = torch.linspace(-1, 1, W)\n    y = torch.linspace(-1, 1, H)\n    grid_x, grid_y = torch.meshgrid(x, y)\n\n    # Stack the grid coordinates along a new dimension\n    grid = torch.stack([grid_x, grid_y], dim=-1)\n\n    # Expand the grid to match the batch size of v_grid\n    grid = grid.unsqueeze(0).expand(v_grid.shape[0], -1, -1, -1)\n\n    # Add the input grid values to the normalized grid\n    normalized_grid = grid + v_grid.permute(0, 2, 3, 1)\n\n    return normalized_grid\n```\n\nExplanation:\n\n1. We extract the height `H` and width `W` of the input grid `v_grid` from its shape.\n\n2. We create a grid of coordinates in the range [-1, 1] using `torch.linspace()`. We create two 1D tensors `x` and `y` representing the coordinates along the width and height dimensions, respectively.\n\n3. We use `torch.meshgrid()` to create a 2D grid of coordinates from `x` and `y`. This gives us `grid_x` and `grid_y`, which are 2D tensors representing the x and y coordinates of each point in the grid.\n\n4. We stack `grid_x` and `grid_y` along a new dimension using `torch.stack()` to create a 3D tensor `grid` of shape (H, W, 2). Each element of `grid` represents the (x, y) coordinates of a point in the normalized grid.\n\n5. We expand the `grid` tensor to match the batch size of `v_grid` using `unsqueeze()` and `expand()`. This ensures that we have a separate grid for each sample in the batch.\n\n6. We add the input grid values `v_grid` to the normalized grid `grid`. Since `v_grid` has shape (B, 2, H, W) and `grid` has shape (B, H, W, 2), we use `permute()` to rearrange the dimensions of `v_grid` to match the shape of `grid`.\n\n7. Finally, we return the normalized grid `normalized_grid` of shape (B, H, W, 2), where each element represents the normalized (x, y) coordinates of a point in the grid, with the input grid values added to it.\n\nThis implementation assumes that the input `v_grid` is a 4D tensor of shape (B, 2, H, W), where B is the batch size, 2 represents the (x, y) coordinates, and H and W are the height and width of the grid, respectively. The function normalizes the grid values to the range [-1, 1] and returns the normalized grid of shape (B, H, W, 2).",
  "canonical_solution": "    \n    _, _, H, W = v_grid.size()\n\n    # scale grid to [-1,1]\n    v_grid_norm = torch.zeros_like(v_grid)\n    v_grid_norm[:, 0, :, :] = 2.0 * v_grid[:, 0, :, :] / (W - 1) - 1.0\n    v_grid_norm[:, 1, :, :] = 2.0 * v_grid[:, 1, :, :] / (H - 1) - 1.0\n    return v_grid_norm.permute(0, 2, 3, 1)  # BHW2\n\n\n",
  "unprocess_testcases": "v_grid = torch.rand((2, 2, 4, 4))\nassert((SOLUTION_SIGNATURE(v_grid) == norm_grid(v_grid)).all())\n\nv_grid = torch.tensor([[[[0, 1], [0, 1]], [[0, 0], [1, 1]]]])\nassert((SOLUTION_SIGNATURE(v_grid) == norm_grid(v_grid)).all())\n\nv_grid = torch.rand((1, 2, 3, 5))\nassert((SOLUTION_SIGNATURE(v_grid) == norm_grid(v_grid)).all())\n"
 },
 {
  "task_id": "RWPB/55",
  "prompt": "import numpy as np\nimport torch\n\n\ndef numpy_to_pytorch(x):\n    \"\"\"\n    Converts a numpy array to a PyTorch tensor with specific preprocessing.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        A numpy array to be converted to a PyTorch tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        A PyTorch tensor with the same underlying data as the input numpy array,\n        normalized and reshaped to have a batch dimension.\n\n    Notes\n    -----\n    This function performs the following operations on the input numpy array:\n    - Converts the data type to np.float32.\n    - Normalizes the values by dividing by 255.0, which is often used for image pixel values.\n    - Adds a batch dimension by prepending a singleton dimension (None).\n    - Creates a contiguous array to ensure the data layout is compatible with PyTorch.\n    - Converts the numpy array to a PyTorch tensor using torch.from_numpy.\n    - Casts the resulting tensor to a float type.\n\n    This is a common preprocessing step when preparing numpy arrays for use with PyTorch models,\n    especially in computer vision tasks where image data is involved.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `numpy_to_pytorch` function:\n\n```python\nimport numpy as np\nimport torch\n\n\ndef numpy_to_pytorch(x):\n    \"\"\"\n    Converts a numpy array to a PyTorch tensor with specific preprocessing.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        A numpy array to be converted to a PyTorch tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        A PyTorch tensor with the same underlying data as the input numpy array,\n        normalized and reshaped to have a batch dimension.\n\n    Notes\n    -----\n    This function performs the following operations on the input numpy array:\n    - Converts the data type to np.float32.\n    - Normalizes the values by dividing by 255.0, which is often used for image pixel values.\n    - Adds a batch dimension by prepending a singleton dimension (None).\n    - Creates a contiguous array to ensure the data layout is compatible with PyTorch.\n    - Converts the numpy array to a PyTorch tensor using torch.from_numpy.\n    - Casts the resulting tensor to a float type.\n\n    This is a common preprocessing step when preparing numpy arrays for use with PyTorch models,\n    especially in computer vision tasks where image data is involved.\n    \"\"\"\n    # Convert the data type to np.float32\n    x = x.astype(np.float32)\n\n    # Normalize the values by dividing by 255.0\n    x /= 255.0\n\n    # Add a batch dimension by prepending a singleton dimension (None)\n    x = x[None, ...]\n\n    # Create a contiguous array to ensure the data layout is compatible with PyTorch\n    x = np.ascontiguousarray(x)\n\n    # Convert the numpy array to a PyTorch tensor\n    x = torch.from_numpy(x)\n\n    # Cast the resulting tensor to a float type\n    x = x.float()\n\n    return x\n```\n\nThis implementation follows the steps described in the function's docstring:\n\n1. It converts the data type of the input numpy array to `np.float32` using `x.astype(np.float32)`.\n\n2. It normalizes the values by dividing by 255.0 using `x /= 255.0`. This is a common normalization step for image data.\n\n3. It adds a batch dimension by prepending a singleton dimension (None) using `x = x[None, ...]`. This step is necessary because PyTorch expects the input tensor to have a batch dimension.\n\n4. It creates a contiguous array using `np.ascontiguousarray(x)` to ensure the data layout is compatible with PyTorch.\n\n5. It converts the numpy array to a PyTorch tensor using `torch.from_numpy(x)`.\n\n6. Finally, it casts the resulting tensor to a float type using `x.float()`.\n\nThe function returns the preprocessed PyTorch tensor, which can be used as input to a PyTorch model.\n\nNote: As requested, I have not provided any unit test cases for this function.",
  "canonical_solution": "    \n    # Convert numpy array to float32 and normalize by dividing by 255.0\n    y = x.astype(np.float32) / 255.0\n\n    # Add a batch dimension by prepending a singleton dimension\n    y = y[None]\n\n    # Ensure the array is contiguous in memory for compatibility with PyTorch\n    y = np.ascontiguousarray(y.copy())\n\n    # Convert the numpy array to a PyTorch tensor and cast it to float\n    y = torch.from_numpy(y).float()\n\n    return y\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([[10, 20, 30], [40, 50, 60]])) == numpy_to_pytorch(np.array([[10, 20, 30], [40, 50, 60]]))).all())\nassert((SOLUTION_SIGNATURE(np.array([[255]])) == numpy_to_pytorch(np.array([[255]]))).all())\nassert((SOLUTION_SIGNATURE(np.array([[[255, 0, 0], [0, 255, 0], [0, 0, 255]], [[100, 100, 100], [150, 150, 150], [200, 200, 200]], [[50, 25, 75], [125, 75, 175], [225, 125, 25]]])) == numpy_to_pytorch(np.array([[[255, 0, 0], [0, 255, 0], [0, 0, 255]], [[100, 100, 100], [150, 150, 150], [200, 200, 200]], [[50, 25, 75], [125, 75, 175], [225, 125, 25]]]))).all())\n"
 },
 {
  "task_id": "RWPB/92",
  "prompt": "import math\nimport numpy as np\nfrom decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\ndef round(number, num_decimal_places, leave_int=False):\n    '''\n    Purpose: round a number to the specified number of decimal places. Existing \n        round functions may not round correctly so that's why I built my own.\n    Args:\n        - number: float, number to round\n        - num_decimal_places: int, number of decimal places to round number to\n        - leave_int: bool, whether to leave integers unchanged or convert them to floats\n\n    Returns:\n        - float: rounded number\n    '''\n",
  "solution": "Here's the completed code to solve the rounding problem without providing unit test cases:\n\n```python\nimport math\nimport numpy as np\nfrom decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\ndef round(number, num_decimal_places, leave_int=False):\n    '''\n    Purpose: round a number to the specified number of decimal places. Existing \n        round functions may not round correctly so that's why I built my own.\n    Args:\n        - number: float, number to round\n        - num_decimal_places: int, number of decimal places to round number to\n        - leave_int: bool, whether to leave integers unchanged or convert them to floats\n\n    Returns:\n        - float: rounded number\n    '''\n    if isinstance(number, int) and leave_int:\n        return number\n    \n    # Convert the number to a string\n    number_str = str(number)\n    \n    # Split the number into integer and decimal parts\n    parts = number_str.split('.')\n    \n    # Check if the number has a decimal part\n    if len(parts) == 1:\n        # If no decimal part, return the integer as a float\n        return float(parts[0])\n    \n    integer_part, decimal_part = parts\n    \n    # Pad the decimal part with zeros if necessary\n    decimal_part = decimal_part.ljust(num_decimal_places, '0')\n    \n    # Truncate the decimal part to the specified number of decimal places\n    decimal_part = decimal_part[:num_decimal_places]\n    \n    # Reconstruct the rounded number\n    rounded_number = float(integer_part + '.' + decimal_part)\n    \n    return rounded_number\n```\n\nExplanation:\n\n1. First, we check if the input `number` is an integer and `leave_int` is set to `True`. If both conditions are met, we return the number as is without any rounding.\n\n2. We convert the input `number` to a string using `str(number)` to handle both float and integer inputs.\n\n3. We split the number string into integer and decimal parts using the `split('.')` method. If the number doesn't have a decimal part, we return the integer part as a float using `float(parts[0])`.\n\n4. If the number has a decimal part, we separate the integer and decimal parts into `integer_part` and `decimal_part` variables.\n\n5. We pad the `decimal_part` with zeros using the `ljust()` method to ensure it has the specified number of decimal places. This step is necessary to handle cases where the decimal part has fewer digits than the desired number of decimal places.\n\n6. We truncate the `decimal_part` to the specified number of decimal places using slicing: `decimal_part[:num_decimal_places]`. This ensures that the rounded number has the desired number of decimal places.\n\n7. Finally, we reconstruct the rounded number by concatenating the `integer_part` and the truncated `decimal_part` with a dot ('.') in between, and convert it back to a float using `float()`.\n\n8. The function returns the rounded number as a float.\n\nThis implementation provides a custom rounding function that rounds a number to the specified number of decimal places, handling both float and integer inputs, and provides the option to leave integers unchanged if desired.",
  "canonical_solution": "    \n    if leave_int and int(number) == number:\n        return number\n    decimal_str = '1.'\n    for decimal_place in range(num_decimal_places):\n        decimal_str += '1'\n    return float(Decimal(str(number)).quantize(Decimal(decimal_str), rounding=ROUND_HALF_UP))\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(3.14159265, 4) == round(3.14159265, 4))\nassert(SOLUTION_SIGNATURE(200, 2, leave_int=True) == round(200, 2, leave_int=True))\nassert(SOLUTION_SIGNATURE(2.789, 0) == round(2.789, 0))\n"
 },
 {
  "task_id": "RWPB/61",
  "prompt": "import torch\nimport torch.nn.functional as F\n\n\ndef layer_norm_ref(x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False):\n    \"\"\"\n    Applies Layer Normalization to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        The input tensor.\n    weight : torch.Tensor\n        The weight tensor for scaling the normalized input.\n    bias : torch.Tensor or None\n        The bias tensor for shifting the normalized input. If None, no bias is added.\n    residual : torch.Tensor or None\n        The residual tensor to be added to the normalized input. If None, no residual is added.\n    eps : float, optional\n        A small constant added for numerical stability.\n    prenorm : bool, optional\n        If True, the function returns both the normalized output and the original input.\n    upcast : bool, optional\n        If True, upcasts all tensors to float type for computation.\n\n    Returns\n    -------\n    torch.Tensor or tuple of torch.Tensor\n        The layer-normalized output tensor. If `prenorm` is True, returns a tuple\n        containing the output tensor and the original input tensor.\n\n    Notes\n    -----\n    Layer Normalization normalizes the input tensor by subtracting the mean and dividing by\n    the standard deviation of each individual layer. This function performs the normalization,\n    applies the weight and bias, and optionally adds a residual connection. If `upcast` is True,\n    all tensors are upcast to float type before the operations for precision.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `layer_norm_ref` function:\n\n```python\nimport torch\nimport torch.nn.functional as F\n\n\ndef layer_norm_ref(x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False):\n    \"\"\"\n    Applies Layer Normalization to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        The input tensor.\n    weight : torch.Tensor\n        The weight tensor for scaling the normalized input.\n    bias : torch.Tensor or None\n        The bias tensor for shifting the normalized input. If None, no bias is added.\n    residual : torch.Tensor or None\n        The residual tensor to be added to the normalized input. If None, no residual is added.\n    eps : float, optional\n        A small constant added for numerical stability.\n    prenorm : bool, optional\n        If True, the function returns both the normalized output and the original input.\n    upcast : bool, optional\n        If True, upcasts all tensors to float type for computation.\n\n    Returns\n    -------\n    torch.Tensor or tuple of torch.Tensor\n        The layer-normalized output tensor. If `prenorm` is True, returns a tuple\n        containing the output tensor and the original input tensor.\n\n    Notes\n    -----\n    Layer Normalization normalizes the input tensor by subtracting the mean and dividing by\n    the standard deviation of each individual layer. This function performs the normalization,\n    applies the weight and bias, and optionally adds a residual connection. If `upcast` is True,\n    all tensors are upcast to float type before the operations for precision.\n    \"\"\"\n    if upcast:\n        x = x.float()\n        if residual is not None:\n            residual = residual.float()\n        weight = weight.float()\n        if bias is not None:\n            bias = bias.float()\n\n    mean = x.mean(-1, keepdim=True)\n    var = x.var(-1, keepdim=True, unbiased=False)\n    y = (x - mean) * torch.rsqrt(var + eps)\n\n    y = y * weight\n    if bias is not None:\n        y = y + bias\n\n    if residual is not None:\n        y = y + residual\n\n    if prenorm:\n        return y, x\n    else:\n        return y\n```\n\nThe `layer_norm_ref` function performs the following steps:\n\n1. If `upcast` is True, it upcasts all tensors to float type for computation.\n\n2. It calculates the mean of the input tensor `x` along the last dimension (-1) and keeps the dimension using `keepdim=True`.\n\n3. It calculates the variance of the input tensor `x` along the last dimension (-1) and keeps the dimension using `keepdim=True`. The `unbiased=False` argument ensures that the variance is calculated using the biased estimator.\n\n4. It normalizes the input tensor by subtracting the mean and dividing by the square root of the variance plus a small constant `eps` for numerical stability.\n\n5. It scales the normalized tensor by multiplying it with the `weight` tensor.\n\n6. If `bias` is not None, it adds the `bias` tensor to the scaled normalized tensor.\n\n7. If `residual` is not None, it adds the `residual` tensor to the output tensor.\n\n8. If `prenorm` is True, it returns a tuple containing the output tensor and the original input tensor. Otherwise, it returns only the output tensor.\n\nThis implementation follows the standard Layer Normalization formula and allows for optional residual connections and pre-normalization. The `upcast` argument provides the option to upcast all tensors to float type for computation, which can be useful for precision in certain scenarios.",
  "canonical_solution": "\n    # Preserve the original dtype\n    dtype = x.dtype\n\n    # Upcast weight, bias, and x to float if necessary\n    if upcast:\n        weight = weight.float()\n        bias = bias.float() if bias is not None else None\n        x = x.float()\n        residual = residual.float() if residual is not None else residual\n\n    # Add residual to input if provided\n    if residual is not None:\n        x = (x + residual).to(dtype)\n\n    # Apply layer normalization\n    # Note: It was x.to(weight.dtype) but it should be x because F.layer_norm expects the input tensor x.\n    out = F.layer_norm(x, x.shape[-1:], weight=weight, bias=bias, eps=eps).to(dtype)\n\n    # Return the output only or the output with the original input if prenorm is True\n    return out if not prenorm else (out, x)\n\n\n",
  "unprocess_testcases": "a = torch.rand(10, 5)\nb = torch.rand(5,)\n\nassert((SOLUTION_SIGNATURE(a, b, b) == layer_norm_ref(a, b, b)).all())\nassert((SOLUTION_SIGNATURE(a, b, b, a, 1e-5, False, True) == layer_norm_ref(a, b, b, a, 1e-5, False, True)).all())\nassert((SOLUTION_SIGNATURE(a, b, None, None, 1e-6, True, False)[0] == layer_norm_ref(a, b, None, None, 1e-6, True, False)[0]).all())\nassert((SOLUTION_SIGNATURE(a, b, None, None, 1e-6, True, False)[1] == layer_norm_ref(a, b, None, None, 1e-6, True, False)[1]).all())\nassert((SOLUTION_SIGNATURE(a, b, None, b, 1e-6, True, False)[0] == layer_norm_ref(a, b, None, b, 1e-6, True, False)[0]).all())\nassert((SOLUTION_SIGNATURE(a, b, None, b, 1e-6, True, False)[1] == layer_norm_ref(a, b, None, b, 1e-6, True, False)[1]).all())\nassert((SOLUTION_SIGNATURE(a, b, None) == layer_norm_ref(a, b, None)).all())\n"
 },
 {
  "task_id": "RWPB/30",
  "prompt": "import numpy as np\n\ndef get_confusion_matrix(gt_label, pred_label, num_classes):\n    \"\"\"\n    Calculates the confusion matrix from the ground truth and predicted labels.\n    If the length of gt_label is not equal to the length of pred_label, please return ValueError \n\n    Args:\n        gt_label (np.array): An array of ground truth labels.\n        pred_label (np.array): An array of predicted labels from the model.\n        num_classes (int): The number of classes in the dataset.\n\n    Returns:\n        np.array: A 2D array (matrix) of shape (num_classes, num_classes) representing the confusion matrix,\n                  where each row corresponds to the true classes and each column to the predicted classes.\n\n    The function calculates a confusion matrix which helps in evaluating the accuracy of a classification model.\n    It does so by cross-tabulating each class label with its corresponding prediction, thus providing insight\n    into misclassifications.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport numpy as np\n\ndef get_confusion_matrix(gt_label, pred_label, num_classes):\n    \"\"\"\n    Calculates the confusion matrix from the ground truth and predicted labels.\n    If the length of gt_label is not equal to the length of pred_label, please return ValueError \n\n    Args:\n        gt_label (np.array): An array of ground truth labels.\n        pred_label (np.array): An array of predicted labels from the model.\n        num_classes (int): The number of classes in the dataset.\n\n    Returns:\n        np.array: A 2D array (matrix) of shape (num_classes, num_classes) representing the confusion matrix,\n                  where each row corresponds to the true classes and each column to the predicted classes.\n\n    The function calculates a confusion matrix which helps in evaluating the accuracy of a classification model.\n    It does so by cross-tabulating each class label with its corresponding prediction, thus providing insight\n    into misclassifications.\n    \"\"\"\n    if len(gt_label) != len(pred_label):\n        raise ValueError(\"The length of gt_label and pred_label must be equal.\")\n\n    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n\n    for true_label, predicted_label in zip(gt_label, pred_label):\n        confusion_matrix[true_label][predicted_label] += 1\n\n    return confusion_matrix\n```\n\nExplanation:\n\n1. The function first checks if the length of `gt_label` and `pred_label` are equal. If they are not equal, it raises a `ValueError` with an appropriate error message.\n\n2. It creates a 2D array called `confusion_matrix` of shape `(num_classes, num_classes)` initialized with zeros using `np.zeros()`. This array will store the counts of true labels versus predicted labels.\n\n3. The function then iterates over each pair of true label and predicted label using `zip(gt_label, pred_label)`.\n\n4. For each pair, it increments the value in the `confusion_matrix` at the position corresponding to the true label (row) and predicted label (column). This is done using `confusion_matrix[true_label][predicted_label] += 1`.\n\n5. Finally, the function returns the computed `confusion_matrix`.\n\nThe confusion matrix provides a tabular summary of the classification performance. Each row represents the true class labels, and each column represents the predicted class labels. The values in the matrix indicate the number of instances that belong to each combination of true and predicted labels.\n\nThe diagonal elements of the confusion matrix represent the correctly classified instances for each class, while the off-diagonal elements represent misclassifications.\n\nThis implementation assumes that the class labels are represented as integers starting from 0 to `num_classes - 1`. If the class labels are not in this format, you may need to modify the code accordingly.",
  "canonical_solution": "\n    if gt_label.shape[-1] != pred_label.shape[-1]:\n        return ValueError\n    \n    # Compute the linear index for each element of the confusion matrix\n    index = (gt_label * num_classes + pred_label).astype('int32')\n\n    # Count the occurrence of each index\n    label_count = np.bincount(index, minlength=num_classes*num_classes)\n\n    # Initialize the confusion matrix with zeros\n    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n\n    # Fill the confusion matrix with counts\n    for i_label in range(num_classes):\n        for i_pred_label in range(num_classes):\n            cur_index = i_label * num_classes + i_pred_label\n            confusion_matrix[i_label, i_pred_label] = label_count[cur_index]\n\n    return confusion_matrix\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([0, 1, 2, 1, 0]), np.array([0, 2, 2, 1, 0]), 3) == get_confusion_matrix(np.array([0, 1, 2, 1, 0]), np.array([0, 2, 2, 1, 0]), 3)).all())\nassert((SOLUTION_SIGNATURE(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0]), 1) == get_confusion_matrix(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0]), 1)).all())\nassert(SOLUTION_SIGNATURE(np.array([0, 1, 2, 3]), np.array([0, 1, 2]), 4) == get_confusion_matrix(np.array([0, 1, 2, 3]), np.array([0, 1, 2]), 4))\n"
 },
 {
  "task_id": "RWPB/20",
  "prompt": "import math\n\n\ndef get_line_angle(x1, y1, x2, y2):\n    '''\n    Calculates the angle of a line segment relative to the horizontal axis.\n\n    Parameters\n    ----------\n    x1, y1 : float\n        Coordinates of the first point (x1, y1).\n    x2, y2 : float\n        Coordinates of the second point (x2, y2).\n\n    Return\n    ------\n    float\n        The angle of the line segment from point (x1, y1) to (x2, y2) in degrees.\n\n    The function computes the slope of the line segment and then uses the arctangent\n    function to determine the angle in radians. This angle is converted to degrees.\n    The function includes handling for vertical lines where the slope would be undefined.\n    '''\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport math\n\n\ndef get_line_angle(x1, y1, x2, y2):\n    '''\n    Calculates the angle of a line segment relative to the horizontal axis.\n\n    Parameters\n    ----------\n    x1, y1 : float\n        Coordinates of the first point (x1, y1).\n    x2, y2 : float\n        Coordinates of the second point (x2, y2).\n\n    Return\n    ------\n    float\n        The angle of the line segment from point (x1, y1) to (x2, y2) in degrees.\n\n    The function computes the slope of the line segment and then uses the arctangent\n    function to determine the angle in radians. This angle is converted to degrees.\n    The function includes handling for vertical lines where the slope would be undefined.\n    '''\n    # Calculate the difference in x and y coordinates\n    dx = x2 - x1\n    dy = y2 - y1\n\n    # Check if the line is vertical\n    if dx == 0:\n        if dy > 0:\n            angle = 90.0\n        elif dy < 0:\n            angle = -90.0\n        else:\n            angle = 0.0\n    else:\n        # Calculate the slope of the line\n        slope = dy / dx\n\n        # Calculate the angle in radians using arctangent\n        angle_rad = math.atan(slope)\n\n        # Convert the angle from radians to degrees\n        angle = math.degrees(angle_rad)\n\n    return angle\n```\n\nThe completed code does the following:\n\n1. It calculates the difference in x and y coordinates between the two points (`dx` and `dy`).\n\n2. It checks if the line is vertical by comparing `dx` to zero. If `dx` is zero, it means the line is vertical, and the angle is determined based on the value of `dy`:\n   - If `dy` is greater than zero, the angle is 90 degrees.\n   - If `dy` is less than zero, the angle is -90 degrees.\n   - If `dy` is equal to zero, the angle is 0 degrees.\n\n3. If the line is not vertical, it calculates the slope of the line by dividing `dy` by `dx`.\n\n4. It calculates the angle in radians using the `math.atan()` function, which returns the arctangent of the slope.\n\n5. It converts the angle from radians to degrees using the `math.degrees()` function.\n\n6. Finally, it returns the calculated angle in degrees.\n\nThis code handles both non-vertical and vertical lines and returns the angle of the line segment relative to the horizontal axis in degrees.",
  "canonical_solution": "\n    # Check for a vertical line to avoid division by zero\n    if x2 == x1:\n        return 90 if y2 > y1 else -90  # Directly return 90 or -90 degrees depending on the direction\n\n    # Calculate the slope of the line\n    slope = (y2 - y1) / (x2 - x1)\n\n    # Calculate the angle in radians\n    angle_radians = math.atan(slope)\n\n    # Convert the angle to degrees\n    angle_degrees = math.degrees(angle_radians)\n\n    return angle_degrees\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(0, 10, 10, 0) == get_line_angle(0, 10, 10, 0))\nassert(SOLUTION_SIGNATURE(5, 5, 5, 10) == get_line_angle(5, 5, 5, 10))\nassert(SOLUTION_SIGNATURE(0, 0, 10, 0) == get_line_angle(0, 0, 10, 0))\n"
 },
 {
  "task_id": "RWPB/71",
  "prompt": "from math import factorial\n\n\ndef getLambertCoefs(numOfLambertCoefs):\n    \"\"\"\n    Compute the Lambert coefficients for a given number of coefficients.\n    Args:\n        - numOfLambertCoefs: int, the number of coefficients\n    Returns:\n        lambert_coefs: list, the computed Lambert coefficients\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of computing Lambert coefficients:\n\n```python\nfrom math import factorial\n\n\ndef getLambertCoefs(numOfLambertCoefs):\n    \"\"\"\n    Compute the Lambert coefficients for a given number of coefficients.\n    Args:\n        - numOfLambertCoefs: int, the number of coefficients\n    Returns:\n        lambert_coefs: list, the computed Lambert coefficients\n    \"\"\"\n    lambert_coefs = []\n    \n    for k in range(numOfLambertCoefs):\n        coef = 0\n        for i in range(k + 1):\n            coef += (-1) ** i * factorial(k) // (factorial(i) * factorial(k - i)) * (k - i) ** k\n        \n        lambert_coefs.append(coef / factorial(k + 1))\n    \n    return lambert_coefs\n```\n\nExplanation:\n1. We initialize an empty list called `lambert_coefs` to store the computed Lambert coefficients.\n\n2. We start a loop that iterates from 0 to `numOfLambertCoefs - 1` (inclusive) to compute each coefficient.\n\n3. For each coefficient, we initialize a variable `coef` to store the sum of the terms.\n\n4. We start an inner loop that iterates from 0 to `k` (inclusive) to compute each term of the coefficient.\n\n5. For each term, we calculate the following:\n   - `(-1) ** i`: Alternating sign based on the value of `i`.\n   - `factorial(k) // (factorial(i) * factorial(k - i))`: Binomial coefficient using factorials.\n   - `(k - i) ** k`: The term `(k - i)` raised to the power of `k`.\n\n6. We add each term to the `coef` variable.\n\n7. After computing all the terms for a coefficient, we divide `coef` by `factorial(k + 1)` to get the final value of the coefficient.\n\n8. We append the computed coefficient to the `lambert_coefs` list.\n\n9. After computing all the coefficients, we return the `lambert_coefs` list.\n\nThis code computes the Lambert coefficients based on the given number of coefficients (`numOfLambertCoefs`). The Lambert coefficients are used in various mathematical and computational contexts, such as solving equations involving the Lambert W function.\n\nNote: The code assumes that the `math` module is imported to use the `factorial` function.",
  "canonical_solution": "    \n    maxFactorial = factorial(numOfLambertCoefs-1)\n    return [maxFactorial*i**(i-1)//factorial(i) for i in range(1,numOfLambertCoefs)]\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(5) == getLambertCoefs(5))\nassert(SOLUTION_SIGNATURE(1) == getLambertCoefs(1))\nassert(SOLUTION_SIGNATURE(20) == getLambertCoefs(20))\nassert(SOLUTION_SIGNATURE(12) == getLambertCoefs(12))\nassert(SOLUTION_SIGNATURE(15) == getLambertCoefs(15))\n"
 },
 {
  "task_id": "RWPB/82",
  "prompt": "import importlib.util\nimport numpy as np\nimport torch\n\ndef is_tensor(x):\n    \"\"\"\n    Determines if the input `x` is a `torch.Tensor` or a `np.ndarray`.\n\n    Parameters:\n    x : any type\n        The input variable to be checked.\n\n    Returns:\n    bool\n        Returns True if `x` is either a `torch.Tensor` (if the PyTorch library is available) \n        or a `np.ndarray`. Otherwise, returns False.\n\n    Process:\n    1. Checks if the PyTorch library is available using `importlib.util.find_spec(\"torch\")`.\n    2. If PyTorch is available, imports the library and checks if `x` is an instance of `torch.Tensor`.\n    3. If `x` is not a `torch.Tensor` or if PyTorch is not available, it then checks if `x` is an instance of `np.ndarray`.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport importlib.util\nimport numpy as np\n\ndef is_tensor(x):\n    \"\"\"\n    Determines if the input `x` is a `torch.Tensor` or a `np.ndarray`.\n\n    Parameters:\n    x : any type\n        The input variable to be checked.\n\n    Returns:\n    bool\n        Returns True if `x` is either a `torch.Tensor` (if the PyTorch library is available) \n        or a `np.ndarray`. Otherwise, returns False.\n\n    Process:\n    1. Checks if the PyTorch library is available using `importlib.util.find_spec(\"torch\")`.\n    2. If PyTorch is available, imports the library and checks if `x` is an instance of `torch.Tensor`.\n    3. If `x` is not a `torch.Tensor` or if PyTorch is not available, it then checks if `x` is an instance of `np.ndarray`.\n    \"\"\"\n    # Check if PyTorch is available\n    if importlib.util.find_spec(\"torch\") is not None:\n        import torch\n        if isinstance(x, torch.Tensor):\n            return True\n    \n    # Check if x is a NumPy array\n    if isinstance(x, np.ndarray):\n        return True\n    \n    return False\n```\n\nExplanation:\n\n1. The code first checks if the PyTorch library is available using `importlib.util.find_spec(\"torch\")`. If PyTorch is found, it imports the library.\n\n2. If PyTorch is available and `x` is an instance of `torch.Tensor`, the function returns `True`.\n\n3. If `x` is not a `torch.Tensor` or if PyTorch is not available, the code then checks if `x` is an instance of `np.ndarray` using `isinstance(x, np.ndarray)`. If it is, the function returns `True`.\n\n4. If `x` is neither a `torch.Tensor` nor a `np.ndarray`, the function returns `False`.\n\nThis implementation follows the process outlined in the problem description and checks for both `torch.Tensor` and `np.ndarray` based on the availability of the PyTorch library.",
  "canonical_solution": "    \n    if importlib.util.find_spec(\"torch\") is not None:\n        import torch\n\n        if isinstance(x, torch.Tensor):\n            return True\n\n    return isinstance(x, np.ndarray)\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(np.array([1, 2, 3])) == is_tensor(np.array([1, 2, 3])))\nassert(SOLUTION_SIGNATURE([1, 2, 3]) == is_tensor([1, 2, 3]))\nassert(SOLUTION_SIGNATURE(torch.tensor([1, 2, 3])) == is_tensor(torch.tensor([1, 2, 3])))\nassert(SOLUTION_SIGNATURE(0) == is_tensor(0))\n"
 },
 {
  "task_id": "RWPB/45",
  "prompt": "import torch\n\ndef invalid_to_zeros(arr, valid_mask, ndim=999):\n    \"\"\"\n    Replaces invalid entries in an array with zeros, based on a validity mask.\n\n    Parameters\n    ----------\n    arr : torch.Tensor\n        The input tensor with potentially invalid entries.\n    valid_mask : torch.Tensor (boolean)\n        A boolean mask with the same number of elements as 'arr', where True indicates\n        a valid entry and False indicates an invalid entry that should be replaced with zero.\n    ndim : int, optional\n        The desired number of dimensions for the output tensor. If 'arr' has more than\n        'ndim' dimensions, it will be flattened accordingly. Default is 999, meaning no flattening.\n\n    Returns\n    -------\n    torch.Tensor\n        The input tensor 'arr' with invalid entries replaced by zeros.\n    int or torch.Tensor\n        The number of non-zero entries in the validity mask (nnz), which corresponds to the\n        number of valid points per image.\n\n    Notes\n    -----\n    This function is particularly useful in the context of point cloud processing or similar\n    applications where some data points may be invalid or missing. By setting these invalid\n    points to zero, the resulting tensor can be used for further processing, such as neural\n    network input.\n\n    If 'valid_mask' is provided, the function updates 'arr' in-place to zero out the invalid\n    entries. It also calculates the number of non-zero entries in the mask (nnz), which is\n    returned separately. If 'valid_mask' is None, it assumes all entries are valid and calculates\n    nnz based on the total number of elements in 'arr' divided by the first dimension size.\n\n    If 'arr' has more dimensions than specified by 'ndim', the function flattens it to the\n    desired number of dimensions.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `invalid_to_zeros` function:\n\n```python\nimport torch\n\ndef invalid_to_zeros(arr, valid_mask, ndim=999):\n    \"\"\"\n    Replaces invalid entries in an array with zeros, based on a validity mask.\n\n    Parameters\n    ----------\n    arr : torch.Tensor\n        The input tensor with potentially invalid entries.\n    valid_mask : torch.Tensor (boolean)\n        A boolean mask with the same number of elements as 'arr', where True indicates\n        a valid entry and False indicates an invalid entry that should be replaced with zero.\n    ndim : int, optional\n        The desired number of dimensions for the output tensor. If 'arr' has more than\n        'ndim' dimensions, it will be flattened accordingly. Default is 999, meaning no flattening.\n\n    Returns\n    -------\n    torch.Tensor\n        The input tensor 'arr' with invalid entries replaced by zeros.\n    int or torch.Tensor\n        The number of non-zero entries in the validity mask (nnz), which corresponds to the\n        number of valid points per image.\n\n    Notes\n    -----\n    This function is particularly useful in the context of point cloud processing or similar\n    applications where some data points may be invalid or missing. By setting these invalid\n    points to zero, the resulting tensor can be used for further processing, such as neural\n    network input.\n\n    If 'valid_mask' is provided, the function updates 'arr' in-place to zero out the invalid\n    entries. It also calculates the number of non-zero entries in the mask (nnz), which is\n    returned separately. If 'valid_mask' is None, it assumes all entries are valid and calculates\n    nnz based on the total number of elements in 'arr' divided by the first dimension size.\n\n    If 'arr' has more dimensions than specified by 'ndim', the function flattens it to the\n    desired number of dimensions.\n    \"\"\"\n    if valid_mask is not None:\n        # Update 'arr' in-place to zero out invalid entries\n        arr[~valid_mask] = 0\n\n        # Calculate the number of non-zero entries in the validity mask\n        nnz = torch.sum(valid_mask, dim=-1)\n    else:\n        # Assume all entries are valid\n        nnz = arr.shape[-1]\n\n    # Flatten 'arr' to the desired number of dimensions if necessary\n    if arr.ndim > ndim:\n        new_shape = arr.shape[:-1] + (-1,)\n        arr = arr.reshape(new_shape)\n\n    return arr, nnz\n```\n\nThis code implements the functionality described in the docstring:\n\n1. If `valid_mask` is provided, it updates `arr` in-place to zero out the invalid entries using boolean indexing. It then calculates the number of non-zero entries in the validity mask (`nnz`) using `torch.sum()` along the last dimension.\n\n2. If `valid_mask` is `None`, it assumes all entries are valid and sets `nnz` to the size of the last dimension of `arr`.\n\n3. If `arr` has more dimensions than specified by `ndim`, it flattens `arr` to the desired number of dimensions using `reshape()`. The new shape is determined by keeping the dimensions up to `ndim-1` and flattening the remaining dimensions into a single dimension.\n\n4. Finally, it returns the updated `arr` with invalid entries replaced by zeros, and the number of non-zero entries (`nnz`).\n\nThis function can be used to preprocess data by setting invalid entries to zero based on a validity mask, making it suitable for further processing or as input to neural networks.",
  "canonical_solution": "\n    if valid_mask is not None:\n        arr = arr.clone()  # Create a copy of arr to avoid modifying the original tensor\n        arr[~valid_mask] = 0  # Set invalid entries (where mask is False) to zero\n        nnz = valid_mask.view(len(valid_mask), -1).sum(1)  # Calculate the number of valid points per image\n    else:\n        # If no valid_mask is provided, calculate nnz based on total elements\n        nnz = arr.numel() // len(arr) if len(arr) else 0\n\n    # If the number of dimensions in arr is greater than ndim, flatten it\n    if arr.ndim > ndim:\n        if ndim > 0:\n            arr = arr.reshape([arr.shape[0], -1])[:ndim]\n        else:\n            arr = arr.flatten()\n\n    return arr, nnz\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([1, 2, 3, 4]), valid_mask = torch.tensor([True, False, True, True]))[0] == invalid_to_zeros(torch.tensor([1, 2, 3, 4]), valid_mask = torch.tensor([True, False, True, True]))[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 2, 3, 4]), valid_mask = torch.tensor([True, False, True, True]))[1] == invalid_to_zeros(torch.tensor([1, 2, 3, 4]), valid_mask = torch.tensor([True, False, True, True]))[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 2, 3, 4]), torch.tensor([False, False, False, False]))[0] == invalid_to_zeros(torch.tensor([1, 2, 3, 4]), torch.tensor([False, False, False, False]))[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 2, 3, 4]), torch.tensor([False, False, False, False]))[1] == invalid_to_zeros(torch.tensor([1, 2, 3, 4]), torch.tensor([False, False, False, False]))[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]), torch.tensor([[[True, True], [False, True]], [[True, False], [True, True]]]), 1)[0] == invalid_to_zeros(torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]), torch.tensor([[[True, True], [False, True]], [[True, False], [True, True]]]), 1)[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]), torch.tensor([[[True, True], [False, True]], [[True, False], [True, True]]]), 1)[1] == invalid_to_zeros(torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]), torch.tensor([[[True, True], [False, True]], [[True, False], [True, True]]]), 1)[1]).all())\n"
 },
 {
  "task_id": "RWPB/14",
  "prompt": "import numpy as np\n\ndef align_vector_to_another(a=np.array([0, 0, 1]), b=np.array([1, 0, 0])):\n    '''Aligns vector a to vector b with axis angle rotation.\n    If array a is equal array b, please return (None, None)\n    Args:\n        - a: numpy.array. the first vector. \n        - b: numpy.array. the second vector.\n    Return:\n        - axis: numpy.array. the axis of vector\n        - angle: numpy.array. the angle of vector\n    '''\n",
  "solution": "Here's the completed code to align vector `a` to vector `b` using axis-angle rotation:\n\n```python\nimport numpy as np\n\ndef align_vector_to_another(a=np.array([0, 0, 1]), b=np.array([1, 0, 0])):\n    '''Aligns vector a to vector b with axis angle rotation.\n    If array a is equal array b, please return (None, None)\n    Args:\n        - a: numpy.array. the first vector. \n        - b: numpy.array. the second vector.\n    Return:\n        - axis: numpy.array. the axis of vector\n        - angle: numpy.array. the angle of vector\n    '''\n    # Check if the vectors are equal\n    if np.array_equal(a, b):\n        return None, None\n\n    # Normalize the input vectors\n    a_norm = a / np.linalg.norm(a)\n    b_norm = b / np.linalg.norm(b)\n\n    # Compute the cross product between the normalized vectors\n    axis = np.cross(a_norm, b_norm)\n\n    # Compute the dot product between the normalized vectors\n    dot_product = np.dot(a_norm, b_norm)\n\n    # Compute the angle between the vectors\n    angle = np.arccos(np.clip(dot_product, -1.0, 1.0))\n\n    # Normalize the axis vector\n    axis_norm = axis / np.linalg.norm(axis)\n\n    return axis_norm, angle\n```\n\nExplanation:\n\n1. First, we check if the input vectors `a` and `b` are equal using `np.array_equal()`. If they are equal, we return `(None, None)` as specified in the problem statement.\n\n2. If the vectors are not equal, we normalize both vectors `a` and `b` by dividing them by their respective Euclidean norms using `np.linalg.norm()`. This ensures that the vectors have unit length.\n\n3. We compute the cross product between the normalized vectors `a_norm` and `b_norm` using `np.cross()`. The resulting vector represents the axis of rotation.\n\n4. We compute the dot product between the normalized vectors `a_norm` and `b_norm` using `np.dot()`. The dot product gives the cosine of the angle between the vectors.\n\n5. We compute the angle between the vectors using `np.arccos()` on the dot product. The `np.clip()` function is used to ensure that the dot product is within the range [-1, 1] to avoid numerical instability.\n\n6. Finally, we normalize the axis vector by dividing it by its Euclidean norm using `np.linalg.norm()` to obtain a unit vector.\n\n7. We return the normalized axis vector `axis_norm` and the angle `angle` as the result of the function.\n\nThis code aligns vector `a` to vector `b` by computing the axis and angle of rotation required to transform `a` into `b`. The axis is represented by a unit vector, and the angle is in radians.",
  "canonical_solution": "\n    if np.array_equal(a, b):\n        return None, None\n    axis_ = np.cross(a, b)\n    axis_ = axis_ / np.linalg.norm(axis_)\n    angle = np.arccos(np.dot(a, b))\n\n    return axis_, angle\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(a = [1, 0, 0], b = [1, 0, 0])[0] == align_vector_to_another(a = [1, 0, 0], b = [1, 0, 0])[0])\nassert((SOLUTION_SIGNATURE(a = [0, 0, 1], b = [0, 1, 0])[0] == align_vector_to_another(a = [0, 0, 1], b = [0, 1, 0])[0]).all())\nassert((SOLUTION_SIGNATURE(a = [1, 1, 0], b = [-1, 0, 0])[0] == align_vector_to_another(a = [1, 1, 0], b = [-1, 0, 0])[0]).all())\nassert(SOLUTION_SIGNATURE(a = [1, 0, 0], b = [1, 0, 0])[1] == align_vector_to_another(a = [1, 0, 0], b = [1, 0, 0])[1])\nassert((SOLUTION_SIGNATURE(a = [0, 0, 1], b = [0, 1, 0])[1] == align_vector_to_another(a = [0, 0, 1], b = [0, 1, 0])[1]).all())\nassert((SOLUTION_SIGNATURE(a = [1, 1, 0], b = [-1, 0, 0])[1] == align_vector_to_another(a = [1, 1, 0], b = [-1, 0, 0])[1]).all())\n"
 },
 {
  "task_id": "RWPB/118",
  "prompt": "import numpy as np\n\ndef filter_nan(ts, vs):\n  \"\"\"\n  Filter NaN values from a time series.\n  Args:\n      - ts: numpy.ndarray, the time series\n      - vs: numpy.ndarray, the values corresponding to the time series\n  Returns:\n      filtered_ts: numpy.ndarray, the time series with NaN values removed\n      filtered_vs: numpy.ndarray, the corresponding values with NaN values removed\n  \"\"\"\n",
  "solution": "Certainly! Here's the completed code to filter NaN values from a time series:\n\n```python\nimport numpy as np\n\ndef filter_nan(ts, vs):\n    \"\"\"\n    Filter NaN values from a time series.\n    Args:\n        - ts: numpy.ndarray, the time series\n        - vs: numpy.ndarray, the values corresponding to the time series\n    Returns:\n        filtered_ts: numpy.ndarray, the time series with NaN values removed\n        filtered_vs: numpy.ndarray, the corresponding values with NaN values removed\n    \"\"\"\n    # Create a boolean mask for non-NaN values\n    mask = ~np.isnan(vs)\n    \n    # Apply the mask to the time series and values\n    filtered_ts = ts[mask]\n    filtered_vs = vs[mask]\n    \n    return filtered_ts, filtered_vs\n```\n\nExplanation:\n1. We create a boolean mask `mask` using `~np.isnan(vs)`, which returns `True` for non-NaN values and `False` for NaN values in the `vs` array.\n\n2. We apply the `mask` to both the `ts` and `vs` arrays using boolean indexing. This selects only the elements where the corresponding value in `mask` is `True`, effectively filtering out the NaN values.\n\n3. The filtered time series and values are assigned to `filtered_ts` and `filtered_vs`, respectively.\n\n4. Finally, we return the `filtered_ts` and `filtered_vs` arrays.\n\nThis code will filter out any NaN values from the input time series and corresponding values, and return the filtered arrays.",
  "canonical_solution": "  \n  good_vs = np.logical_not(np.isnan(vs))\n  return ts[good_vs], vs[good_vs]\n\n\n",
  "unprocess_testcases": "ts = np.array([1, 2, 3, 4, 5])\nvs = np.array([1.1, 2.2, 3.3, 4.4, 5.5])\nassert((SOLUTION_SIGNATURE(ts, vs)[0] == filter_nan(ts, vs)[0]).all())\nassert((SOLUTION_SIGNATURE(ts, vs)[1] == filter_nan(ts, vs)[1]).all())\n\nts = np.array([1, 2, 3, 4, 5])\nvs = np.array([np.nan, np.nan, np.nan, np.nan, np.nan])\nassert((SOLUTION_SIGNATURE(ts, vs)[0] == filter_nan(ts, vs)[0]).all())\nassert((SOLUTION_SIGNATURE(ts, vs)[1] == filter_nan(ts, vs)[1]).all())\n\nts = np.array([1, 2, 3, 4, 5])\nvs = np.array([1.1, np.nan, 3.3, np.nan, 5.5])\nassert((SOLUTION_SIGNATURE(ts, vs)[0] == filter_nan(ts, vs)[0]).all())\nassert((SOLUTION_SIGNATURE(ts, vs)[1] == filter_nan(ts, vs)[1]).all())\n"
 },
 {
  "task_id": "RWPB/6",
  "prompt": "def format_seconds(seconds: float) -> str:\n    \"\"\"Converts a float number (seconds) to a string in the format:\n\n    - HH:MM:SS.SSS when greater than or equal to one hour\n    - MM:SS.SSS when less than one hour\n\n    Args:\n        seconds: The float number to convert (in seconds).\n\n    Returns:\n        str: The formatted string representation of the time.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without unit test cases:\n\n```python\ndef format_seconds(seconds: float) -> str:\n    \"\"\"Converts a float number (seconds) to a string in the format:\n\n    - HH:MM:SS.SSS when greater than or equal to one hour\n    - MM:SS.SSS when less than one hour\n\n    Args:\n        seconds: The float number to convert (in seconds).\n\n    Returns:\n        str: The formatted string representation of the time.\n    \"\"\"\n    hours = int(seconds // 3600)\n    minutes = int((seconds % 3600) // 60)\n    remaining_seconds = seconds % 60\n\n    if hours > 0:\n        return f\"{hours:02d}:{minutes:02d}:{remaining_seconds:06.3f}\"\n    else:\n        return f\"{minutes:02d}:{remaining_seconds:06.3f}\"\n```\n\nExplanation:\n\n1. We first calculate the number of hours by dividing `seconds` by 3600 (the number of seconds in an hour) using integer division (`//`). This gives us the whole number of hours.\n\n2. We then calculate the number of minutes by taking the remainder of `seconds` divided by 3600 (using the modulo operator `%`) and dividing the result by 60 (the number of seconds in a minute) using integer division. This gives us the whole number of minutes.\n\n3. Finally, we calculate the remaining seconds by taking the remainder of `seconds` divided by 60 using the modulo operator.\n\n4. If the number of hours is greater than 0, we format the time as \"HH:MM:SS.SSS\" using f-strings. The `:02d` format specifier ensures that the hours and minutes are padded with leading zeros to a width of 2, and the `:06.3f` format specifier ensures that the remaining seconds are padded with leading zeros to a width of 6 and displayed with 3 decimal places.\n\n5. If the number of hours is 0, we format the time as \"MM:SS.SSS\" using f-strings, with the same format specifiers as before for minutes and remaining seconds.\n\n6. The function returns the formatted string representation of the time.\n\nThis code should correctly format the time in seconds according to the specified requirements.",
  "canonical_solution": "    \n    hours = int(seconds // 3600)  # Extract whole hours\n    minutes = int((seconds % 3600) // 60)  # Extract whole minutes\n    seconds = seconds % 60  # Extract remaining seconds\n\n    # Use f-strings for clean formatting\n    if hours > 0:\n        return f\"{hours:02d}:{minutes:02d}:{seconds:06.3f}\"\n    else:\n        return f\"{minutes:02d}:{seconds:06.3f}\"\n    \n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(90.123) == format_seconds(90.123))\nassert(SOLUTION_SIGNATURE(3600.0) == format_seconds(3600.0))\nassert(SOLUTION_SIGNATURE(5436.789) == format_seconds(5436.789))"
 },
 {
  "task_id": "RWPB/99",
  "prompt": "import math\n\ndef focal2fov(focal, pixels):\n    \"\"\"\n    Converts focal length to field of view (FOV).\n    If focal is zero, the code should return KeyError\n\n    Args:\n    focal (float): Focal length.\n    pixels (int): Number of pixels.\n\n    Returns:\n    float: Field of view in radians.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport math\n\ndef focal2fov(focal, pixels):\n    \"\"\"\n    Converts focal length to field of view (FOV).\n    If focal is zero, the code should return KeyError\n\n    Args:\n    focal (float): Focal length.\n    pixels (int): Number of pixels.\n\n    Returns:\n    float: Field of view in radians.\n    \"\"\"\n    if focal == 0:\n        raise KeyError(\"Focal length cannot be zero.\")\n\n    return 2 * math.atan(pixels / (2 * focal))\n```\n\nExplanation:\n\n1. The function `focal2fov` takes two arguments: `focal` (focal length) and `pixels` (number of pixels).\n\n2. Inside the function, we first check if the `focal` length is equal to zero. If it is, we raise a `KeyError` with an appropriate error message, indicating that the focal length cannot be zero.\n\n3. If the `focal` length is non-zero, we calculate the field of view using the formula:\n   ```\n   FOV = 2 * atan(pixels / (2 * focal))\n   ```\n   - `pixels / (2 * focal)` calculates the tangent of half the field of view angle.\n   - We then use the `math.atan()` function to compute the arctangent (inverse tangent) of the result, which gives us half the field of view angle in radians.\n   - Finally, we multiply the result by 2 to get the full field of view angle in radians.\n\n4. The function returns the calculated field of view in radians.\n\nThis code converts the focal length to the field of view in radians, taking into account the number of pixels. If the focal length is zero, it raises a `KeyError` to indicate an invalid input.",
  "canonical_solution": "    \n    if int(focal) == 0:\n        return KeyError\n    return 2 * math.atan(pixels / (2 * focal))\n\n",
  "unprocess_testcases": "\nfocal = 49.1\npixels = 50000\nassert(SOLUTION_SIGNATURE(focal, pixels) == focal2fov(focal, pixels))\n\nfocal = 50.0\npixels = 1920\nassert(SOLUTION_SIGNATURE(focal, pixels) == focal2fov(focal, pixels))\n\nfocal = 1.0\npixels = 1920\nassert(SOLUTION_SIGNATURE(focal, pixels) == focal2fov(focal, pixels))\n\nfocal = 0\npixels = 2048\nassert(SOLUTION_SIGNATURE(focal, pixels) == focal2fov(focal, pixels))\n"
 },
 {
  "task_id": "RWPB/137",
  "prompt": "import numpy as np\nimport torch\nfrom sklearn.linear_model import LinearRegression\n\ndef fit_params(\n    x,\n    y,\n    fun,\n    a_range=(-10, 10),\n    b_range=(-10, 10),\n    grid_number=101,\n    iteration=3,\n    verbose=True,\n):\n    \"\"\"\n    fit a, b, c, d such that\n\n    .. math::\n        |y-(cf(ax+b)+d)|^2\n\n    is minimized. Both x and y are 1D array. Sweep a and b, find the best fitted model.\n\n    Args:\n    -----\n        x : 1D array\n            x values\n        y : 1D array\n            y values\n        fun : function\n            symbolic function\n        a_range : tuple\n            sweeping range of a\n        b_range : tuple\n            sweeping range of b\n        grid_num : int\n            number of steps along a and b\n        iteration : int\n            number of zooming in\n        verbose : bool\n            print extra information if True\n\n    Returns:\n    --------\n        a_best : float\n            best fitted a\n        b_best : float\n            best fitted b\n        c_best : float\n            best fitted c\n        d_best : float\n            best fitted d\n        r2_best : float\n            best r2 (coefficient of determination)\n\n    Example\n    -------\n    >>> num = 100\n    >>> x = torch.linspace(-1,1,steps=num)\n    >>> noises = torch.normal(0,1,(num,)) * 0.02\n    >>> y = 5.0*torch.sin(3.0*x + 2.0) + 0.7 + noises\n    >>> fit_params(x, y, torch.sin)\n    r2 is 0.9999727010726929\n    (tensor([2.9982, 1.9996, 5.0053, 0.7011]), tensor(1.0000))\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport numpy as np\nimport torch\nfrom sklearn.linear_model import LinearRegression\n\ndef fit_params(\n    x,\n    y,\n    fun,\n    a_range=(-10, 10),\n    b_range=(-10, 10),\n    grid_number=101,\n    iteration=3,\n    verbose=True,\n):\n    a_best, b_best, c_best, d_best, r2_best = None, None, None, None, -np.inf\n\n    for _ in range(iteration):\n        a_vals = torch.linspace(a_range[0], a_range[1], grid_number)\n        b_vals = torch.linspace(b_range[0], b_range[1], grid_number)\n\n        for a in a_vals:\n            for b in b_vals:\n                x_trans = fun(a * x + b)\n                model = LinearRegression().fit(x_trans.reshape(-1, 1), y)\n                c, d = model.coef_[0], model.intercept_\n\n                y_pred = c * x_trans + d\n                r2 = 1 - torch.sum((y - y_pred) ** 2) / torch.sum((y - torch.mean(y)) ** 2)\n\n                if r2 > r2_best:\n                    a_best, b_best, c_best, d_best, r2_best = a, b, c, d, r2\n\n        # Zoom in around the best values for the next iteration\n        a_range = (a_best - (a_range[1] - a_range[0]) / (2 * grid_number),\n                   a_best + (a_range[1] - a_range[0]) / (2 * grid_number))\n        b_range = (b_best - (b_range[1] - b_range[0]) / (2 * grid_number),\n                   b_best + (b_range[1] - b_range[0]) / (2 * grid_number))\n\n    if verbose:\n        print(f\"r2 is {r2_best}\")\n        print(f\"(tensor([{a_best:.4f}, {b_best:.4f}, {c_best:.4f}, {d_best:.4f}]), tensor({r2_best:.4f}))\")\n\n    return a_best, b_best, c_best, d_best, r2_best\n```\n\nThe completed code does the following:\n\n1. It initializes the best values for `a`, `b`, `c`, `d`, and `r2` to `None` and negative infinity, respectively.\n\n2. It performs a grid search over the specified ranges of `a` and `b` for the given number of iterations.\n\n3. For each combination of `a` and `b`, it applies the given function `fun` to transform the input `x` values.\n\n4. It fits a linear regression model using the transformed `x` values and the corresponding `y` values to obtain the coefficients `c` and `d`.\n\n5. It calculates the predicted `y` values using the fitted model and computes the coefficient of determination (R-squared) to measure the goodness of fit.\n\n6. If the current R-squared value is better than the best R-squared value seen so far, it updates the best values for `a`, `b`, `c`, `d`, and `r2`.\n\n7. After each iteration, it zooms in around the best values of `a` and `b` by reducing the search range by a factor of `grid_number`.\n\n8. If `verbose` is set to `True`, it prints the best R-squared value and the corresponding best values for `a`, `b`, `c`, and `d`.\n\n9. Finally, it returns the best values for `a`, `b`, `c`, `d`, and `r2`.\n\nThis code should solve the problem of fitting the parameters `a`, `b`, `c`, and `d` to minimize the squared error between the transformed input `x` and the corresponding output `y` using the given function `fun`.",
  "canonical_solution": "    \n    # fit a, b, c, d such that y=c*fun(a*x+b)+d; both x and y are 1D array.\n    # sweep a and b, choose the best fitted model\n    for _ in range(iteration):\n        a_ = torch.linspace(a_range[0], a_range[1], steps=grid_number)\n        b_ = torch.linspace(b_range[0], b_range[1], steps=grid_number)\n        a_grid, b_grid = torch.meshgrid(a_, b_, indexing=\"ij\")\n        post_fun = fun(\n            a_grid[None, :, :] * x[:, None, None] + b_grid[None, :, :]\n        )\n        x_mean = torch.mean(post_fun, dim=[0], keepdim=True)\n        y_mean = torch.mean(y, dim=[0], keepdim=True)\n        numerator = (\n            torch.sum((post_fun - x_mean) * (y - y_mean)[:, None, None], dim=0)\n            ** 2\n        )\n        denominator = torch.sum((post_fun - x_mean) ** 2, dim=0) * torch.sum(\n            (y - y_mean)[:, None, None] ** 2, dim=0\n        )\n        r2 = numerator / (denominator + 1e-4)\n        r2 = torch.nan_to_num(r2)\n\n        best_id = torch.argmax(r2)\n        a_id, b_id = (\n            torch.div(best_id, grid_number, rounding_mode=\"floor\"),\n            best_id % grid_number,\n        )\n\n        if (\n            a_id == 0\n            or a_id == grid_number - 1\n            or b_id == 0\n            or b_id == grid_number - 1\n        ):\n            if _ == 0 and verbose:\n                pass\n            if a_id == 0:\n                a_arange = [a_[0], a_[1]]  # noqa\n            if a_id == grid_number - 1:\n                a_arange = [a_[-2], a_[-1]]  # noqa\n            if b_id == 0:\n                b_arange = [b_[0], b_[1]]  # noqa\n            if b_id == grid_number - 1:\n                b_arange = [b_[-2], b_[-1]]  # noqa\n\n        else:\n            a_range = [a_[a_id - 1], a_[a_id + 1]]\n            b_range = [b_[b_id - 1], b_[b_id + 1]]\n\n    a_best = a_[a_id]\n    b_best = b_[b_id]\n    post_fun = fun(a_best * x + b_best)\n    r2_best = r2[a_id, b_id]\n\n\n    post_fun = torch.nan_to_num(post_fun)\n    reg = LinearRegression().fit(\n        post_fun[:, None].detach().numpy(), y.detach().numpy()\n    )\n    c_best = torch.from_numpy(reg.coef_)[0]\n    d_best = torch.from_numpy(np.array(reg.intercept_))\n    return torch.stack([a_best, b_best, c_best, d_best]), r2_best\n\n\n\n\n",
  "unprocess_testcases": "x = torch.linspace(-1, 1, steps=100)\ny = 5.0 * torch.sin(3.0 * x + 2.0) + 0.7 + torch.normal(0, 1, (100,)) * 0.02\nassert((SOLUTION_SIGNATURE(x, y, torch.sin)[0] == fit_params(x, y, torch.sin)[0]).all())\nassert((SOLUTION_SIGNATURE(x, y, torch.sin)[1] == fit_params(x, y, torch.sin)[1]).all())\n\n\nx = torch.linspace(-2, 2, steps=50)\ny = 4.0 * torch.cos(2.0 * x + 1.0) + 1.0 + torch.normal(0, 1, (50,)) * 0.05\nassert((SOLUTION_SIGNATURE(x, y, torch.cos, a_range=(-2, 2), b_range=(-2, 2), grid_number=5)[0] == fit_params(x, y, torch.cos, a_range=(-2, 2), b_range=(-2, 2), grid_number=5)[0]).all())\nassert((SOLUTION_SIGNATURE(x, y, torch.cos, a_range=(-2, 2), b_range=(-2, 2), grid_number=5)[1] == fit_params(x, y, torch.cos, a_range=(-2, 2), b_range=(-2, 2), grid_number=5)[1]).all())\n\nx = torch.linspace(-3, 3, steps=150)\ny = 2.0 * torch.tanh(0.5 * x + 3.0) + 0.5 + torch.normal(0, 1, (150,)) * 0.1\nassert((SOLUTION_SIGNATURE(x, y, torch.tanh, iteration=5)[0] == fit_params(x, y, torch.tanh, iteration=5)[0]).all())\nassert((SOLUTION_SIGNATURE(x, y, torch.tanh, iteration=5)[1] == fit_params(x, y, torch.tanh, iteration=5)[1]).all())\n"
 },
 {
  "task_id": "RWPB/103",
  "prompt": "MAX_VAL = (1 << 256) - 1\n\n\n\n# does not revert on overflow\ndef unsafeAdd(x, y):\n    \"\"\"\n    Add two integers, ignoring overflow.\n    Args:\n        - x: int, first operand\n        - y: int, second operand\n    Returns:\n        result: int, the sum of x and y, with overflow ignored\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nMAX_VAL = (1 << 256) - 1\n\ndef unsafeAdd(x, y):\n    \"\"\"\n    Add two integers, ignoring overflow.\n    Args:\n        - x: int, first operand\n        - y: int, second operand\n    Returns:\n        result: int, the sum of x and y, with overflow ignored\n    \"\"\"\n    result = x + y\n    if result > MAX_VAL:\n        result = result & MAX_VAL\n    return result\n```\n\nExplanation:\n1. The function `unsafeAdd` takes two integers `x` and `y` as input.\n2. It adds `x` and `y` using the `+` operator and stores the result in the `result` variable.\n3. It then checks if the `result` is greater than `MAX_VAL`. If it is, it means an overflow has occurred.\n4. To handle the overflow, the code performs a bitwise AND operation between `result` and `MAX_VAL`. This operation effectively truncates the `result` to the maximum value that can be represented within the given bit range (256 bits in this case).\n5. Finally, the function returns the `result`, which is either the sum of `x` and `y` if no overflow occurred, or the truncated value if an overflow occurred.\n\nBy performing the bitwise AND operation with `MAX_VAL`, the code ensures that the result stays within the valid range and ignores any overflow that may have occurred during the addition.\n\nNote: This code assumes that the integers are unsigned and that the maximum value is determined by the `MAX_VAL` constant, which is set to `(1 << 256) - 1` in this case, representing the maximum value that can be represented with 256 bits.",
  "canonical_solution": "    \n    return (x + y) & MAX_VAL\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(10, 20) == unsafeAdd(10, 20))\nassert(SOLUTION_SIGNATURE(MAX_VAL - 5, 10) == unsafeAdd(MAX_VAL - 5, 10))\nassert(SOLUTION_SIGNATURE(MAX_VAL // 2, (MAX_VAL // 2) + 1) == unsafeAdd(MAX_VAL // 2, (MAX_VAL // 2) + 1))\n"
 },
 {
  "task_id": "RWPB/113",
  "prompt": "import torch\n\ndef _linspace(start: torch.Tensor, stop: torch.Tensor, num: int):\n    \"\"\"\n    Generate a sequence of evenly spaced values between start and stop tensors.\n\n    Args:\n        start (torch.Tensor): The starting values of the sequence.\n        stop (torch.Tensor): The stopping values of the sequence.\n        num (int): The number of values to generate.\n\n    Returns:\n        torch.Tensor: A tensor containing `num` evenly spaced values between `start` and `stop`.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `_linspace` function:\n\n```python\nimport torch\n\ndef _linspace(start: torch.Tensor, stop: torch.Tensor, num: int):\n    \"\"\"\n    Generate a sequence of evenly spaced values between start and stop tensors.\n\n    Args:\n        start (torch.Tensor): The starting values of the sequence.\n        stop (torch.Tensor): The stopping values of the sequence.\n        num (int): The number of values to generate.\n\n    Returns:\n        torch.Tensor: A tensor containing `num` evenly spaced values between `start` and `stop`.\n    \"\"\"\n    if num == 1:\n        return start.unsqueeze(0)\n    \n    step = (stop - start) / (num - 1)\n    values = torch.arange(num, dtype=start.dtype, device=start.device)\n    values = values * step + start\n    \n    return values\n```\n\nExplanation:\n\n1. We first check if `num` is equal to 1. If it is, we simply return the `start` tensor as a single-element tensor by unsqueezing it along dimension 0.\n\n2. If `num` is greater than 1, we calculate the step size between consecutive values by dividing the difference between `stop` and `start` by `num - 1`. This ensures that the last value in the sequence will be equal to `stop`.\n\n3. We create a tensor `values` using `torch.arange()` with the same data type and device as `start`. It generates a sequence of integers from 0 to `num - 1`.\n\n4. We multiply `values` by the `step` size and add `start` to each element. This generates a sequence of evenly spaced values between `start` and `stop`.\n\n5. Finally, we return the `values` tensor containing the evenly spaced values.\n\nThis implementation uses PyTorch tensors and operations to generate the evenly spaced values efficiently on the same device as the input tensors.",
  "canonical_solution": "    \n    if num == 1:\n        return start.clone()\n\n    steps = torch.linspace(0, 1, num, dtype=start.dtype, device=start.device)\n\n    steps = steps.view(*([1] * (start.dim() - 1)), num)\n    out = start.unsqueeze(-1) + (stop - start).unsqueeze(-1) * steps\n    return out\n\n\n",
  "unprocess_testcases": "start = torch.tensor([0.])\nstop = torch.tensor([10.])\nnum = 5\nassert((SOLUTION_SIGNATURE(start, stop, num) == _linspace(start, stop, num)).all())\n\nstart = torch.tensor([1., 2., 3.])\nstop = torch.tensor([1., 2., 3.])\nnum = 1\nassert((SOLUTION_SIGNATURE(start, stop, num) == _linspace(start, stop, num)).all())\n\nstart = torch.tensor([0, 10], dtype=torch.int32)\nstop = torch.tensor([5, 15], dtype=torch.int32)\nnum = 3\nassert((SOLUTION_SIGNATURE(start, stop, num) == _linspace(start, stop, num)).all())\n\nstart = torch.tensor([5., 5., 5.])\nstop = torch.tensor([5., 5., 5.])\nnum = 3\nassert((SOLUTION_SIGNATURE(start, stop, num) == _linspace(start, stop, num)).all())\n\nstart = torch.tensor([[0., 0.], [10., 10.]])\nstop = torch.tensor([[5., 10.], [15., 20.]])\nnum = 4\nassert((SOLUTION_SIGNATURE(start, stop, num) == _linspace(start, stop, num)).all())\n"
 },
 {
  "task_id": "RWPB/127",
  "prompt": "import torch\n\ndef shift_dim(x, src_dim=-1, dest_dim=-1, make_contiguous=True):\n    \"\"\"\n    Shifts a specified dimension of a tensor to a new position.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        src_dim (int): The original position of the dimension to move. Negative values count from the end.\n        dest_dim (int): The destination position for the dimension. Negative values count from the end.\n        make_contiguous (bool): If True, returns a contiguous tensor. This can be necessary after a permute\n                                operation because permute can lead to non-contiguous memory layout, potentially\n                                affecting performance.\n\n    Returns:\n        torch.Tensor: A tensor with the specified dimension shifted to the new position.\n\n    Example:\n        # Assuming x is a tensor with shape (batch, channels, time, height, width)\n        # and we want to move the 'channels' dimension to the end:\n        shifted_tensor = shift_dim(x, 1, -1)\n        # shifted_tensor will have the shape (batch, time, height, width, channels)\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport torch\n\ndef shift_dim(x, src_dim=-1, dest_dim=-1, make_contiguous=True):\n    \"\"\"\n    Shifts a specified dimension of a tensor to a new position.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        src_dim (int): The original position of the dimension to move. Negative values count from the end.\n        dest_dim (int): The destination position for the dimension. Negative values count from the end.\n        make_contiguous (bool): If True, returns a contiguous tensor. This can be necessary after a permute\n                                operation because permute can lead to non-contiguous memory layout, potentially\n                                affecting performance.\n\n    Returns:\n        torch.Tensor: A tensor with the specified dimension shifted to the new position.\n\n    Example:\n        # Assuming x is a tensor with shape (batch, channels, time, height, width)\n        # and we want to move the 'channels' dimension to the end:\n        shifted_tensor = shift_dim(x, 1, -1)\n        # shifted_tensor will have the shape (batch, time, height, width, channels)\n    \"\"\"\n    if src_dim < 0:\n        src_dim = x.dim() + src_dim\n    if dest_dim < 0:\n        dest_dim = x.dim() + dest_dim\n\n    dims = list(range(x.dim()))\n    dims.pop(src_dim)\n    dims.insert(dest_dim, src_dim)\n\n    shifted_tensor = x.permute(dims)\n    if make_contiguous:\n        shifted_tensor = shifted_tensor.contiguous()\n\n    return shifted_tensor\n```\n\nExplanation:\n1. We first check if `src_dim` and `dest_dim` are negative. If they are, we convert them to positive indices by adding the tensor's dimension to them. This allows using negative indices to count from the end.\n\n2. We create a list `dims` containing the dimensions of the tensor using `range(x.dim())`.\n\n3. We remove the dimension at index `src_dim` from the `dims` list using `dims.pop(src_dim)`.\n\n4. We insert the `src_dim` value at the `dest_dim` position in the `dims` list using `dims.insert(dest_dim, src_dim)`. This effectively shifts the dimension to the new position.\n\n5. We use `x.permute(dims)` to permute the tensor according to the new dimension order specified by `dims`.\n\n6. If `make_contiguous` is set to `True`, we call `shifted_tensor.contiguous()` to ensure that the resulting tensor has a contiguous memory layout. This can be important for performance reasons, especially after a permute operation.\n\n7. Finally, we return the shifted tensor.\n\nThis code should solve the problem of shifting a specified dimension of a tensor to a new position while providing the option to make the resulting tensor contiguous.",
  "canonical_solution": "    \n    n_dims = len(x.shape)  # Number of dimensions in the input tensor\n\n    # Adjust src_dim and dest_dim to be positive if they are given as negative\n    if src_dim < 0:\n        src_dim += n_dims\n    if dest_dim < 0:\n        dest_dim += n_dims\n\n    # Ensure the source and destination dimensions are within the valid range\n    assert 0 <= src_dim < n_dims and 0 <= dest_dim < n_dims, \"Source and destination dimensions must be within the valid range\"\n\n    # Create a list of dimensions except for the source dimension\n    dims = list(range(n_dims))\n    del dims[src_dim]  # Remove the source dimension from the list\n\n    # Build a new permutation of dimensions\n    permutation = []\n    ctr = 0\n    for i in range(n_dims):\n        if i == dest_dim:\n            permutation.append(src_dim)  # Insert the source dimension at the destination index\n        else:\n            permutation.append(dims[ctr])  # Fill in the rest of dimensions\n            ctr += 1\n\n    # Permute the dimensions of the tensor according to the new order\n    x = x.permute(permutation)\n\n    # Make the tensor contiguous if required\n    if make_contiguous:\n        x = x.contiguous()\n\n    return x\n\n\n",
  "unprocess_testcases": "tensor = torch.randn(2, 3, 4, 5)\nassert((SOLUTION_SIGNATURE(tensor, src_dim=1, dest_dim=-1) == shift_dim(tensor, src_dim=1, dest_dim=-1)).all())\n\ntensor = torch.randn(3, 4, 5)\nassert((SOLUTION_SIGNATURE(tensor, src_dim=-3, dest_dim=0, make_contiguous=False) == shift_dim(tensor, src_dim=-3, dest_dim=0, make_contiguous=False)).all())\n\n\ntensor = torch.randn(1, 2, 3, 4, 5)\nassert((SOLUTION_SIGNATURE(tensor, src_dim=0, dest_dim=4, make_contiguous=True) == shift_dim(tensor, src_dim=0, dest_dim=4, make_contiguous=True)).all())\n"
 },
 {
  "task_id": "RWPB/89",
  "prompt": "import math\n\n\ndef format_ratio(ratio):\n    \"\"\"\n    Formats a ratio into a percentage string\n\n    Args:\n        - ratio: float, ratio to be formatted\n    Returns:\n        formatted_ratio: str, formatted ratio in percentage\n\n    Examples:\n        - input: 3.1245133412\n        - output: percentage:3.124\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport math\n\n\ndef format_ratio(ratio):\n    \"\"\"\n    Formats a ratio into a percentage string\n\n    Args:\n        - ratio: float, ratio to be formatted\n    Returns:\n        formatted_ratio: str, formatted ratio in percentage\n\n    Examples:\n        - input: 3.1245133412\n        - output: percentage:3.124\n    \"\"\"\n    percentage = ratio * 100\n    formatted_ratio = f\"percentage:{percentage:.3f}\"\n    return formatted_ratio\n```\n\nExplanation:\n\n1. We multiply the `ratio` by 100 to convert it into a percentage.\n\n2. We use an f-string to format the `percentage` value. The `.3f` format specifier ensures that the percentage is displayed with three decimal places.\n\n3. We concatenate the string `\"percentage:\"` with the formatted percentage value to create the final formatted string.\n\n4. Finally, we return the `formatted_ratio` string.\n\nThis function takes a ratio as input, converts it into a percentage, formats it with three decimal places, and returns the formatted string in the desired format.",
  "canonical_solution": "    \n    percentage = ratio * 100\n    return f\"{percentage:3.3f}%\"\n\n\n",
  "unprocess_testcases": "assert(format(3.14) == format(3.14))\nassert(format(6.29) == format(6.29))\nassert(format(0) == format(0))\n"
 },
 {
  "task_id": "RWPB/2",
  "prompt": "import torch\nfrom scipy import special, pi\n\ndef logbessel_I_scipy(nu, z, check = True):\n\t'''\n\tPytorch version of scipy computation of modified Bessel functions \n\tof the 1st kind I(nu,z).\n\tParameters\n\t----------\n\tnu: positive int, float\n\t\tOrder of modified Bessel function of 1st kind.\n\tz: int/float or tensor, shape (N,) \n\t\tArgument of Bessel function.\n\tcheck: bool\n\t\tIf True, check if argument of log is non zero.\n\t\n\tReturn\n\t------\n\tresult: tensor, shape (N,)\n\t'''\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport torch\nfrom scipy import special, pi\n\ndef logbessel_I_scipy(nu, z, check = True):\n    '''\n    Pytorch version of scipy computation of modified Bessel functions \n    of the 1st kind I(nu,z).\n    Parameters\n    ----------\n    nu: positive int, float\n        Order of modified Bessel function of 1st kind.\n    z: int/float or tensor, shape (N,) \n        Argument of Bessel function.\n    check: bool\n        If True, check if argument of log is non zero.\n    \n    Return\n    ------\n    result: tensor, shape (N,)\n    '''\n    if isinstance(z, torch.Tensor):\n        z = z.cpu().numpy()\n    \n    result = special.ive(nu, z)\n    \n    if check:\n        result = torch.tensor(result, dtype=torch.float)\n        result[result == 0] = torch.tensor(1e-300)\n        result = torch.log(result)\n    else:\n        result = torch.log(torch.tensor(result, dtype=torch.float))\n    \n    return result\n```\n\nExplanation:\n1. We first check if `z` is a PyTorch tensor. If it is, we convert it to a NumPy array using `z.cpu().numpy()`. This is because `scipy.special.ive()` expects a NumPy array as input.\n\n2. We then compute the modified Bessel function of the first kind using `scipy.special.ive()` with the given order `nu` and argument `z`. The result is stored in the `result` variable.\n\n3. If `check` is True, we perform the following steps:\n   - Convert the `result` to a PyTorch tensor using `torch.tensor()` with `dtype=torch.float`.\n   - Check if any elements in `result` are equal to zero. If found, replace those elements with a small value (e.g., 1e-300) to avoid taking the logarithm of zero.\n   - Take the logarithm of the modified `result` using `torch.log()`.\n\n4. If `check` is False, we directly take the logarithm of the `result` converted to a PyTorch tensor.\n\n5. Finally, we return the computed logarithm of the modified Bessel function of the first kind.\n\nThis code provides a PyTorch-compatible version of the `scipy.special.ive()` function for computing the logarithm of the modified Bessel function of the first kind.",
  "canonical_solution": "\n\tif not isinstance(z, torch.Tensor):\n\t\tz = torch.tensor(z)\n\tz = z.reshape(-1)\n\n\tresult = special.ive(nu, z)\n\tif check:\n\t\tassert len(result[ result == 0]) == 0\n\tresult = torch.log(result) + z\n\treturn result\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(nu=1, z=5.0) == logbessel_I_scipy(nu=1, z=5.0)).all())\nassert((SOLUTION_SIGNATURE(nu=0.5, z=torch.tensor([0.1, 1.0, 10.0])) == logbessel_I_scipy(nu=0.5, z=torch.tensor([0.1, 1.0, 10.0]))).all())\nassert((SOLUTION_SIGNATURE(nu=-0.0001, z=0.0001) == logbessel_I_scipy(nu=-0.0001, z=0.0001)).all())"
 },
 {
  "task_id": "RWPB/107",
  "prompt": "import torch\n\ndef pad_camera_extrinsics_4x4(extrinsics):\n    \"\"\"\n    Pads a given extrinsics matrix to a 4x4 matrix if it is not already.\n\n    Args:\n        extrinsics (torch.Tensor): Input extrinsics matrix of shape (..., 3, 4).\n\n    Returns:\n        torch.Tensor: Padded extrinsics matrix of shape (..., 4, 4).\n    \"\"\"\n",
  "solution": "Here's the completed code to pad a camera extrinsics matrix to a 4x4 matrix:\n\n```python\nimport torch\n\ndef pad_camera_extrinsics_4x4(extrinsics):\n    \"\"\"\n    Pads a given extrinsics matrix to a 4x4 matrix if it is not already.\n\n    Args:\n        extrinsics (torch.Tensor): Input extrinsics matrix of shape (..., 3, 4).\n\n    Returns:\n        torch.Tensor: Padded extrinsics matrix of shape (..., 4, 4).\n    \"\"\"\n    if extrinsics.shape[-2:] == (4, 4):\n        return extrinsics\n\n    if extrinsics.shape[-2:] != (3, 4):\n        raise ValueError(f\"Invalid extrinsics shape: {extrinsics.shape}. Expected (..., 3, 4) or (..., 4, 4).\")\n\n    # Create a new tensor with an additional row of [0, 0, 0, 1]\n    bottom_row = torch.tensor([0, 0, 0, 1], dtype=extrinsics.dtype, device=extrinsics.device)\n    bottom_row = bottom_row.view(*[1] * (extrinsics.dim() - 1), 1, 4)\n    bottom_row = bottom_row.expand(*extrinsics.shape[:-2], 1, 4)\n\n    # Concatenate the input extrinsics with the bottom row\n    padded_extrinsics = torch.cat([extrinsics, bottom_row], dim=-2)\n\n    return padded_extrinsics\n```\n\nExplanation:\n\n1. First, we check if the input `extrinsics` matrix is already a 4x4 matrix. If so, we return it as is.\n\n2. If the `extrinsics` matrix is not a 3x4 or 4x4 matrix, we raise a `ValueError` with an appropriate error message.\n\n3. We create a new tensor `bottom_row` with the values `[0, 0, 0, 1]`. This tensor represents the bottom row that we want to append to the `extrinsics` matrix.\n\n4. We reshape `bottom_row` to have the same number of dimensions as `extrinsics`, except for the last two dimensions. We use `view` and `expand` to broadcast the `bottom_row` tensor to match the shape of `extrinsics`.\n\n5. Finally, we concatenate the input `extrinsics` matrix with the `bottom_row` tensor along the second-to-last dimension (i.e., the row dimension) using `torch.cat`. This appends the bottom row to the `extrinsics` matrix.\n\n6. The resulting `padded_extrinsics` matrix has a shape of `(..., 4, 4)`, where the `...` represents any leading dimensions from the input `extrinsics` matrix.\n\nThis code ensures that the input `extrinsics` matrix is padded to a 4x4 matrix if it is not already, while preserving any leading dimensions.",
  "canonical_solution": "    \n    if extrinsics.shape[-2] == 4:\n        return extrinsics\n    padding = torch.tensor([[0, 0, 0, 1]]).to(extrinsics)\n    if extrinsics.ndim == 3:\n        padding = padding.unsqueeze(0).repeat(extrinsics.shape[0], 1, 1)\n    extrinsics = torch.cat([extrinsics, padding], dim=-2)\n    return extrinsics\n\n\n",
  "unprocess_testcases": "extrinsics = torch.rand((2, 4, 4))\nassert((SOLUTION_SIGNATURE(extrinsics) == pad_camera_extrinsics_4x4(extrinsics)).all())\n\nextrinsics = torch.rand((3, 3, 4))\nassert((SOLUTION_SIGNATURE(extrinsics) == pad_camera_extrinsics_4x4(extrinsics)).all())\n\nextrinsics = torch.rand((3, 4))\nassert((SOLUTION_SIGNATURE(extrinsics) == pad_camera_extrinsics_4x4(extrinsics)).all())\n\nextrinsics = torch.rand((2, 1, 4))\nassert((SOLUTION_SIGNATURE(extrinsics) == pad_camera_extrinsics_4x4(extrinsics)).all())\n"
 },
 {
  "task_id": "RWPB/133",
  "prompt": "from PIL import Image\n\ndef divide_to_patches(image, patch_size):\n    \"\"\"\n    Divides an image into patches of a specified size.\n\n    Args:\n        image (PIL.Image.Image): The input image.\n        patch_size (int): The size of each patch.\n\n    Returns:\n        list: A list of PIL.Image.Image objects representing the patches.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of dividing an image into patches of a specified size:\n\n```python\nfrom PIL import Image\n\ndef divide_to_patches(image, patch_size):\n    \"\"\"\n    Divides an image into patches of a specified size.\n\n    Args:\n        image (PIL.Image.Image): The input image.\n        patch_size (int): The size of each patch.\n\n    Returns:\n        list: A list of PIL.Image.Image objects representing the patches.\n    \"\"\"\n    width, height = image.size\n    patches = []\n\n    for i in range(0, height, patch_size):\n        for j in range(0, width, patch_size):\n            box = (j, i, j + patch_size, i + patch_size)\n            patch = image.crop(box)\n            patches.append(patch)\n\n    return patches\n```\n\nExplanation:\n\n1. We start by extracting the width and height of the input image using `image.size`.\n\n2. We initialize an empty list called `patches` to store the extracted patches.\n\n3. We use two nested loops to iterate over the image in a grid-like manner:\n   - The outer loop iterates over the rows of the image, starting from 0 and incrementing by `patch_size` in each iteration.\n   - The inner loop iterates over the columns of the image, starting from 0 and incrementing by `patch_size` in each iteration.\n\n4. Inside the nested loops, we define a `box` tuple that represents the coordinates of the current patch. The `box` tuple contains four values: the left, upper, right, and lower coordinates of the patch.\n\n5. We use `image.crop(box)` to extract the current patch from the image based on the coordinates specified in the `box` tuple. This creates a new PIL.Image.Image object representing the patch.\n\n6. We append the extracted patch to the `patches` list.\n\n7. After the loops finish, we have extracted all the patches from the image. We return the `patches` list containing the PIL.Image.Image objects representing the patches.\n\nYou can now use this function to divide an image into patches of a specified size. For example:\n\n```python\nimage = Image.open(\"example.jpg\")\npatch_size = 128\npatches = divide_to_patches(image, patch_size)\n```\n\nThis will divide the `example.jpg` image into patches of size 128x128 pixels and store them in the `patches` list.",
  "canonical_solution": "    \n    patches = []\n    width, height = image.size\n    for i in range(0, height, patch_size):\n        for j in range(0, width, patch_size):\n            box = (j, i, j + patch_size, i + patch_size)\n            patch = image.crop(box)\n            patches.append(patch)\n\n    return patches\n\n\n",
  "unprocess_testcases": "img = Image.new('RGB', (256, 256))\npatch_size = 64\nassert(SOLUTION_SIGNATURE(img, patch_size) == divide_to_patches(img, patch_size))\n\nimg = Image.new('RGB', (300, 150))\npatch_size = 50\nassert(SOLUTION_SIGNATURE(img, patch_size) == divide_to_patches(img, patch_size))\n\nimg = Image.new('RGB', (30, 30))\npatch_size = 50\nassert(SOLUTION_SIGNATURE(img, patch_size) == divide_to_patches(img, patch_size))\n"
 },
 {
  "task_id": "RWPB/123",
  "prompt": "import torch\nimport torch.nn as nn\n\n\ndef fuse_deconv_and_bn(deconv, bn):\n    \"\"\"\n    Fuses a ConvTranspose2d layer with a BatchNorm2d layer into a single ConvTranspose2d layer.\n\n    Arguments:\n    deconv : torch.nn.ConvTranspose2d\n        The convolutional transpose layer to be fused.\n    bn : torch.nn.BatchNorm2d\n        The batch normalization layer to be fused.\n\n    Returns:\n    torch.nn.ConvTranspose2d\n        The resulting fused ConvTranspose2d layer.\n    \"\"\"\n",
  "solution": "To fuse a ConvTranspose2d layer with a BatchNorm2d layer into a single ConvTranspose2d layer, you can follow these steps:\n\n1. Compute the equivalent weights and biases of the fused layer by combining the parameters of the ConvTranspose2d and BatchNorm2d layers.\n2. Create a new ConvTranspose2d layer with the computed weights and biases.\n\nHere's the completed code:\n\n```python\nimport torch\nimport torch.nn as nn\n\n\ndef fuse_deconv_and_bn(deconv, bn):\n    \"\"\"\n    Fuses a ConvTranspose2d layer with a BatchNorm2d layer into a single ConvTranspose2d layer.\n\n    Arguments:\n    deconv : torch.nn.ConvTranspose2d\n        The convolutional transpose layer to be fused.\n    bn : torch.nn.BatchNorm2d\n        The batch normalization layer to be fused.\n\n    Returns:\n    torch.nn.ConvTranspose2d\n        The resulting fused ConvTranspose2d layer.\n    \"\"\"\n    # Check if the ConvTranspose2d layer has bias\n    has_bias = deconv.bias is not None\n\n    # Compute the equivalent weights and biases\n    w_deconv = deconv.weight.data\n    if has_bias:\n        b_deconv = deconv.bias.data\n    else:\n        b_deconv = torch.zeros_like(bn.running_mean)\n\n    factor = bn.weight.data / torch.sqrt(bn.running_var + bn.eps)\n    w_fused = w_deconv * factor.view(-1, 1, 1, 1)\n    b_fused = (b_deconv - bn.running_mean) * factor + bn.bias.data\n\n    # Create a new ConvTranspose2d layer with the fused weights and biases\n    fused_deconv = nn.ConvTranspose2d(\n        in_channels=deconv.in_channels,\n        out_channels=deconv.out_channels,\n        kernel_size=deconv.kernel_size,\n        stride=deconv.stride,\n        padding=deconv.padding,\n        output_padding=deconv.output_padding,\n        groups=deconv.groups,\n        bias=True,\n        dilation=deconv.dilation,\n    )\n    fused_deconv.weight.data = w_fused\n    fused_deconv.bias.data = b_fused\n\n    return fused_deconv\n```\n\nHere's how the code works:\n\n1. We check if the ConvTranspose2d layer has a bias term using `deconv.bias is not None`.\n\n2. We compute the equivalent weights and biases for the fused layer:\n   - The weights of the fused layer (`w_fused`) are obtained by multiplying the weights of the ConvTranspose2d layer (`w_deconv`) with the scaling factor computed from the BatchNorm2d layer's parameters (`factor`). The scaling factor is calculated as `bn.weight / sqrt(bn.running_var + bn.eps)`.\n   - The biases of the fused layer (`b_fused`) are obtained by subtracting the running mean of the BatchNorm2d layer (`bn.running_mean`) from the biases of the ConvTranspose2d layer (`b_deconv`), multiplying the result by the scaling factor, and adding the biases of the BatchNorm2d layer (`bn.bias`).\n\n3. We create a new ConvTranspose2d layer (`fused_deconv`) with the same parameters as the original ConvTranspose2d layer, except for the weights and biases, which are set to the computed `w_fused` and `b_fused`, respectively.\n\n4. Finally, we return the fused ConvTranspose2d layer.\n\nThis code fuses the ConvTranspose2d layer with the BatchNorm2d layer into a single ConvTranspose2d layer, eliminating the need for a separate BatchNorm2d layer during inference.",
  "canonical_solution": "    \n    # Create a new ConvTranspose2d layer with the same parameters as the original but with bias enabled\n    fuseddconv = nn.ConvTranspose2d(\n        in_channels=deconv.in_channels,\n        out_channels=deconv.out_channels,\n        kernel_size=deconv.kernel_size,\n        stride=deconv.stride,\n        padding=deconv.padding,\n        output_padding=deconv.output_padding,\n        dilation=deconv.dilation,\n        groups=deconv.groups,\n        bias=True  # Always true to accommodate the fused bias\n    ).requires_grad_(False).to(deconv.weight.device)  # Disable grad and move to the same device\n\n    # Prepare the weights for fusion\n    w_deconv = deconv.weight.clone().view(deconv.out_channels, -1)\n    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))\n    fuseddconv.weight.copy_(\n        torch.mm(w_bn, w_deconv).view(fuseddconv.weight.shape))  # Apply the BN transformation to weights\n\n    # Prepare and fuse the biases\n    b_conv = torch.zeros(deconv.weight.shape[1], device=deconv.weight.device) if deconv.bias is None else deconv.bias\n    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n    fuseddconv.bias.copy_(\n        torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)  # Combine biases from BN and deconv layers\n\n    return fuseddconv\n\n\n",
  "unprocess_testcases": "a = torch.randn(20, 3, 50, 100)\ndeconv = nn.ConvTranspose2d(3, 3, 3, stride=2, padding=1, output_padding=1, dilation=1, groups=1, bias=None)\nbn = nn.BatchNorm2d(3)\nassert((SOLUTION_SIGNATURE(deconv, bn)(a) == fuse_deconv_and_bn(deconv, bn)(a)).all())\n\ninitial_bias = torch.randn(3)\ndeconv = nn.ConvTranspose2d(3, 3, 3, stride=2, padding=1, output_padding=1, dilation=1, groups=1, bias=True)\ndeconv.bias.data = initial_bias.clone()\nbn = nn.BatchNorm2d(3)\nassert((SOLUTION_SIGNATURE(deconv, bn)(a) == fuse_deconv_and_bn(deconv, bn)(a)).all())\n\ndeconv = nn.ConvTranspose2d(3, 4, (5, 5), (2, 2), (1, 1), output_padding=(1, 1), dilation=(2, 2))\nbn = nn.BatchNorm2d(4)\nassert((SOLUTION_SIGNATURE(deconv, bn)(a) == fuse_deconv_and_bn(deconv, bn)(a)).all())\n\n"
 },
 {
  "task_id": "RWPB/117",
  "prompt": "import os.path\n\ndef path_spliter(path: str):\n    \"\"\"\n    Splits the given path into its components.\n\n    Args:\n        path (str): The path to split.\n\n    Returns:\n        list: A list of path components.\n    \"\"\"\n",
  "solution": "Certainly! Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport os.path\n\ndef path_spliter(path: str):\n    \"\"\"\n    Splits the given path into its components.\n\n    Args:\n        path (str): The path to split.\n\n    Returns:\n        list: A list of path components.\n    \"\"\"\n    components = []\n    while True:\n        head, tail = os.path.split(path)\n        if tail:\n            components.insert(0, tail)\n        if head == path:\n            if head:\n                components.insert(0, head)\n            break\n        path = head\n    return components\n```\n\nExplanation:\n1. We initialize an empty list called `components` to store the path components.\n\n2. We start a loop that continues until we have processed the entire path.\n\n3. Inside the loop, we use `os.path.split()` to split the path into its head (directory) and tail (file or directory name) components.\n\n4. If the `tail` is not empty, we insert it at the beginning of the `components` list using `insert(0, tail)`. This ensures that the components are added in the correct order.\n\n5. We then check if the `head` is equal to the current `path`. If it is, it means we have reached the root directory or the top-level component of the path.\n\n6. If the `head` is not empty, we insert it at the beginning of the `components` list. This handles the case where the path starts with a root directory.\n\n7. We break out of the loop since we have processed the entire path.\n\n8. If the `head` is not equal to the current `path`, we update the `path` variable to be the `head` component and continue the loop to process the remaining parts of the path.\n\n9. Finally, we return the `components` list containing the split path components.\n\nThis code uses the `os.path.split()` function to recursively split the path into its components. The resulting `components` list will contain the individual path components in the correct order.\n\nNote: This code assumes that the input `path` is a valid path string. It does not include any error handling for invalid paths.",
  "canonical_solution": "    \n    res = []\n    if path == '':\n        return []\n\n    while path != '' and path != '/':\n        res.append(os.path.split(path)[1])\n        path = os.path.split(path)[0]\n    if path != '':\n        res.append(path)\n    \n\n    return res[::-1]\n\n\n",
  "unprocess_testcases": "str = '/usr/bin/python'\nassert(SOLUTION_SIGNATURE(str) == path_spliter(str))\n\nstr = '../folder/subfolder/file.txt'\nassert(SOLUTION_SIGNATURE(str) == path_spliter(str))\n\nstr = './../xx/yy/./../yty/file.txt'\nassert(SOLUTION_SIGNATURE(str) == path_spliter(str))\n\nstr = ''\nassert(SOLUTION_SIGNATURE(str) == path_spliter(str))\n"
 },
 {
  "task_id": "RWPB/3",
  "prompt": "import numpy as np\n\ndef get_focalLength_from_fieldOfView(fov=60, img_size=512):\n    \"\"\"\n    Compute the focal length of the camera lens by assuming a certain FOV for the entire image\n    Args:\n        - fov: float, expressed in degree\n        - img_size: int\n    Return:\n        focal: float\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of computing the focal length from the field of view (FOV) and image size:\n\n```python\nimport numpy as np\n\ndef get_focalLength_from_fieldOfView(fov=60, img_size=512):\n    \"\"\"\n    Compute the focal length of the camera lens by assuming a certain FOV for the entire image\n    Args:\n        - fov: float, expressed in degree\n        - img_size: int\n    Return:\n        focal: float\n    \"\"\"\n    # Convert FOV from degrees to radians\n    fov_rad = np.deg2rad(fov)\n\n    # Compute the focal length using the formula: focal = (img_size / 2) / tan(FOV / 2)\n    focal = (img_size / 2) / np.tan(fov_rad / 2)\n\n    return focal\n```\n\nExplanation:\n1. The function `get_focalLength_from_fieldOfView` takes two parameters:\n   - `fov`: The field of view of the camera lens, expressed in degrees (default value is 60).\n   - `img_size`: The size of the image (default value is 512).\n\n2. The first step is to convert the FOV from degrees to radians using `np.deg2rad()`. This is necessary because the `np.tan()` function expects the angle to be in radians.\n\n3. The focal length is computed using the formula: `focal = (img_size / 2) / tan(FOV / 2)`.\n   - `img_size / 2` represents half of the image size, which is used as the distance from the center of the image to the edge.\n   - `np.tan(fov_rad / 2)` computes the tangent of half the FOV angle. Dividing the FOV by 2 gives the angle from the center of the image to the edge.\n\n4. Finally, the computed focal length is returned.\n\nThis function provides a way to estimate the focal length of a camera lens based on the field of view and image size. The focal length is an important parameter in computer vision and 3D reconstruction tasks, as it relates the size of the image sensor to the field of view of the lens.",
  "canonical_solution": "    \n    focal = img_size / (2 * np.tan(np.radians(fov) /2))\n    return focal\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(fov = 60, img_size = 512) == get_focalLength_from_fieldOfView(fov = 60, img_size = 512))\nassert(SOLUTION_SIGNATURE(fov = 1, img_size = 512) == get_focalLength_from_fieldOfView(fov = 1, img_size = 512))\nassert(SOLUTION_SIGNATURE(fov = 120, img_size = 2048) == get_focalLength_from_fieldOfView(fov = 120, img_size = 2048))"
 },
 {
  "task_id": "RWPB/106",
  "prompt": "import numpy as np\n\ndef interpolate_features(features,\n                         input_rate,\n                         output_rate,\n                         output_len):\n    \"\"\"\n    Interpolate DeepSpeech features.\n\n    Parameters\n    ----------\n    features : np.array\n        DeepSpeech features.\n    input_rate : int\n        input rate (FPS).\n    output_rate : int\n        Output rate (FPS).\n    output_len : int\n        Output data length.\n\n    Returns\n    -------\n    np.array\n        Interpolated data.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport numpy as np\n\ndef interpolate_features(features,\n                         input_rate,\n                         output_rate,\n                         output_len):\n    \"\"\"\n    Interpolate DeepSpeech features.\n\n    Parameters\n    ----------\n    features : np.array\n        DeepSpeech features.\n    input_rate : int\n        input rate (FPS).\n    output_rate : int\n        Output rate (FPS).\n    output_len : int\n        Output data length.\n\n    Returns\n    -------\n    np.array\n        Interpolated data.\n    \"\"\"\n    input_len = features.shape[0]\n    \n    # Calculate the ratio between input and output rates\n    ratio = input_rate / output_rate\n    \n    # Create an array of input indices\n    input_indices = np.arange(input_len)\n    \n    # Create an array of output indices\n    output_indices = np.linspace(0, input_len - 1, output_len)\n    \n    # Perform linear interpolation along the time axis\n    interpolated_features = np.zeros((output_len, features.shape[1]))\n    for i in range(features.shape[1]):\n        interpolated_features[:, i] = np.interp(output_indices, input_indices, features[:, i])\n    \n    return interpolated_features\n```\n\nExplanation:\n1. We calculate the ratio between the input and output rates, which will be used to determine the spacing of the output indices.\n\n2. We create an array of input indices using `np.arange()`, which represents the original indices of the input features.\n\n3. We create an array of output indices using `np.linspace()`, which generates evenly spaced indices from 0 to `input_len - 1` with a total of `output_len` elements. These indices represent the desired positions in the interpolated output.\n\n4. We initialize an array called `interpolated_features` with the shape `(output_len, features.shape[1])` to store the interpolated features.\n\n5. We iterate over each feature dimension (column) of the input features using a loop.\n\n6. For each feature dimension, we perform linear interpolation using `np.interp()`. It takes the output indices, input indices, and the corresponding feature values as arguments and returns the interpolated values at the output indices.\n\n7. We assign the interpolated values to the corresponding column of `interpolated_features`.\n\n8. Finally, we return the interpolated features.\n\nThis code interpolates the DeepSpeech features from the input rate to the output rate, generating an array of interpolated features with the desired output length.",
  "canonical_solution": "    \n    input_len = features.shape[0]\n    num_features = features.shape[1]\n    input_timestamps = np.arange(input_len) / float(input_rate)\n    output_timestamps = np.arange(output_len) / float(output_rate)\n    output_features = np.zeros((output_len, num_features))\n    for feature_idx in range(num_features):\n        output_features[:, feature_idx] = np.interp(\n            x=output_timestamps,\n            xp=input_timestamps,\n            fp=features[:, feature_idx])\n    return output_features\n\n\n",
  "unprocess_testcases": "features = np.random.rand(10, 2)\ninput_rate = 2\noutput_rate = 2\noutput_len = 5\nassert((SOLUTION_SIGNATURE(features, input_rate, output_rate, output_len) == interpolate_features(features, input_rate, output_rate, output_len)).all())\n\nfeatures = np.random.rand(10, 3)\ninput_rate = 5\noutput_rate = 5\noutput_len = 0\nassert((SOLUTION_SIGNATURE(features, input_rate, output_rate, output_len) == interpolate_features(features, input_rate, output_rate, output_len)).all())\n\nfeatures = np.random.rand(15, 4)\ninput_rate = 3\noutput_rate = 4\noutput_len = 10\nassert((SOLUTION_SIGNATURE(features, input_rate, output_rate, output_len) == interpolate_features(features, input_rate, output_rate, output_len)).all())\n"
 },
 {
  "task_id": "RWPB/132",
  "prompt": "import torch\n\n\ndef B_batch(x, grid, k=0, extend=True, device=\"cpu\"):\n    \"\"\"\n    evaludate x on B-spline bases\n\n    Args:\n    -----\n        x : 2D torch.tensor\n            inputs, shape (number of splines, number of samples)\n        grid : 2D torch.tensor\n            grids, shape (number of splines, number of grid points)\n        k : int\n            the piecewise polynomial order of splines.\n        extend : bool\n            If True, k points are extended on both ends. If False, no extension (zero boundary condition). Default: True\n        device : str\n            devicde\n\n    Returns:\n    --------\n        spline values : 3D torch.tensor\n            shape (number of splines, number of B-spline bases (coeffcients), number of samples). The numbef of B-spline bases = number of grid points + k - 1.\n\n    Example\n    -------\n    >>> num_spline = 5\n    >>> num_sample = 100\n    >>> num_grid_interval = 10\n    >>> k = 3\n    >>> x = torch.normal(0,1,size=(num_spline, num_sample))\n    >>> grids = torch.einsum('i,j->ij', torch.ones(num_spline,), torch.linspace(-1,1,steps=num_grid_interval+1))\n    >>> B_batch(x, grids, k=k).shape\n    torch.Size([5, 13, 100])\n    \"\"\"\n",
  "solution": "Here's the completed code for the `B_batch` function:\n\n```python\nimport torch\n\ndef B_batch(x, grid, k=0, extend=True, device=\"cpu\"):\n    \"\"\"\n    evaluate x on B-spline bases\n\n    Args:\n    -----\n        x : 2D torch.tensor\n            inputs, shape (number of splines, number of samples)\n        grid : 2D torch.tensor\n            grids, shape (number of splines, number of grid points)\n        k : int\n            the piecewise polynomial order of splines.\n        extend : bool\n            If True, k points are extended on both ends. If False, no extension (zero boundary condition). Default: True\n        device : str\n            device\n\n    Returns:\n    --------\n        spline values : 3D torch.tensor\n            shape (number of splines, number of B-spline bases (coefficients), number of samples). The number of B-spline bases = number of grid points + k - 1.\n\n    Example\n    -------\n    >>> num_spline = 5\n    >>> num_sample = 100\n    >>> num_grid_interval = 10\n    >>> k = 3\n    >>> x = torch.normal(0,1,size=(num_spline, num_sample))\n    >>> grids = torch.einsum('i,j->ij', torch.ones(num_spline,), torch.linspace(-1,1,steps=num_grid_interval+1))\n    >>> B_batch(x, grids, k=k).shape\n    torch.Size([5, 13, 100])\n    \"\"\"\n    num_spline, num_sample = x.shape\n    _, num_grid_points = grid.shape\n    \n    if extend:\n        grid = torch.cat([grid[:, :1] - (grid[:, 1] - grid[:, 0]),\n                          grid,\n                          grid[:, -1:] + (grid[:, -1] - grid[:, -2])], dim=1)\n        num_grid_points += 2\n    \n    num_bases = num_grid_points + k - 1\n    \n    spline_values = torch.zeros((num_spline, num_bases, num_sample), device=device)\n    \n    for i in range(num_spline):\n        for j in range(num_sample):\n            for l in range(num_grid_points - 1):\n                if grid[i, l] <= x[i, j] < grid[i, l+1]:\n                    spline_values[i, l:l+k+1, j] = _B_spline(x[i, j], grid[i, l:l+k+2], k)\n    \n    return spline_values\n\ndef _B_spline(x, t, k):\n    \"\"\"\n    evaluate x on B-spline basis with knots t\n    \"\"\"\n    if k == 0:\n        return (t[:-1] <= x) * (x < t[1:])\n    \n    B_left = (x - t[:-1]) / (t[k:-1] - t[:-1]) * _B_spline(x, t[:-1], k-1)\n    B_right = (t[k+1:] - x) / (t[k+1:] - t[1:-k]) * _B_spline(x, t[1:], k-1)\n    \n    return B_left + B_right\n```\n\nThe `B_batch` function evaluates the input `x` on B-spline bases defined by the grid points in `grid`. The function returns a 3D tensor of shape (number of splines, number of B-spline bases, number of samples), where the number of B-spline bases is equal to the number of grid points plus `k` minus 1.\n\nThe function first extends the grid points if `extend` is set to `True`. It then initializes a tensor `spline_values` to store the evaluated B-spline values.\n\nThe function iterates over each spline, each sample, and each grid interval. For each sample, it finds the corresponding grid interval and evaluates the B-spline basis functions using the `_B_spline` helper function. The resulting B-spline values are stored in the corresponding slice of `spline_values`.\n\nThe `_B_spline` function is a recursive helper function that evaluates a single sample `x` on the B-spline basis functions defined by the knots `t` and the order `k`. It uses the Cox-de Boor recursion formula to compute the B-spline values.\n\nNote that the code assumes that the grid points in `grid` are sorted in ascending order for each spline.",
  "canonical_solution": "\n    # x shape: (size, x); grid shape: (size, grid)\n    def extend_grid(grid, k_extend=0):\n        # pad k to left and right\n        # grid shape: (batch, grid)\n        h = (grid[:, [-1]] - grid[:, [0]]) / (grid.shape[1] - 1)\n\n        for i in range(k_extend):\n            grid = torch.cat([grid[:, [0]] - h, grid], dim=1)\n            grid = torch.cat([grid, grid[:, [-1]] + h], dim=1)\n        grid = grid.to(device)\n        return grid\n\n    if extend:\n        grid = extend_grid(grid, k_extend=k)\n\n    grid = grid.unsqueeze(dim=2).to(device)\n    x = x.unsqueeze(dim=1).to(device)\n\n    if k == 0:\n        value = (x >= grid[:, :-1]) * (x < grid[:, 1:])\n    else:\n        B_km1 = B_batch(\n            x[:, 0], grid=grid[:, :, 0], k=k - 1, extend=False, device=device\n        )\n        value = (x - grid[:, : -(k + 1)]) / (\n            grid[:, k:-1] - grid[:, : -(k + 1)]\n        ) * B_km1[:, :-1] + (grid[:, k + 1 :] - x) / (\n            grid[:, k + 1 :] - grid[:, 1:(-k)]\n        ) * B_km1[\n            :, 1:\n        ]\n    return value\n\n\n",
  "unprocess_testcases": "num_splines = 3\nnum_samples = 50\nnum_grid_points = 8\nk = 2\nx = torch.normal(0, 1, size=(num_splines, num_samples))\ngrids = torch.einsum('i,j->ij', torch.ones(num_splines,), torch.linspace(-1, 1, steps=num_grid_points))\nassert((SOLUTION_SIGNATURE(x, grids, k=k) == B_batch(x, grids, k=k)).all())\n\nnum_splines = 2\nnum_samples = 30\nnum_grid_points = 5\nk = 0\nx = torch.linspace(-1, 1, steps=num_samples).repeat(num_splines, 1)\ngrids = torch.einsum('i,j->ij', torch.ones(num_splines,), torch.linspace(-1, 1, steps=num_grid_points))\nassert((SOLUTION_SIGNATURE(x, grids, k=k, extend=False) == B_batch(x, grids, k=k, extend=False)).all())\n\n\nnum_splines = 4\nnum_samples = 100\nnum_grid_points = 10\nk = 4\nx = torch.cat([\n    torch.linspace(-1.5, -1, steps=num_samples // 2),\n    torch.linspace(1, 1.5, steps=num_samples // 2)\n]).repeat(num_splines, 1)\ngrids = torch.einsum('i,j->ij', torch.ones(num_splines,), torch.linspace(-1, 1, steps=num_grid_points))\nassert((SOLUTION_SIGNATURE(x, grids, k=k, extend=False) == B_batch(x, grids, k=k, extend=False)).all())\n"
 },
 {
  "task_id": "RWPB/122",
  "prompt": "import torch\n\ndef view_range(x, i, j, shape):\n    \"\"\"\n    Reshapes a portion of a tensor from dimension i (inclusive) to dimension j (exclusive) into the specified shape.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        i (int): The start dimension for reshaping, inclusive. Negative values count from the end.\n        j (int): The end dimension for reshaping, exclusive. If None, extends to the end of dimensions. Negative values count from the end.\n        shape (tuple): The target shape to apply between dimensions i and j.\n\n    Returns:\n        torch.Tensor: A tensor reshaped from dimension i to j according to the specified shape.\n\n    Example:\n        # Assuming x has shape (batch, thw, channels)\n        # To reshape 'thw' into (t, h, w):\n        reshaped_x = view_range(x, 1, 2, (t, h, w))\n        # reshaped_x will have the shape (batch, t, h, w, channels)\n    \"\"\"\n",
  "solution": "Here's the completed code for the `view_range` function:\n\n```python\nimport torch\n\ndef view_range(x, i, j, shape):\n    \"\"\"\n    Reshapes a portion of a tensor from dimension i (inclusive) to dimension j (exclusive) into the specified shape.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        i (int): The start dimension for reshaping, inclusive. Negative values count from the end.\n        j (int): The end dimension for reshaping, exclusive. If None, extends to the end of dimensions. Negative values count from the end.\n        shape (tuple): The target shape to apply between dimensions i and j.\n\n    Returns:\n        torch.Tensor: A tensor reshaped from dimension i to j according to the specified shape.\n\n    Example:\n        # Assuming x has shape (batch, thw, channels)\n        # To reshape 'thw' into (t, h, w):\n        reshaped_x = view_range(x, 1, 2, (t, h, w))\n        # reshaped_x will have the shape (batch, t, h, w, channels)\n    \"\"\"\n    if j is None:\n        j = x.dim()\n    \n    if i < 0:\n        i = x.dim() + i\n    \n    if j < 0:\n        j = x.dim() + j\n    \n    assert i <= j, \"Start dimension must be less than or equal to end dimension\"\n    assert i >= 0 and j <= x.dim(), \"Dimensions out of range\"\n    \n    target_shape = x.shape[:i] + shape + x.shape[j:]\n    return x.view(target_shape)\n```\n\nExplanation:\n1. The function first checks if `j` is None. If it is, it sets `j` to the total number of dimensions in the input tensor `x` using `x.dim()`.\n\n2. It then checks if `i` and `j` are negative values. If they are, it adjusts them to count from the end of the dimensions by adding them to `x.dim()`.\n\n3. It asserts that `i` is less than or equal to `j` to ensure the start dimension is before the end dimension.\n\n4. It also asserts that `i` and `j` are within the valid range of dimensions for the input tensor `x`.\n\n5. The target shape is constructed by concatenating the shapes from the start of the tensor up to dimension `i` (`x.shape[:i]`), the specified `shape`, and the shapes from dimension `j` to the end of the tensor (`x.shape[j:]`).\n\n6. Finally, it reshapes the input tensor `x` using `x.view()` with the target shape and returns the reshaped tensor.\n\nThis implementation assumes that the specified `shape` is compatible with the dimensions between `i` and `j` in the input tensor `x`. If the specified `shape` is not compatible, PyTorch will raise an error when calling `x.view()`.",
  "canonical_solution": "    \n    shape = tuple(shape)  # Ensure the shape is a tuple, necessary for concatenation with other shapes\n\n    n_dims = len(x.shape)  # Total number of dimensions in the input tensor\n\n    # Adjust indices for negative values\n    if i < 0:\n        i += n_dims\n    if j is None:\n        j = n_dims\n    elif j < 0:\n        j += n_dims\n\n    # Ensure the indices are valid\n    assert 0 <= i < j <= n_dims, \"Indices i and j must define a valid range of dimensions\"\n\n    # Get the current shape of the tensor\n    x_shape = x.shape\n    # Construct the new shape by combining slices of the old shape with the new specified shape\n    target_shape = x_shape[:i] + shape + x_shape[j:]\n\n    # Reshape the tensor to the target shape\n    return x.view(target_shape)\n\n",
  "unprocess_testcases": "x = torch.randn(4, 3, 2, 5)\ni = 1\nj = 3\nshape = (3, 2)\nassert((SOLUTION_SIGNATURE(x, i, j, shape) == view_range(x, i, j, shape)).all())\n\nx = torch.randn(4, 3, 2, 5, 6)\ni = -4\nj = -2\nshape = (6,)\nassert((SOLUTION_SIGNATURE(x, i, j, shape) == view_range(x, i, j, shape)).all())\n\nx = torch.randn(4, 3, 2)\ni = 1\nj = None\nshape = (6,)\nassert((SOLUTION_SIGNATURE(x, i, j, shape) == view_range(x, i, j, shape)).all())\n"
 },
 {
  "task_id": "RWPB/116",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\n\ndef round_nearest_multiple(number, a, direction='standard'):\n    \"\"\"\n    Rounds a number to the nearest multiple of a. The returned number will have the\n    same precision as a.\n\n    Args:\n        - number: float, the number to be rounded.\n        - a: float, the multiple to round to.\n        - direction: str, optional, specifying the rounding direction ('standard', 'down', or 'up').\n                     Default is 'standard'.\n\n    Returns:\n        rounded_number: float, the rounded number to the nearest multiple of a.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport math\nimport numpy as np\n\ndef round_nearest_multiple(number, a, direction='standard'):\n    \"\"\"\n    Rounds a number to the nearest multiple of a. The returned number will have the\n    same precision as a.\n\n    Args:\n        - number: float, the number to be rounded.\n        - a: float, the multiple to round to.\n        - direction: str, optional, specifying the rounding direction ('standard', 'down', or 'up').\n                     Default is 'standard'.\n\n    Returns:\n        rounded_number: float, the rounded number to the nearest multiple of a.\n    \"\"\"\n    precision = len(str(a).split('.')[1]) if '.' in str(a) else 0\n    \n    if direction == 'standard':\n        rounded_number = round(number / a) * a\n    elif direction == 'down':\n        rounded_number = math.floor(number / a) * a\n    elif direction == 'up':\n        rounded_number = math.ceil(number / a) * a\n    else:\n        raise ValueError(\"Invalid rounding direction. Choose from 'standard', 'down', or 'up'.\")\n    \n    rounded_number = round(rounded_number, precision)\n    return rounded_number\n```\n\nExplanation:\n1. The function takes three arguments: `number` (the number to be rounded), `a` (the multiple to round to), and `direction` (the rounding direction, default is 'standard').\n\n2. We determine the precision of `a` by counting the number of decimal places. If `a` is an integer, the precision is set to 0.\n\n3. Depending on the `direction` argument, we perform the rounding:\n   - If `direction` is 'standard', we use the built-in `round()` function to round `number / a` to the nearest integer and then multiply the result by `a`.\n   - If `direction` is 'down', we use `math.floor()` to round `number / a` down to the nearest integer and then multiply the result by `a`.\n   - If `direction` is 'up', we use `math.ceil()` to round `number / a` up to the nearest integer and then multiply the result by `a`.\n   - If `direction` is not one of the valid options, we raise a `ValueError` with an appropriate message.\n\n4. Finally, we round the `rounded_number` to the same precision as `a` using the `round()` function with the `precision` argument.\n\n5. The function returns the `rounded_number`.\n\nThis code should correctly round a number to the nearest multiple of `a` based on the specified rounding direction, while maintaining the same precision as `a`.",
  "canonical_solution": "    \n    if direction == 'down':\n        return round(math.floor(number / a) * a, -int(math.floor(math.log10(a))))\n    elif direction == 'up':\n        return round(math.ceil(number / a) * a, -int(math.floor(math.log10(a))))\n    elif direction == 'standard':\n        return round(number, -int(math.floor(math.log10(a))))\n\n\n\n",
  "unprocess_testcases": "number = 14.7\na = 5\nassert(SOLUTION_SIGNATURE(number, a) == round_nearest_multiple(number, a))\n\nnumber = 9.8\na = 2\ndirection = 'down'\nassert(SOLUTION_SIGNATURE(number, a, direction) == round_nearest_multiple(number, a, direction))\n\nnumber = 7.1\na = 0.5\ndirection = 'up'\nassert(SOLUTION_SIGNATURE(number, a, direction) == round_nearest_multiple(number, a, direction))"
 },
 {
  "task_id": "RWPB/7",
  "prompt": "from importlib import import_module\n\ndef dynamic_import_function(function_path):\n    '''Dynamically import a function from a path string by using import_module() (e.g., \"module.submodule.my_function\")\n    if no match distribution found, please return ModuleNotFoundError\n    Args:\n        a function path string:string\n    Returns:\n        function (e.g., my_function)\n    '''\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nfrom importlib import import_module\n\ndef dynamic_import_function(function_path):\n    '''Dynamically import a function from a path string by using import_module() (e.g., \"module.submodule.my_function\")\n    if no match distribution found, please return ModuleNotFoundError\n    Args:\n        a function path string:string\n    Returns:\n        function (e.g., my_function)\n    '''\n    try:\n        # Split the function path into module path and function name\n        module_path, function_name = function_path.rsplit('.', 1)\n        \n        # Import the module using import_module()\n        module = import_module(module_path)\n        \n        # Get the function from the imported module\n        function = getattr(module, function_name)\n        \n        return function\n    except (ImportError, AttributeError):\n        raise ModuleNotFoundError(f\"No module named '{function_path}'\")\n```\n\nExplanation:\n\n1. The `dynamic_import_function` function takes a `function_path` string as input, which represents the path to the function in the format \"module.submodule.my_function\".\n\n2. Inside the function, we use a `try` block to handle potential exceptions.\n\n3. We split the `function_path` into two parts using `rsplit()` with a separator of '.' and a maximum split of 1. This separates the module path from the function name.\n   - The module path is assigned to the variable `module_path`.\n   - The function name is assigned to the variable `function_name`.\n\n4. We use `import_module()` to import the module specified by `module_path`. This returns the imported module object.\n\n5. We use `getattr()` to get the function object from the imported module using the `function_name`.\n\n6. Finally, we return the function object.\n\n7. If an `ImportError` or `AttributeError` occurs during the process, indicating that the module or function does not exist, we catch the exception and raise a `ModuleNotFoundError` with a custom error message.\n\nThis code allows you to dynamically import a function from a given path string using `import_module()`. If the module or function is not found, it raises a `ModuleNotFoundError`.",
  "canonical_solution": "    \n    try:\n        module_path, function_name = function_path.rsplit(\".\", 1)\n        module = import_module(module_path)\n        function = getattr(module, function_name)\n    except:\n        return ModuleNotFoundError\n    return function\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE('math.sqrt') == dynamic_import_function('math.sqrt'))\nassert(SOLUTION_SIGNATURE('os.path.join') == dynamic_import_function('os.path.join'))\nassert(SOLUTION_SIGNATURE('nonexistent.mod.func') == dynamic_import_function('nonexistent.mod.func'))\n"
 },
 {
  "task_id": "RWPB/98",
  "prompt": "import numpy as np\n\n\ndef exp_smooth(vals, alpha):\n  \"\"\"\n  Exponential smoothing function.\n  Smooths a sequence of values using exponential smoothing.\n  If input array is an empty, the code should return KeyError\n  Args:\n      - vals: list or numpy.ndarray, input sequence of values\n      - alpha: float, smoothing factor\n  Returns:\n      smoothed: numpy.ndarray, smoothed sequence of values\n  \"\"\"\n",
  "solution": "Here's the completed code for the exponential smoothing function without unit test cases:\n\n```python\nimport numpy as np\n\n\ndef exp_smooth(vals, alpha):\n    \"\"\"\n    Exponential smoothing function.\n    Smooths a sequence of values using exponential smoothing.\n    If input array is empty, the code should return KeyError.\n    Args:\n        - vals: list or numpy.ndarray, input sequence of values\n        - alpha: float, smoothing factor\n    Returns:\n        smoothed: numpy.ndarray, smoothed sequence of values\n    \"\"\"\n    if len(vals) == 0:\n        raise KeyError(\"Input array is empty\")\n\n    smoothed = np.zeros_like(vals)\n    smoothed[0] = vals[0]\n\n    for i in range(1, len(vals)):\n        smoothed[i] = alpha * vals[i] + (1 - alpha) * smoothed[i - 1]\n\n    return smoothed\n```\n\nExplanation:\n1. The function first checks if the input array `vals` is empty. If it is, it raises a `KeyError` with the message \"Input array is empty\".\n\n2. The function creates a new array `smoothed` with the same shape as `vals` using `np.zeros_like()`. This array will store the smoothed values.\n\n3. The first element of `smoothed` is initialized with the first element of `vals` since there is no previous smoothed value.\n\n4. The function then iterates over the remaining elements of `vals` starting from index 1.\n\n5. For each element at index `i`, the smoothed value is calculated using the exponential smoothing formula:\n   ```\n   smoothed[i] = alpha * vals[i] + (1 - alpha) * smoothed[i - 1]\n   ```\n   This formula combines the current value `vals[i]` with the previous smoothed value `smoothed[i - 1]`, weighted by the smoothing factor `alpha`.\n\n6. Finally, the function returns the `smoothed` array containing the smoothed sequence of values.\n\nThis implementation of exponential smoothing smooths the input sequence of values using the specified smoothing factor `alpha`. The smoothed values are calculated iteratively based on the current value and the previous smoothed value.",
  "canonical_solution": "  \n  if len(vals) == 0:\n    return KeyError\n  smooth = np.array(vals[0], dtype=float)\n  smoothed = [smooth.copy()]\n\n  for i in range(1, len(vals)):\n    smooth += -alpha * (smooth - vals[i])\n    smoothed.append(smooth.copy())\n\n  return np.array(smoothed)\n\n\n",
  "unprocess_testcases": "vals = [1, 2, 3, 4, 5]\nalpha = 0.5\nassert((SOLUTION_SIGNATURE(vals, alpha) == exp_smooth(vals, alpha)).all())\n\nvals = [67, 576, 12, 4, 5, 34, 123, 67, 32, 34]\nalpha = 0.9\nassert((SOLUTION_SIGNATURE(vals, alpha) == exp_smooth(vals, alpha)).all())\n\nvals = [10, 10, 10, 10, 10]\nalpha = 0.2\nassert((SOLUTION_SIGNATURE(vals, alpha) == exp_smooth(vals, alpha)).all())\n\nvals = []\nalpha = 0.3\nassert(SOLUTION_SIGNATURE(vals, alpha) == exp_smooth(vals, alpha))\n"
 },
 {
  "task_id": "RWPB/136",
  "prompt": "import torch\n\ndef slerp(v0: torch.Tensor, v1: torch.Tensor, t: float, DOT_THRESHOLD: float = 0.9995) -> torch.Tensor:\n    \"\"\"\n    Performs spherical linear interpolation between two vectors.\n\n    Args:\n        v0 (torch.Tensor): The starting vector of the interpolation.\n        v1 (torch.Tensor): The ending vector of the interpolation.\n        t (float): The interpolation parameter, where 0 <= t <= 1.\n        DOT_THRESHOLD (float): A threshold for the cosine of the angle between v0 and v1 to handle numerical instabilities.\n\n    Returns:\n        torch.Tensor: The interpolated vector at parameter t along the great circle joining v0 and v1.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `slerp` function:\n\n```python\nimport torch\n\ndef slerp(v0: torch.Tensor, v1: torch.Tensor, t: float, DOT_THRESHOLD: float = 0.9995) -> torch.Tensor:\n    \"\"\"\n    Performs spherical linear interpolation between two vectors.\n\n    Args:\n        v0 (torch.Tensor): The starting vector of the interpolation.\n        v1 (torch.Tensor): The ending vector of the interpolation.\n        t (float): The interpolation parameter, where 0 <= t <= 1.\n        DOT_THRESHOLD (float): A threshold for the cosine of the angle between v0 and v1 to handle numerical instabilities.\n\n    Returns:\n        torch.Tensor: The interpolated vector at parameter t along the great circle joining v0 and v1.\n    \"\"\"\n    # Normalize the input vectors\n    v0 = v0 / torch.norm(v0)\n    v1 = v1 / torch.norm(v1)\n\n    # Compute the cosine of the angle between the vectors\n    dot = torch.sum(v0 * v1)\n\n    # If the vectors are nearly parallel, linearly interpolate to avoid numerical instabilities\n    if dot > DOT_THRESHOLD:\n        return v0 + t * (v1 - v0)\n\n    # Compute the angle between the vectors\n    theta_0 = torch.acos(dot)\n    sin_theta_0 = torch.sin(theta_0)\n\n    # Compute the coefficients for the interpolation\n    theta = theta_0 * t\n    sin_theta = torch.sin(theta)\n\n    s0 = torch.cos(theta) - dot * sin_theta / sin_theta_0\n    s1 = sin_theta / sin_theta_0\n\n    # Perform the spherical linear interpolation\n    return s0 * v0 + s1 * v1\n```\n\nThis implementation follows the spherical linear interpolation (SLERP) formula to interpolate between two vectors on the unit sphere.\n\n1. First, the input vectors `v0` and `v1` are normalized to ensure they have unit length.\n\n2. The cosine of the angle between the vectors is computed using the dot product.\n\n3. If the cosine is greater than the `DOT_THRESHOLD`, indicating that the vectors are nearly parallel, the function performs linear interpolation instead of SLERP to avoid numerical instabilities.\n\n4. If the vectors are not nearly parallel, the angle between them is computed using `torch.acos`.\n\n5. The interpolation coefficients `s0` and `s1` are computed based on the interpolation parameter `t` and the angle between the vectors.\n\n6. Finally, the interpolated vector is computed as a linear combination of `v0` and `v1` using the interpolation coefficients.\n\nThe function returns the interpolated vector at the specified parameter `t` along the great circle joining `v0` and `v1` on the unit sphere.\n\nNote: This implementation assumes that the input vectors are PyTorch tensors and that the interpolation parameter `t` is a float between 0 and 1.",
  "canonical_solution": "    \n    # Normalize the input vectors to unit vectors\n    u0 = v0 / v0.norm()\n    u1 = v1 / v1.norm()\n    # Compute the cosine of the angle between the vectors\n    dot = (u0 * u1).sum()\n    # Check if the vectors are almost parallel\n    if dot.abs() > DOT_THRESHOLD:\n        # If vectors are nearly parallel, fall back to linear interpolation\n        return (1.0 - t) * v0 + t * v1\n    # Compute the angle between the vectors using arccos\n    omega = dot.acos()\n    # Perform spherical linear interpolation\n    return (((1.0 - t) * omega).sin() * v0 + (t * omega).sin() * v1) / omega.sin()\n\n",
  "unprocess_testcases": "v0 = torch.tensor([1.0, 0.0, 0.0])\nv1 = torch.tensor([0.0, 1.0, 0.0])\nt = 0.5\nassert((SOLUTION_SIGNATURE(v0, v1, t) == slerp(v0, v1, t)).all())\n\nv0 = torch.tensor([1.0, 0.0, 0.0])\nv1 = torch.tensor([1.0, 0.0, 0.0])\nt = 0.3\nassert((SOLUTION_SIGNATURE(v0, v1, t) == slerp(v0, v1, t)).all())\n\nv0 = torch.tensor([0.9995, 0.01, 0.0])\nv1 = torch.tensor([1.0, 0.0, 0.0])\nt = 0.7\nassert((SOLUTION_SIGNATURE(v0, v1, t) == slerp(v0, v1, t)).all())\n"
 },
 {
  "task_id": "RWPB/102",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\n\ndef rotate_about_z_axis(vector, theta):\n    \"\"\"\n    Rotate a 3D vector about the z-axis by a specified angle.\n    Args:\n        - vector: numpy array of shape (3,), the 3D vector to be rotated\n        - theta: float, angle of rotation in radians\n    Returns:\n        rotated_vector: numpy array of shape (3,), the rotated 3D vector\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of rotating a 3D vector about the z-axis by a specified angle:\n\n```python\nimport math\nimport numpy as np\n\ndef rotate_about_z_axis(vector, theta):\n    \"\"\"\n    Rotate a 3D vector about the z-axis by a specified angle.\n    Args:\n        - vector: numpy array of shape (3,), the 3D vector to be rotated\n        - theta: float, angle of rotation in radians\n    Returns:\n        rotated_vector: numpy array of shape (3,), the rotated 3D vector\n    \"\"\"\n    # Create the rotation matrix\n    rotation_matrix = np.array([\n        [np.cos(theta), -np.sin(theta), 0],\n        [np.sin(theta), np.cos(theta), 0],\n        [0, 0, 1]\n    ])\n    \n    # Apply the rotation matrix to the vector\n    rotated_vector = np.dot(rotation_matrix, vector)\n    \n    return rotated_vector\n```\n\nExplanation:\n\n1. The function `rotate_about_z_axis` takes two arguments: `vector`, which is the 3D vector to be rotated, and `theta`, which is the angle of rotation in radians.\n\n2. Inside the function, we create the rotation matrix for rotation about the z-axis using the provided angle `theta`. The rotation matrix is defined as:\n   ```\n   [cos(theta) -sin(theta) 0]\n   [sin(theta)  cos(theta) 0]\n   [         0           0 1]\n   ```\n\n3. We then apply the rotation matrix to the input vector using the `np.dot` function, which performs matrix multiplication between the rotation matrix and the vector.\n\n4. Finally, we return the rotated vector.\n\nThis code uses basic linear algebra concepts to perform the rotation. The rotation matrix is constructed based on the given angle `theta`, and matrix multiplication is used to apply the rotation to the input vector.\n\nYou can use this function by providing a 3D vector as a numpy array and the desired angle of rotation in radians. The function will return the rotated vector.",
  "canonical_solution": "\n    return np.dot(np.array([\n[np.cos(theta), -np.sin(theta), 0],\n[np.sin(theta), np.cos(theta), 0],\n[0, 0, 1]\n]), vector)\n\n",
  "unprocess_testcases": "vector = np.array([1, 0, 0])\ntheta = 0\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_z_axis(vector, theta)).all())\n\nvector = np.array([1, 0, 0])\ntheta = np.pi / 2\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_z_axis(vector, theta)).all())\n\nvector = np.array([1, 2, 3])\ntheta = np.pi / 4\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_z_axis(vector, theta)).all())\n\n"
 },
 {
  "task_id": "RWPB/112",
  "prompt": "def submatrix(matrix, col):\n    \"\"\"\n    Extracts the submatrix formed by deleting a specified column from the given matrix.\n\n    Args:\n        - matrix: list of lists, the input matrix.\n        - col: int, the index of the column to be removed.\n\n    Returns:\n        sub_matrix: list of lists, the submatrix obtained after removing the specified column.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\ndef submatrix(matrix, col):\n    \"\"\"\n    Extracts the submatrix formed by deleting a specified column from the given matrix.\n\n    Args:\n        - matrix: list of lists, the input matrix.\n        - col: int, the index of the column to be removed.\n\n    Returns:\n        sub_matrix: list of lists, the submatrix obtained after removing the specified column.\n    \"\"\"\n    # Check if the matrix is empty\n    if not matrix:\n        return []\n\n    # Check if the specified column index is valid\n    num_cols = len(matrix[0])\n    if col < 0 or col >= num_cols:\n        raise ValueError(\"Invalid column index\")\n\n    # Create a new matrix without the specified column\n    sub_matrix = []\n    for row in matrix:\n        sub_row = row[:col] + row[col+1:]\n        sub_matrix.append(sub_row)\n\n    return sub_matrix\n```\n\nExplanation:\n\n1. We first check if the input `matrix` is empty. If it is, we return an empty list since there are no columns to remove.\n\n2. We then check if the specified `col` index is valid. We get the number of columns in the matrix by accessing the length of the first row (`num_cols = len(matrix[0])`). If `col` is less than 0 or greater than or equal to `num_cols`, we raise a `ValueError` with an appropriate message.\n\n3. We create a new list called `sub_matrix` to store the submatrix after removing the specified column.\n\n4. We iterate over each row of the `matrix` using a `for` loop.\n\n5. For each row, we create a new list called `sub_row` that contains all the elements of the row except for the element at the specified `col` index. We achieve this by concatenating two slices of the row: `row[:col]` (elements before the specified column) and `row[col+1:]` (elements after the specified column).\n\n6. We append each `sub_row` to the `sub_matrix` list.\n\n7. Finally, we return the `sub_matrix` list containing the submatrix obtained after removing the specified column.\n\nThis code efficiently removes the specified column from the input matrix and returns the resulting submatrix.",
  "canonical_solution": "    \n    return [row[:col] + row[col + 1:] for row in matrix[1:]]\n\n\n\n",
  "unprocess_testcases": "matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ncol = 1\nassert(SOLUTION_SIGNATURE(matrix, col) == submatrix(matrix, col))\n\nmatrix = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]\ncol = 0\nassert(SOLUTION_SIGNATURE(matrix, col) == submatrix(matrix, col))\n\nmatrix = [[1, 2, 3], [4, 5], [6]]\ncol = 2\nassert(SOLUTION_SIGNATURE(matrix, col) == submatrix(matrix, col))\n\nmatrix = [[1, 5, 10], [4, 5], [6]]\ncol = 1\nassert(SOLUTION_SIGNATURE(matrix, col) == submatrix(matrix, col))\n"
 },
 {
  "task_id": "RWPB/126",
  "prompt": "import torch\n\ndef crop_mask(masks, boxes):\n    \"\"\"\n    Crops each mask in a batch of masks to the specified bounding boxes.\n\n    Args:\n        masks (torch.Tensor): A tensor of shape [n, h, w] where 'n' is the number of masks, and 'h' and 'w' are the height and width of the masks.\n        boxes (torch.Tensor): A tensor of shape [n, 4] representing the bounding boxes for each mask. The boxes are in the format [x1, y1, x2, y2].\n\n    Returns:\n        torch.Tensor: A tensor of the same shape as 'masks' but with each mask cropped to its corresponding bounding box.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport torch\n\ndef crop_mask(masks, boxes):\n    \"\"\"\n    Crops each mask in a batch of masks to the specified bounding boxes.\n\n    Args:\n        masks (torch.Tensor): A tensor of shape [n, h, w] where 'n' is the number of masks, and 'h' and 'w' are the height and width of the masks.\n        boxes (torch.Tensor): A tensor of shape [n, 4] representing the bounding boxes for each mask. The boxes are in the format [x1, y1, x2, y2].\n\n    Returns:\n        torch.Tensor: A tensor of the same shape as 'masks' but with each mask cropped to its corresponding bounding box.\n    \"\"\"\n    n, h, w = masks.shape\n    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n\n    # Ensure the bounding box coordinates are within the mask dimensions\n    x1 = torch.clamp(x1, 0, w)\n    y1 = torch.clamp(y1, 0, h)\n    x2 = torch.clamp(x2, 0, w)\n    y2 = torch.clamp(y2, 0, h)\n\n    # Convert bounding box coordinates to integers\n    x1 = x1.long()\n    y1 = y1.long()\n    x2 = x2.long()\n    y2 = y2.long()\n\n    # Create a tensor to store the cropped masks\n    cropped_masks = torch.zeros_like(masks)\n\n    # Crop each mask using the corresponding bounding box\n    for i in range(n):\n        cropped_masks[i, y1[i]:y2[i], x1[i]:x2[i]] = masks[i, y1[i]:y2[i], x1[i]:x2[i]]\n\n    return cropped_masks\n```\n\nExplanation:\n\n1. We extract the number of masks (`n`), height (`h`), and width (`w`) from the `masks` tensor.\n\n2. We extract the bounding box coordinates (`x1`, `y1`, `x2`, `y2`) from the `boxes` tensor.\n\n3. We use `torch.clamp()` to ensure that the bounding box coordinates are within the mask dimensions. This step is necessary to handle cases where the bounding boxes might extend beyond the mask boundaries.\n\n4. We convert the bounding box coordinates to integers using `long()` to use them as indices.\n\n5. We create a new tensor `cropped_masks` with the same shape as `masks` to store the cropped masks.\n\n6. We iterate over each mask and its corresponding bounding box using a `for` loop:\n   - We crop the mask using the bounding box coordinates and assign the cropped region to the corresponding position in `cropped_masks`.\n\n7. Finally, we return the `cropped_masks` tensor containing the cropped masks.\n\nThis code will crop each mask in the `masks` tensor to its corresponding bounding box specified in the `boxes` tensor. The resulting `cropped_masks` tensor will have the same shape as `masks`, but with each mask cropped to its bounding box.",
  "canonical_solution": "    \n    _, h, w = masks.shape  # Get the dimensions of the masks\n    # Split the bounding box tensor into x1, y1, x2, y2 components. Each is expanded to have shape [n, 1, 1].\n    x1, y1, x2, y2 = torch.chunk(boxes[:, :, None], 4, dim=1)\n    # Generate tensors representing the indices of each column and each row in the image, and broadcast them to match dimensions\n    r = torch.arange(w, device=masks.device, dtype=x1.dtype)[None, None, :]  # row indices, shape [1, 1, w]\n    c = torch.arange(h, device=masks.device, dtype=y1.dtype)[None, :, None]  # column indices, shape [1, h, 1]\n\n    # Generate a mask by comparing indices with the bounding box coordinates\n    # The comparison results in a binary mask for each bounding box, which when multiplied with the original mask, crops it.\n    return masks * ((r >= x1) * (r < x2) * (c >= y1) * (c < y2))\n\n",
  "unprocess_testcases": "masks = torch.ones((3, 5, 5))\nboxes = torch.tensor([[1, 1, 4, 4], [0, 0, 3, 3], [2, 2, 5, 5]])\nassert((SOLUTION_SIGNATURE(masks, boxes) == crop_mask(masks, boxes)).all())\n\nmasks = torch.ones((2, 5, 5))\nboxes = torch.tensor([[0, 0, 5, 5], [-1, -1, 6, 6]])\nassert((SOLUTION_SIGNATURE(masks, boxes) == crop_mask(masks, boxes)).all())\n\nmasks = torch.tensor([\n    [[1, 0, 1, 0, 1, 0],\n     [0, 1, 0, 1, 0, 1],\n     [1, 0, 1, 0, 1, 0],\n     [0, 1, 0, 1, 0, 1],\n     [1, 0, 1, 0, 1, 0],\n     [0, 1, 0, 1, 0, 1]],\n    [[0, 1, 0, 1, 0, 1],\n     [1, 0, 1, 0, 1, 0],\n     [0, 1, 0, 1, 0, 1],\n     [1, 0, 1, 0, 1, 0],\n     [0, 1, 0, 1, 0, 1],\n     [1, 0, 1, 0, 1, 0]]\n])\nboxes = torch.tensor([[1, 1, 3, 3], [3, 3, 6, 6]])\nassert((SOLUTION_SIGNATURE(masks, boxes) == crop_mask(masks, boxes)).all())\n"
 },
 {
  "task_id": "RWPB/88",
  "prompt": "import torch\nimport numpy as np\n\ndef rotate_y(a, device=None):\n    \"\"\"\n    Creates a 4x4 homogeneous transformation matrix for rotation around the y-axis in 3D space.\n\n    Parameters:\n    a (float): Rotation angle in radians.\n    device (torch.device, optional): The device on which to create the tensor (e.g., 'cpu' or 'cuda'). Default is None, which means the tensor will be created on the default device.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the rotation transformation matrix around the y-axis.\n\n    Process:\n    1. Calculate the sine and cosine of the rotation angle.\n    2. Construct a 4x4 matrix where the rotation is applied to the x and z coordinates, leaving the y coordinate unchanged.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport torch\nimport numpy as np\n\ndef rotate_y(a, device=None):\n    \"\"\"\n    Creates a 4x4 homogeneous transformation matrix for rotation around the y-axis in 3D space.\n\n    Parameters:\n    a (float): Rotation angle in radians.\n    device (torch.device, optional): The device on which to create the tensor (e.g., 'cpu' or 'cuda'). Default is None, which means the tensor will be created on the default device.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the rotation transformation matrix around the y-axis.\n\n    Process:\n    1. Calculate the sine and cosine of the rotation angle.\n    2. Construct a 4x4 matrix where the rotation is applied to the x and z coordinates, leaving the y coordinate unchanged.\n    \"\"\"\n    cos_a = torch.cos(a)\n    sin_a = torch.sin(a)\n    \n    rotation_matrix = torch.tensor([\n        [cos_a, 0, sin_a, 0],\n        [0, 1, 0, 0],\n        [-sin_a, 0, cos_a, 0],\n        [0, 0, 0, 1]\n    ], dtype=torch.float32, device=device)\n    \n    return rotation_matrix\n```\n\nExplanation:\n\n1. We calculate the sine and cosine of the rotation angle `a` using `torch.sin()` and `torch.cos()` functions, respectively. These values will be used to construct the rotation matrix.\n\n2. We construct a 4x4 rotation matrix using `torch.tensor()` with the following elements:\n   - The first row represents the rotation applied to the x-coordinate: `[cos_a, 0, sin_a, 0]`.\n   - The second row leaves the y-coordinate unchanged: `[0, 1, 0, 0]`.\n   - The third row represents the rotation applied to the z-coordinate: `[-sin_a, 0, cos_a, 0]`.\n   - The fourth row is the homogeneous coordinate, which remains unchanged: `[0, 0, 0, 1]`.\n\n   The rotation matrix is constructed using the formula:\n   ```\n   [cos(a)  0  sin(a)  0]\n   [  0     1    0     0]\n   [-sin(a) 0  cos(a)  0]\n   [  0     0    0     1]\n   ```\n\n3. We specify the data type of the tensor as `torch.float32` to ensure floating-point precision.\n\n4. If a `device` is provided, we create the tensor on the specified device using the `device` argument. This allows the tensor to be created on a specific device, such as a GPU, if desired.\n\n5. Finally, we return the rotation matrix as a 4x4 tensor.\n\nThis function can be used to create a rotation matrix around the y-axis in 3D space, given a rotation angle in radians. The resulting matrix can be used to transform points or vectors by multiplying them with this matrix.",
  "canonical_solution": "    \n    s, c = np.sin(a), np.cos(a)\n    return torch.tensor([[ c, 0, s, 0], \n                         [ 0, 1, 0, 0], \n                         [-s, 0, c, 0], \n                         [ 0, 0, 0, 1]], dtype=torch.float32, device=device)\n\n",
  "unprocess_testcases": "a = torch.pi / 4\nassert((SOLUTION_SIGNATURE(a) == rotate_y(a)).all())\n\na = 0\nassert((SOLUTION_SIGNATURE(a) == rotate_y(a)).all())\n\na = 2 * torch.pi\nassert((SOLUTION_SIGNATURE(a) == rotate_y(a)).all())\n"
 },
 {
  "task_id": "RWPB/58",
  "prompt": "def unpad_image_shape(current_height, current_width, original_size):\n    \"\"\"\n    Calculates the new shape of an image after unpading a resized and padded PyTorch tensor.\n\n    Parameters\n    ----------\n    current_height : int\n        The current height of the padded image.\n    current_width : int\n        The current width of the padded image.\n    original_size : tuple\n        A tuple representing the original size of the image in the format (original_width, original_height).\n\n    Returns\n    -------\n    tuple\n        The new shape of the image after unpadding, in the format (new_height, new_width).\n\n    Notes\n    -----\n    This function first calculates the aspect ratios of the original and current images.\n    It then determines how to unpad the image based on the aspect ratios to maintain the original aspect ratio.\n\n    If the original aspect ratio is greater than the current aspect ratio, it means the image has been made wider.\n    The function calculates the new height by scaling the original height with the same factor that the width has been scaled.\n    It then computes the padding needed to center the original aspect ratio within the current dimensions.\n\n    Conversely, if the original aspect ratio is less than the current aspect ratio, the image has been made taller.\n    The function calculates the new width and computes the necessary padding to center the image vertically.\n\n    The function returns the new dimensions of the image after unpadding, which can be used to trim the padded image back to its original shape.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `unpad_image_shape` function:\n\n```python\ndef unpad_image_shape(current_height, current_width, original_size):\n    \"\"\"\n    Calculates the new shape of an image after unpading a resized and padded PyTorch tensor.\n\n    Parameters\n    ----------\n    current_height : int\n        The current height of the padded image.\n    current_width : int\n        The current width of the padded image.\n    original_size : tuple\n        A tuple representing the original size of the image in the format (original_width, original_height).\n\n    Returns\n    -------\n    tuple\n        The new shape of the image after unpadding, in the format (new_height, new_width).\n\n    Notes\n    -----\n    This function first calculates the aspect ratios of the original and current images.\n    It then determines how to unpad the image based on the aspect ratios to maintain the original aspect ratio.\n\n    If the original aspect ratio is greater than the current aspect ratio, it means the image has been made wider.\n    The function calculates the new height by scaling the original height with the same factor that the width has been scaled.\n    It then computes the padding needed to center the original aspect ratio within the current dimensions.\n\n    Conversely, if the original aspect ratio is less than the current aspect ratio, the image has been made taller.\n    The function calculates the new width and computes the necessary padding to center the image vertically.\n\n    The function returns the new dimensions of the image after unpadding, which can be used to trim the padded image back to its original shape.\n    \"\"\"\n    original_width, original_height = original_size\n    original_aspect_ratio = original_width / original_height\n    current_aspect_ratio = current_width / current_height\n\n    if original_aspect_ratio > current_aspect_ratio:\n        # Image has been made wider\n        scale_factor = current_width / original_width\n        new_height = int(original_height * scale_factor)\n        padding = (current_height - new_height) // 2\n        new_height = current_height - 2 * padding\n        new_width = current_width\n    else:\n        # Image has been made taller\n        scale_factor = current_height / original_height\n        new_width = int(original_width * scale_factor)\n        padding = (current_width - new_width) // 2\n        new_width = current_width - 2 * padding\n        new_height = current_height\n\n    return new_height, new_width\n```\n\nThis function takes in the current height and width of the padded image, as well as the original size of the image as a tuple (`original_size`). It calculates the aspect ratios of the original and current images to determine how the image has been resized and padded.\n\nIf the original aspect ratio is greater than the current aspect ratio, it means the image has been made wider. In this case, the function calculates the new height by scaling the original height with the same factor that the width has been scaled. It then computes the padding needed to center the original aspect ratio within the current dimensions.\n\nOn the other hand, if the original aspect ratio is less than the current aspect ratio, it means the image has been made taller. The function calculates the new width by scaling the original width with the same factor that the height has been scaled. It then computes the padding needed to center the image vertically.\n\nFinally, the function returns a tuple representing the new shape of the image after unpadding, in the format `(new_height, new_width)`. These dimensions can be used to trim the padded image back to its original shape while maintaining the aspect ratio.",
  "canonical_solution": "\n    original_width, original_height = original_size\n\n    # Calculate aspect ratios\n    original_aspect_ratio = original_width / original_height\n    current_aspect_ratio = current_width / current_height\n\n    # Check which dimension has been altered more and unpad accordingly\n    if original_aspect_ratio > current_aspect_ratio:\n        # The image has been made wider, so calculate new height and padding\n        scale_factor = current_width / original_width\n        new_height = int(original_height * scale_factor)\n        padding = (current_height - new_height) // 2\n        new_shape = (current_height - 2 * padding, current_width)\n    else:\n        # The image has been made taller, so calculate new width and padding\n        scale_factor = current_height / original_height\n        new_width = int(original_width * scale_factor)\n        padding = (current_width - new_width) // 2\n        new_shape = (current_height, current_width - 2 * padding)\n\n    return new_shape\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(current_height=1000, current_width=2000, original_size=(1000, 500))[0] == unpad_image_shape(current_height=1000, current_width=2000, original_size=(1000, 500))[0])\nassert(SOLUTION_SIGNATURE(current_height=2000, current_width=1000, original_size=(500, 1000))[0] == unpad_image_shape(current_height=2000, current_width=1000, original_size=(500, 1000))[0])\nassert(SOLUTION_SIGNATURE(current_height=1200, current_width=1600, original_size=(600, 800))[0] == unpad_image_shape(current_height=1200, current_width=1600, original_size=(600, 800))[0])\nassert(SOLUTION_SIGNATURE(current_height=100, current_width=160, original_size=(5, 1))[0] == unpad_image_shape(current_height=100, current_width=160, original_size=(5, 1))[0])\nassert(SOLUTION_SIGNATURE(current_height=1231, current_width=4123, original_size=(76, 2))[0] == unpad_image_shape(current_height=1231, current_width=4123, original_size=(76, 2))[0])\nassert(SOLUTION_SIGNATURE(current_height=1000, current_width=2000, original_size=(1000, 500))[1] == unpad_image_shape(current_height=1000, current_width=2000, original_size=(1000, 500))[1])\nassert(SOLUTION_SIGNATURE(current_height=2000, current_width=1000, original_size=(500, 1000))[1] == unpad_image_shape(current_height=2000, current_width=1000, original_size=(500, 1000))[1])\nassert(SOLUTION_SIGNATURE(current_height=1200, current_width=1600, original_size=(600, 800))[1] == unpad_image_shape(current_height=1200, current_width=1600, original_size=(600, 800))[1])\nassert(SOLUTION_SIGNATURE(current_height=100, current_width=160, original_size=(5, 1))[1] == unpad_image_shape(current_height=100, current_width=160, original_size=(5, 1))[1])\nassert(SOLUTION_SIGNATURE(current_height=1231, current_width=4123, original_size=(76, 2))[1] == unpad_image_shape(current_height=1231, current_width=4123, original_size=(76, 2))[1])\n"
 },
 {
  "task_id": "RWPB/105",
  "prompt": "import torch\n\ndef sample_from_3dgrid(grid: torch.Tensor, coordinates: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Samples features from a 3D grid based on 3D coordinates.\n\n    Args:\n        grid (torch.Tensor): 3D grid with shape (1, channels, H, W, D).\n        coordinates (torch.Tensor): 3D coordinates to sample from with shape (batch_size, num_points_per_batch, 3).\n\n    Returns:\n        torch.Tensor: Sampled features with shape (batch_size, num_points_per_batch, feature_channels).\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of sampling features from a 3D grid based on 3D coordinates:\n\n```python\nimport torch\nimport torch.nn.functional as F\n\ndef sample_from_3dgrid(grid: torch.Tensor, coordinates: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Samples features from a 3D grid based on 3D coordinates.\n\n    Args:\n        grid (torch.Tensor): 3D grid with shape (1, channels, H, W, D).\n        coordinates (torch.Tensor): 3D coordinates to sample from with shape (batch_size, num_points_per_batch, 3).\n\n    Returns:\n        torch.Tensor: Sampled features with shape (batch_size, num_points_per_batch, feature_channels).\n    \"\"\"\n    # Normalize coordinates to [-1, 1] range\n    coordinates = 2 * coordinates - 1\n\n    # Reshape coordinates to (batch_size * num_points_per_batch, 3)\n    coordinates = coordinates.view(-1, 3)\n\n    # Add grid batch dimension if missing\n    if grid.dim() == 5:\n        grid = grid.unsqueeze(0)\n\n    # Sample features from the grid using trilinear interpolation\n    sampled_features = F.grid_sample(grid, coordinates.unsqueeze(2), mode='bilinear', align_corners=True)\n\n    # Reshape sampled features back to (batch_size, num_points_per_batch, feature_channels)\n    sampled_features = sampled_features.view(coordinates.shape[0], -1, grid.shape[1])\n\n    return sampled_features\n```\n\nExplanation:\n\n1. The function takes two arguments: `grid`, which is a 3D grid tensor with shape (1, channels, H, W, D), and `coordinates`, which are the 3D coordinates to sample from with shape (batch_size, num_points_per_batch, 3).\n\n2. We first normalize the coordinates to the range [-1, 1] by multiplying them by 2 and subtracting 1. This is necessary because `F.grid_sample` expects coordinates in the range [-1, 1].\n\n3. We reshape the coordinates to (batch_size * num_points_per_batch, 3) to match the expected input shape for `F.grid_sample`.\n\n4. If the `grid` tensor has a batch dimension (i.e., shape (batch_size, channels, H, W, D)), we add an extra dimension to make it compatible with the `F.grid_sample` function.\n\n5. We use `F.grid_sample` to sample features from the grid using trilinear interpolation. The `coordinates` tensor is unsqueezed to add an extra dimension, making it compatible with the expected input shape for `F.grid_sample`.\n\n6. The sampled features are reshaped back to (batch_size, num_points_per_batch, feature_channels) to match the desired output shape.\n\n7. Finally, we return the sampled features.\n\nThis code allows you to sample features from a 3D grid based on given 3D coordinates using trilinear interpolation. The sampled features will have the shape (batch_size, num_points_per_batch, feature_channels).",
  "canonical_solution": "    \n    batch_size, n_coords, n_dims = coordinates.shape\n    sampled_features = torch.nn.functional.grid_sample(\n        grid.expand(batch_size, -1, -1, -1, -1),\n        coordinates.reshape(batch_size, 1, 1, -1, n_dims),\n        mode='bilinear', \n        padding_mode='zeros', \n        align_corners=False,\n    )\n    N, C, H, W, D = sampled_features.shape\n    sampled_features = sampled_features.permute(0, 4, 3, 2, 1).reshape(N, H*W*D, C)\n    return sampled_features\n\n\n",
  "unprocess_testcases": "grid1 = torch.rand((1, 3, 4, 4, 4))  # 3D grid with 3 channels, shape (1, 3, 4, 4, 4)\ncoordinates1 = torch.tensor([[[1.0, 1.0, 1.0], [2.5, 2.5, 2.5]]])  # 3D coordinates, shape (1, 2, 3)\nassert((SOLUTION_SIGNATURE(grid1, coordinates1) == sample_from_3dgrid(grid1, coordinates1)).all())\n\ngrid2 = torch.rand((1, 2, 5, 5, 5))  # 3D grid with 2 channels, shape (1, 2, 5, 5, 5)\ncoordinates2 = torch.tensor([[[6.0, 6.0, 6.0], [-1.0, -1.0, -1.0]]])  # Out of bounds coordinates, shape (1, 2, 3)\nassert((SOLUTION_SIGNATURE(grid2, coordinates2) == sample_from_3dgrid(grid2, coordinates2)).all())\n\ngrid3 = torch.rand((1, 4, 6, 6, 6))  # 3D grid with 4 channels, shape (1, 4, 6, 6, 6)\ncoordinates3 = torch.tensor([\n    [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0]],\n    [[0.5, 0.5, 0.5], [4.5, 4.5, 4.5], [5.5, 5.5, 5.5]]\n])  # 3D coordinates, shape (2, 3, 3)\nassert((SOLUTION_SIGNATURE(grid3, coordinates3) == sample_from_3dgrid(grid3, coordinates3)).all())\n"
 },
 {
  "task_id": "RWPB/131",
  "prompt": "import torch\nimport numpy as np\n\ndef box_iou(box1, box2, eps=1e-7):\n    \"\"\"\n    Calculate intersection-over-union (IoU) of boxes. Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n    Based on https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n\n    Args:\n        box1 (torch.Tensor): A tensor of shape (N, 4) representing N bounding boxes.\n        box2 (torch.Tensor): A tensor of shape (M, 4) representing M bounding boxes.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n\n    Returns:\n        (torch.Tensor): An NxM tensor containing the pairwise IoU values for every element in box1 and box2.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `box_iou` function:\n\n```python\nimport torch\nimport numpy as np\n\ndef box_iou(box1, box2, eps=1e-7):\n    \"\"\"\n    Calculate intersection-over-union (IoU) of boxes. Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n    Based on https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n\n    Args:\n        box1 (torch.Tensor): A tensor of shape (N, 4) representing N bounding boxes.\n        box2 (torch.Tensor): A tensor of shape (M, 4) representing M bounding boxes.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n\n    Returns:\n        (torch.Tensor): An NxM tensor containing the pairwise IoU values for every element in box1 and box2.\n    \"\"\"\n    # Get the coordinates of bounding boxes\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1.split(1, dim=-1)\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2.split(1, dim=-1)\n\n    # Intersection coordinates\n    inter_x1 = torch.max(b1_x1, b2_x1.transpose(0, 1))\n    inter_y1 = torch.max(b1_y1, b2_y1.transpose(0, 1))\n    inter_x2 = torch.min(b1_x2, b2_x2.transpose(0, 1))\n    inter_y2 = torch.min(b1_y2, b2_y2.transpose(0, 1))\n\n    # Intersection area\n    inter_area = torch.clamp(inter_x2 - inter_x1 + 1, min=0) * torch.clamp(inter_y2 - inter_y1 + 1, min=0)\n\n    # Union area\n    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n    union_area = b1_area + b2_area.transpose(0, 1) - inter_area\n\n    # IoU\n    iou = inter_area / (union_area + eps)\n\n    return iou\n```\n\nExplanation:\n1. The function takes two tensors `box1` and `box2` representing bounding boxes in the format (x1, y1, x2, y2), where (x1, y1) are the coordinates of the top-left corner and (x2, y2) are the coordinates of the bottom-right corner.\n\n2. We split the coordinates of `box1` and `box2` into separate variables using `split()` along the last dimension.\n\n3. We calculate the coordinates of the intersection region by taking the maximum of the top-left coordinates (x1, y1) and the minimum of the bottom-right coordinates (x2, y2) between each pair of boxes from `box1` and `box2`. We use `transpose()` to align the dimensions for element-wise comparison.\n\n4. The intersection area is calculated by taking the maximum of 0 and the differences between the intersection coordinates, and then multiplying the resulting width and height.\n\n5. The union area is calculated by adding the areas of `box1` and `box2` and subtracting the intersection area. We use `transpose()` to align the dimensions for element-wise addition.\n\n6. Finally, we compute the IoU by dividing the intersection area by the union area, adding a small value `eps` to avoid division by zero.\n\n7. The function returns an NxM tensor containing the pairwise IoU values for every element in `box1` and `box2`.\n\nThis implementation calculates the IoU for all pairs of bounding boxes between `box1` and `box2` efficiently using tensor operations in PyTorch.",
  "canonical_solution": "\n    # NOTE: need float32 to get accurate iou values\n    box1 = torch.as_tensor(box1, dtype=torch.float32)\n    box2 = torch.as_tensor(box2, dtype=torch.float32)\n    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n    (a1, a2), (b1, b2) = box1.unsqueeze(1).chunk(2, 2), box2.unsqueeze(0).chunk(2, 2)\n    inter = (torch.min(a2, b2) - torch.max(a1, b1)).clamp_(0).prod(2)\n\n    # IoU = inter / (area1 + area2 - inter)\n    return inter / ((a2 - a1).prod(2) + (b2 - b1).prod(2) - inter + eps)\n\n",
  "unprocess_testcases": "box1 = np.array([[0, 0, 2, 2]])\nbox2 = np.array([[1, 1, 3, 3]])\nassert((SOLUTION_SIGNATURE(box1, box2) == box_iou(box1, box2)).all())\n\nbox1 = np.array([[0, 0, 1, 1]])\nbox2 = np.array([[2, 2, 3, 3]])\nassert((SOLUTION_SIGNATURE(box1, box2) == box_iou(box1, box2)).all())\n\nbox1 = np.array([[0, 0, 3, 3], [4, 4, 5, 5]])\nbox2 = np.array([[1, 1, 4, 4], [0, 0, 2, 2]])\nassert((SOLUTION_SIGNATURE(box1, box2) == box_iou(box1, box2)).all())\n\nbox1 = np.array([[0, 0, 3, 3], [4, 4, 5, 5]])\nbox2 = np.array([[1, 1, 4, 4], [0, 0, 2, 2]])\nassert((SOLUTION_SIGNATURE(box1, box2, 2e-5) == box_iou(box1, box2, 2e-5)).all())\n\nbox1 = np.array([[1, 3, 3, 6], [7, 7, 2, 6]])\nbox2 = np.array([[9, 3, 5, 4], [0, 1, 3, 8]])\nassert((SOLUTION_SIGNATURE(box1, box2) == box_iou(box1, box2)).all())\n"
 },
 {
  "task_id": "RWPB/121",
  "prompt": "def isprime(n):\n    \"\"\"\n    Check if a number is a prime number.\n\n    Args:\n        - n: int, number to be checked for primality\n\n    Returns:\n        - bool: True if n is prime, False otherwise\n    \"\"\"\n",
  "solution": "Certainly! Here's the completed code for the `isprime` function without unit test cases:\n\n```python\ndef isprime(n):\n    \"\"\"\n    Check if a number is a prime number.\n\n    Args:\n        - n: int, number to be checked for primality\n\n    Returns:\n        - bool: True if n is prime, False otherwise\n    \"\"\"\n    if n <= 1:\n        return False\n    if n == 2:\n        return True\n    if n % 2 == 0:\n        return False\n\n    for i in range(3, int(n**0.5) + 1, 2):\n        if n % i == 0:\n            return False\n\n    return True\n```\n\nExplanation:\n1. We first check if the number `n` is less than or equal to 1. If it is, we return `False` because numbers less than or equal to 1 are not considered prime.\n\n2. If `n` is equal to 2, we return `True` since 2 is the smallest prime number.\n\n3. If `n` is even and not equal to 2, we return `False` because even numbers (except 2) are not prime.\n\n4. We then iterate from 3 to the square root of `n` (inclusive) with a step of 2. We only need to check odd numbers up to the square root of `n` because if a number is not prime, it must have at least one factor less than or equal to its square root.\n\n5. Inside the loop, we check if `n` is divisible by any of the numbers `i`. If it is, we return `False` because `n` is not prime.\n\n6. If the loop completes without finding any factors, it means `n` is prime, so we return `True`.\n\nThis implementation efficiently checks whether a given number `n` is prime or not.",
  "canonical_solution": "    \n    if n == 2:\n        return True\n    if n % 2 == 0:\n        return False\n\n    max = n**0.5+1\n    i = 3\n    \n    while i <= max:\n        if n % i == 0:\n            return False\n        i+=2\n    return True\n\n\n",
  "unprocess_testcases": "n = 2\nassert(SOLUTION_SIGNATURE(n) == isprime(n))\n\nn = 4\nassert(SOLUTION_SIGNATURE(n) == isprime(n))\n\nn = 29\nassert(SOLUTION_SIGNATURE(n) == isprime(n))\n"
 },
 {
  "task_id": "RWPB/115",
  "prompt": "import torch\nimport torch.nn as nn\n\ndef compute_tri_normal(geometry, tris):\n    \"\"\"\n    Compute the normal vectors for triangles defined by vertex indices.\n\n    Args:\n        geometry (torch.Tensor): A tensor of shape (batch_size, num_points, 3) representing the 3D coordinates of points.\n        tris (torch.Tensor): A tensor of shape (num_tris, 3) representing the vertex indices that form triangles.\n\n    Returns:\n        torch.Tensor: A tensor of shape (batch_size, num_tris, 3) representing the normal vectors of the triangles.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of computing triangle normals:\n\n```python\nimport torch\nimport torch.nn as nn\n\ndef compute_tri_normal(geometry, tris):\n    \"\"\"\n    Compute the normal vectors for triangles defined by vertex indices.\n\n    Args:\n        geometry (torch.Tensor): A tensor of shape (batch_size, num_points, 3) representing the 3D coordinates of points.\n        tris (torch.Tensor): A tensor of shape (num_tris, 3) representing the vertex indices that form triangles.\n\n    Returns:\n        torch.Tensor: A tensor of shape (batch_size, num_tris, 3) representing the normal vectors of the triangles.\n    \"\"\"\n    # Extract the vertex coordinates for each triangle\n    v0 = geometry[:, tris[:, 0], :]  # (batch_size, num_tris, 3)\n    v1 = geometry[:, tris[:, 1], :]  # (batch_size, num_tris, 3)\n    v2 = geometry[:, tris[:, 2], :]  # (batch_size, num_tris, 3)\n\n    # Compute the edge vectors\n    e1 = v1 - v0  # (batch_size, num_tris, 3)\n    e2 = v2 - v0  # (batch_size, num_tris, 3)\n\n    # Compute the cross product of the edge vectors to get the normal vectors\n    normals = torch.cross(e1, e2, dim=-1)  # (batch_size, num_tris, 3)\n\n    # Normalize the normal vectors\n    normals = nn.functional.normalize(normals, p=2, dim=-1)  # (batch_size, num_tris, 3)\n\n    return normals\n```\n\nExplanation:\n1. We extract the vertex coordinates for each triangle using the vertex indices provided in `tris`. The resulting tensors `v0`, `v1`, and `v2` have shape `(batch_size, num_tris, 3)`, where each row represents the coordinates of a vertex for each triangle in the batch.\n\n2. We compute the edge vectors `e1` and `e2` by subtracting the vertex coordinates. `e1` represents the vector from `v0` to `v1`, and `e2` represents the vector from `v0` to `v2`.\n\n3. We compute the cross product of the edge vectors `e1` and `e2` using `torch.cross()` to obtain the normal vectors for each triangle. The resulting tensor `normals` has shape `(batch_size, num_tris, 3)`.\n\n4. Finally, we normalize the normal vectors using `nn.functional.normalize()` with `p=2` (Euclidean norm) along the last dimension to ensure unit length.\n\n5. The function returns the computed normal vectors for each triangle in the batch.\n\nThis code computes the normal vectors for triangles defined by vertex indices, taking into account the batch dimension of the geometry tensor.",
  "canonical_solution": "    \n    geometry = geometry.permute(0, 2, 1)\n    tri_1 = tris[:, 0]\n    tri_2 = tris[:, 1]\n    tri_3 = tris[:, 2]\n\n    vert_1 = torch.index_select(geometry, 2, tri_1)\n    vert_2 = torch.index_select(geometry, 2, tri_2)\n    vert_3 = torch.index_select(geometry, 2, tri_3)\n\n    nnorm = torch.cross(vert_2 - vert_1, vert_3 - vert_1, 1)\n    normal = nn.functional.normalize(nnorm).permute(0, 2, 1)\n    return normal\n\n\n",
  "unprocess_testcases": "geometry = torch.tensor([[[0., 0., 0.],\n                  [1., 0., 0.],\n                  [0., 1., 0.]]], dtype=torch.float32)\ntris = torch.tensor([[0, 1, 2]], dtype=torch.int64)\nassert((SOLUTION_SIGNATURE(geometry, tris) == compute_tri_normal(geometry, tris)).all())\n\ngeometry = torch.tensor([[[0., 0., 0.], [1., 0., 0.], [0., 1., 0.], [1., 1., 0.], [2., 0., 0.], [0., 2., 0.]],\n                 [[0., 0., 0.], [0., 1., 0.], [1., 0., 0.], [0., 0., 1.], [1., 1., 1.], [1., 0., 1.]]],\n                dtype=torch.float32)\ntris = torch.tensor([[0, 1, 2], [3, 4, 5]], dtype=torch.int64)\nassert((SOLUTION_SIGNATURE(geometry, tris) == compute_tri_normal(geometry, tris)).all())\n\ngeometry = torch.tensor([[[0., 0., 0.],\n                  [1., 1., 1.],\n                  [2., 2., 2.]]], dtype=torch.float32)\ntris = torch.tensor([[0, 1, 2]], dtype=torch.int64)\nassert((SOLUTION_SIGNATURE(geometry, tris) == compute_tri_normal(geometry, tris)).all())\n"
 },
 {
  "task_id": "RWPB/48",
  "prompt": "def select_best_resolution(original_size, possible_resolutions):\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        original_size (tuple): The original size of the image in the format (width, height).\n        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2), ...].\n\n    Returns:\n        tuple: The best fit resolution in the format (width, height).\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of selecting the best resolution from a list of possible resolutions based on the original size:\n\n```python\ndef select_best_resolution(original_size, possible_resolutions):\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        original_size (tuple): The original size of the image in the format (width, height).\n        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2), ...].\n\n    Returns:\n        tuple: The best fit resolution in the format (width, height).\n    \"\"\"\n    original_width, original_height = original_size\n    aspect_ratio = original_width / original_height\n\n    min_diff = float('inf')\n    best_resolution = None\n\n    for resolution in possible_resolutions:\n        width, height = resolution\n        res_aspect_ratio = width / height\n\n        diff = abs(res_aspect_ratio - aspect_ratio)\n\n        if diff < min_diff:\n            min_diff = diff\n            best_resolution = resolution\n\n    return best_resolution\n```\n\nExplanation:\n1. We extract the original width and height from the `original_size` tuple.\n2. We calculate the aspect ratio of the original size by dividing the width by the height.\n3. We initialize `min_diff` to positive infinity (`float('inf')`) to keep track of the minimum difference in aspect ratios.\n4. We initialize `best_resolution` to `None` to store the best fit resolution.\n5. We iterate over each resolution in the `possible_resolutions` list.\n6. For each resolution, we extract the width and height from the resolution tuple.\n7. We calculate the aspect ratio of the current resolution by dividing the width by the height.\n8. We calculate the absolute difference between the aspect ratio of the current resolution and the aspect ratio of the original size.\n9. If the calculated difference is smaller than the current `min_diff`, we update `min_diff` with the new difference and update `best_resolution` with the current resolution.\n10. After iterating through all possible resolutions, we return the `best_resolution`, which represents the resolution with the aspect ratio closest to the original size.\n\nThis code selects the best resolution from the list of possible resolutions by comparing the aspect ratios of each resolution with the aspect ratio of the original size. The resolution with the aspect ratio closest to the original size is considered the best fit and is returned.",
  "canonical_solution": "\n    original_width, original_height = original_size\n    best_fit = None\n    max_effective_resolution = 0\n    min_wasted_resolution = float('inf')\n\n    for width, height in possible_resolutions:\n        scale = min(width / original_width, height / original_height)\n        downscaled_width, downscaled_height = int(original_width * scale), int(original_height * scale)\n        effective_resolution = min(downscaled_width * downscaled_height, original_width * original_height)\n        wasted_resolution = (width * height) - effective_resolution\n\n        if effective_resolution > max_effective_resolution or (effective_resolution == max_effective_resolution and wasted_resolution < min_wasted_resolution):\n            max_effective_resolution = effective_resolution\n            min_wasted_resolution = wasted_resolution\n            best_fit = (width, height)\n\n    return best_fit\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE((1920, 1080), [(1280, 720), (1920, 1080), (2560, 1440)])[0] == select_best_resolution((1920, 1080), [(1280, 720), (1920, 1080), (2560, 1440)])[0])\nassert(SOLUTION_SIGNATURE((1920, 1080), [(1024, 768), (1280, 1024), (1600, 1200)])[0] == select_best_resolution((1920, 1080), [(1024, 768), (1280, 1024), (1600, 1200)])[0])\nassert(SOLUTION_SIGNATURE((800, 600), [(1024, 768), (1280, 960), (1920, 1440)])[0] == select_best_resolution((800, 600), [(1024, 768), (1280, 960), (1920, 1440)])[0])\nassert(SOLUTION_SIGNATURE((1920, 1080), [(1280, 720), (1920, 1080), (2560, 1440)])[1] == select_best_resolution((1920, 1080), [(1280, 720), (1920, 1080), (2560, 1440)])[1])\nassert(SOLUTION_SIGNATURE((1920, 1080), [(1024, 768), (1280, 1024), (1600, 1200)])[1] == select_best_resolution((1920, 1080), [(1024, 768), (1280, 1024), (1600, 1200)])[1])\nassert(SOLUTION_SIGNATURE((800, 600), [(1024, 768), (1280, 960), (1920, 1440)])[1] == select_best_resolution((800, 600), [(1024, 768), (1280, 960), (1920, 1440)])[1])\n"
 },
 {
  "task_id": "RWPB/19",
  "prompt": "def rank_accuracy(preds, references):\n    '''\n    Computes the accuracy of ranking predictions compared to reference rankings.\n\n    Parameters\n    ----------\n    preds : list\n        A list of numeric predictions, typically confidence scores, where higher\n        values represent stronger predictions.\n    references : list\n        A list of actual values or labels, with the same ordering principle as\n        'preds', used as a ground truth for accuracy calculation.\n\n    Return\n    ------\n    float\n        The proportion of correct ranking relations in 'preds' relative to 'references'.\n\n    This function first generates all possible pairs of different predictions and checks if\n    the ranking (greater or lesser) of the first element relative to the second in each pair\n    is correct based on the corresponding reference values.\n    '''\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\ndef rank_accuracy(preds, references):\n    '''\n    Computes the accuracy of ranking predictions compared to reference rankings.\n\n    Parameters\n    ----------\n    preds : list\n        A list of numeric predictions, typically confidence scores, where higher\n        values represent stronger predictions.\n    references : list\n        A list of actual values or labels, with the same ordering principle as\n        'preds', used as a ground truth for accuracy calculation.\n\n    Return\n    ------\n    float\n        The proportion of correct ranking relations in 'preds' relative to 'references'.\n\n    This function first generates all possible pairs of different predictions and checks if\n    the ranking (greater or lesser) of the first element relative to the second in each pair\n    is correct based on the corresponding reference values.\n    '''\n    if len(preds) != len(references):\n        raise ValueError(\"Predictions and references must have the same length.\")\n\n    total_pairs = 0\n    correct_pairs = 0\n\n    for i in range(len(preds)):\n        for j in range(i + 1, len(preds)):\n            total_pairs += 1\n            if (preds[i] > preds[j] and references[i] > references[j]) or \\\n               (preds[i] < preds[j] and references[i] < references[j]) or \\\n               (preds[i] == preds[j] and references[i] == references[j]):\n                correct_pairs += 1\n\n    if total_pairs == 0:\n        return 1.0  # If there are no pairs, consider it as 100% accuracy\n\n    accuracy = correct_pairs / total_pairs\n    return accuracy\n```\n\nExplanation:\n\n1. The function first checks if the lengths of `preds` and `references` are equal. If not, it raises a `ValueError` with an appropriate message.\n\n2. It initializes two variables: `total_pairs` to keep track of the total number of pairs considered, and `correct_pairs` to count the number of correctly ranked pairs.\n\n3. It uses nested loops to generate all possible pairs of different predictions. The outer loop iterates from index 0 to the second-to-last index, and the inner loop iterates from the next index after the outer loop's index to the last index.\n\n4. For each pair, it increments `total_pairs` by 1 and checks the ranking relation between the predictions and the corresponding references.\n   - If the first prediction is greater than the second prediction and the first reference is also greater than the second reference, it counts as a correct pair.\n   - Similarly, if the first prediction is less than the second prediction and the first reference is also less than the second reference, it counts as a correct pair.\n   - If both predictions are equal and both references are equal, it also counts as a correct pair.\n   - If any of these conditions are met, `correct_pairs` is incremented by 1.\n\n5. After considering all pairs, it checks if `total_pairs` is 0. If so, it means there were no pairs to compare, and the function returns 1.0 (100% accuracy) to handle this edge case.\n\n6. Finally, it calculates the accuracy by dividing `correct_pairs` by `total_pairs` and returns the result.\n\nThis function computes the accuracy of ranking predictions by comparing the ranking relations of all possible pairs of predictions with their corresponding reference values.",
  "canonical_solution": "\n    # Initialize an empty list to store the pairs and their comparison results\n    pairs = []\n\n    # Generate all pairwise comparisons from preds except self-comparisons\n    for i, pred in enumerate(preds):\n        for j, pred2 in enumerate(preds):\n            if i == j:\n                continue  # Skip comparing the same elements\n            pairs.append((i, j, pred > pred2))  # Store index and comparison result\n\n    # Count how many of these predicted comparisons are correct according to the references\n    correct = 0\n    for i, ref in enumerate(references):\n        for j, ref2 in enumerate(references):\n            if i == j:\n                continue  # Skip self-comparisons\n            if (i, j, ref > ref2) in pairs:\n                correct += 1  # Increment if the prediction comparison is correct\n\n    # Calculate the accuracy as the ratio of correct predictions to total comparisons\n    return correct / len(pairs) if pairs else 0  # Handle the case where pairs might be empty\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(preds = [0.9, 0.1, 0.5], references = [3, 1, 2]) == rank_accuracy(preds = [0.9, 0.1, 0.5], references = [3, 1, 2]))\nassert(SOLUTION_SIGNATURE(preds = [0.5, 0.9, 0.1], references = [3, 1, 2]) == rank_accuracy(preds = [0.5, 0.9, 0.1], references = [3, 1, 2]))\nassert(SOLUTION_SIGNATURE(preds = [0.5, 0.5, 0.1], references = [2, 2, 1]) == rank_accuracy(preds = [0.5, 0.5, 0.1], references = [2, 2, 1]))\n"
 },
 {
  "task_id": "RWPB/4",
  "prompt": "import numpy as np\nimport torch.nn.functional as F\nimport torch\nimport roma\n\ndef rot6d_to_rotmat(x):\n    \"\"\"\n    6D rotation representation to 3x3 rotation matrix. \n    If the size of matrix is not (B,6), please return ValueError\n\n    Args:\n        x: (B,6) Batch of 6-D rotation representations.\n    Returns:\n        torch.Tensor: Batch of corresponding rotation matrices with shape (B,3,3).\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of converting a 6D rotation representation to a 3x3 rotation matrix:\n\n```python\nimport numpy as np\nimport torch.nn.functional as F\nimport torch\nimport roma\n\ndef rot6d_to_rotmat(x):\n    \"\"\"\n    6D rotation representation to 3x3 rotation matrix. \n    If the size of matrix is not (B,6), please return ValueError\n\n    Args:\n        x: (B,6) Batch of 6-D rotation representations.\n    Returns:\n        torch.Tensor: Batch of corresponding rotation matrices with shape (B,3,3).\n    \"\"\"\n    if x.shape[-1] != 6:\n        raise ValueError(\"Invalid input shape. Expected (B, 6), but got {}\".format(x.shape))\n\n    # Normalize the input\n    x_normalized = F.normalize(x, dim=-1)\n\n    # Reshape the input to (B, 2, 3)\n    x_reshaped = x_normalized.view(-1, 2, 3)\n\n    # Compute the rotation matrix using roma library\n    rotation_matrix = roma.unitquat_to_rotmat(roma.special_gramschmidt(x_reshaped))\n\n    return rotation_matrix\n```\n\nExplanation:\n\n1. We first check if the input `x` has the expected shape of (B, 6), where B is the batch size. If the shape is not (B, 6), we raise a `ValueError` with an appropriate error message.\n\n2. We normalize the input `x` using `F.normalize` along the last dimension (dim=-1) to ensure that each pair of 3D vectors in the 6D representation has unit length.\n\n3. We reshape the normalized input `x_normalized` to (B, 2, 3) using `view()`. This step is necessary because the `special_gramschmidt` function from the `roma` library expects the input to have this shape.\n\n4. We compute the rotation matrix using the `unitquat_to_rotmat` function from the `roma` library. The `special_gramschmidt` function is used to orthogonalize the pair of 3D vectors in the 6D representation before converting them to a rotation matrix.\n\n5. Finally, we return the computed rotation matrix, which has a shape of (B, 3, 3).\n\nThis code assumes that you have the `roma` library installed, which provides the necessary functions for orthogonalization and conversion to rotation matrices.\n\nNote: Make sure to handle any potential exceptions that may occur if the input does not meet the expected shape or if there are any issues with the `roma` library.",
  "canonical_solution": "    \n    if x.size(1) != 6:\n        return ValueError\n    x = x.reshape(-1,2,3).permute(0, 2, 1).contiguous()\n    y = roma.special_gramschmidt(x)\n    return y\n\n\n",
  "unprocess_testcases": "a = torch.randn(3, 6)\nb = torch.randn(1, 6)\nc = torch.randn(3, 5)\n\nassert((SOLUTION_SIGNATURE(a) == rot6d_to_rotmat(a)).all())\nassert((SOLUTION_SIGNATURE(b) == rot6d_to_rotmat(b)).all())\nassert(SOLUTION_SIGNATURE(c) == rot6d_to_rotmat(c))"
 },
 {
  "task_id": "RWPB/39",
  "prompt": "import re\nfrom typing import List\n\n\ndef clean_html(\n        html_to_clean: str,\n        tags_to_remove: List[str] = [\"style\", \"svg\", \"script\"],\n        attributes_to_keep: List[str] = [\"id\", \"href\"],\n) -> str:\n    \"\"\"\n    Cleans HTML content by removing specified tags and their contents, and strips out all attributes except those specified.\n\n    Args:\n        html_to_clean (str): The HTML content to clean.\n        tags_to_remove (List[str]): List of HTML tags to remove from the content. Default tags are 'style', 'svg', and 'script'.\n        attributes_to_keep (List[str]): List of HTML attributes to retain in the remaining HTML tags. Default is ['id', 'href'].\n\n    Returns:\n        str: The cleaned HTML content, free of specified tags and with only desired attributes retained.\n\n    This function first removes all specified tags and their contents using regular expressions. It then eliminates all attributes\n    from the remaining tags except for those explicitly listed to be kept. This is useful for preparing HTML content for environments\n    where only basic styling or functionality is needed, or for security purposes to remove potentially malicious content.\n\n    Example:\n        >>> clean_html('<div id=\"main\" style=\"color:red\">Hello <script>alert(\"World\")</script></div>',\n                       tags_to_remove=['script'], attributes_to_keep=['id'])\n        '<div id=\"main\">Hello </div>'\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport re\nfrom typing import List\n\n\ndef clean_html(\n        html_to_clean: str,\n        tags_to_remove: List[str] = [\"style\", \"svg\", \"script\"],\n        attributes_to_keep: List[str] = [\"id\", \"href\"],\n) -> str:\n    \"\"\"\n    Cleans HTML content by removing specified tags and their contents, and strips out all attributes except those specified.\n\n    Args:\n        html_to_clean (str): The HTML content to clean.\n        tags_to_remove (List[str]): List of HTML tags to remove from the content. Default tags are 'style', 'svg', and 'script'.\n        attributes_to_keep (List[str]): List of HTML attributes to retain in the remaining HTML tags. Default is ['id', 'href'].\n\n    Returns:\n        str: The cleaned HTML content, free of specified tags and with only desired attributes retained.\n\n    This function first removes all specified tags and their contents using regular expressions. It then eliminates all attributes\n    from the remaining tags except for those explicitly listed to be kept. This is useful for preparing HTML content for environments\n    where only basic styling or functionality is needed, or for security purposes to remove potentially malicious content.\n\n    Example:\n        >>> clean_html('<div id=\"main\" style=\"color:red\">Hello <script>alert(\"World\")</script></div>',\n                       tags_to_remove=['script'], attributes_to_keep=['id'])\n        '<div id=\"main\">Hello </div>'\n    \"\"\"\n    # Remove specified tags and their contents\n    for tag in tags_to_remove:\n        html_to_clean = re.sub(rf'<{tag}.*?</{tag}>', '', html_to_clean, flags=re.DOTALL)\n\n    # Remove all attributes except those specified\n    attributes_regex = '|'.join(attributes_to_keep)\n    html_to_clean = re.sub(rf'(<[^>]+) (?!(?:{attributes_regex})=)[^>]+(?=>)', r'\\1', html_to_clean)\n\n    return html_to_clean\n```\n\nExplanation:\n\n1. We iterate over the `tags_to_remove` list and use a regular expression to remove each tag and its contents from the `html_to_clean` string. The regular expression `<{tag}.*?</{tag}>` matches the opening tag, any content inside the tag (including nested tags), and the closing tag. The `re.DOTALL` flag ensures that the dot (`.`) matches newline characters as well.\n\n2. We create a regular expression pattern `attributes_regex` by joining the `attributes_to_keep` list with the `|` operator. This pattern will match any of the specified attributes.\n\n3. We use another regular expression to remove all attributes except those specified in `attributes_to_keep`. The regular expression `(<[^>]+) (?!(?:{attributes_regex})=)[^>]+(?=>)` matches the following:\n   - `(<[^>]+)` captures the opening part of the tag (e.g., `<div`).\n   - `(?!(?:{attributes_regex})=)` is a negative lookahead assertion that ensures the attribute name is not one of the specified attributes to keep.\n   - `[^>]+` matches the attribute name and value.\n   - `(?=>)` is a positive lookahead assertion that ensures the attribute is followed by the closing part of the tag.\n\n   The replacement pattern `r'\\1'` replaces the matched attribute with an empty string, effectively removing it.\n\n4. Finally, we return the cleaned HTML content.\n\nThis implementation should handle the cleaning of HTML content according to the specified requirements.",
  "canonical_solution": "\n    # Remove specified tags and their contents completely\n    for tag in tags_to_remove:\n        html_to_clean = re.sub(\n            rf\"<{tag}[^>]*>.*?</{tag}>\", \"\", html_to_clean, flags=re.DOTALL\n        )\n\n    # Compile a pattern that matches any attribute not in the list of attributes to keep\n    attributes_to_keep = \"|\".join(attributes_to_keep)\n    pattern = rf'\\b(?!({attributes_to_keep})\\b)\\w+(?:-\\w+)?\\s*=\\s*[\"\\'][^\"\\']*[\"\\']'\n\n    # Remove all attributes that are not explicitly listed to be kept\n    cleaned_html = re.sub(pattern, \"\", html_to_clean)\n\n    cleaned_html = re.sub(rf'\\s*>', \">\", cleaned_html)\n\n    return cleaned_html\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE('<div><style> .text { font-size: 12px; } </style><p id=\"info\">Visit our <a href=\"http://example.com\" style=\"color: red;\">website</a></p></div>') == clean_html('<div><style> .text { font-size: 12px; } </style><p id=\"info\">Visit our <a href=\"http://example.com\" style=\"color: red;\">website</a></p></div>'))\nassert(SOLUTION_SIGNATURE('<div id=\"container\"><script>alert(\\'Hello\\');</script><svg><circle cx=\"50\" cy=\"50\" r=\"40\" stroke=\"black\" fill=\"red\"></circle></svg><a href=\"#nothing\" onclick=\"javascript:void(0);\" style=\"color: black;\">Click here</a></div>') == clean_html('<div id=\"container\"><script>alert(\\'Hello\\');</script><svg><circle cx=\"50\" cy=\"50\" r=\"40\" stroke=\"black\" fill=\"red\"></circle></svg><a href=\"#nothing\" onclick=\"javascript:void(0);\" style=\"color: black;\">Click here</a></div>'))\nassert(SOLUTION_SIGNATURE('<header style=\"background: black;\"><nav id=\"top-nav\" class=\"navigation\" data-toggle=\"collapse\"><a href=\"/home\" title=\"Home\">Home</a></nav></header>') == clean_html('<header style=\"background: black;\"><nav id=\"top-nav\" class=\"navigation\" data-toggle=\"collapse\"><a href=\"/home\" title=\"Home\">Home</a></nav></header>'))\n"
 },
 {
  "task_id": "RWPB/68",
  "prompt": "import numpy as np\n\ndef last_before(val, arr):\n  \"\"\"\n  Find the index of the last element in a sorted array that is less than or equal to a given value.Requires sorted (monotonic) arr.\n  Args:\n      - val: int, the value to search for\n      - arr: numpy.ndarray, the sorted array\n  Returns:\n      idx: int, the index of the last element in the array that is less than or equal to val\n  \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport numpy as np\n\ndef last_before(val, arr):\n    \"\"\"\n    Find the index of the last element in a sorted array that is less than or equal to a given value.\n    Requires sorted (monotonic) arr.\n    Args:\n        - val: int, the value to search for\n        - arr: numpy.ndarray, the sorted array\n    Returns:\n        idx: int, the index of the last element in the array that is less than or equal to val\n    \"\"\"\n    left = 0\n    right = len(arr) - 1\n    idx = -1\n\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] <= val:\n            idx = mid\n            left = mid + 1\n        else:\n            right = mid - 1\n\n    return idx\n```\n\nExplanation:\n1. We initialize two pointers, `left` and `right`, pointing to the start and end of the array, respectively. We also initialize `idx` to -1, which will store the index of the last element less than or equal to `val`.\n\n2. We perform a binary search while `left` is less than or equal to `right`:\n   - We calculate the middle index `mid` using integer division.\n   - If the element at index `mid` is less than or equal to `val`, we update `idx` to `mid` and move `left` to `mid + 1` to search in the right half of the array.\n   - If the element at index `mid` is greater than `val`, we move `right` to `mid - 1` to search in the left half of the array.\n\n3. We repeat step 2 until `left` becomes greater than `right`, indicating that we have searched the entire array.\n\n4. Finally, we return `idx`, which represents the index of the last element in the array that is less than or equal to `val`. If no such element is found, `idx` will remain -1.\n\nThis implementation assumes that the input array `arr` is sorted in ascending order. The binary search approach allows us to efficiently find the last element less than or equal to `val` with a time complexity of O(log n), where n is the length of the array.",
  "canonical_solution": "  \n  assert np.all(np.diff(arr) >= 0)\n  return np.searchsorted(arr, val) - 1\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(val = 5, arr = np.array([1, 3, 5, 7])) == last_before(val = 5, arr = np.array([1, 3, 5, 7])))\nassert(SOLUTION_SIGNATURE(val = 4, arr = np.array([1, 3, 5, 7])) == last_before(val = 4, arr = np.array([1, 3, 5, 7])))\nassert(SOLUTION_SIGNATURE(val = 0, arr = np.array([1, 3, 5, 7])) == last_before(val = 0, arr = np.array([1, 3, 5, 7])))\n"
 },
 {
  "task_id": "RWPB/135",
  "prompt": "import torch\n\ndef mask_iou(mask1, mask2, eps=1e-7):\n    \"\"\"\n    Calculate masks IoU.\n\n    Args:\n        mask1 (torch.Tensor): A tensor of shape (N, n) where N is the number of ground truth objects and n is the\n                        product of image width and height.\n        mask2 (torch.Tensor): A tensor of shape (M, n) where M is the number of predicted objects and n is the\n                        product of image width and height.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n\n    Returns:\n        (torch.Tensor): A tensor of shape (N, M) representing masks IoU.\n    \"\"\"\n",
  "solution": "To complete the `mask_iou` function and calculate the IoU between masks, you can use the following code:\n\n```python\nimport torch\n\ndef mask_iou(mask1, mask2, eps=1e-7):\n    \"\"\"\n    Calculate masks IoU.\n\n    Args:\n        mask1 (torch.Tensor): A tensor of shape (N, n) where N is the number of ground truth objects and n is the\n                        product of image width and height.\n        mask2 (torch.Tensor): A tensor of shape (M, n) where M is the number of predicted objects and n is the\n                        product of image width and height.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n\n    Returns:\n        (torch.Tensor): A tensor of shape (N, M) representing masks IoU.\n    \"\"\"\n    intersection = torch.matmul(mask1, mask2.t())\n    area1 = torch.sum(mask1, dim=1, keepdim=True)\n    area2 = torch.sum(mask2, dim=1)\n    union = area1 + area2 - intersection\n    iou = intersection / (union + eps)\n    return iou\n```\n\nExplanation:\n1. The `intersection` between the masks is calculated using matrix multiplication (`torch.matmul`) between `mask1` and the transpose of `mask2`. This gives us a tensor of shape (N, M) representing the element-wise intersection between each pair of masks.\n\n2. The `area1` is calculated by summing along the second dimension of `mask1` using `torch.sum` with `dim=1` and `keepdim=True`. This gives us a tensor of shape (N, 1) representing the area of each mask in `mask1`.\n\n3. The `area2` is calculated by summing along the second dimension of `mask2` using `torch.sum` with `dim=1`. This gives us a tensor of shape (M,) representing the area of each mask in `mask2`.\n\n4. The `union` is calculated by adding `area1` and `area2` and subtracting the `intersection`. This gives us a tensor of shape (N, M) representing the union area between each pair of masks.\n\n5. Finally, the `iou` is calculated by dividing the `intersection` by the `union` (with a small value `eps` added to avoid division by zero). This gives us a tensor of shape (N, M) representing the IoU between each pair of masks.\n\nThe function returns the `iou` tensor.\n\nNote: Make sure that `mask1` and `mask2` have the same number of elements along the second dimension (i.e., the product of image width and height).",
  "canonical_solution": "\n    intersection = torch.matmul(mask1, mask2.T).clamp_(0)\n    union = (mask1.sum(1)[:, None] + mask2.sum(1)[None]) - intersection  # (area1 + area2) - intersection\n    return intersection / (union + eps)\n\n\n",
  "unprocess_testcases": "mask1 = torch.tensor([\n    [1, 0, 1, 0],  # Object 1\n    [0, 1, 0, 1]   # Object 2\n])\nmask2 = torch.tensor([\n    [1, 0, 0, 1],  # Prediction 1\n    [0, 1, 1, 0],  # Prediction 2\n    [1, 1, 1, 1]   # Prediction 3\n])\nassert((SOLUTION_SIGNATURE(mask1, mask2) == mask_iou(mask1, mask2)).all())\n\nmask1 = torch.tensor([\n    [1, 0, 0, 0],  # Object 1\n    [0, 1, 0, 0]   # Object 2\n])\nmask2 = torch.tensor([\n    [0, 0, 1, 0],  # Prediction 1\n    [0, 0, 0, 1]   # Prediction 2\n])\nassert((SOLUTION_SIGNATURE(mask1, mask2) == mask_iou(mask1, mask2)).all())\n\n\nmask1 = torch.tensor([\n    [1, 1, 0, 0, 1, 1],  # Object 1\n    [0, 1, 1, 1, 0, 0],  # Object 2\n    [1, 0, 1, 0, 1, 0]   # Object 3\n])\nmask2 = torch.tensor([\n    [1, 0, 0, 1, 1, 1],  # Prediction 1\n    [0, 1, 1, 0, 0, 1]   # Prediction 2\n])\nassert((SOLUTION_SIGNATURE(mask1, mask2) == mask_iou(mask1, mask2)).all())\n"
 },
 {
  "task_id": "RWPB/101",
  "prompt": "import torch\n\ndef generate_grids(lower_limit, upper_limit, grid_size):\n    \"\"\"\n    Generate a grid of points within the specified limits.\n\n    Args:\n        lower_limit (torch.Tensor): Lower boundary of the grid.\n        upper_limit (torch.Tensor): Upper boundary of the grid.\n        grid_size (torch.Tensor): Number of grid points per dimension.\n\n    Returns:\n        tuple: Two tensors, each containing the lower and upper bounds of each grid cell.\n    \"\"\"\n",
  "solution": "Here's the completed code to generate a grid of points within the specified limits:\n\n```python\nimport torch\n\ndef generate_grids(lower_limit, upper_limit, grid_size):\n    \"\"\"\n    Generate a grid of points within the specified limits.\n\n    Args:\n        lower_limit (torch.Tensor): Lower boundary of the grid.\n        upper_limit (torch.Tensor): Upper boundary of the grid.\n        grid_size (torch.Tensor): Number of grid points per dimension.\n\n    Returns:\n        tuple: Two tensors, each containing the lower and upper bounds of each grid cell.\n    \"\"\"\n    # Calculate the size of each grid cell\n    cell_size = (upper_limit - lower_limit) / (grid_size - 1)\n\n    # Generate the grid points for each dimension\n    grid_points = [torch.linspace(lower_limit[i], upper_limit[i], grid_size[i]) for i in range(len(grid_size))]\n\n    # Create a meshgrid from the grid points\n    meshgrid = torch.meshgrid(grid_points)\n\n    # Stack the meshgrid tensors to create a tensor of grid points\n    grid_tensor = torch.stack(meshgrid, dim=-1)\n\n    # Calculate the lower and upper bounds of each grid cell\n    lower_bounds = grid_tensor[..., :-1, :]\n    upper_bounds = grid_tensor[..., 1:, :]\n\n    return lower_bounds, upper_bounds\n```\n\nExplanation:\n\n1. We calculate the size of each grid cell by dividing the difference between the upper and lower limits by the number of grid points minus one. This ensures that the grid points are evenly spaced.\n\n2. We generate the grid points for each dimension using `torch.linspace()`. This function creates a 1D tensor of evenly spaced values between the lower and upper limits for each dimension.\n\n3. We create a meshgrid from the grid points using `torch.meshgrid()`. This function takes the 1D tensors for each dimension and generates a multi-dimensional grid of points.\n\n4. We stack the meshgrid tensors along a new dimension using `torch.stack()` to create a tensor of grid points. The resulting tensor has a shape of `(*grid_size, num_dimensions)`, where `*grid_size` represents the grid size for each dimension.\n\n5. We calculate the lower and upper bounds of each grid cell by slicing the grid tensor. The lower bounds are obtained by slicing the grid tensor from the beginning to the second-to-last element along each dimension. The upper bounds are obtained by slicing the grid tensor from the second element to the end along each dimension.\n\n6. Finally, we return the lower and upper bounds of each grid cell as a tuple of tensors.\n\nThis code generates a grid of points within the specified limits and returns the lower and upper bounds of each grid cell. The resulting tensors can be used for various purposes, such as sampling points within each grid cell or performing spatial operations on the grid.",
  "canonical_solution": "    \n    ndim = lower_limit.size(0)\n    assert ndim == upper_limit.size(0)\n    assert lower_limit.ndim == upper_limit.ndim == 1\n    grids = [None] * ndim\n    steps = (upper_limit - lower_limit) / grid_size\n    for d in range(ndim):\n        grids[d] = torch.linspace(\n            lower_limit[d], upper_limit[d], grid_size[d] + 1, device=lower_limit.device\n        )[: grid_size[d]]\n    lower = torch.cartesian_prod(*grids)\n    upper = lower + steps\n    return lower, upper\n\n",
  "unprocess_testcases": "\nlower_limit = torch.tensor([0.0])\nupper_limit = torch.tensor([1.0])\ngrid_size = torch.tensor([2])\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[0] == generate_grids(lower_limit, upper_limit, grid_size)[0]).all())\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[1] == generate_grids(lower_limit, upper_limit, grid_size)[1]).all())\n\nlower_limit = torch.tensor([0.0, 0.0])\nupper_limit = torch.tensor([1.0, 1.0])\ngrid_size = torch.tensor([2, 2])\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[0] == generate_grids(lower_limit, upper_limit, grid_size)[0]).all())\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[1] == generate_grids(lower_limit, upper_limit, grid_size)[1]).all())\n\nlower_limit = torch.tensor([-1.0, 0.0, 2.0])\nupper_limit = torch.tensor([1.0, 1.0, 5.0])\ngrid_size = torch.tensor([2, 3, 1])\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[0] == generate_grids(lower_limit, upper_limit, grid_size)[0]).all())\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[1] == generate_grids(lower_limit, upper_limit, grid_size)[1]).all())\n"
 },
 {
  "task_id": "RWPB/111",
  "prompt": "import torch\n\ndef mesh_grid(B, H, W):\n    \"\"\"Create a mesh grid.\n\n    Args:\n        B (int): Batch size.\n        H (int): Height of the grid.\n        W (int): Width of the grid.\n\n    Returns:\n        torch.Tensor: A tensor of shape (B, 2, H, W) containing the mesh grid coordinates.\n    \"\"\"\n",
  "solution": "Certainly! Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport torch\n\ndef mesh_grid(B, H, W):\n    \"\"\"Create a mesh grid.\n\n    Args:\n        B (int): Batch size.\n        H (int): Height of the grid.\n        W (int): Width of the grid.\n\n    Returns:\n        torch.Tensor: A tensor of shape (B, 2, H, W) containing the mesh grid coordinates.\n    \"\"\"\n    # Create 1D tensors for x and y coordinates\n    x_coords = torch.linspace(-1, 1, W)\n    y_coords = torch.linspace(-1, 1, H)\n\n    # Create 2D tensors for x and y coordinates by repeating and expanding dimensions\n    x_grid = x_coords.repeat(H, 1)\n    y_grid = y_coords.view(-1, 1).repeat(1, W)\n\n    # Stack x and y coordinates along a new dimension\n    grid = torch.stack([x_grid, y_grid], dim=0)\n\n    # Repeat the grid for each batch\n    grid_batch = grid.unsqueeze(0).repeat(B, 1, 1, 1)\n\n    return grid_batch\n```\n\nExplanation:\n1. We create 1D tensors `x_coords` and `y_coords` using `torch.linspace()` to generate evenly spaced coordinates between -1 and 1 for the width and height dimensions, respectively.\n\n2. We create 2D tensors `x_grid` and `y_grid` by repeating and expanding the dimensions of `x_coords` and `y_coords`. This is done using `repeat()` and `view()` to match the desired shape.\n\n3. We stack the `x_grid` and `y_grid` tensors along a new dimension using `torch.stack()` to create a tensor of shape (2, H, W) representing the mesh grid coordinates.\n\n4. To support multiple batches, we unsqueeze a new dimension at index 0 using `unsqueeze()` and repeat the grid tensor along this dimension using `repeat()`. This creates a tensor of shape (B, 2, H, W), where B is the batch size.\n\n5. Finally, we return the `grid_batch` tensor containing the mesh grid coordinates for each batch.\n\nThis implementation creates a mesh grid with coordinates ranging from -1 to 1 for both the x and y dimensions. The resulting tensor has a shape of (B, 2, H, W), where B is the batch size, 2 represents the x and y coordinates, H is the height of the grid, and W is the width of the grid.",
  "canonical_solution": "    \n    x_base = torch.arange(0, W).repeat(B, H, 1)  # BHW\n    y_base = torch.arange(0, H).repeat(B, W, 1).transpose(1, 2)  # BHW\n\n    base_grid = torch.stack([x_base, y_base], 1)  # B2HW\n    return base_grid\n\n",
  "unprocess_testcases": "B=1\nH=1\nW=1\nassert((SOLUTION_SIGNATURE(B, H, W) == mesh_grid(B, H, W)).all())\n\nB=1\nH=5\nW=10\nassert((SOLUTION_SIGNATURE(B, H, W) == mesh_grid(B, H, W)).all())\n\nB=100\nH=10\nW=10\nassert((SOLUTION_SIGNATURE(B, H, W) == mesh_grid(B, H, W)).all())\n\nB=3\nH=10\nW=5\nassert((SOLUTION_SIGNATURE(B, H, W) == mesh_grid(B, H, W)).all())\n"
 },
 {
  "task_id": "RWPB/140",
  "prompt": "from typing import Callable, Dict, Tuple, Any\nimport inspect\nfrom functools import partial\n\ndef get_args_for_func(func: Callable, params: Dict) -> Tuple[Callable, Dict[str, Any]]:\n    \"\"\"\n    Filters keyword arguments from a dictionary based on a function's parameter list and creates a partially\n    applied function that already includes the matched keyword arguments.\n\n    Parameters:\n        func (Callable): The function for which the keyword arguments need to be matched.\n        params (Dict): A dictionary containing potential keyword arguments to pass to the function.\n\n    Returns:\n        Tuple[Callable, Dict[str, Any]]:\n            A tuple containing:\n            - A partial function with pre-applied keyword arguments that are valid for the specified function.\n            - A dictionary of the keyword arguments that were applicable to the function.\n\n    Example:\n        input_dict = {\"a\": \"foo\", \"b\": \"bar\"}\n        def example_func(a: str):\n            return a\n        func, kwargs = get_args_for_func(example_func, input_dict)\n        # func can now be called as func() and it would execute example_func(a=\"foo\")\n        # kwargs will be {'a': 'foo'}\n    \"\"\"\n",
  "solution": "Here's the completed code for the `get_args_for_func` function:\n\n```python\nfrom typing import Callable, Dict, Tuple, Any\nimport inspect\nfrom functools import partial\n\ndef get_args_for_func(func: Callable, params: Dict) -> Tuple[Callable, Dict[str, Any]]:\n    \"\"\"\n    Filters keyword arguments from a dictionary based on a function's parameter list and creates a partially\n    applied function that already includes the matched keyword arguments.\n\n    Parameters:\n        func (Callable): The function for which the keyword arguments need to be matched.\n        params (Dict): A dictionary containing potential keyword arguments to pass to the function.\n\n    Returns:\n        Tuple[Callable, Dict[str, Any]]:\n            A tuple containing:\n            - A partial function with pre-applied keyword arguments that are valid for the specified function.\n            - A dictionary of the keyword arguments that were applicable to the function.\n\n    Example:\n        input_dict = {\"a\": \"foo\", \"b\": \"bar\"}\n        def example_func(a: str):\n            return a\n        func, kwargs = get_args_for_func(example_func, input_dict)\n        # func can now be called as func() and it would execute example_func(a=\"foo\")\n        # kwargs will be {'a': 'foo'}\n    \"\"\"\n    # Get the parameter names and default values of the function\n    sig = inspect.signature(func)\n    func_params = sig.parameters\n\n    # Filter the keyword arguments that match the function's parameter list\n    matching_kwargs = {\n        param: value\n        for param, value in params.items()\n        if param in func_params\n    }\n\n    # Create a partially applied function with the matching keyword arguments\n    partial_func = partial(func, **matching_kwargs)\n\n    return partial_func, matching_kwargs\n```\n\nExplanation:\n1. The `inspect.signature()` function is used to get the signature of the `func` function, which includes information about its parameters.\n\n2. The `sig.parameters` attribute is used to retrieve a dictionary-like object (`func_params`) containing the parameter names as keys and parameter objects as values.\n\n3. A dictionary comprehension is used to filter the keyword arguments from `params` that match the parameter names of `func`. The resulting dictionary (`matching_kwargs`) contains only the keyword arguments that are applicable to `func`.\n\n4. The `partial()` function from the `functools` module is used to create a new function (`partial_func`) that partially applies the matching keyword arguments to `func`. This means that when `partial_func` is called, it will automatically include the pre-applied keyword arguments.\n\n5. Finally, the function returns a tuple containing the `partial_func` and the `matching_kwargs` dictionary.\n\nNow, when you call `get_args_for_func` with a function and a dictionary of potential keyword arguments, it will return a tuple containing a partially applied function with the valid keyword arguments and a dictionary of the matching keyword arguments.",
  "canonical_solution": "    \n    # Extracts function argument names using inspect\n    _kwargs = {k: v for k, v in params.items() if k in inspect.getfullargspec(func).args}\n    # Creates a partial function that pre-applies these keyword arguments\n    return (\n        partial(func, **_kwargs),\n        _kwargs,\n    )\n\n\n",
  "unprocess_testcases": "def add(x, y):\n    return x + y\n\ndef multiply(x, y):\n    return x * y\n\ndef greet(name, msg=\"Hello\"):\n    return f\"{msg}, {name}\"\n\nfunc, kwargs = get_args_for_func(add, {'x': 10, 'y': 5})\n\nassert(SOLUTION_SIGNATURE(add, {'x': 10, 'y': 5})[0]() == 15)\nassert(SOLUTION_SIGNATURE(add, {'x': 10, 'y': 5})[1] == get_args_for_func(add, {'x': 10, 'y': 5})[1])\n\nassert(SOLUTION_SIGNATURE(multiply, {'x': 2, 'y': 8, 'z': 15})[0]() == 16)\nassert(SOLUTION_SIGNATURE(multiply, {'x': 2, 'y': 8, 'z': 15})[1] == get_args_for_func(multiply, {'x': 2, 'y': 8, 'z': 15})[1])\n\nassert(SOLUTION_SIGNATURE(greet, {'name': 'Alice'})[0]() == \"Hello, Alice\")\nassert(SOLUTION_SIGNATURE(greet, {'name': 'Alice'})[1] == get_args_for_func(greet, {'name': 'Alice'})[1])"
 },
 {
  "task_id": "RWPB/125",
  "prompt": "import numpy as np\n\ndef resample_segments(segments, n=1000):\n    \"\"\"\n    Resamples each segment in a list to have a specified number of equidistant points.\n\n    Args:\n        segments (list): A list of numpy arrays of shape (n, 2), where each array represents a segment with n points.\n        n (int): The number of points to resample each segment to, defaults to 1000.\n\n    Returns:\n        list: A list of numpy arrays, each reshaped to (n, 2), representing the resampled segments.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport numpy as np\n\ndef resample_segments(segments, n=1000):\n    \"\"\"\n    Resamples each segment in a list to have a specified number of equidistant points.\n\n    Args:\n        segments (list): A list of numpy arrays of shape (n, 2), where each array represents a segment with n points.\n        n (int): The number of points to resample each segment to, defaults to 1000.\n\n    Returns:\n        list: A list of numpy arrays, each reshaped to (n, 2), representing the resampled segments.\n    \"\"\"\n    resampled_segments = []\n\n    for segment in segments:\n        # Compute the cumulative distance along the segment\n        distances = np.sqrt(np.sum(np.diff(segment, axis=0) ** 2, axis=1))\n        cumulative_distances = np.concatenate(([0], np.cumsum(distances)))\n\n        # Normalize the cumulative distances to the range [0, 1]\n        normalized_distances = cumulative_distances / cumulative_distances[-1]\n\n        # Create a new set of equidistant points along the normalized distance range\n        resampled_distances = np.linspace(0, 1, n)\n\n        # Interpolate the x and y coordinates at the resampled distances\n        resampled_points = np.interp(resampled_distances, normalized_distances, segment.T)\n\n        # Reshape the resampled points to (n, 2)\n        resampled_segment = resampled_points.T.reshape(n, 2)\n\n        resampled_segments.append(resampled_segment)\n\n    return resampled_segments\n```\n\nExplanation:\n1. We initialize an empty list called `resampled_segments` to store the resampled segments.\n\n2. We iterate over each segment in the `segments` list.\n\n3. For each segment, we compute the cumulative distance along the segment using the Euclidean distance formula. We calculate the distances between consecutive points using `np.diff()` and then take the square root of the sum of squared differences using `np.sqrt()` and `np.sum()`. We concatenate a zero at the beginning to represent the distance at the starting point.\n\n4. We normalize the cumulative distances to the range [0, 1] by dividing each distance by the total cumulative distance (the last element of `cumulative_distances`).\n\n5. We create a new set of equidistant points along the normalized distance range using `np.linspace()`. This gives us `n` evenly spaced points between 0 and 1.\n\n6. We interpolate the x and y coordinates at the resampled distances using `np.interp()`. We pass the resampled distances, the normalized distances, and the transposed segment coordinates to interpolate along each axis separately.\n\n7. We reshape the resampled points to the desired shape of (n, 2) using `reshape()`.\n\n8. We append the resampled segment to the `resampled_segments` list.\n\n9. Finally, we return the list of resampled segments.\n\nThis code resamples each segment in the `segments` list to have `n` equidistant points, ensuring that the resampled segments have the same number of points and are evenly spaced along the original segment.",
  "canonical_solution": "    \n    for i, s in enumerate(segments):\n        # Ensure the segment is closed by appending the first point to the end\n        s = np.concatenate((s, s[0:1, :]), axis=0)\n        # Create an array of the desired number of points spaced between 0 and the last index of the segment\n        x = np.linspace(0, len(s) - 1, n)\n        # Create an array of existing indices\n        xp = np.arange(len(s))\n        # Interpolate new points for both x and y coordinates\n        segments[i] = np.stack(\n            [np.interp(x, xp, s[:, j]) for j in range(2)], axis=-1\n        )  # Stack interpolated x and y coordinates\n    return segments\n\n",
  "unprocess_testcases": "segment1 = np.array([[0, 0], [2.5, 2.5], [5, 5], [7.5, 7.5], [10, 10]])\nsegment2 = np.array([[0, 0], [1, 5], [2, 10], [3, 15], [4, 20], [5, 25]])\nsegments = [segment1, segment2]\nassert((SOLUTION_SIGNATURE(segments, n=100)[i] == resample_segments(segments, n=100)[i]).all() for i in range(len(resample_segments(segments, n=100))))\n\nsegment = np.array([[0, 0], [1, 1]])\nsegments = [segment]\nassert((SOLUTION_SIGNATURE(segments, n=3)[i] == resample_segments(segments, n=3)[i]).all for i in range(len(resample_segments(segments, n=3))))\n\nsegment = np.array([[0, 0], [1, 1]])\nsegments = [segment]\nassert((SOLUTION_SIGNATURE(segments, n=1)[i] == resample_segments(segments, n=1)[i]).all() for i in range(len(resample_segments(segments, n=1))))\n\nrepeated_point_segment = np.array([[1, 1]] * 5)\ncircle = np.array([[np.cos(theta), np.sin(theta)] for theta in np.linspace(0, 2 * np.pi, 100, endpoint=False)])\nsegments = [repeated_point_segment, circle]\nassert((SOLUTION_SIGNATURE(segments, n=50)[i] == resample_segments(segments, n=50)[i]).all() for i in range(len(resample_segments(segments, n=50))))\n"
 },
 {
  "task_id": "RWPB/78",
  "prompt": "import pandas\n\ndef check_all_columns_numeric(df):\n    \"\"\"\n    Check if all columns in a DataFrame are numeric. Return True if so, False otherwise.\n    \"\"\"\n",
  "solution": "Certainly! Here's the completed code to check if all columns in a DataFrame are numeric:\n\n```python\nimport pandas as pd\n\ndef check_all_columns_numeric(df):\n    \"\"\"\n    Check if all columns in a DataFrame are numeric. Return True if so, False otherwise.\n    \"\"\"\n    numeric_columns = df.select_dtypes(include=[pd.np.number]).columns\n    return len(numeric_columns) == len(df.columns)\n```\n\nExplanation:\n\n1. We import the `pandas` library as `pd`.\n\n2. Inside the `check_all_columns_numeric` function, we use the `select_dtypes` method of the DataFrame `df` to select only the columns that have a numeric data type. The `include` parameter is set to `[pd.np.number]`, which includes all numeric data types (int, float, etc.).\n\n3. We store the selected numeric columns in the `numeric_columns` variable.\n\n4. Finally, we compare the length of `numeric_columns` with the total number of columns in the DataFrame `df.columns`. If they are equal, it means all columns are numeric, and we return `True`. Otherwise, we return `False`.\n\nThis function will return `True` if all columns in the DataFrame are numeric, and `False` if any column is non-numeric.\n\nNote: This solution assumes that the input `df` is a valid pandas DataFrame. If you want to handle the case where `df` might not be a DataFrame, you can add an additional check at the beginning of the function to ensure it is a DataFrame before proceeding.",
  "canonical_solution": "    \n    return df.select_dtypes(include=[int, float]).shape[1] == df.shape[1]\n\n\n",
  "unprocess_testcases": "\nimport pandas as pd\ndf_numeric = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4.1, 5.2, 6.3]\n})\nassert(SOLUTION_SIGNATURE(df_numeric) == check_all_columns_numeric(df_numeric))\n\ndf_mixed = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': ['x', 'y', 'z']\n})\nassert(SOLUTION_SIGNATURE(df_mixed) == check_all_columns_numeric(df_mixed))\n\ndf_empty = pd.DataFrame()\nassert(SOLUTION_SIGNATURE(df_empty) == check_all_columns_numeric(df_empty))\n\ndf_varied_types = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [1.5, 2.5, 3.5],\n    'C': [True, False, True]\n})\nassert(SOLUTION_SIGNATURE(df_varied_types) == check_all_columns_numeric(df_varied_types))\n"
 },
 {
  "task_id": "RWPB/29",
  "prompt": "import torch\n\ndef iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    \"\"\"\n    Computes the Intersection over Union (IoU) for the foreground class in binary segmentation.\n\n    Args:\n        preds (Tensor): Predictions from the model, where 1 indicates foreground and 0 indicates background.\n        labels (Tensor): Ground truth binary masks corresponding to the predictions.\n        EMPTY (float, optional): The value to return if there is no union between predicted and ground truth pixels.\n                                 Default is 1.0.\n        ignore (int, optional): Label value to ignore during IoU calculation. This is useful for ignoring certain\n                                pixels in evaluation, such as boundary or undefined areas.\n        per_image (bool, optional): If True, calculates IoU independently for each image and then averages them.\n                                    If False, calculates IoU across the entire batch as a whole. Default is True.\n\n    Returns:\n        float: The IoU score multiplied by 100 to convert it to a percentage.\n\n    This function calculates the IoU score for the foreground class, accounting for any ignore labels\n    and handling both individual image evaluations and batch-wide evaluations. The IoU is calculated\n    by determining the intersection and union of the predicted and actual foreground areas, and handling\n    cases where the union might be zero.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport torch\n\ndef iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    \"\"\"\n    Computes the Intersection over Union (IoU) for the foreground class in binary segmentation.\n\n    Args:\n        preds (Tensor): Predictions from the model, where 1 indicates foreground and 0 indicates background.\n        labels (Tensor): Ground truth binary masks corresponding to the predictions.\n        EMPTY (float, optional): The value to return if there is no union between predicted and ground truth pixels.\n                                 Default is 1.0.\n        ignore (int, optional): Label value to ignore during IoU calculation. This is useful for ignoring certain\n                                pixels in evaluation, such as boundary or undefined areas.\n        per_image (bool, optional): If True, calculates IoU independently for each image and then averages them.\n                                    If False, calculates IoU across the entire batch as a whole. Default is True.\n\n    Returns:\n        float: The IoU score multiplied by 100 to convert it to a percentage.\n\n    This function calculates the IoU score for the foreground class, accounting for any ignore labels\n    and handling both individual image evaluations and batch-wide evaluations. The IoU is calculated\n    by determining the intersection and union of the predicted and actual foreground areas, and handling\n    cases where the union might be zero.\n    \"\"\"\n    if per_image:\n        ious = []\n        for pred, label in zip(preds, labels):\n            iou = iou_binary(pred.unsqueeze(0), label.unsqueeze(0), EMPTY=EMPTY, ignore=ignore, per_image=False)\n            ious.append(iou)\n        iou = torch.mean(torch.tensor(ious))\n    else:\n        if ignore is not None:\n            preds = preds[labels != ignore]\n            labels = labels[labels != ignore]\n\n        intersection = torch.logical_and(preds, labels).sum()\n        union = torch.logical_or(preds, labels).sum()\n\n        if union == 0:\n            iou = EMPTY\n        else:\n            iou = intersection.float() / union.float()\n\n    return iou * 100\n```\n\nExplanation:\n\n1. The function first checks if `per_image` is True. If it is, it calculates the IoU for each image independently by recursively calling `iou_binary` with `per_image=False` for each image in the batch. The resulting IoU scores are collected in the `ious` list, and the average IoU is computed using `torch.mean()`.\n\n2. If `per_image` is False or if the function is called recursively for individual images, it proceeds to calculate the IoU for the entire batch or a single image.\n\n3. If an `ignore` label is provided, the function filters out the predictions and labels corresponding to the ignore label using boolean indexing.\n\n4. The intersection between the predicted and actual foreground areas is calculated using `torch.logical_and()` and summed using `sum()`.\n\n5. The union of the predicted and actual foreground areas is calculated using `torch.logical_or()` and summed using `sum()`.\n\n6. If the union is zero, the function returns the `EMPTY` value specified as an argument (default is 1.0).\n\n7. Otherwise, the IoU is calculated by dividing the intersection by the union and converting the result to a float using `float()`.\n\n8. Finally, the IoU score is multiplied by 100 to convert it to a percentage before returning it.\n\nThis function provides a flexible way to calculate the IoU for binary segmentation tasks, allowing for both per-image and batch-wide evaluations, and handling cases where certain pixels should be ignored or where the union might be zero.",
  "canonical_solution": "\n    # Helper function to calculate mean\n    def mean(l):\n        return sum(l) / len(l) if l else EMPTY\n\n    # Handle batch-wide calculation by wrapping predictions and labels in tuples if per_image is False\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n\n    ious = []\n    for pred, label in zip(preds, labels):\n        # Calculate intersection and union for each image\n        intersection = ((label == 1) & (pred == 1)).sum()\n        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n\n        # Calculate IoU, handling cases with no union\n        if not union:\n            iou = EMPTY\n        else:\n            iou = float(intersection) / float(union)\n\n        ious.append(iou)\n\n    # Calculate the mean IoU across images if per_image, or for the batch\n    iou = mean(ious)\n    return 100 * iou  # Return IoU as a percentage\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(torch.tensor([[1,0,1,0],[0,1,0,1]]), torch.tensor([[1,0,1,0],[0,1,0,1]]), 1.0, None, True) == iou_binary(torch.tensor([[1,0,1,0],[0,1,0,1]]), torch.tensor([[1,0,1,0],[0,1,0,1]]), 1.0, None, True))\nassert(SOLUTION_SIGNATURE(torch.tensor([[0,0,0,0],[1,1,1,1]]), torch.tensor([[0,0,0,0],[0,0,0,0]]), 0.5, 1, True) == iou_binary(torch.tensor([[0,0,0,0],[1,1,1,1]]), torch.tensor([[0,0,0,0],[0,0,0,0]]), 0.5, 1, True))\nassert(SOLUTION_SIGNATURE(torch.tensor([[1,1,0,0],[0,0,1,1]]), torch.tensor([[1,0,0,1],[1,1,0,0]]), 1.0, 0, False) == iou_binary(torch.tensor([[1,1,0,0],[0,0,1,1]]), torch.tensor([[1,0,0,1],[1,1,0,0]]), 1.0, 0, False))\n"
 },
 {
  "task_id": "RWPB/5",
  "prompt": "def seconds_to_hh_mm_ss(seconds):\n    \"\"\"\n    Converts a given number of seconds to a string in hh:mm:ss format.\n\n    Args:\n        seconds: An integer representing the number of seconds.\n\n    Returns:\n        A string in hh:mm:ss format, or \"00:00:00\" if seconds is negative.\n    \"\"\"\n",
  "solution": "Certainly! Here's the completed code to solve the problem:\n\n```python\ndef seconds_to_hh_mm_ss(seconds):\n    \"\"\"\n    Converts a given number of seconds to a string in hh:mm:ss format.\n\n    Args:\n        seconds: An integer representing the number of seconds.\n\n    Returns:\n        A string in hh:mm:ss format, or \"00:00:00\" if seconds is negative.\n    \"\"\"\n    if seconds < 0:\n        return \"00:00:00\"\n\n    hours = seconds // 3600\n    seconds %= 3600\n    minutes = seconds // 60\n    seconds %= 60\n\n    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n```\n\nExplanation:\n\n1. We start by checking if the `seconds` argument is negative. If it is, we return the string \"00:00:00\" as specified in the problem statement.\n\n2. If `seconds` is non-negative, we proceed with the conversion.\n\n3. We calculate the number of hours by dividing `seconds` by 3600 (the number of seconds in an hour) using integer division (`//`). This gives us the whole number of hours.\n\n4. We update `seconds` by taking the remainder of `seconds` divided by 3600 using the modulo operator (`%`). This gives us the remaining seconds after extracting the hours.\n\n5. We calculate the number of minutes by dividing the remaining `seconds` by 60 (the number of seconds in a minute) using integer division (`//`). This gives us the whole number of minutes.\n\n6. We update `seconds` again by taking the remainder of `seconds` divided by 60 using the modulo operator (`%`). This gives us the remaining seconds after extracting the minutes.\n\n7. Finally, we return a formatted string using an f-string. The `{hours:02d}`, `{minutes:02d}`, and `{seconds:02d}` syntax ensures that each value is padded with leading zeros to a width of 2 digits.\n\nThis code will convert the given number of seconds to a string in the format \"hh:mm:ss\". If the input is negative, it will return \"00:00:00\" as requested.",
  "canonical_solution": "    if seconds < 0:\n        return \"00:00:00\"\n\n    hours = seconds // 3600  # Get hours by dividing by total seconds in an hour\n    seconds %= 3600  # Get remaining seconds after hours calculation\n\n    minutes = seconds // 60  # Get minutes by dividing by total seconds in a minute\n    seconds %= 60  # Get remaining seconds after minutes calculation\n\n    # Format hour, minute, and second values with leading zeros\n    hours_str = f\"{hours:02d}\"\n    minutes_str = f\"{minutes:02d}\"\n    seconds_str = f\"{seconds:02d}\"\n\n    return f\"{hours_str}:{minutes_str}:{seconds_str}\"\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(-10) == seconds_to_hh_mm_ss(-10))\nassert(SOLUTION_SIGNATURE(3661) == seconds_to_hh_mm_ss(3661))\nassert(SOLUTION_SIGNATURE(7200) == seconds_to_hh_mm_ss(7200))"
 },
 {
  "task_id": "RWPB/38",
  "prompt": "from typing import List\n\ndef extract_imports_from_lines(lines: List[str]) -> str:\n    \"\"\"\n    Extracts import statements from a list of Python code lines.\n\n    Args:\n        lines (List[str]): A list of strings, each representing a line of Python code.\n\n    Returns:\n        str: A single string containing all extracted import and from-import lines, separated by new lines.\n\n    This function filters through a list of Python code lines, identifying and extracting lines that are import statements.\n    It supports both 'import' and 'from ... import ...' formats. This is useful for analyzing or consolidating imports\n    from a set of Python code lines, such as when preparing a script or checking for consistency and redundancy in imports.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without unit test cases:\n\n```python\nfrom typing import List\n\ndef extract_imports_from_lines(lines: List[str]) -> str:\n    \"\"\"\n    Extracts import statements from a list of Python code lines.\n\n    Args:\n        lines (List[str]): A list of strings, each representing a line of Python code.\n\n    Returns:\n        str: A single string containing all extracted import and from-import lines, separated by new lines.\n\n    This function filters through a list of Python code lines, identifying and extracting lines that are import statements.\n    It supports both 'import' and 'from ... import ...' formats. This is useful for analyzing or consolidating imports\n    from a set of Python code lines, such as when preparing a script or checking for consistency and redundancy in imports.\n    \"\"\"\n    import_lines = []\n    for line in lines:\n        line = line.strip()\n        if line.startswith('import ') or line.startswith('from '):\n            import_lines.append(line)\n    return '\\n'.join(import_lines)\n```\n\nThis implementation does the following:\n\n1. It initializes an empty list called `import_lines` to store the extracted import statements.\n\n2. It iterates over each line in the `lines` list using a `for` loop.\n\n3. For each line, it applies the `strip()` method to remove leading and trailing whitespace.\n\n4. It checks if the stripped line starts with either `'import '` or `'from '` using the `startswith()` method. This identifies lines that are import statements.\n\n5. If the line is an import statement, it appends the line to the `import_lines` list.\n\n6. After iterating through all the lines, it joins the `import_lines` list into a single string using the `'\\n'.join()` method. This creates a string containing all the extracted import statements, separated by new lines.\n\n7. Finally, it returns the resulting string containing the extracted import statements.\n\nThis function effectively filters through the list of Python code lines and extracts only the lines that are import statements. It supports both the `import` and `from ... import ...` formats. The extracted import statements are returned as a single string, with each import statement on a separate line.",
  "canonical_solution": "\n    # Use list comprehension to filter out lines that start with 'import' or 'from', indicating import statements\n    return \"\\n\".join(\n        [line for line in lines if line.startswith(\"from\") or line.startswith(\"import\")]\n    )\n\n\n\n",
  "unprocess_testcases": "a = [\n    \"import os\",\n    \"x = 10\",                 # Non-import line\n    \"from sys import argv\",\n    \"print('Hello World!')\",  # Non-import line\n    \"import math\"\n]\nassert(SOLUTION_SIGNATURE(a) == extract_imports_from_lines(a))\n\nb = [\n    \"Import os\",                  # Incorrect case\n    \"# from datetime import datetime\",  # Commented import\n    \"from collections import deque\",    # Correct import\n    \"IMPORT sys\",                 # Incorrect case\n    \"from math import pi\"         # Correct import\n]\nassert(SOLUTION_SIGNATURE(b) == extract_imports_from_lines(b))\n\nc = [\n    \"from numpy import array, random\",\n    \"import scipy.stats as stats\",  # Aliased import\n    \"from os.path import join, exists\",  # Multiple imports from same module\n    \"x = 'import should not be detected'\",  # Import within a string\n    \"import pandas as pd\"  # Aliased import\n]\nassert(SOLUTION_SIGNATURE(c) == extract_imports_from_lines(c))\n"
 },
 {
  "task_id": "RWPB/69",
  "prompt": "import numpy as np\nimport math\n\ndef fovx_to_fovy(fovx, aspect):\n    \"\"\"\n    Converts the horizontal field of view (FoVx) to the vertical field of view (FoVy) based on the aspect ratio.\n\n    Parameters:\n    fovx (float): The horizontal field of view in radians.\n    aspect (float): The aspect ratio of the image (width/height).\n\n    Returns:\n    float: The vertical field of view in radians.\n\n    Process:\n    1. Calculate the tangent of half the horizontal field of view (FoVx/2).\n    2. Divide this value by the aspect ratio to get the tangent of half the vertical field of view.\n    3. Use the arctangent function to find half the vertical field of view from its tangent.\n    4. Multiply by 2 to obtain the full vertical field of view (FoVy).\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of converting the horizontal field of view (FoVx) to the vertical field of view (FoVy) based on the aspect ratio:\n\n```python\nimport numpy as np\nimport math\n\ndef fovx_to_fovy(fovx, aspect):\n    \"\"\"\n    Converts the horizontal field of view (FoVx) to the vertical field of view (FoVy) based on the aspect ratio.\n\n    Parameters:\n    fovx (float): The horizontal field of view in radians.\n    aspect (float): The aspect ratio of the image (width/height).\n\n    Returns:\n    float: The vertical field of view in radians.\n\n    Process:\n    1. Calculate the tangent of half the horizontal field of view (FoVx/2).\n    2. Divide this value by the aspect ratio to get the tangent of half the vertical field of view.\n    3. Use the arctangent function to find half the vertical field of view from its tangent.\n    4. Multiply by 2 to obtain the full vertical field of view (FoVy).\n    \"\"\"\n    # Step 1: Calculate the tangent of half the horizontal field of view\n    tan_half_fovx = math.tan(fovx / 2)\n    \n    # Step 2: Divide by the aspect ratio to get the tangent of half the vertical field of view\n    tan_half_fovy = tan_half_fovx / aspect\n    \n    # Step 3: Use the arctangent function to find half the vertical field of view\n    half_fovy = math.atan(tan_half_fovy)\n    \n    # Step 4: Multiply by 2 to obtain the full vertical field of view\n    fovy = 2 * half_fovy\n    \n    return fovy\n```\n\nThis code follows the process outlined in the comments to convert the horizontal field of view (FoVx) to the vertical field of view (FoVy) based on the aspect ratio.\n\n1. It calculates the tangent of half the horizontal field of view (FoVx/2) using the `math.tan()` function.\n2. It divides this value by the aspect ratio to get the tangent of half the vertical field of view.\n3. It uses the `math.atan()` function to find half the vertical field of view from its tangent.\n4. Finally, it multiplies the result by 2 to obtain the full vertical field of view (FoVy).\n\nThe function takes two parameters: `fovx`, which is the horizontal field of view in radians, and `aspect`, which is the aspect ratio of the image (width/height). It returns the vertical field of view in radians.",
  "canonical_solution": "    \n    return np.arctan(np.tan(fovx / 2) / aspect) * 2.0\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(math.pi/4, 16/9) == fovx_to_fovy(math.pi/4, 16/9))\nassert(SOLUTION_SIGNATURE(math.pi/3, 0.5) == fovx_to_fovy(math.pi/3, 0.5))\nassert(SOLUTION_SIGNATURE(math.pi, 1) == fovx_to_fovy(math.pi, 1))\n"
 },
 {
  "task_id": "RWPB/134",
  "prompt": "def select_best_resolution(original_size, possible_resolutions):\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        original_size (tuple): The original size of the image in the format (width, height).\n        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2), ...].\n\n    Returns:\n        tuple: The best fit resolution in the format (width, height).\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without unit test cases:\n\n```python\ndef select_best_resolution(original_size, possible_resolutions):\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        original_size (tuple): The original size of the image in the format (width, height).\n        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2), ...].\n\n    Returns:\n        tuple: The best fit resolution in the format (width, height).\n    \"\"\"\n    # Calculate the aspect ratio of the original size\n    original_aspect_ratio = original_size[0] / original_size[1]\n\n    # Initialize variables to store the best fit resolution\n    best_fit_resolution = None\n    min_aspect_ratio_diff = float('inf')\n\n    # Iterate over each possible resolution\n    for resolution in possible_resolutions:\n        # Calculate the aspect ratio of the current resolution\n        current_aspect_ratio = resolution[0] / resolution[1]\n\n        # Calculate the absolute difference between the current and original aspect ratios\n        aspect_ratio_diff = abs(current_aspect_ratio - original_aspect_ratio)\n\n        # Check if the current resolution has a smaller aspect ratio difference\n        if aspect_ratio_diff < min_aspect_ratio_diff:\n            min_aspect_ratio_diff = aspect_ratio_diff\n            best_fit_resolution = resolution\n\n    return best_fit_resolution\n```\n\nThis code does the following:\n\n1. It calculates the aspect ratio of the original size by dividing the width by the height.\n\n2. It initializes variables to store the best fit resolution and the minimum aspect ratio difference. The `best_fit_resolution` is initially set to `None`, and the `min_aspect_ratio_diff` is set to positive infinity using `float('inf')`.\n\n3. It iterates over each possible resolution in the `possible_resolutions` list.\n\n4. For each resolution, it calculates the aspect ratio by dividing the width by the height.\n\n5. It calculates the absolute difference between the current resolution's aspect ratio and the original aspect ratio using `abs(current_aspect_ratio - original_aspect_ratio)`.\n\n6. It checks if the current resolution has a smaller aspect ratio difference compared to the previously stored minimum aspect ratio difference.\n\n7. If the current resolution has a smaller aspect ratio difference, it updates the `min_aspect_ratio_diff` with the new minimum value and updates the `best_fit_resolution` to the current resolution.\n\n8. After iterating over all possible resolutions, it returns the `best_fit_resolution`, which represents the resolution with the aspect ratio closest to the original size.\n\nThis code selects the best resolution from the list of possible resolutions based on the aspect ratio that is closest to the original size's aspect ratio.",
  "canonical_solution": "    \n    original_width, original_height = original_size\n    best_fit = None\n    max_effective_resolution = 0\n    min_wasted_resolution = float('inf')\n\n    for width, height in possible_resolutions:\n        scale = min(width / original_width, height / original_height)\n        downscaled_width, downscaled_height = int(original_width * scale), int(original_height * scale)\n        effective_resolution = min(downscaled_width * downscaled_height, original_width * original_height)\n        wasted_resolution = (width * height) - effective_resolution\n\n        if effective_resolution > max_effective_resolution or (effective_resolution == max_effective_resolution and wasted_resolution < min_wasted_resolution):\n            max_effective_resolution = effective_resolution\n            min_wasted_resolution = wasted_resolution\n            best_fit = (width, height)\n\n    return best_fit\n\n\n",
  "unprocess_testcases": "original_size = (1920, 1080)\npossible_resolutions = [(800, 600), (1920, 1080), (2560, 1440)]\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[0] == select_best_resolution(original_size, possible_resolutions)[0])\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[1] == select_best_resolution(original_size, possible_resolutions)[1])\n\noriginal_size = (1024, 768)\npossible_resolutions = [(800, 600), (1600, 1200), (1280, 960)]\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[0] == select_best_resolution(original_size, possible_resolutions)[0])\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[1] == select_best_resolution(original_size, possible_resolutions)[1])\n\n\noriginal_size = (1280, 720)\npossible_resolutions = [(1024, 768), (1920, 1080), (1600, 900)]\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[0] == select_best_resolution(original_size, possible_resolutions)[0])\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[1] == select_best_resolution(original_size, possible_resolutions)[1])\n\n\n"
 },
 {
  "task_id": "RWPB/100",
  "prompt": "import torch\n\ndef project_onto_planes(planes: torch.Tensor, coordinates: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Projects 3D coordinates onto a batch of 2D planes, returning 2D plane coordinates.\n\n    Args:\n        planes (torch.Tensor): Tensor defining plane axes with shape (n_planes, 3, 3).\n        coordinates (torch.Tensor): Tensor of 3D coordinates with shape (N, M, 3).\n\n    Returns:\n        torch.Tensor: Projected 2D coordinates with shape (N*n_planes, M, 2).\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport torch\n\ndef project_onto_planes(planes: torch.Tensor, coordinates: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Projects 3D coordinates onto a batch of 2D planes, returning 2D plane coordinates.\n\n    Args:\n        planes (torch.Tensor): Tensor defining plane axes with shape (n_planes, 3, 3).\n        coordinates (torch.Tensor): Tensor of 3D coordinates with shape (N, M, 3).\n\n    Returns:\n        torch.Tensor: Projected 2D coordinates with shape (N*n_planes, M, 2).\n    \"\"\"\n    N, M, _ = coordinates.shape\n    n_planes = planes.shape[0]\n\n    # Reshape coordinates to (N, 1, M, 3) to broadcast with planes\n    coordinates = coordinates.unsqueeze(1)\n\n    # Expand planes to match coordinates shape (N, n_planes, 1, 3, 3)\n    planes = planes.view(1, n_planes, 1, 3, 3).expand(N, n_planes, 1, 3, 3)\n\n    # Transform coordinates to plane space\n    transformed_coords = torch.matmul(coordinates, planes)\n\n    # Extract x and y coordinates from the transformed coordinates\n    x_coords = transformed_coords[..., 0, 0]\n    y_coords = transformed_coords[..., 0, 1]\n\n    # Stack x and y coordinates along the last dimension\n    projected_coords = torch.stack([x_coords, y_coords], dim=-1)\n\n    # Reshape the projected coordinates to (N*n_planes, M, 2)\n    projected_coords = projected_coords.view(N * n_planes, M, 2)\n\n    return projected_coords\n```\n\nExplanation:\n1. We extract the dimensions `N`, `M`, and `n_planes` from the input tensors.\n\n2. We reshape `coordinates` to have shape `(N, 1, M, 3)` by adding an extra dimension at index 1. This allows us to broadcast `coordinates` with `planes` along the `n_planes` dimension.\n\n3. We expand `planes` to match the shape of `coordinates` by adding dimensions and expanding them accordingly. The resulting shape is `(N, n_planes, 1, 3, 3)`.\n\n4. We perform a matrix multiplication between `coordinates` and `planes` using `torch.matmul()`. This transforms the coordinates to the plane space. The resulting shape is `(N, n_planes, M, 3)`.\n\n5. We extract the x and y coordinates from the transformed coordinates by indexing the appropriate dimensions.\n\n6. We stack the x and y coordinates along the last dimension using `torch.stack()` to create the projected 2D coordinates with shape `(N, n_planes, M, 2)`.\n\n7. Finally, we reshape the projected coordinates to `(N*n_planes, M, 2)` by flattening the first two dimensions.\n\n8. The function returns the projected 2D coordinates.\n\nThis code projects the given 3D coordinates onto the specified planes and returns the corresponding 2D plane coordinates.",
  "canonical_solution": "    \n    N, M, C = coordinates.shape\n    n_planes, _, _ = planes.shape\n    coordinates = coordinates.unsqueeze(1).expand(-1, n_planes, -1, -1).reshape(N*n_planes, M, 3)\n    inv_planes = torch.linalg.inv(planes).unsqueeze(0).expand(N, -1, -1, -1).reshape(N*n_planes, 3, 3)\n    projections = torch.bmm(coordinates, inv_planes)\n    return projections[..., :2]\n\n\n",
  "unprocess_testcases": "planes = torch.tensor([[[1, 0, 0], [0, 1, 0], [0, 0, 1]], \n                       [[0, 1, 0], [1, 0, 0], [0, 0, 1]]], dtype=torch.float32)\ncoordinates = torch.tensor([[[1, 2, 3], [4, 5, 6]]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(planes, coordinates) == project_onto_planes(planes, coordinates)).all())\n\nplanes = torch.tensor([[[1, 0, 0], [0, 1, 0], [0, 0, 1]], \n                       [[0, 1, 0], [1, 0, 0], [0, 0, 1]],\n                       [[1, 0, 0], [0, 0, 1], [0, 1, 0]]], dtype=torch.float32)\ncoordinates = torch.tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n                            [[9, 8, 7], [6, 5, 4], [3, 2, 1]]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(planes, coordinates) == project_onto_planes(planes, coordinates)).all())\n\nplanes = torch.tensor([[[1, 0, 0], [0, 1, 0], [0, 0, 1]]], dtype=torch.float32)\ncoordinates = torch.tensor([[[0, -1, -2]]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(planes, coordinates) == project_onto_planes(planes, coordinates)).all())\n"
 },
 {
  "task_id": "RWPB/110",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\n\ndef find_intersection(plane_normal,plane_point,ray_direction,ray_point):\n    \"\"\"\n    Finds the intersection point of a line and a plane.\n\n    Args:\n        - plane_normal: numpy array, representing a normal vector of the plane.\n        - plane_point: numpy array, representing a point on the plane.\n        - ray_direction: numpy array, representing a direction vector of the line.\n        - ray_point: numpy array, representing a point on the line.\n\n    Returns:\n        Psi: numpy array or None, the intersection point of the line and the plane.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of finding the intersection point of a line and a plane:\n\n```python\nimport math\nimport numpy as np\n\ndef find_intersection(plane_normal, plane_point, ray_direction, ray_point):\n    \"\"\"\n    Finds the intersection point of a line and a plane.\n\n    Args:\n        - plane_normal: numpy array, representing a normal vector of the plane.\n        - plane_point: numpy array, representing a point on the plane.\n        - ray_direction: numpy array, representing a direction vector of the line.\n        - ray_point: numpy array, representing a point on the line.\n\n    Returns:\n        Psi: numpy array or None, the intersection point of the line and the plane.\n    \"\"\"\n    # Calculate the dot product of the plane normal and the ray direction\n    denominator = np.dot(plane_normal, ray_direction)\n\n    # Check if the line is parallel to the plane\n    if abs(denominator) < 1e-6:\n        return None\n\n    # Calculate the vector from the ray point to a point on the plane\n    vector = plane_point - ray_point\n\n    # Calculate the dot product of the plane normal and the vector\n    numerator = np.dot(plane_normal, vector)\n\n    # Calculate the distance along the ray direction to reach the plane\n    t = numerator / denominator\n\n    # Calculate the intersection point\n    Psi = ray_point + t * ray_direction\n\n    return Psi\n```\n\nExplanation:\n\n1. The function takes four parameters: `plane_normal` (the normal vector of the plane), `plane_point` (a point on the plane), `ray_direction` (the direction vector of the line), and `ray_point` (a point on the line).\n\n2. It calculates the dot product of the plane normal and the ray direction and stores it in the `denominator` variable. This value determines whether the line is parallel to the plane or not.\n\n3. If the absolute value of the `denominator` is very small (less than 1e-6), it means the line is parallel to the plane, and there is no intersection. In this case, the function returns `None`.\n\n4. If the line is not parallel to the plane, the function calculates the vector from the ray point to a point on the plane by subtracting `ray_point` from `plane_point` and stores it in the `vector` variable.\n\n5. It then calculates the dot product of the plane normal and the `vector` and stores it in the `numerator` variable.\n\n6. The distance along the ray direction to reach the plane is calculated by dividing the `numerator` by the `denominator` and stored in the variable `t`.\n\n7. Finally, the intersection point `Psi` is calculated by adding the scaled ray direction (`t * ray_direction`) to the ray point (`ray_point`).\n\n8. The function returns the intersection point `Psi` as a numpy array.\n\nThis code solves the problem of finding the intersection point between a line and a plane using the given parameters. If the line is parallel to the plane, it returns `None` to indicate no intersection.",
  "canonical_solution": "    \n    epsilon=1e-6\n    ndotu = plane_normal.dot(ray_direction)\n    if abs(ndotu) < epsilon:\n        # no intersection or line is within plane\n        return None\n    w = ray_point - plane_point\n    si = -plane_normal.dot(w) / ndotu\n    Psi = w + si * ray_direction + plane_point #intersections\n    return Psi\n\n\n",
  "unprocess_testcases": "plane_normal = np.array([0, 0, 1])\nplane_point = np.array([0, 0, 5])\nray_direction = np.array([1, 1, -1])\nray_point = np.array([1, 1, 10])\nassert((SOLUTION_SIGNATURE(plane_point, plane_point, ray_direction, ray_point) == find_intersection(plane_point, plane_point, ray_direction, ray_point)).all())\n\nplane_normal = np.array([0, 0, 1])\nplane_point = np.array([0, 3, 5])\nray_direction = np.array([1, 1, -1])\nray_point = np.array([1, 1, 9])\nassert((SOLUTION_SIGNATURE(plane_point, plane_point, ray_direction, ray_point) == find_intersection(plane_point, plane_point, ray_direction, ray_point)).all())\n\nplane_normal = np.array([0, 1, 0])\nplane_point = np.array([0, 2, 0])\nray_direction = np.array([1, 0, 0])\nray_point = np.array([0, 0, 0])\nassert(SOLUTION_SIGNATURE(plane_point, plane_point, ray_direction, ray_point) == find_intersection(plane_point, plane_point, ray_direction, ray_point))\n\nplane_normal = np.array([0, 0, 1])\nplane_point = np.array([0, 0, 0])\nray_direction = np.array([1, 1, 0])\nray_point = np.array([1, 1, 0])\nassert(SOLUTION_SIGNATURE(plane_point, plane_point, ray_direction, ray_point) == find_intersection(plane_point, plane_point, ray_direction, ray_point))\n"
 },
 {
  "task_id": "RWPB/124",
  "prompt": "import math\nimport torch\n\n\ndef make_divisible(x, divisor):\n    \"\"\"\n    Adjusts x to the nearest value that is divisible by divisor.\n\n    Arguments:\n    x : int or float\n        The value to be made divisible.\n    divisor : int or torch.Tensor\n        The divisor to be used. If a torch.Tensor is provided, uses its maximum value.\n\n    Returns:\n    int\n        The nearest value to x that is divisible by the specified divisor.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport math\nimport torch\n\n\ndef make_divisible(x, divisor):\n    \"\"\"\n    Adjusts x to the nearest value that is divisible by divisor.\n\n    Arguments:\n    x : int or float\n        The value to be made divisible.\n    divisor : int or torch.Tensor\n        The divisor to be used. If a torch.Tensor is provided, uses its maximum value.\n\n    Returns:\n    int\n        The nearest value to x that is divisible by the specified divisor.\n    \"\"\"\n    if isinstance(divisor, torch.Tensor):\n        divisor = divisor.max()\n\n    return int(math.ceil(x / divisor) * divisor)\n```\n\nExplanation:\n1. The function first checks if the `divisor` is a `torch.Tensor`. If it is, it takes the maximum value of the tensor using `divisor.max()`. This ensures that if a tensor is provided as the divisor, only its maximum value is used.\n\n2. The function then calculates the nearest value to `x` that is divisible by the `divisor`. It does this by dividing `x` by the `divisor` using `x / divisor`, rounding up the result to the nearest integer using `math.ceil()`, and then multiplying the result by the `divisor`.\n\n3. Finally, the function returns the calculated value as an integer using `int()`.\n\nThis implementation handles both integer and float values for `x` and supports both integer and `torch.Tensor` values for `divisor`. It adjusts `x` to the nearest value that is divisible by the specified `divisor`.",
  "canonical_solution": "    \n    # Convert divisor to int if it is a torch.Tensor, taking the maximum value in the tensor\n    if isinstance(divisor, torch.Tensor):\n        divisor = int(divisor.max())  # Convert to int\n\n    # Compute the nearest value to x that is divisible by divisor\n    return math.ceil(x / divisor) * divisor  # Use ceil to ensure it rounds up to the nearest divisor\n\n",
  "unprocess_testcases": "x = 23\ndivisor = 5\nassert(SOLUTION_SIGNATURE(x, divisor) == make_divisible(x, divisor))\n\nx = 14.7\ndivisor = 3\nassert(SOLUTION_SIGNATURE(x, divisor) == make_divisible(x, divisor))\n\nx = 100\ndivisor = torch.Tensor([2,8,10])\nassert(SOLUTION_SIGNATURE(x, divisor) == make_divisible(x, divisor))\n"
 },
 {
  "task_id": "RWPB/79",
  "prompt": "import numpy as np\n\ndef pr_from_grav(grav):\n  \"\"\"\n  Compute pitch and roll angles from a matrix of gravity vectors in body frame.\n  Args:\n      - grav: numpy.ndarray, matrix of gravity vectors in body frame (shape: (n, 3))\n  Returns:\n      - pitches: numpy.ndarray, pitch angles (shape: (n,))\n      - rolls: numpy.ndarray, roll angles (shape: (n,))\n  \"\"\"\n",
  "solution": "Certainly! Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport numpy as np\n\ndef pr_from_grav(grav):\n    \"\"\"\n    Compute pitch and roll angles from a matrix of gravity vectors in body frame.\n    Args:\n        - grav: numpy.ndarray, matrix of gravity vectors in body frame (shape: (n, 3))\n    Returns:\n        - pitches: numpy.ndarray, pitch angles (shape: (n,))\n        - rolls: numpy.ndarray, roll angles (shape: (n,))\n    \"\"\"\n    # Extract the x, y, and z components of the gravity vectors\n    gx = grav[:, 0]\n    gy = grav[:, 1]\n    gz = grav[:, 2]\n\n    # Compute pitch angles using the arctan2 function\n    pitches = np.arctan2(-gx, np.sqrt(gy**2 + gz**2))\n\n    # Compute roll angles using the arctan2 function\n    rolls = np.arctan2(gy, gz)\n\n    return pitches, rolls\n```\n\nExplanation:\n1. The function takes a matrix `grav` of gravity vectors in the body frame as input. The shape of `grav` is assumed to be `(n, 3)`, where `n` is the number of gravity vectors.\n\n2. We extract the x, y, and z components of the gravity vectors using array indexing. `gx`, `gy`, and `gz` are numpy arrays of shape `(n,)` containing the respective components.\n\n3. To compute the pitch angles, we use the `arctan2` function from numpy. The pitch angle is calculated as the arctangent of the negative x-component divided by the square root of the sum of squares of the y and z components. This formula is derived from the definition of pitch angle in the body frame.\n\n4. To compute the roll angles, we use the `arctan2` function again. The roll angle is calculated as the arctangent of the y-component divided by the z-component. This formula is derived from the definition of roll angle in the body frame.\n\n5. Finally, the function returns two numpy arrays: `pitches` containing the computed pitch angles and `rolls` containing the computed roll angles. Both arrays have a shape of `(n,)`.\n\nNote: The `arctan2` function is used instead of `arctan` to handle the case when the denominator is zero and to determine the quadrant of the angle based on the signs of the arguments.\n\nThis code should solve the problem of computing pitch and roll angles from a matrix of gravity vectors in the body frame.",
  "canonical_solution": "  \n  pitches = np.arcsin(-grav[:, 0])\n  rolls = np.arctan2(grav[:, 1], grav[:, 2])\n  return pitches, rolls\n\n\n",
  "unprocess_testcases": "a = np.array([[0, 0, -1],  # No tilt (pitch=roll=0)\n          [0, 0.7071, 0.7071],  # Tilted 45 degrees along Y-axis\n          [0.5, 0, 0.866],  # Tilted 30 degrees along X-axis\n          [0, -1, 0]])  # Upside down along Y-axis\n\nb = np.array([[-1, 0, 0],    # Pitch -90 degrees (pointing directly downward)\n          [1, 0, 0],     # Pitch 90 degrees (pointing directly upward)\n          [0, 1, 0],     # Roll 90 degrees\n          [0, 0, 1],     # No tilt, gravity perfectly aligned\n          [0, -1, 1e-6]]) # Near vertical Y-axis, small Z to avoid zero\n\nc = np.array([[0, 0, 0],  # Zero vector\n          [0, 0, 0],  # Repeated zero vector to test consistent handling\n          [0, 0, 0]])\n\nd = np.array([[1e-12, 0, -1],   # Extremely small x component\n          [0, 1e-12, -1],   # Extremely small y component\n          [1e-12, 1e-12, -1]])  # Small x and y components\n\ne = np.array([[1, 0, 0],   # Along X-axis\n          [0, 1, 0],   # Along Y-axis\n          [0, 0, 1],   # Along Z-axis\n          [0, 0, -1],  # Along -Z-axis\n          [-1, 0, 0],  # Along -X-axis\n          [0, -1, 0]]) # Along -Y-axis\n\nassert((SOLUTION_SIGNATURE(a)[0] == pr_from_grav(a)[0]).all())\nassert((SOLUTION_SIGNATURE(b)[0] == pr_from_grav(b)[0]).all())\nassert((SOLUTION_SIGNATURE(c)[0] == pr_from_grav(c)[0]).all())\nassert((SOLUTION_SIGNATURE(d)[0] == pr_from_grav(d)[0]).all())\nassert((SOLUTION_SIGNATURE(e)[0] == pr_from_grav(e)[0]).all())\nassert((SOLUTION_SIGNATURE(a)[1] == pr_from_grav(a)[1]).all())\nassert((SOLUTION_SIGNATURE(b)[1] == pr_from_grav(b)[1]).all())\nassert((SOLUTION_SIGNATURE(c)[1] == pr_from_grav(c)[1]).all())\nassert((SOLUTION_SIGNATURE(d)[1] == pr_from_grav(d)[1]).all())\nassert((SOLUTION_SIGNATURE(e)[1] == pr_from_grav(e)[1]).all())"
 },
 {
  "task_id": "RWPB/28",
  "prompt": "import numpy as np\nimport math\n\n\ndef mean(l, ignore_nan=False, empty=0):\n    \"\"\"\n    Calculates the mean of a collection of numbers, handling NaN values and empty collections.\n\n    Parameters\n    ----------\n    l : iterable\n        An iterable of numbers (e.g., list, tuple, generator).\n    ignore_nan : bool, optional\n        If True, NaN values are ignored in the calculation. Default is False.\n    empty : int, str, optional\n        Defines the behavior when the input iterable is empty:\n            - If 0, the function returns 0.\n            - If 'raise', the function return a ValueError.\n            - For other values, the function returns the value of `empty`.\n\n    Returns\n    -------\n    float\n        The mean of the input numbers, excluding NaN values if `ignore_nan` is True.\n\n    or\n    ------\n    ValueError\n        If the input iterable is empty and `empty` is set to 'raise'.\n\n    Notes\n    -----\n    This function is similar to the built-in `mean` function in numpy, but it is designed to work\n    with generators and other iterables that do not support indexing.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `mean` function based on the provided problem description:\n\n```python\nimport numpy as np\nimport math\n\n\ndef mean(l, ignore_nan=False, empty=0):\n    \"\"\"\n    Calculates the mean of a collection of numbers, handling NaN values and empty collections.\n\n    Parameters\n    ----------\n    l : iterable\n        An iterable of numbers (e.g., list, tuple, generator).\n    ignore_nan : bool, optional\n        If True, NaN values are ignored in the calculation. Default is False.\n    empty : int, str, optional\n        Defines the behavior when the input iterable is empty:\n            - If 0, the function returns 0.\n            - If 'raise', the function return a ValueError.\n            - For other values, the function returns the value of `empty`.\n\n    Returns\n    -------\n    float\n        The mean of the input numbers, excluding NaN values if `ignore_nan` is True.\n\n    or\n    ------\n    ValueError\n        If the input iterable is empty and `empty` is set to 'raise'.\n\n    Notes\n    -----\n    This function is similar to the built-in `mean` function in numpy, but it is designed to work\n    with generators and other iterables that do not support indexing.\n    \"\"\"\n    total = 0\n    count = 0\n\n    for item in l:\n        if ignore_nan and np.isnan(item):\n            continue\n        total += item\n        count += 1\n\n    if count == 0:\n        if empty == 'raise':\n            raise ValueError(\"Mean of empty iterable and 'empty' set to 'raise'\")\n        return empty\n\n    return total / count\n```\n\nExplanation:\n1. We initialize two variables: `total` to keep track of the sum of the numbers and `count` to keep track of the number of valid (non-NaN) items.\n\n2. We iterate over each item in the input iterable `l`.\n\n3. For each item, we check if `ignore_nan` is True and if the item is NaN using `np.isnan()`. If both conditions are met, we skip the item using the `continue` statement.\n\n4. If the item is not NaN or `ignore_nan` is False, we add the item to the `total` and increment the `count`.\n\n5. After the loop, we check if the `count` is zero, which means the iterable was empty or contained only NaN values (if `ignore_nan` is True).\n\n6. If `count` is zero, we check the value of the `empty` parameter:\n   - If `empty` is set to 'raise', we raise a `ValueError` with an appropriate message.\n   - Otherwise, we return the value of `empty`.\n\n7. If `count` is non-zero, we calculate the mean by dividing the `total` by the `count` and return the result.\n\nThis implementation handles NaN values and empty iterables as specified in the problem description. It also provides flexibility in defining the behavior when the input iterable is empty through the `empty` parameter.",
  "canonical_solution": "\n    # Create an iterator from the input iterable\n    l = iter(l)\n\n    # Initialize the accumulator and the count of elements\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        # If the iterable is empty, handle it based on the value of `empty`\n        if empty == 'raise':\n            return ValueError\n        return empty\n\n    # Sum the elements of the iterable and count them\n    for n, v in enumerate(l, 2):\n        acc += v\n\n    # Calculate the mean, taking care of the case when there's only one element\n    return acc / n if n > 1 else acc\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE([231, 2.5, 44, 5.0, 10.0], True, 0) == mean([231, 2.5, 44, 5.0, 10.0], True, 0))\nassert(SOLUTION_SIGNATURE([], False, 'raise') == mean([], False, 'raise'))\nassert(SOLUTION_SIGNATURE([1, 2.5, 3, 4.5], False, 0) == mean([1, 2.5, 3, 4.5], False, 0))\n"
 },
 {
  "task_id": "RWPB/1",
  "prompt": "from typing import Optional\n\nimport torch\nfrom matplotlib import cm\nfrom torchtyping import TensorType\n\ndef apply_colormap(\n    image: TensorType[\"bs\":..., 1],\n    cmap=\"viridis\",\n) -> TensorType[\"bs\":..., \"rgb\":3]:\n    \"\"\"Convert single channel to a color image. \n    Args:\n        image: Single channel image.\n        cmap: Colormap for image.\n    Returns:\n        TensorType: Colored image\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nfrom typing import Optional\n\nimport torch\nfrom matplotlib import cm\nfrom torchtyping import TensorType\n\ndef apply_colormap(\n    image: TensorType[\"bs\":..., 1],\n    cmap=\"viridis\",\n) -> TensorType[\"bs\":..., \"rgb\":3]:\n    \"\"\"Convert single channel to a color image. \n    Args:\n        image: Single channel image.\n        cmap: Colormap for image.\n    Returns:\n        TensorType: Colored image\n    \"\"\"\n    # Ensure image is a PyTorch tensor\n    if not isinstance(image, torch.Tensor):\n        image = torch.tensor(image)\n\n    # Normalize image to the range [0, 1]\n    image_normalized = (image - image.min()) / (image.max() - image.min())\n\n    # Apply colormap to the normalized image\n    colormap = cm.get_cmap(cmap)\n    colored_image = colormap(image_normalized.cpu().numpy())\n\n    # Convert the colored image from RGBA to RGB\n    colored_image = colored_image[..., :3]\n\n    # Convert the colored image back to a PyTorch tensor\n    colored_image_tensor = torch.from_numpy(colored_image).permute(0, 3, 1, 2)\n\n    return colored_image_tensor\n```\n\nExplanation:\n\n1. We first ensure that the input `image` is a PyTorch tensor. If it's not, we convert it to a tensor using `torch.tensor()`.\n\n2. We normalize the image to the range [0, 1] by subtracting the minimum value and dividing by the range (maximum - minimum). This step is necessary because colormaps expect input values in the range [0, 1].\n\n3. We get the colormap specified by the `cmap` argument using `cm.get_cmap()` from the matplotlib library.\n\n4. We apply the colormap to the normalized image using `colormap(image_normalized.cpu().numpy())`. We convert the image tensor to a NumPy array and move it to the CPU before applying the colormap.\n\n5. The colormap returns an RGBA image, so we extract only the RGB channels by slicing the last dimension `[..., :3]`.\n\n6. Finally, we convert the colored image back to a PyTorch tensor using `torch.from_numpy()` and permute the dimensions to match the expected output shape `[\"bs\":..., \"rgb\":3]` using `permute(0, 3, 1, 2)`.\n\n7. We return the colored image tensor.\n\nThis code takes a single channel image tensor, applies the specified colormap to it, and returns a colored image tensor with the shape `[\"bs\":..., \"rgb\":3]`.",
  "canonical_solution": "\n    colormap = cm.get_cmap(cmap)\n    colormap = torch.tensor(colormap.colors).to(image.device)  # type: ignore\n    image_long = (image * 255).long()\n    image_long_min = torch.min(image_long)\n    image_long_max = torch.max(image_long)\n    assert image_long_min >= 0, f\"the min value is {image_long_min}\"\n    assert image_long_max <= 255, f\"the max value is {image_long_max}\"\n    return colormap[image_long[..., 0]]\n\n\n",
  "unprocess_testcases": "a = torch.rand(2, 10, 10, 1)\nb = torch.rand(1, 100, 100, 1)\nc = torch.tensor([[[[0.0]], [[1.0]]]])\n\nassert((SOLUTION_SIGNATURE(a) == apply_colormap(a)).all())\nassert((SOLUTION_SIGNATURE(b, 'plasma') == apply_colormap(b, 'plasma')).all())\nassert((SOLUTION_SIGNATURE(c) == apply_colormap(c)).all())"
 },
 {
  "task_id": "RWPB/59",
  "prompt": "import torch\n\ndef modify_logit_for_repetition_penalty(logits, prev_output_tokens, repetition_penalty=1.0):\n    \"\"\"\n    Adjusts logits to penalize or favor the repetition of tokens that have already appeared,\n    based on the provided repetition penalty.\n\n    Args:\n        logits (torch.Tensor): The logits from a language model, of shape (batch_size, vocab_size).\n        prev_output_tokens (torch.Tensor): The indices of previously generated tokens for each sequence\n                                           in the batch, of shape (batch_size, seq_len).\n        repetition_penalty (float): The penalty factor for repetition. Values >1 penalize, values <1 favor\n                                    repetition. The default value of 1.0 means no penalty.\n\n    Returns:\n        torch.Tensor: The adjusted logits.\n\n    This function applies a repetition penalty as described in the paper:\n    \"CTRL: A Conditional Transformer Language Model for Controllable Generation\".\n    It decreases the probability of tokens that have appeared previously if the penalty is >1,\n    and increases if <1 but not equal to zero.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport torch\n\ndef modify_logit_for_repetition_penalty(logits, prev_output_tokens, repetition_penalty=1.0):\n    \"\"\"\n    Adjusts logits to penalize or favor the repetition of tokens that have already appeared,\n    based on the provided repetition penalty.\n\n    Args:\n        logits (torch.Tensor): The logits from a language model, of shape (batch_size, vocab_size).\n        prev_output_tokens (torch.Tensor): The indices of previously generated tokens for each sequence\n                                           in the batch, of shape (batch_size, seq_len).\n        repetition_penalty (float): The penalty factor for repetition. Values >1 penalize, values <1 favor\n                                    repetition. The default value of 1.0 means no penalty.\n\n    Returns:\n        torch.Tensor: The adjusted logits.\n\n    This function applies a repetition penalty as described in the paper:\n    \"CTRL: A Conditional Transformer Language Model for Controllable Generation\".\n    It decreases the probability of tokens that have appeared previously if the penalty is >1,\n    and increases if <1 but not equal to zero.\n    \"\"\"\n    if repetition_penalty != 1.0:\n        batch_size, vocab_size = logits.shape\n        seq_len = prev_output_tokens.shape[1]\n\n        # Create a mask tensor to identify repeated tokens\n        mask = torch.zeros((batch_size, seq_len, vocab_size), dtype=torch.bool, device=logits.device)\n        for i in range(batch_size):\n            for j in range(seq_len):\n                token = prev_output_tokens[i, j].item()\n                mask[i, j, token] = 1\n\n        # Apply the repetition penalty to the logits\n        penalty = torch.zeros_like(logits)\n        penalty.masked_fill_(mask.any(dim=1), -float('inf') if repetition_penalty > 1 else repetition_penalty)\n        logits += penalty\n\n    return logits\n```\n\nThis code modifies the logits based on the repetition penalty as follows:\n\n1. If the `repetition_penalty` is not equal to 1.0 (i.e., a penalty is desired), the code proceeds with the penalty calculation.\n\n2. It retrieves the batch size and vocabulary size from the `logits` tensor and the sequence length from the `prev_output_tokens` tensor.\n\n3. It creates a boolean mask tensor of shape `(batch_size, seq_len, vocab_size)` to identify repeated tokens. For each token in `prev_output_tokens`, the corresponding position in the mask is set to 1.\n\n4. It creates a penalty tensor of the same shape as `logits` and initializes it with zeros.\n\n5. Using the `masked_fill_` function, it applies the repetition penalty to the positions in the penalty tensor where the mask is True (i.e., where tokens are repeated). If the `repetition_penalty` is greater than 1, it sets the penalty to negative infinity, effectively preventing the repetition of those tokens. If the `repetition_penalty` is less than 1 but not equal to zero, it sets the penalty to the `repetition_penalty` value, favoring the repetition of those tokens.\n\n6. Finally, it adds the penalty tensor to the `logits` tensor, modifying the logits based on the repetition penalty.\n\n7. The modified logits are returned.\n\nThis code implements the repetition penalty as described in the paper \"CTRL: A Conditional Transformer Language Model for Controllable Generation\", allowing you to penalize or favor the repetition of tokens based on the provided `repetition_penalty` value.",
  "canonical_solution": "\n    if repetition_penalty == 1.0:\n        return logits  # No modification needed if penalty is 1\n\n    # Gather scores of previously generated tokens from logits\n    score = torch.gather(logits, 1, prev_output_tokens)\n\n    # Apply repetition penalty:\n    # If score < 0 (unlikely tokens), multiplying by a penalty > 1 will make them even less likely\n    # If score > 0 (likely tokens), dividing by a penalty > 1 will make them less likely\n    adjusted_score = torch.where(score < 0, score * repetition_penalty, score / repetition_penalty)\n\n    # Scatter the adjusted scores back into the logits tensor at the positions of the previous tokens\n    logits.scatter_(1, prev_output_tokens, adjusted_score)\n\n    return logits\n\n\n",
  "unprocess_testcases": "a = torch.rand(1, 5)\n\nassert((SOLUTION_SIGNATURE(a, torch.tensor([[0, 1]])) == modify_logit_for_repetition_penalty(a, torch.tensor([[0, 1]]))).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[10.0, -5.0, 0.0, 3.0, -1.0]]), torch.tensor([[0, 3]]), 2.0) == modify_logit_for_repetition_penalty(torch.tensor([[10.0, -5.0, 0.0, 3.0, -1.0]]), torch.tensor([[0, 3]]), 2.0)).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[2.0, -2.0, 4.0, -1.0, 0.0]]), torch.tensor([[0, 1, 3]]), 0.5) == modify_logit_for_repetition_penalty(torch.tensor([[2.0, -2.0, 4.0, -1.0, 0.0]]), torch.tensor([[0, 1, 3]]), 0.5)).all())\n"
 },
 {
  "task_id": "RWPB/104",
  "prompt": "MAX_VAL = (1 << 256) - 1\n\n\n# does not overflow\ndef mulMod(x, y, z):\n    \"\"\"\n    Compute (x * y) % z without overflow.\n    Args:\n        - x: int, multiplicand\n        - y: int, multiplier\n        - z: int, modulus\n    Returns:\n        result: int, the result of (x * y) % z\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nMAX_VAL = (1 << 256) - 1\n\n\ndef mulMod(x, y, z):\n    \"\"\"\n    Compute (x * y) % z without overflow.\n    Args:\n        - x: int, multiplicand\n        - y: int, multiplier\n        - z: int, modulus\n    Returns:\n        result: int, the result of (x * y) % z\n    \"\"\"\n    result = 0\n    x = x % z\n    while y > 0:\n        if y % 2 == 1:\n            result = (result + x) % z\n        x = (x * 2) % z\n        y //= 2\n    return result\n```\n\nExplanation:\n- The function `mulMod` takes three arguments: `x` (multiplicand), `y` (multiplier), and `z` (modulus).\n- The variable `result` is initialized to 0, which will store the final result.\n- `x` is first reduced modulo `z` to ensure it is within the range [0, z-1].\n- The function uses a bitwise approach to compute the modular multiplication.\n- The loop continues as long as `y` is greater than 0.\n- In each iteration:\n  - If `y` is odd (i.e., the least significant bit is 1), `result` is updated by adding `x` to it and taking the modulo with `z`.\n  - `x` is doubled (multiplied by 2) and reduced modulo `z`.\n  - `y` is divided by 2 (right-shifted by 1 bit) to process the next bit.\n- Finally, the function returns the computed `result`, which is equal to `(x * y) % z`.\n\nThis implementation avoids overflow by performing modular arithmetic at each step, ensuring that the intermediate values always remain within the range [0, z-1].\n\nNote: The `MAX_VAL` constant is defined as `(1 << 256) - 1`, which represents the maximum value that can be represented in 256 bits. However, it is not used in the provided solution since the modular arithmetic operations handle overflow implicitly.",
  "canonical_solution": "    \n    return (x * y) % z\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(10, 20, 7) == mulMod(10, 20, 7))\nassert(SOLUTION_SIGNATURE(3214, 34, 423) == mulMod(3214, 34, 423))\n\nlarge_x = (1 << 255)\nlarge_y = (1 << 255)\nlarge_z = (1 << 256) - 1\nassert(SOLUTION_SIGNATURE(large_x, large_y, large_z) == mulMod(large_x, large_y, large_z))\n\nassert(SOLUTION_SIGNATURE(0, 10, 3) == mulMod(0, 10, 3))\nassert(SOLUTION_SIGNATURE(10, 0, 3) == mulMod(10, 0, 3))\nassert(SOLUTION_SIGNATURE(10, 20, 1) == mulMod(10, 20, 1))\n"
 },
 {
  "task_id": "RWPB/130",
  "prompt": "import numpy as np\n\ndef bbox_ioa(box1, box2, iou=False, eps=1e-7):\n    \"\"\"\n    Calculate the intersection over box2 area given box1 and box2. Boxes are in x1y1x2y2 format.\n\n    Args:\n        box1 (np.ndarray): A numpy array of shape (n, 4) representing n bounding boxes.\n        box2 (np.ndarray): A numpy array of shape (m, 4) representing m bounding boxes.\n        iou (bool): Calculate the standard IoU if True else return inter_area/box2_area.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n\n    Returns:\n        (np.ndarray): A numpy array of shape (n, m) representing the intersection over box2 area.\n    \"\"\"\n",
  "solution": "Here's the completed code to calculate the intersection over box2 area (or IoU if specified) given two sets of bounding boxes:\n\n```python\nimport numpy as np\n\ndef bbox_ioa(box1, box2, iou=False, eps=1e-7):\n    \"\"\"\n    Calculate the intersection over box2 area given box1 and box2. Boxes are in x1y1x2y2 format.\n\n    Args:\n        box1 (np.ndarray): A numpy array of shape (n, 4) representing n bounding boxes.\n        box2 (np.ndarray): A numpy array of shape (m, 4) representing m bounding boxes.\n        iou (bool): Calculate the standard IoU if True else return inter_area/box2_area.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n\n    Returns:\n        (np.ndarray): A numpy array of shape (n, m) representing the intersection over box2 area.\n    \"\"\"\n    # Get the coordinates of bounding boxes\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1.T\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2.T\n\n    # Calculate the intersection coordinates\n    inter_x1 = np.maximum(b1_x1[:, np.newaxis], b2_x1)\n    inter_y1 = np.maximum(b1_y1[:, np.newaxis], b2_y1)\n    inter_x2 = np.minimum(b1_x2[:, np.newaxis], b2_x2)\n    inter_y2 = np.minimum(b1_y2[:, np.newaxis], b2_y2)\n\n    # Calculate the intersection area\n    inter_area = np.maximum(inter_x2 - inter_x1 + 1, 0) * np.maximum(inter_y2 - inter_y1 + 1, 0)\n\n    # Calculate the area of box2\n    box2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n\n    if iou:\n        # Calculate the area of box1\n        box1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n        union_area = box1_area[:, np.newaxis] + box2_area - inter_area\n        return inter_area / (union_area + eps)\n    else:\n        return inter_area / (box2_area + eps)\n```\n\nExplanation:\n\n1. The function takes two numpy arrays `box1` and `box2` representing the bounding boxes in the format `x1y1x2y2`. It also has an optional parameter `iou` to specify whether to calculate the standard IoU or the intersection over box2 area (default). The `eps` parameter is used to avoid division by zero.\n\n2. The coordinates of the bounding boxes are extracted using array slicing and transposition.\n\n3. The intersection coordinates are calculated using element-wise maximum and minimum operations between the corresponding coordinates of `box1` and `box2`.\n\n4. The intersection area is calculated by taking the maximum of the difference between the intersection coordinates plus 1 and 0. This ensures that the area is non-negative.\n\n5. The area of `box2` is calculated by taking the difference between the coordinates plus 1.\n\n6. If `iou` is set to `True`, the area of `box1` is also calculated, and the union area is computed by adding the areas of `box1` and `box2` and subtracting the intersection area. The IoU is then calculated by dividing the intersection area by the union area (plus `eps` to avoid division by zero).\n\n7. If `iou` is set to `False` (default), the intersection over box2 area is calculated by dividing the intersection area by the area of `box2` (plus `eps` to avoid division by zero).\n\n8. The function returns a numpy array of shape (n, m) representing the intersection over box2 area (or IoU if specified) for each pair of bounding boxes.\n\nThis implementation assumes that the input bounding boxes are in the format `x1y1x2y2`, where `(x1, y1)` represents the top-left corner and `(x2, y2)` represents the bottom-right corner of the bounding box.",
  "canonical_solution": "\n    # Get the coordinates of bounding boxes\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1.T\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2.T\n\n    # Intersection area\n    inter_area = (np.minimum(b1_x2[:, None], b2_x2) - np.maximum(b1_x1[:, None], b2_x1)).clip(0) * (\n        np.minimum(b1_y2[:, None], b2_y2) - np.maximum(b1_y1[:, None], b2_y1)\n    ).clip(0)\n\n    # Box2 area\n    area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n    if iou:\n        box1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n        area = area + box1_area[:, None] - inter_area\n\n    # Intersection over box2 area\n    return inter_area / (area + eps)\n\n\n",
  "unprocess_testcases": "box1 = np.array([[0, 0, 2, 2]])\nbox2 = np.array([[1, 1, 3, 3]])\niou = False\nassert((SOLUTION_SIGNATURE(box1, box2, iou) == bbox_ioa(box1, box2, iou)).all())\n\nbox1 = np.array([[0, 0, 1, 1]])\nbox2 = np.array([[2, 2, 3, 3]])\niou = False\nassert((SOLUTION_SIGNATURE(box1, box2, iou) == bbox_ioa(box1, box2, iou)).all())\n\nbox1 = np.array([[0, 0, 3, 3], [4, 4, 5, 5]])\nbox2 = np.array([[1, 1, 4, 4], [0, 0, 2, 2]])\niou = True\nassert((SOLUTION_SIGNATURE(box1, box2, iou) == bbox_ioa(box1, box2, iou)).all())\n\nbox1 = np.array([[0, 0, 3, 3], [4, 4, 5, 5]])\nbox2 = np.array([[1, 1, 4, 4], [0, 0, 2, 2]])\niou = True\nassert((SOLUTION_SIGNATURE(box1, box2, iou, 2e-5) == bbox_ioa(box1, box2, iou, 2e-5)).all())\n\nbox1 = np.array([[1, 3, 3, 6], [7, 7, 2, 6]])\nbox2 = np.array([[9, 3, 5, 4], [0, 1, 3, 8]])\niou = True\nassert((SOLUTION_SIGNATURE(box1, box2, iou) == bbox_ioa(box1, box2, iou)).all())\n"
 },
 {
  "task_id": "RWPB/120",
  "prompt": "def parity(cycles):\n    \"\"\"\n    Calculate the parity (even or odd) of a permutation given its disjoint cycles.\n\n    Args:\n        cycles (list): A list of disjoint cycles, where each cycle is represented as a list.\n\n    Returns:\n        int: 0 if the permutation is even, 1 if it is odd.\n    \"\"\"\n",
  "solution": "Certainly! Here's the completed code to calculate the parity of a permutation given its disjoint cycles:\n\n```python\ndef parity(cycles):\n    \"\"\"\n    Calculate the parity (even or odd) of a permutation given its disjoint cycles.\n\n    Args:\n        cycles (list): A list of disjoint cycles, where each cycle is represented as a list.\n\n    Returns:\n        int: 0 if the permutation is even, 1 if it is odd.\n    \"\"\"\n    total_length = sum(len(cycle) for cycle in cycles)\n    num_cycles = len(cycles)\n    \n    parity = (total_length - num_cycles) % 2\n    \n    return parity\n```\n\nExplanation:\n\n1. The `parity` function takes a list of disjoint cycles (`cycles`) as input, where each cycle is represented as a list.\n\n2. We calculate the total length of the permutation by summing the lengths of all the cycles using a list comprehension: `sum(len(cycle) for cycle in cycles)`. This gives us the total number of elements in the permutation.\n\n3. We count the number of cycles by getting the length of the `cycles` list using `len(cycles)`.\n\n4. The parity of a permutation is determined by the difference between the total length of the permutation and the number of cycles. If this difference is even, the permutation is even (parity is 0). If the difference is odd, the permutation is odd (parity is 1).\n\n5. We calculate the parity using the modulo operator `%`: `(total_length - num_cycles) % 2`. This gives us 0 if the difference is even and 1 if the difference is odd.\n\n6. Finally, we return the calculated parity.\n\nThe function will return 0 if the permutation is even and 1 if it is odd, based on the given disjoint cycles.",
  "canonical_solution": "    \n    res = 0\n    for i in cycles:\n        if len(i) > 1:\n            res += len(i) - 1\n    return res % 2\n\n",
  "unprocess_testcases": "cycles = [[1,2,3,4]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1, 2], [3, 4, 5], [6, 7, 8, 9]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1], [2, 3], [4, 5, 6], [7]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = []\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1], [2], [3], [4]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1, 2, 3], [4, 5, 6], [7, 8, 9, 10, 11]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n"
 },
 {
  "task_id": "RWPB/114",
  "prompt": "import torch\n\ndef reshape_weight_task_tensors(task_tensors, weights):\n    \"\"\"\n    Reshapes `weights` to match the shape of `task_tensors` by unsqeezing in the remaining dimensions.\n\n    Args:\n        task_tensors (torch.Tensor): The tensors that will be used to reshape `weights`.\n        weights (torch.Tensor): The tensor to be reshaped.\n\n    Returns:\n        torch.Tensor: The reshaped tensor.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport torch\n\ndef reshape_weight_task_tensors(task_tensors, weights):\n    \"\"\"\n    Reshapes `weights` to match the shape of `task_tensors` by unsqeezing in the remaining dimensions.\n\n    Args:\n        task_tensors (torch.Tensor): The tensors that will be used to reshape `weights`.\n        weights (torch.Tensor): The tensor to be reshaped.\n\n    Returns:\n        torch.Tensor: The reshaped tensor.\n    \"\"\"\n    # Get the shape of task_tensors\n    task_shape = task_tensors.shape\n\n    # Get the number of dimensions in task_tensors\n    num_dims = len(task_shape)\n\n    # Get the number of dimensions in weights\n    num_weights_dims = len(weights.shape)\n\n    # Calculate the number of dimensions to unsqueeze\n    num_unsqueeze_dims = num_dims - num_weights_dims\n\n    # Unsqueeze weights to match the shape of task_tensors\n    reshaped_weights = weights\n    for _ in range(num_unsqueeze_dims):\n        reshaped_weights = reshaped_weights.unsqueeze(-1)\n\n    return reshaped_weights\n```\n\nExplanation:\n1. We first get the shape of `task_tensors` using `task_tensors.shape` and store it in the `task_shape` variable.\n\n2. We calculate the number of dimensions in `task_tensors` using `len(task_shape)` and store it in the `num_dims` variable.\n\n3. We calculate the number of dimensions in `weights` using `len(weights.shape)` and store it in the `num_weights_dims` variable.\n\n4. We calculate the number of dimensions to unsqueeze by subtracting `num_weights_dims` from `num_dims` and store it in the `num_unsqueeze_dims` variable.\n\n5. We create a new variable `reshaped_weights` and initialize it with `weights`.\n\n6. We start a loop that iterates `num_unsqueeze_dims` times. In each iteration:\n   - We unsqueeze `reshaped_weights` by adding a new dimension at the end using `reshaped_weights.unsqueeze(-1)`.\n   - This step ensures that `reshaped_weights` matches the shape of `task_tensors` by adding the necessary dimensions.\n\n7. Finally, we return the `reshaped_weights` tensor, which now has the same shape as `task_tensors`.\n\nThis code reshapes the `weights` tensor to match the shape of `task_tensors` by unsqueezing the weights tensor in the remaining dimensions. The resulting `reshaped_weights` tensor will have the same shape as `task_tensors`, allowing for element-wise operations between the two tensors.",
  "canonical_solution": "    \n    new_shape = weights.shape + (1,) * (task_tensors.dim() - weights.dim())\n    weights = weights.view(new_shape)\n    return weights\n\n\n",
  "unprocess_testcases": "task_tensors = torch.rand((2, 3, 4))\nweights = torch.rand((2))\nassert((SOLUTION_SIGNATURE(task_tensors, weights) == reshape_weight_task_tensors(task_tensors, weights)).all())\n\ntask_tensors = torch.rand((5, 6, 7, 8))\nweights = torch.rand((5, 6))\nassert((SOLUTION_SIGNATURE(task_tensors, weights) == reshape_weight_task_tensors(task_tensors, weights)).all())\n\ntask_tensors = torch.rand((10, 20))\nweights = torch.rand((10))\nassert((SOLUTION_SIGNATURE(task_tensors, weights) == reshape_weight_task_tensors(task_tensors, weights)).all())\n"
 },
 {
  "task_id": "RWPB/49",
  "prompt": "import torch\n\ndef make_even_first_dim(tensor):\n    \"\"\"\n    Ensures that the first dimension of the tensor is even. If it's odd, reduces it by one.\n\n    Args:\n    tensor (torch.Tensor): Input tensor.\n\n    Returns:\n    torch.Tensor: Tensor with an even first dimension.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport torch\n\ndef make_even_first_dim(tensor):\n    \"\"\"\n    Ensures that the first dimension of the tensor is even. If it's odd, reduces it by one.\n\n    Args:\n    tensor (torch.Tensor): Input tensor.\n\n    Returns:\n    torch.Tensor: Tensor with an even first dimension.\n    \"\"\"\n    if tensor.shape[0] % 2 != 0:\n        return tensor[:-1]\n    return tensor\n```\n\nExplanation:\n1. The function takes a `tensor` as input.\n2. It checks if the first dimension of the tensor (accessed using `tensor.shape[0]`) is odd by using the modulo operator `%`. If the remainder of dividing the first dimension by 2 is not equal to 0, it means the first dimension is odd.\n3. If the first dimension is odd, the function returns a new tensor that excludes the last element along the first dimension using the slicing notation `tensor[:-1]`. This effectively reduces the first dimension by one.\n4. If the first dimension is already even, the function simply returns the original tensor without any modifications.\n\nThis code ensures that the returned tensor always has an even first dimension. If the input tensor already has an even first dimension, it is returned unchanged. If the input tensor has an odd first dimension, a new tensor with the last element along the first dimension removed is returned.",
  "canonical_solution": "\n    size = list(tensor.size())\n    if size[0] % 2 == 1:\n        size[0] -= 1\n        return tensor[:size[0]]\n    return tensor\n\n",
  "unprocess_testcases": "a = torch.randn(4, 3, 2)\nb = torch.randn(5, 3, 2)\nc = torch.randn(1, 4, 4)\n\nassert((SOLUTION_SIGNATURE(a) == make_even_first_dim(a)).all())\nassert((SOLUTION_SIGNATURE(b) == make_even_first_dim(b)).all())\nassert((SOLUTION_SIGNATURE(c) == make_even_first_dim(c)).all())\n"
 },
 {
  "task_id": "RWPB/18",
  "prompt": "import numpy as np\n\ndef last_before(val, arr):\n  \"\"\"\n  Find the index of the last element in a sorted array that is less than or equal to a given value.Requires sorted (monotonic) arr.\n  Args:\n      - val: int, the value to search for\n      - arr: numpy.ndarray, the sorted array\n  Returns:\n      idx: int, the index of the last element in the array that is less than or equal to val\n  \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport numpy as np\n\ndef last_before(val, arr):\n    \"\"\"\n    Find the index of the last element in a sorted array that is less than or equal to a given value.\n    Requires sorted (monotonic) arr.\n    Args:\n        - val: int, the value to search for\n        - arr: numpy.ndarray, the sorted array\n    Returns:\n        idx: int, the index of the last element in the array that is less than or equal to val\n    \"\"\"\n    left = 0\n    right = len(arr) - 1\n\n    while left <= right:\n        mid = (left + right) // 2\n\n        if arr[mid] <= val:\n            if mid == len(arr) - 1 or arr[mid + 1] > val:\n                return mid\n            left = mid + 1\n        else:\n            right = mid - 1\n\n    return -1\n```\n\nExplanation:\n1. We initialize two pointers, `left` and `right`, pointing to the start and end of the array, respectively.\n2. We use a binary search approach to find the last element less than or equal to the given value `val`.\n3. In each iteration of the while loop:\n   - We calculate the middle index `mid` using integer division.\n   - If the element at index `mid` is less than or equal to `val`, we check if it is the last element in the array or if the next element is greater than `val`. If either condition is true, we have found the last element less than or equal to `val`, so we return `mid`.\n   - If the element at index `mid` is greater than `val`, we update `right` to `mid - 1` to search in the left half of the array.\n   - If the element at index `mid` is less than or equal to `val` but not the last element, we update `left` to `mid + 1` to search in the right half of the array.\n4. If the while loop completes without finding the desired element, it means the value `val` is smaller than all elements in the array. In this case, we return -1 to indicate that no element satisfies the condition.\n\nThe time complexity of this solution is O(log n) since it uses binary search to find the last element less than or equal to the given value. The space complexity is O(1) as it only uses a constant amount of extra space.",
  "canonical_solution": "\n  assert np.all(np.diff(arr) >= 0)\n  return np.searchsorted(arr, val) - 1\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(val = 5, arr = np.array([1, 2, 3, 4, 5])) == last_before(val = 5, arr = np.array([1, 2, 3, 4, 5])))\nassert(SOLUTION_SIGNATURE(val = 3.5, arr = np.array([1, 2, 3, 4, 5])) == last_before(val = 3.5, arr = np.array([1, 2, 3, 4, 5])))\nassert(SOLUTION_SIGNATURE(val = 0, arr = np.array([1, 2, 3, 4, 5])) == last_before(val = 0, arr = np.array([1, 2, 3, 4, 5])))\n"
 },
 {
  "task_id": "RWPB/66",
  "prompt": "import numpy as np\n\ndef focal_length_to_fovy(focal_length, sensor_height):\n    \"\"\"\n    Calculates the vertical field of view (FoVy) from the focal length of the lens and the sensor height.\n\n    Parameters:\n    focal_length (float): The focal length of the lens.\n    sensor_height (float): The height of the camera sensor.\n\n    Returns:\n    float: The vertical field of view in radians.\n\n    Process:\n    1. Compute half the sensor height.\n    2. Divide half the sensor height by the focal length to get the tangent of half the vertical field of view.\n    3. Use the arctangent function to determine half the vertical field of view.\n    4. Multiply by 2 to get the full vertical field of view (FoVy).\n    \"\"\"\n",
  "solution": "Here's the completed code to calculate the vertical field of view (FoVy) from the focal length and sensor height:\n\n```python\nimport numpy as np\n\ndef focal_length_to_fovy(focal_length, sensor_height):\n    \"\"\"\n    Calculates the vertical field of view (FoVy) from the focal length of the lens and the sensor height.\n\n    Parameters:\n    focal_length (float): The focal length of the lens.\n    sensor_height (float): The height of the camera sensor.\n\n    Returns:\n    float: The vertical field of view in radians.\n\n    Process:\n    1. Compute half the sensor height.\n    2. Divide half the sensor height by the focal length to get the tangent of half the vertical field of view.\n    3. Use the arctangent function to determine half the vertical field of view.\n    4. Multiply by 2 to get the full vertical field of view (FoVy).\n    \"\"\"\n    half_sensor_height = sensor_height / 2\n    tan_half_fovy = half_sensor_height / focal_length\n    half_fovy = np.arctan(tan_half_fovy)\n    fovy = 2 * half_fovy\n    return fovy\n```\n\nThis code follows the process outlined in the comments:\n\n1. It computes half the sensor height by dividing the `sensor_height` by 2 and assigns it to the variable `half_sensor_height`.\n\n2. It calculates the tangent of half the vertical field of view by dividing `half_sensor_height` by the `focal_length` and assigns it to the variable `tan_half_fovy`.\n\n3. It uses the `np.arctan()` function to determine half the vertical field of view and assigns it to the variable `half_fovy`.\n\n4. Finally, it multiplies `half_fovy` by 2 to get the full vertical field of view (FoVy) and returns the result.\n\nThe function takes two parameters: `focal_length` and `sensor_height`, both of which are expected to be floats representing the focal length of the lens and the height of the camera sensor, respectively.\n\nThe function returns the vertical field of view (FoVy) in radians.",
  "canonical_solution": "    \n    return 2 * np.arctan(0.5 * sensor_height / focal_length)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(focal_length=50, sensor_height=24) == focal_length_to_fovy(focal_length=50, sensor_height=24))\nassert(SOLUTION_SIGNATURE(focal_length=1, sensor_height=24) == focal_length_to_fovy(focal_length=1, sensor_height=24))\nassert(SOLUTION_SIGNATURE(focal_length=1000, sensor_height=24) == focal_length_to_fovy(focal_length=1000, sensor_height=24))\n"
 },
 {
  "task_id": "RWPB/37",
  "prompt": "import math\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\n\ndef euler_and_translation_to_matrix(euler_angles, translation_vector):\n    \"\"\"\n    Converts Euler angles and a translation vector into a 4x4 transformation matrix.\n\n    Args:\n        euler_angles (list or np.array): Euler angles in degrees for rotation around the x, y, and z axes.\n        translation_vector (list or np.array): Translation vector for x, y, and z coordinates.\n\n    Returns:\n        np.array: A 4x4 transformation matrix combining both rotation and translation.\n\n    This function constructs a homogeneous transformation matrix that combines both rotation and translation.\n    The rotation is specified by Euler angles (roll, pitch, yaw), which are converted into a rotation matrix.\n    The translation is represented as a vector. This transformation matrix is useful in applications such as\n    robotics, 3D graphics, and computer vision, where transformations in 3D space are common.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport math\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\n\ndef euler_and_translation_to_matrix(euler_angles, translation_vector):\n    \"\"\"\n    Converts Euler angles and a translation vector into a 4x4 transformation matrix.\n\n    Args:\n        euler_angles (list or np.array): Euler angles in degrees for rotation around the x, y, and z axes.\n        translation_vector (list or np.array): Translation vector for x, y, and z coordinates.\n\n    Returns:\n        np.array: A 4x4 transformation matrix combining both rotation and translation.\n\n    This function constructs a homogeneous transformation matrix that combines both rotation and translation.\n    The rotation is specified by Euler angles (roll, pitch, yaw), which are converted into a rotation matrix.\n    The translation is represented as a vector. This transformation matrix is useful in applications such as\n    robotics, 3D graphics, and computer vision, where transformations in 3D space are common.\n    \"\"\"\n    # Convert Euler angles from degrees to radians\n    euler_angles_rad = np.radians(euler_angles)\n\n    # Create a rotation object from Euler angles\n    rotation = R.from_euler('xyz', euler_angles_rad)\n\n    # Convert the rotation object to a 3x3 rotation matrix\n    rotation_matrix = rotation.as_matrix()\n\n    # Create a 4x4 transformation matrix with the rotation matrix and translation vector\n    transformation_matrix = np.eye(4)\n    transformation_matrix[:3, :3] = rotation_matrix\n    transformation_matrix[:3, 3] = translation_vector\n\n    return transformation_matrix\n```\n\nExplanation:\n1. We first convert the Euler angles from degrees to radians using `np.radians()` since the `scipy.spatial.transform.Rotation` class expects angles in radians.\n\n2. We create a rotation object using `R.from_euler()`, specifying the order of rotation axes ('xyz' in this case) and the Euler angles in radians.\n\n3. We convert the rotation object to a 3x3 rotation matrix using the `as_matrix()` method.\n\n4. We create a 4x4 transformation matrix using `np.eye(4)`, which initializes an identity matrix.\n\n5. We assign the 3x3 rotation matrix to the top-left 3x3 portion of the transformation matrix using `transformation_matrix[:3, :3] = rotation_matrix`.\n\n6. We assign the translation vector to the last column of the transformation matrix using `transformation_matrix[:3, 3] = translation_vector`.\n\n7. Finally, we return the 4x4 transformation matrix that combines both rotation and translation.\n\nThis function takes Euler angles (in degrees) and a translation vector as input and returns a 4x4 transformation matrix that represents the combined rotation and translation in 3D space.",
  "canonical_solution": "\n    # Create a rotation object from the Euler angles\n    rotation = R.from_euler('xyz', euler_angles, degrees=True)\n    # Convert the rotation object to a 3x3 rotation matrix\n    rotation_matrix = rotation.as_matrix()\n\n    # Initialize a 4x4 identity matrix\n    matrix = np.eye(4)\n    # Insert the rotation matrix into the top-left 3x3 part of the identity matrix\n    matrix[:3, :3] = rotation_matrix\n    # Insert the translation vector into the first three elements of the fourth column\n    matrix[:3, 3] = translation_vector\n\n    return matrix\n\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([45, 45, 45]), np.array([10, 0, 5])) == euler_and_translation_to_matrix(np.array([45, 45, 45]), np.array([10, 0, 5]))).all())\nassert((SOLUTION_SIGNATURE([0, 0, 0], [5, 5, 5]) == euler_and_translation_to_matrix([0, 0, 0], [5, 5, 5])).all())\nassert((SOLUTION_SIGNATURE([-90, 360, 270], [-1, -1, -1]) == euler_and_translation_to_matrix([-90, 360, 270], [-1, -1, -1])).all())\n"
 },
 {
  "task_id": "RWPB/95",
  "prompt": "import torch\n\ndef lines_focal(o, d):\n    \"\"\"\n    Estimates the focal point of a set of lines in 3D space given their origins and directions.\n\n    Parameters:\n    o (torch.Tensor): A tensor of shape (N, 3) representing the origins of N lines in 3D space.\n    d (torch.Tensor): A tensor of shape (N, 3) representing the directions of N lines in 3D space.\n\n    Returns:\n    torch.Tensor: A tensor of shape (3,) representing the estimated focal point in 3D space.\n\n    Process:\n    1. Normalize the direction vectors `d` to unit length along the last dimension.\n    2. Create a 3x3 identity matrix `I` with the same dtype and device as the input tensors.\n    3. Compute the sum `S` of the outer products of each direction vector with itself, subtracting the identity matrix `I` from each outer product. This results in a tensor of shape (3, 3).\n    4. Compute the sum `C` of the matrix products of each adjusted direction outer product with the corresponding origin vector. This results in a tensor of shape (3,).\n    5. Compute the pseudo-inverse of `S` using `torch.linalg.pinv`.\n    6. Multiply the pseudo-inverse of `S` with `C` to obtain the estimated focal point.\n\n    Returns the estimated focal point as a tensor of shape (3,).\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of estimating the focal point of a set of lines in 3D space:\n\n```python\nimport torch\n\ndef lines_focal(o, d):\n    \"\"\"\n    Estimates the focal point of a set of lines in 3D space given their origins and directions.\n\n    Parameters:\n    o (torch.Tensor): A tensor of shape (N, 3) representing the origins of N lines in 3D space.\n    d (torch.Tensor): A tensor of shape (N, 3) representing the directions of N lines in 3D space.\n\n    Returns:\n    torch.Tensor: A tensor of shape (3,) representing the estimated focal point in 3D space.\n\n    Process:\n    1. Normalize the direction vectors `d` to unit length along the last dimension.\n    2. Create a 3x3 identity matrix `I` with the same dtype and device as the input tensors.\n    3. Compute the sum `S` of the outer products of each direction vector with itself, subtracting the identity matrix `I` from each outer product. This results in a tensor of shape (3, 3).\n    4. Compute the sum `C` of the matrix products of each adjusted direction outer product with the corresponding origin vector. This results in a tensor of shape (3,).\n    5. Compute the pseudo-inverse of `S` using `torch.linalg.pinv`.\n    6. Multiply the pseudo-inverse of `S` with `C` to obtain the estimated focal point.\n\n    Returns the estimated focal point as a tensor of shape (3,).\n    \"\"\"\n    # Normalize the direction vectors\n    d = d / torch.norm(d, dim=-1, keepdim=True)\n\n    # Create a 3x3 identity matrix\n    I = torch.eye(3, dtype=o.dtype, device=o.device)\n\n    # Compute the sum of the outer products of each direction vector with itself,\n    # subtracting the identity matrix from each outer product\n    S = torch.sum(torch.einsum('ij,ik->ijk', d, d) - I, dim=0)\n\n    # Compute the sum of the matrix products of each adjusted direction outer product\n    # with the corresponding origin vector\n    C = torch.sum(torch.einsum('ij,ik,il->jl', d, d, o) - o, dim=0)\n\n    # Compute the pseudo-inverse of S\n    S_pinv = torch.linalg.pinv(S)\n\n    # Multiply the pseudo-inverse of S with C to obtain the estimated focal point\n    focal_point = torch.matmul(S_pinv, C)\n\n    return focal_point\n```\n\nThis implementation follows the process outlined in the problem description:\n\n1. The direction vectors `d` are normalized to unit length along the last dimension using `torch.norm`.\n2. A 3x3 identity matrix `I` is created with the same dtype and device as the input tensors using `torch.eye`.\n3. The sum `S` of the outer products of each direction vector with itself is computed using `torch.einsum`, subtracting the identity matrix `I` from each outer product.\n4. The sum `C` of the matrix products of each adjusted direction outer product with the corresponding origin vector is computed using `torch.einsum`.\n5. The pseudo-inverse of `S` is computed using `torch.linalg.pinv`.\n6. The pseudo-inverse of `S` is multiplied with `C` using `torch.matmul` to obtain the estimated focal point.\n\nThe function returns the estimated focal point as a tensor of shape (3,).",
  "canonical_solution": "    \n    d = d / torch.norm(d, dim=-1, keepdim=True)\n    I = torch.eye(3, dtype=o.dtype, device=o.device)\n    S = torch.sum(d[..., None] @ torch.transpose(d[..., None], 1, 2) - I[None, ...], dim=0)\n    C = torch.sum((d[..., None] @ torch.transpose(d[..., None], 1, 2) - I[None, ...]) @ o[..., None], dim=0).squeeze(1)\n    return torch.linalg.pinv(S) @ C\n\n\n",
  "unprocess_testcases": "o = torch.tensor([[0, 0, 0], [1, 1, 1]], dtype=torch.float32)\nd = torch.tensor([[1, 0, 0], [0, 1, 0]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(o, d) == lines_focal(o, d)).all())\n\no = torch.tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=torch.float32)\nd = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(o, d) == lines_focal(o, d)).all())\n\no = torch.tensor([[0, 0, 0], [1, 1, 1], [2, 2, 2]], dtype=torch.float32)\nd = torch.tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(o, d) == lines_focal(o, d)).all())\n"
 },
 {
  "task_id": "RWPB/52",
  "prompt": "import torch\n\ndef reshape_tensor(x, heads):\n    \"\"\"\n    Reshape a tensor to fit a multi-head attention mechanism structure.\n    If the shape of x is invalid, please return ValueError.\n\n    Arguments:\n    x : torch.Tensor\n        The input tensor with shape (batch_size, length, width), where 'width'\n        is typically the embedding dimension or a multiple of 'heads'.\n    heads : int\n        The number of heads in the multi-head attention mechanism.\n\n    Returns:\n    torch.Tensor\n        A reshaped tensor with dimensions corresponding to multi-head structures,\n        specifically (batch_size * n_heads, length, dim_per_head).\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport torch\n\ndef reshape_tensor(x, heads):\n    \"\"\"\n    Reshape a tensor to fit a multi-head attention mechanism structure.\n    If the shape of x is invalid, please return ValueError.\n\n    Arguments:\n    x : torch.Tensor\n        The input tensor with shape (batch_size, length, width), where 'width'\n        is typically the embedding dimension or a multiple of 'heads'.\n    heads : int\n        The number of heads in the multi-head attention mechanism.\n\n    Returns:\n    torch.Tensor\n        A reshaped tensor with dimensions corresponding to multi-head structures,\n        specifically (batch_size * n_heads, length, dim_per_head).\n    \"\"\"\n    # Check if the input tensor has the correct number of dimensions\n    if x.dim() != 3:\n        raise ValueError(\"Input tensor must have 3 dimensions (batch_size, length, width)\")\n\n    batch_size, length, width = x.size()\n\n    # Check if the width is divisible by the number of heads\n    if width % heads != 0:\n        raise ValueError(\"The width of the input tensor must be divisible by the number of heads\")\n\n    dim_per_head = width // heads\n\n    # Reshape the tensor to fit the multi-head attention structure\n    reshaped_tensor = x.view(batch_size, length, heads, dim_per_head)\n    reshaped_tensor = reshaped_tensor.permute(0, 2, 1, 3)\n    reshaped_tensor = reshaped_tensor.contiguous().view(batch_size * heads, length, dim_per_head)\n\n    return reshaped_tensor\n```\n\nExplanation:\n1. First, we check if the input tensor `x` has the correct number of dimensions, which should be 3 (batch_size, length, width). If the number of dimensions is not 3, we raise a `ValueError` with an appropriate message.\n\n2. We extract the `batch_size`, `length`, and `width` from the size of the input tensor `x`.\n\n3. We check if the `width` of the input tensor is divisible by the number of `heads`. If it's not divisible, we raise a `ValueError` with an appropriate message.\n\n4. We calculate the `dim_per_head` by dividing the `width` by the number of `heads`.\n\n5. We reshape the input tensor `x` to fit the multi-head attention structure:\n   - First, we use `x.view(batch_size, length, heads, dim_per_head)` to reshape the tensor into a 4-dimensional tensor with dimensions (batch_size, length, heads, dim_per_head).\n   - Then, we use `permute(0, 2, 1, 3)` to rearrange the dimensions to (batch_size, heads, length, dim_per_head).\n   - Finally, we use `contiguous().view(batch_size * heads, length, dim_per_head)` to reshape the tensor into the desired shape (batch_size * heads, length, dim_per_head).\n\n6. We return the reshaped tensor.\n\nThis code takes an input tensor `x` with shape (batch_size, length, width) and reshapes it to fit the multi-head attention mechanism structure with dimensions (batch_size * heads, length, dim_per_head). It also includes error handling to raise a `ValueError` if the input tensor has an invalid shape or if the width is not divisible by the number of heads.",
  "canonical_solution": "\n    bs, length, width = x.shape\n    # Reshape to introduce a new dimension for heads, assuming that width can be evenly divided by heads.\n    # width is divided into 'heads' and the dimension per head is calculated as width // heads\n    try:\n        x = x.view(bs, length, heads, -1)\n    except:\n        return ValueError\n    # (bs, length, heads, dim_per_head) --> (bs, heads, length, dim_per_head)\n    # Transpose to swap the 'length' and 'heads' dimensions to align heads for parallel processing\n    x = x.transpose(1, 2)\n    # (bs, heads, length, dim_per_head) --> (bs * heads, length, dim_per_head)\n    # Reshape to merge batch size and heads for processing each head independently in subsequent operations\n    x = x.reshape(bs * heads, length, -1)\n    return x\n\n\n",
  "unprocess_testcases": "a = torch.rand(2, 5, 10)\nb = torch.rand(3, 8, 15)\nc = torch.rand(1, 1, 4)\n\nassert((SOLUTION_SIGNATURE(a, heads=2) == reshape_tensor(a, heads=2)).all())\nassert(SOLUTION_SIGNATURE(b, heads=4) == reshape_tensor(b, heads=4))\nassert((SOLUTION_SIGNATURE(c, heads=2) == reshape_tensor(c, heads=2)).all())\n"
 },
 {
  "task_id": "RWPB/42",
  "prompt": "from PIL import Image\nimport numpy as np\n\ndef resize_and_center_crop(image, target_width, target_height):\n    \"\"\"\n    Resizes an input image and then crops it to the target dimensions, centered on the resized image.\n\n    Args:\n        image (np.array): The input image as a NumPy array.\n        target_width (int): The desired width of the output image.\n        target_height (int): The desired height of the output image.\n\n    Returns:\n        np.array: The processed image as a NumPy array, resized and cropped to the target dimensions.\n\n    This function first converts the input image to a PIL Image object, then calculates the scale factor needed\n    to ensure that the resized image will cover the target dimensions. It resizes the image using the LANCZOS filter\n    for high-quality downsampling. After resizing, it calculates the coordinates necessary to crop the image to the\n    target dimensions centered on the resized image, and finally converts the cropped PIL image back to a NumPy array.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nfrom PIL import Image\nimport numpy as np\n\ndef resize_and_center_crop(image, target_width, target_height):\n    \"\"\"\n    Resizes an input image and then crops it to the target dimensions, centered on the resized image.\n\n    Args:\n        image (np.array): The input image as a NumPy array.\n        target_width (int): The desired width of the output image.\n        target_height (int): The desired height of the output image.\n\n    Returns:\n        np.array: The processed image as a NumPy array, resized and cropped to the target dimensions.\n\n    This function first converts the input image to a PIL Image object, then calculates the scale factor needed\n    to ensure that the resized image will cover the target dimensions. It resizes the image using the LANCZOS filter\n    for high-quality downsampling. After resizing, it calculates the coordinates necessary to crop the image to the\n    target dimensions centered on the resized image, and finally converts the cropped PIL image back to a NumPy array.\n    \"\"\"\n    pil_image = Image.fromarray(image)\n    \n    # Calculate the scale factor to ensure the resized image covers the target dimensions\n    scale_factor = max(target_width / pil_image.width, target_height / pil_image.height)\n    \n    # Resize the image using the LANCZOS filter for high-quality downsampling\n    resized_width = int(pil_image.width * scale_factor)\n    resized_height = int(pil_image.height * scale_factor)\n    resized_image = pil_image.resize((resized_width, resized_height), Image.LANCZOS)\n    \n    # Calculate the coordinates to crop the resized image to the target dimensions\n    left = (resized_width - target_width) // 2\n    top = (resized_height - target_height) // 2\n    right = left + target_width\n    bottom = top + target_height\n    \n    # Crop the resized image to the target dimensions\n    cropped_image = resized_image.crop((left, top, right, bottom))\n    \n    # Convert the cropped PIL image back to a NumPy array\n    output_image = np.array(cropped_image)\n    \n    return output_image\n```\n\nThis code completes the `resize_and_center_crop` function according to the problem description. Here's how it works:\n\n1. The input image is converted from a NumPy array to a PIL Image object using `Image.fromarray()`.\n\n2. The scale factor is calculated by taking the maximum of the ratios of the target width and height to the original image width and height. This ensures that the resized image will cover the target dimensions.\n\n3. The image is resized using the calculated scale factor and the LANCZOS filter for high-quality downsampling. The new width and height are calculated by multiplying the original dimensions by the scale factor.\n\n4. The coordinates for cropping the resized image are calculated. The left and top coordinates are calculated by subtracting the target width and height from the resized width and height, respectively, and dividing by 2. The right and bottom coordinates are calculated by adding the target width and height to the left and top coordinates, respectively.\n\n5. The resized image is cropped using the calculated coordinates with the `crop()` method of the PIL Image object.\n\n6. The cropped PIL image is converted back to a NumPy array using `np.array()`.\n\n7. Finally, the processed image as a NumPy array is returned.\n\nThis function takes an input image as a NumPy array, resizes it to cover the target dimensions, crops it to the target dimensions centered on the resized image, and returns the processed image as a NumPy array.",
  "canonical_solution": "\n    # Convert the input NumPy array to a PIL Image object\n    pil_image = Image.fromarray(image)\n    original_width, original_height = pil_image.size\n\n    # Calculate the scale factor to resize the image\n    scale_factor = max(target_width / original_width, target_height / original_height)\n\n    # Calculate the new dimensions\n    resized_width = int(round(original_width * scale_factor))\n    resized_height = int(round(original_height * scale_factor))\n\n    # Resize the image using the LANCZOS filter for high quality\n    resized_image = pil_image.resize((resized_width, resized_height), Image.LANCZOS)\n\n    # Calculate the coordinates for cropping the image to center it\n    left = (resized_width - target_width) / 2\n    top = (resized_height - target_height) / 2\n    right = (resized_width + target_width) / 2\n    bottom = (resized_height + target_height) / 2\n\n    # Crop the image using the calculated coordinates\n    cropped_image = resized_image.crop((left, top, right, bottom))\n\n    # Convert the cropped PIL image back to a NumPy array and return\n    return np.array(cropped_image)\n\n\n",
  "unprocess_testcases": "a = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\nb = np.random.randint(0, 256, (100, 200, 3), dtype=np.uint8)\nc = np.random.randint(0, 256, (1000, 800, 3), dtype=np.uint8)\n\nassert((SOLUTION_SIGNATURE(a, 100, 100) == resize_and_center_crop(a, 100, 100)).all())\nassert((SOLUTION_SIGNATURE(b, 150, 100) == resize_and_center_crop(b, 150, 100)).all())\nassert((SOLUTION_SIGNATURE(c, 200, 150) == resize_and_center_crop(c, 200, 150)).all())\n"
 },
 {
  "task_id": "RWPB/13",
  "prompt": "import numpy as np\n\n\ndef remove_scale_from_camera_pose(pose_matrix):\n    '''pose_matrix is a 4x4 numpy array\n\n    Args:\n        - pose_matrix: numpy.array\n    Return:\n        - pose_matrix: numpy.array\n    '''\n",
  "solution": "To remove the scale from a camera pose matrix, you can normalize the first three columns of the matrix. Here's the completed code:\n\n```python\nimport numpy as np\n\n\ndef remove_scale_from_camera_pose(pose_matrix):\n    '''pose_matrix is a 4x4 numpy array\n\n    Args:\n        - pose_matrix: numpy.array\n    Return:\n        - pose_matrix: numpy.array\n    '''\n    # Extract the rotation matrix (first 3x3 submatrix)\n    rotation_matrix = pose_matrix[:3, :3]\n    \n    # Normalize each column of the rotation matrix\n    column1 = rotation_matrix[:, 0]\n    column2 = rotation_matrix[:, 1]\n    column3 = rotation_matrix[:, 2]\n    \n    column1 = column1 / np.linalg.norm(column1)\n    column2 = column2 / np.linalg.norm(column2)\n    column3 = column3 / np.linalg.norm(column3)\n    \n    # Reconstruct the rotation matrix with normalized columns\n    rotation_matrix[:, 0] = column1\n    rotation_matrix[:, 1] = column2\n    rotation_matrix[:, 2] = column3\n    \n    # Update the pose matrix with the normalized rotation matrix\n    pose_matrix[:3, :3] = rotation_matrix\n    \n    return pose_matrix\n```\n\nExplanation:\n1. We extract the rotation matrix, which is the first 3x3 submatrix of the pose matrix.\n2. We normalize each column of the rotation matrix by dividing it by its Euclidean norm using `np.linalg.norm()`. This removes any scale factor present in the rotation matrix.\n3. We reconstruct the rotation matrix with the normalized columns.\n4. We update the pose matrix by replacing the first 3x3 submatrix with the normalized rotation matrix.\n5. Finally, we return the updated pose matrix.\n\nBy normalizing the columns of the rotation matrix, we ensure that the rotation matrix is orthonormal and has a scale of 1. This effectively removes any scale factor present in the camera pose matrix.",
  "canonical_solution": "\n    # Ensure the matrix is in floating point format to avoid type casting issues in division\n    pose_matrix = pose_matrix.astype(np.float64)\n\n    # For each of the first three columns, normalize to remove scale\n    for i in range(3):\n        column = pose_matrix[:, i]\n        scale_factor = np.linalg.norm(column[:3])  # Compute the norm of the column, excluding the bottom element\n        if scale_factor > 0:  # Avoid division by zero\n            pose_matrix[:, i] /= scale_factor  # Normalize column to remove scale\n\n    # The last column (translation) and the last row are not modified, as they do not contribute to scale\n    return pose_matrix\n\n",
  "unprocess_testcases": "a = np.array([[3, 0, 0, 0],\n          [0, 4, 0, 0],\n          [0, 0, 2, 0],\n          [0, 0, 0, 1]])\nb = np.array([[0, 0, 0, 0],\n          [0, 0, 0, 0],\n          [0, 0, 0, 0],\n          [0, 0, 0, 1]])\nc = np.array([[1, 2, 3, 0],\n          [4, 5, 6, 0],\n          [7, 8, 9, 0],\n          [0, 0, 0, 1]])\n\nassert((SOLUTION_SIGNATURE(a) == remove_scale_from_camera_pose(a)).all())\nassert((SOLUTION_SIGNATURE(b) == remove_scale_from_camera_pose(b)).all())\nassert((SOLUTION_SIGNATURE(c) == remove_scale_from_camera_pose(c)).all())"
 },
 {
  "task_id": "RWPB/85",
  "prompt": "from typing import Optional, Sequence\n\nimport torch\n\ndef window_partition(x: torch.Tensor, window_size: Sequence[int]):\n    \"\"\"\n    Partition an image feature map into smaller windows in an ANE (Apple Neural Engine) friendly manner,\n    avoiding the use of 6D tensors.\n\n    :param x: A 4D tensor representing the image feature map with shape (batch_size, H, W, C), where:\n              - batch_size (B) is the number of images in the batch,\n              - H is the height of the feature map,\n              - W is the width of the feature map,\n              - C is the number of channels.\n    :param window_size: A sequence of two integers (win_h, win_w) specifying the height and width of the windows.\n    :returns: A 4D tensor of partitioned feature map windows with shape (batch_size * num_windows, win_h, win_w, C), where:\n              - num_windows is the total number of windows per image (calculated as (H / win_h) * (W / win_w)),\n              - win_h is the window height,\n              - win_w is the window width,\n              - C is the number of channels.\n    :rtype: torch.Tensor\n\n    The process of partitioning is as follows:\n    1. Reshape the input tensor to split the height dimension (H) into smaller chunks of size win_h,\n       resulting in shape (B, H // win_h, win_h, W, C).\n    2. Flatten the batch and height dimensions together to shape (B * H // win_h, win_h, W, C).\n    3. Reshape again to split the width dimension (W) into smaller chunks of size win_w,\n       resulting in shape (B * H // win_h, win_h, W // win_w, win_w, C).\n    4. Permute the dimensions to arrange the tensor into shape (B * H // win_h, W // win_w, win_h, win_w, C).\n    5. Flatten the batch and width dimensions together to produce the final shape (batch_size * num_windows, win_h, win_w, C).\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the `window_partition` problem:\n\n```python\nfrom typing import Optional, Sequence\n\nimport torch\n\ndef window_partition(x: torch.Tensor, window_size: Sequence[int]):\n    \"\"\"\n    Partition an image feature map into smaller windows in an ANE (Apple Neural Engine) friendly manner,\n    avoiding the use of 6D tensors.\n\n    :param x: A 4D tensor representing the image feature map with shape (batch_size, H, W, C), where:\n              - batch_size (B) is the number of images in the batch,\n              - H is the height of the feature map,\n              - W is the width of the feature map,\n              - C is the number of channels.\n    :param window_size: A sequence of two integers (win_h, win_w) specifying the height and width of the windows.\n    :returns: A 4D tensor of partitioned feature map windows with shape (batch_size * num_windows, win_h, win_w, C), where:\n              - num_windows is the total number of windows per image (calculated as (H / win_h) * (W / win_w)),\n              - win_h is the window height,\n              - win_w is the window width,\n              - C is the number of channels.\n    :rtype: torch.Tensor\n\n    The process of partitioning is as follows:\n    1. Reshape the input tensor to split the height dimension (H) into smaller chunks of size win_h,\n       resulting in shape (B, H // win_h, win_h, W, C).\n    2. Flatten the batch and height dimensions together to shape (B * H // win_h, win_h, W, C).\n    3. Reshape again to split the width dimension (W) into smaller chunks of size win_w,\n       resulting in shape (B * H // win_h, win_h, W // win_w, win_w, C).\n    4. Permute the dimensions to arrange the tensor into shape (B * H // win_h, W // win_w, win_h, win_w, C).\n    5. Flatten the batch and width dimensions together to produce the final shape (batch_size * num_windows, win_h, win_w, C).\n    \"\"\"\n    B, H, W, C = x.shape\n    win_h, win_w = window_size\n\n    # Step 1: Reshape to split height dimension\n    x = x.view(B, H // win_h, win_h, W, C)\n\n    # Step 2: Flatten batch and height dimensions\n    x = x.reshape(-1, win_h, W, C)\n\n    # Step 3: Reshape to split width dimension\n    x = x.view(-1, win_h, W // win_w, win_w, C)\n\n    # Step 4: Permute dimensions\n    x = x.permute(0, 2, 1, 3, 4)\n\n    # Step 5: Flatten batch and width dimensions\n    x = x.reshape(-1, win_h, win_w, C)\n\n    return x\n```\n\nThe code follows the steps outlined in the problem description:\n\n1. It reshapes the input tensor `x` to split the height dimension (`H`) into smaller chunks of size `win_h`, resulting in a tensor with shape `(B, H // win_h, win_h, W, C)`.\n\n2. It flattens the batch and height dimensions together using `reshape()`, resulting in a tensor with shape `(B * H // win_h, win_h, W, C)`.\n\n3. It reshapes the tensor again to split the width dimension (`W`) into smaller chunks of size `win_w`, resulting in a tensor with shape `(B * H // win_h, win_h, W // win_w, win_w, C)`.\n\n4. It permutes the dimensions using `permute()` to arrange the tensor into the desired shape `(B * H // win_h, W // win_w, win_h, win_w, C)`.\n\n5. Finally, it flattens the batch and width dimensions together using `reshape()` to produce the final tensor with shape `(batch_size * num_windows, win_h, win_w, C)`.\n\nThe code returns the partitioned feature map windows as a 4D tensor.",
  "canonical_solution": "    \n    B, H, W, C = x.shape\n    # example partition process: 1, 12, 16, 160 -> 1, 2, 6, 16, 160 -> 2, 6, 16, 160 -> 2, 6, 2, 8, 160 -> ...\n    x = x.reshape(\n        (B, H // window_size[0], window_size[0], W, C)\n    )  # B, H//w_size, w_size, W, C\n    x = x.reshape(\n        (B * H // window_size[0], window_size[0], W, C)\n    )  # B * H // w_size, w_size, W, C\n    x = x.reshape(\n        (\n            B * H // window_size[0],\n            window_size[0],\n            W // window_size[1],\n            window_size[1],\n            -1,\n        )\n    )\n    x = x.permute((0, 2, 1, 3, 4))\n    windows = x.reshape((-1, window_size[0], window_size[1], C))\n    return windows\n\n\n",
  "unprocess_testcases": "a = torch.rand((2, 8, 8, 3))\nws = (4, 4)\nassert((SOLUTION_SIGNATURE(a, ws) == window_partition(a, ws)).all())\n\na = torch.rand((1, 10, 10, 5))\nws = (5, 2)\nassert((SOLUTION_SIGNATURE(a, ws) == window_partition(a, ws)).all())\n\na = torch.rand((1, 6, 8, 3))\nws = (3, 2)\nassert((SOLUTION_SIGNATURE(a, ws) == window_partition(a, ws)).all())\n"
 },
 {
  "task_id": "RWPB/27",
  "prompt": "import torch\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens the scores and labels from a batch of predictions, removing entries with a specified ignore label.\n\n    Parameters\n    ----------\n    scores : torch.Tensor\n        The tensor containing the prediction scores for each item in the batch.\n    labels : torch.Tensor\n        The tensor containing the true labels for each item in the batch.\n    ignore : int or float, optional\n        The label to ignore during the flattening process. If None, all labels are kept.\n\n    Returns\n    -------\n    vscores : torch.Tensor\n        A flattened tensor containing the prediction scores without the ignored labels.\n    vlabels : torch.Tensor\n        A flattened tensor containing the true labels without the ignored labels.\n\n    Notes\n    -----\n    This function is typically used in the binary classification case where 'scores' and 'labels'\n    are expected to be tensors of shape (batch_size, ...) and the function flattens them to a 1D\n    tensor (-1,). If the 'ignore' parameter is provided, it filters out the corresponding labels\n    and scores from the result.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `flatten_binary_scores` function:\n\n```python\nimport torch\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens the scores and labels from a batch of predictions, removing entries with a specified ignore label.\n\n    Parameters\n    ----------\n    scores : torch.Tensor\n        The tensor containing the prediction scores for each item in the batch.\n    labels : torch.Tensor\n        The tensor containing the true labels for each item in the batch.\n    ignore : int or float, optional\n        The label to ignore during the flattening process. If None, all labels are kept.\n\n    Returns\n    -------\n    vscores : torch.Tensor\n        A flattened tensor containing the prediction scores without the ignored labels.\n    vlabels : torch.Tensor\n        A flattened tensor containing the true labels without the ignored labels.\n\n    Notes\n    -----\n    This function is typically used in the binary classification case where 'scores' and 'labels'\n    are expected to be tensors of shape (batch_size, ...) and the function flattens them to a 1D\n    tensor (-1,). If the 'ignore' parameter is provided, it filters out the corresponding labels\n    and scores from the result.\n    \"\"\"\n    scores = scores.view(-1)\n    labels = labels.view(-1)\n\n    if ignore is not None:\n        valid_indices = labels != ignore\n        vscores = scores[valid_indices]\n        vlabels = labels[valid_indices]\n    else:\n        vscores = scores\n        vlabels = labels\n\n    return vscores, vlabels\n```\n\nExplanation:\n\n1. The function takes three parameters: `scores`, `labels`, and an optional `ignore` value.\n\n2. It flattens the `scores` and `labels` tensors using `view(-1)`, which reshapes them into 1D tensors.\n\n3. If the `ignore` parameter is provided (not None), it creates a boolean mask `valid_indices` using `labels != ignore`. This mask identifies the elements in `labels` that are not equal to the `ignore` value.\n\n4. It then applies the `valid_indices` mask to both `scores` and `labels` using boolean indexing, resulting in `vscores` and `vlabels` respectively. This step filters out the scores and labels corresponding to the ignored label.\n\n5. If the `ignore` parameter is not provided (None), it simply assigns `scores` to `vscores` and `labels` to `vlabels` without any filtering.\n\n6. Finally, the function returns the flattened and filtered `vscores` and `vlabels` tensors.\n\nThis implementation assumes that `scores` and `labels` have the same shape and that the `ignore` value, if provided, is a valid label value.",
  "canonical_solution": "\n    # Flatten the scores and labels to 1D tensors\n    scores = scores.view(-1)\n    labels = labels.view(-1)\n\n    # If there's no ignore label, return the flattened scores and labels\n    if ignore is None:\n        return scores, labels\n\n    # Create a boolean mask for entries that do not have the ignore label\n    valid = (labels != ignore)\n\n    # Apply the mask to filter out the ignored labels and return the valid scores and labels\n    vscores = scores[valid]\n    vlabels = labels[valid]\n\n    return vscores, vlabels\n\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([[0.5, 0.2], [0.7, 0.4]]), torch.tensor([[1, 1], [0, 0]]), 2)[0] == flatten_binary_scores(torch.tensor([[0.5, 0.2], [0.7, 0.4]]), torch.tensor([[1, 1], [0, 0]]), 2)[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[0.5, 0.2], [0.7, 0.4]]), torch.tensor([[1, 1], [0, 0]]), 2)[1] == flatten_binary_scores(torch.tensor([[0.5, 0.2], [0.7, 0.4]]), torch.tensor([[1, 1], [0, 0]]), 2)[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[0.1, 0.3], [0.5, 0.7]]), torch.tensor([[1, 2], [1, 2]]), 2)[0] == flatten_binary_scores(torch.tensor([[0.1, 0.3], [0.5, 0.7]]), torch.tensor([[1, 2], [1, 2]]), 2)[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[0.1, 0.3], [0.5, 0.7]]), torch.tensor([[1, 2], [1, 2]]), 2)[1] == flatten_binary_scores(torch.tensor([[0.1, 0.3], [0.5, 0.7]]), torch.tensor([[1, 2], [1, 2]]), 2)[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[0.2, 0.4], [0.6, 0.8]]), torch.tensor([[0, 1], [0, 1]]), None)[0] == flatten_binary_scores(torch.tensor([[0.2, 0.4], [0.6, 0.8]]), torch.tensor([[0, 1], [0, 1]]), None)[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[0.2, 0.4], [0.6, 0.8]]), torch.tensor([[0, 1], [0, 1]]), None)[1] == flatten_binary_scores(torch.tensor([[0.2, 0.4], [0.6, 0.8]]), torch.tensor([[0, 1], [0, 1]]), None)[1]).all())\n"
 },
 {
  "task_id": "RWPB/76",
  "prompt": "import torch\n\ndef quat_to_rotmat(quat: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Convert quaternion representation to rotation matrix.\n    Args:\n        quat (torch.Tensor) of shape (B, 4); 4 <===> (w, x, y, z).\n    Returns:\n        torch.Tensor: Corresponding rotation matrices with shape (B, 3, 3).\n    \"\"\"\n",
  "solution": "Here's the completed code to convert quaternions to rotation matrices using PyTorch:\n\n```python\nimport torch\n\ndef quat_to_rotmat(quat: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Convert quaternion representation to rotation matrix.\n    Args:\n        quat (torch.Tensor) of shape (B, 4); 4 <===> (w, x, y, z).\n    Returns:\n        torch.Tensor: Corresponding rotation matrices with shape (B, 3, 3).\n    \"\"\"\n    # Ensure the quaternion is normalized\n    quat = quat / quat.norm(dim=-1, keepdim=True)\n\n    # Extract quaternion components\n    w, x, y, z = quat[:, 0], quat[:, 1], quat[:, 2], quat[:, 3]\n\n    # Compute rotation matrix elements\n    xx = x * x\n    xy = x * y\n    xz = x * z\n    xw = x * w\n\n    yy = y * y\n    yz = y * z\n    yw = y * w\n\n    zz = z * z\n    zw = z * w\n\n    # Construct rotation matrix\n    rotmat = torch.stack([\n        1 - 2 * (yy + zz), 2 * (xy - zw), 2 * (xz + yw),\n        2 * (xy + zw), 1 - 2 * (xx + zz), 2 * (yz - xw),\n        2 * (xz - yw), 2 * (yz + xw), 1 - 2 * (xx + yy)\n    ], dim=-1).view(-1, 3, 3)\n\n    return rotmat\n```\n\nExplanation:\n1. The input `quat` is a tensor of shape (B, 4) representing quaternions, where B is the batch size and the quaternion components are in the order (w, x, y, z).\n2. The quaternions are normalized to ensure they have unit length using `quat.norm()` and element-wise division.\n3. The quaternion components (w, x, y, z) are extracted using tensor indexing.\n4. The rotation matrix elements are computed using the quaternion components based on the quaternion-to-rotation-matrix formula.\n5. The computed elements are stacked into a tensor of shape (B, 9) using `torch.stack()`, and then reshaped to (B, 3, 3) using `view()` to obtain the final rotation matrices.\n6. The function returns the rotation matrices as a tensor of shape (B, 3, 3).\n\nThis implementation assumes that the input quaternions are represented as (w, x, y, z) and converts them to the corresponding rotation matrices.",
  "canonical_solution": "    \n    norm_quat = quat\n    norm_quat = norm_quat / norm_quat.norm(p=2, dim=1, keepdim=True)\n    w, x, y, z = norm_quat[:, 0], norm_quat[:, 1], norm_quat[:, 2], norm_quat[:, 3]\n\n    B = quat.size(0)\n\n    w2, x2, y2, z2 = w.pow(2), x.pow(2), y.pow(2), z.pow(2)\n    wx, wy, wz = w * x, w * y, w * z\n    xy, xz, yz = x * y, x * z, y * z\n\n    rotMat = torch.stack(\n        [\n            w2 + x2 - y2 - z2,\n            2 * xy - 2 * wz,\n            2 * wy + 2 * xz,\n            2 * wz + 2 * xy,\n            w2 - x2 + y2 - z2,\n            2 * yz - 2 * wx,\n            2 * xz - 2 * wy,\n            2 * wx + 2 * yz,\n            w2 - x2 - y2 + z2,\n        ],\n        dim=1,\n    ).view(B, 3, 3)\n    return rotMat\n\n\n\n\n",
  "unprocess_testcases": "a = torch.tensor([[1.0, 0.0, 0.0, 0.0]])\nb = torch.tensor([\n    [0.7071, 0.7071, 0.0, 0.0],\n    [0.7071, 0.0, 0.7071, 0.0],\n    [0.7071, 0.0, 0.0, 0.7071]\n])\nc = torch.tensor([\n    [2.0, 0.0, 0.0, 0.0],\n    [1.4142, 1.4142, 0.0, 0.0],\n    [10.0, 0.0, 0.0, 10.0]\n])\n\nassert((SOLUTION_SIGNATURE(a) == quat_to_rotmat(a)).all())\nassert((SOLUTION_SIGNATURE(b) == quat_to_rotmat(b)).all())\nassert((SOLUTION_SIGNATURE(c) == quat_to_rotmat(c)).all())"
 },
 {
  "task_id": "RWPB/56",
  "prompt": "import torch\nimport numpy as np\nfrom torch.nn import functional as F\n\n\ndef grid_distortion(images, strength=0.5):\n    \"\"\"\n    Applies a grid-based distortion to a batch of images by perturbing the grid points.\n\n    Arguments:\n    images : torch.Tensor\n        A tensor of shape [B, C, H, W] representing the batch of images.\n    strength : float, optional\n        The strength of the distortion, must be a value between 0 and 1. Default is 0.5.\n\n    Returns:\n    torch.Tensor\n        A tensor of distorted images with the same shape as the input.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the grid distortion problem without unit test cases:\n\n```python\nimport torch\nimport numpy as np\nfrom torch.nn import functional as F\n\n\ndef grid_distortion(images, strength=0.5):\n    \"\"\"\n    Applies a grid-based distortion to a batch of images by perturbing the grid points.\n\n    Arguments:\n    images : torch.Tensor\n        A tensor of shape [B, C, H, W] representing the batch of images.\n    strength : float, optional\n        The strength of the distortion, must be a value between 0 and 1. Default is 0.5.\n\n    Returns:\n    torch.Tensor\n        A tensor of distorted images with the same shape as the input.\n    \"\"\"\n    if not 0 <= strength <= 1:\n        raise ValueError(\"Strength must be a value between 0 and 1.\")\n\n    batch_size, _, height, width = images.shape\n\n    # Create a grid of coordinates\n    grid_x, grid_y = torch.meshgrid(torch.linspace(-1, 1, width), torch.linspace(-1, 1, height))\n    grid = torch.stack([grid_x, grid_y], dim=-1).unsqueeze(0).repeat(batch_size, 1, 1, 1)\n\n    # Generate random noise for perturbation\n    noise_x = torch.rand_like(grid[..., 0]) * 2 - 1\n    noise_y = torch.rand_like(grid[..., 1]) * 2 - 1\n    noise = torch.stack([noise_x, noise_y], dim=-1)\n\n    # Apply perturbation to the grid\n    perturbed_grid = grid + strength * noise\n\n    # Apply grid distortion to the images\n    distorted_images = F.grid_sample(images, perturbed_grid, align_corners=True)\n\n    return distorted_images\n```\n\nExplanation:\n1. We first check if the provided `strength` value is between 0 and 1. If not, we raise a `ValueError`.\n\n2. We extract the batch size, height, and width of the input images.\n\n3. We create a grid of coordinates using `torch.meshgrid`. The grid represents the normalized coordinates in the range [-1, 1] for both height and width dimensions. We repeat the grid for each image in the batch.\n\n4. We generate random noise for perturbation using `torch.rand_like`. The noise values are scaled to the range [-1, 1].\n\n5. We apply the perturbation to the grid by adding the scaled noise to the original grid coordinates. The perturbation strength is controlled by the `strength` parameter.\n\n6. We use `F.grid_sample` to apply the perturbed grid to the input images. This function performs bilinear interpolation to sample the pixel values at the perturbed grid coordinates. We set `align_corners=True` to ensure proper alignment of the grid coordinates.\n\n7. Finally, we return the distorted images.\n\nThis code applies a grid-based distortion to a batch of images by perturbing the grid points. The strength of the distortion can be controlled using the `strength` parameter. The distorted images have the same shape as the input images.",
  "canonical_solution": "\n    B, C, H, W = images.shape\n\n    # Determine the resolution of the grid\n    num_steps = np.random.randint(8, 17)  # Random grid size between 8 and 16\n    grid_steps = torch.linspace(-1, 1, num_steps)  # Steps from -1 to 1 for normalized coordinates\n\n    grids = []  # To store grids for each image in the batch\n    for b in range(B):\n        # Generate distorted x-coordinates\n        x_steps = torch.linspace(0, 1, num_steps)  # Equally spaced steps between 0 and 1\n        # Add random perturbation scaled by the distortion strength\n        x_steps = (x_steps + strength * (torch.rand_like(x_steps) - 0.5) / (num_steps - 1)).clamp(0, 1)\n        x_steps = (x_steps * W).long()  # Scale steps to image width\n        x_steps[0] = 0\n        x_steps[-1] = W  # Ensure edges remain at the borders\n\n        xs = [torch.linspace(grid_steps[i], grid_steps[i + 1], x_steps[i + 1] - x_steps[i]) for i in\n              range(num_steps - 1)]\n        xs = torch.cat(xs, dim=0)  # Concatenate to form a single tensor\n\n        # Generate distorted y-coordinates using similar steps as x-coordinates\n        y_steps = torch.linspace(0, 1, num_steps)\n        y_steps = (y_steps + strength * (torch.rand_like(y_steps) - 0.5) / (num_steps - 1)).clamp(0, 1)\n        y_steps = (y_steps * H).long()\n        y_steps[0] = 0\n        y_steps[-1] = H\n\n        ys = [torch.linspace(grid_steps[i], grid_steps[i + 1], y_steps[i + 1] - y_steps[i]) for i in\n              range(num_steps - 1)]\n        ys = torch.cat(ys, dim=0)\n\n        # Construct the distortion grid using meshgrid\n        grid_x, grid_y = torch.meshgrid(xs, ys, indexing='xy')\n        grid = torch.stack([grid_x, grid_y], dim=-1)  # Stack to form [H, W, 2] grid\n\n        grids.append(grid)\n\n    grids = torch.stack(grids, dim=0).to(images.device)  # Convert list to tensor and move to the same device as images\n\n    # Apply grid sample to distort the images according to the generated grids\n    images = F.grid_sample(images, grids, align_corners=False)\n\n    return images\n\n",
  "unprocess_testcases": "a = torch.rand([2, 3, 32, 32])\nb = torch.rand([1, 1, 64, 64])\nc = torch.rand([1, 1, 64, 64])\nd = torch.rand([10, 3, 256, 256])\n\nassert(SOLUTION_SIGNATURE(a).size() == grid_distortion(a).size())\nassert(SOLUTION_SIGNATURE(b, 0).size()== grid_distortion(b, 0).size())\nassert(SOLUTION_SIGNATURE(c, 1).size() == grid_distortion(c, 1).size())\nassert(SOLUTION_SIGNATURE(d, 0.3).size() == grid_distortion(d, 0.3).size())\n"
 },
 {
  "task_id": "RWPB/33",
  "prompt": "import torch\n\n\ndef top_p_sample(prob_dist: torch.Tensor, top_p: float) -> torch.Tensor:\n    \"\"\"\n    Applies top-p filtering to a probability distribution to focus sampling on a subset of likely outputs.\n\n    Args:\n        prob_dist (torch.Tensor): The original probability distribution from which to sample, shape (batch_size, vocab_size).\n        top_p (float): The cumulative probability threshold used for filtering. Probabilities that add up to `top_p` are kept.\n\n    Returns:\n        torch.Tensor: The modified probability distribution after applying top-p filtering.\n\n    This method is used to reduce the sample space to the most probable tokens whose cumulative probability\n    exceeds the threshold `top_p`. It improves sampling outcomes by eliminating highly improbable options.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `top_p_sample` function:\n\n```python\nimport torch\n\n\ndef top_p_sample(prob_dist: torch.Tensor, top_p: float) -> torch.Tensor:\n    \"\"\"\n    Applies top-p filtering to a probability distribution to focus sampling on a subset of likely outputs.\n\n    Args:\n        prob_dist (torch.Tensor): The original probability distribution from which to sample, shape (batch_size, vocab_size).\n        top_p (float): The cumulative probability threshold used for filtering. Probabilities that add up to `top_p` are kept.\n\n    Returns:\n        torch.Tensor: The modified probability distribution after applying top-p filtering.\n\n    This method is used to reduce the sample space to the most probable tokens whose cumulative probability\n    exceeds the threshold `top_p`. It improves sampling outcomes by eliminating highly improbable options.\n    \"\"\"\n    # Check if the input is a valid probability distribution\n    if prob_dist.dim() != 2:\n        raise ValueError(\"Input probability distribution must be a 2D tensor.\")\n    if not torch.allclose(prob_dist.sum(dim=1), torch.ones(prob_dist.shape[0], device=prob_dist.device)):\n        raise ValueError(\"Input tensor must be a valid probability distribution.\")\n\n    # Sort the probabilities in descending order\n    sorted_probs, sorted_indices = torch.sort(prob_dist, descending=True)\n\n    # Calculate the cumulative probabilities\n    cumulative_probs = torch.cumsum(sorted_probs, dim=1)\n\n    # Find the index where the cumulative probability exceeds top_p\n    mask = cumulative_probs <= top_p\n\n    # Create a new tensor to store the filtered probabilities\n    filtered_probs = torch.zeros_like(prob_dist)\n\n    # Assign the filtered probabilities based on the mask\n    batch_indices = torch.arange(prob_dist.shape[0]).unsqueeze(1).expand_as(sorted_indices)\n    filtered_probs[batch_indices, sorted_indices] = mask.float() * sorted_probs\n\n    # Normalize the filtered probabilities\n    filtered_probs /= filtered_probs.sum(dim=1, keepdim=True)\n\n    return filtered_probs\n```\n\nExplanation:\n\n1. The function first checks if the input `prob_dist` is a valid probability distribution by verifying its shape and ensuring that the probabilities sum up to 1 for each batch.\n\n2. It then sorts the probabilities in descending order using `torch.sort()` to obtain the sorted probabilities and their corresponding indices.\n\n3. The cumulative probabilities are calculated using `torch.cumsum()` along the second dimension (vocab_size).\n\n4. A mask is created using the condition `cumulative_probs <= top_p` to identify the probabilities that should be kept based on the `top_p` threshold.\n\n5. A new tensor `filtered_probs` is created with the same shape as `prob_dist` to store the filtered probabilities.\n\n6. The filtered probabilities are assigned to `filtered_probs` based on the mask. The `batch_indices` tensor is used to align the indices correctly for each batch.\n\n7. Finally, the filtered probabilities are normalized by dividing them by their sum along the second dimension to ensure they form a valid probability distribution.\n\n8. The function returns the modified probability distribution after applying top-p filtering.\n\nThis implementation efficiently applies top-p filtering to a probability distribution, focusing the sampling on a subset of likely outputs based on the specified `top_p` threshold.",
  "canonical_solution": "\n    # Sort the probabilities and their corresponding indices in descending order\n    sorted_probs, sorted_indices = torch.sort(prob_dist, descending=True, dim=-1)\n\n    # Compute the cumulative sum of the sorted probabilities\n    cum_sum_probs = torch.cumsum(sorted_probs, dim=-1)  # Shape: (batch_size, vocab_size)\n\n    # Create a boolean mask where probabilities that exceed the threshold are set to True\n    sorted_indices_to_remove = cum_sum_probs > top_p\n\n    # Shift the indices to the right to include the first token above the threshold\n    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n    sorted_indices_to_remove[:, 0] = 0  # Ensure the first token is always included\n\n    # Convert the mask to boolean and apply it to sorted_probs by setting removed probs to 0\n    sorted_probs[sorted_indices_to_remove] = 0\n\n    # Reverse the sorting process to reconstruct the original order of probabilities\n    reversed_indices = torch.argsort(sorted_indices)\n    prob_dist = torch.gather(sorted_probs, -1, reversed_indices)\n\n    # Normalize the probabilities to ensure they sum to 1\n    prob_dist = prob_dist / prob_dist.sum(dim=-1, keepdim=True)\n\n    return prob_dist\n\n\n",
  "unprocess_testcases": "a = torch.tensor([[0.5, 0.2, 0.1, 0.1, 0.05, 0.05]])\nb = torch.tensor([[0.05, 0.15, 0.2, 0.3, 0.25, 0.05],\n                          [0.4, 0.1, 0.1, 0.1, 0.1, 0.2],\n                          [0.25, 0.25, 0.25, 0.15, 0.05, 0.05]])\nc = torch.tensor([[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]])\n\nassert((SOLUTION_SIGNATURE(a, 0.3) == top_p_sample(a, 0.3)).all())\nassert((SOLUTION_SIGNATURE(b, 0.3) == top_p_sample(b, 0.3)).all())\nassert((SOLUTION_SIGNATURE(c, 0.5) == top_p_sample(c, 0.5)).all())\n"
 },
 {
  "task_id": "RWPB/62",
  "prompt": "import torch\n\n\ndef broadcat(tensors, dim=-1):\n    \"\"\"\n    Broadcasts a list of tensors to a common shape and concatenates them along a specified dimension.\n    The function should return ValueError when the first dimensions of the tensors are incompatible for broadcasting.\n    \n    Arguments:\n    tensors : list of torch.Tensor\n        List of tensors to broadcast and concatenate.\n    dim : int, optional\n        Dimension along which to concatenate the tensors.\n\n    Returns:\n    torch.Tensor\n        A tensor resulting from broadcasting and concatenating the input tensors along the specified dimension.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport torch\n\n\ndef broadcat(tensors, dim=-1):\n    \"\"\"\n    Broadcasts a list of tensors to a common shape and concatenates them along a specified dimension.\n    The function should return ValueError when the first dimensions of the tensors are incompatible for broadcasting.\n    \n    Arguments:\n    tensors : list of torch.Tensor\n        List of tensors to broadcast and concatenate.\n    dim : int, optional\n        Dimension along which to concatenate the tensors.\n\n    Returns:\n    torch.Tensor\n        A tensor resulting from broadcasting and concatenating the input tensors along the specified dimension.\n    \"\"\"\n    # Check if the list of tensors is empty\n    if not tensors:\n        raise ValueError(\"The list of tensors is empty.\")\n\n    # Get the shape of the first tensor\n    first_shape = tensors[0].shape\n\n    # Check if the first dimensions of the tensors are compatible for broadcasting\n    for tensor in tensors[1:]:\n        if tensor.shape[:-1] != first_shape[:-1]:\n            raise ValueError(\"The first dimensions of the tensors are incompatible for broadcasting.\")\n\n    # Broadcast the tensors to a common shape\n    broadcasted_tensors = []\n    for tensor in tensors:\n        if tensor.shape != first_shape:\n            # Add singleton dimensions to match the shape of the first tensor\n            missing_dims = first_shape[:-1] + (1,) * (len(first_shape) - len(tensor.shape))\n            tensor = tensor.view(missing_dims + tensor.shape[-1:])\n        broadcasted_tensors.append(tensor)\n\n    # Concatenate the broadcasted tensors along the specified dimension\n    concatenated_tensor = torch.cat(broadcasted_tensors, dim=dim)\n\n    return concatenated_tensor\n```\n\nExplanation:\n\n1. First, we check if the list of tensors is empty. If it is, we raise a `ValueError` with an appropriate message.\n\n2. We get the shape of the first tensor in the list, which will be used as a reference for broadcasting.\n\n3. We iterate over the remaining tensors in the list and check if their first dimensions (excluding the last dimension) match the first dimensions of the first tensor. If they don't match, we raise a `ValueError` indicating that the first dimensions are incompatible for broadcasting.\n\n4. We create an empty list called `broadcasted_tensors` to store the broadcasted tensors.\n\n5. We iterate over each tensor in the list and check if its shape matches the shape of the first tensor. If they don't match, we add singleton dimensions to the tensor to match the shape of the first tensor. This is done by creating a new shape tuple that combines the first dimensions of the first tensor with the last dimension of the current tensor, and then using `tensor.view()` to reshape the tensor accordingly.\n\n6. We append each broadcasted tensor to the `broadcasted_tensors` list.\n\n7. Finally, we use `torch.cat()` to concatenate the broadcasted tensors along the specified dimension `dim`.\n\n8. We return the concatenated tensor.\n\nThis implementation handles the broadcasting of tensors to a common shape and concatenates them along the specified dimension. It also raises a `ValueError` if the first dimensions of the tensors are incompatible for broadcasting or if the list of tensors is empty.",
  "canonical_solution": "    \n\n    num_tensors = len(tensors)\n    if num_tensors == 0:\n        return ValueError\n\n    # Check that all tensors have the same number of dimensions\n    shape_lens = set(map(lambda t: len(t.shape), tensors))\n    if len(shape_lens) != 1:\n        return ValueError\n    shape_len = list(shape_lens)[0]\n\n    # Handle negative dimension indices\n    dim = (dim + shape_len) if dim < 0 else dim\n    if dim >= shape_len:\n        return ValueError\n\n    # Collect dimensions of all tensors and find maximum dimension sizes for broadcasting\n    dimensions = list(zip(*[t.shape for t in tensors]))\n    max_dims = [max(dim_sizes) for dim_sizes in dimensions]\n\n    # Ensure all dimensions are compatible for broadcasting\n    for i, dim_sizes in enumerate(dimensions):\n        if i != dim and any(size != max_dims[i] and size != 1 for size in dim_sizes):\n            return ValueError\n\n    # Broadcast each tensor to the maximum dimension size\n    broadcasted_tensors = [t.expand(*max_dims) for t in tensors]\n\n    # Concatenate the broadcasted tensors along the specified dimension\n    return torch.cat(broadcasted_tensors, dim=dim)\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE([torch.tensor([[1], [2]]), torch.tensor([[3, 3, 3], [4, 4, 4]])]) == broadcat([torch.tensor([[1], [2]]), torch.tensor([[3, 3, 3], [4, 4, 4]])])).all())\nassert((SOLUTION_SIGNATURE([torch.tensor([[1], [2], [3]]), torch.tensor([[4, 4], [5, 5], [6, 6]])], dim=-1) == broadcat([torch.tensor([[1], [2], [3]]), torch.tensor([[4, 4], [5, 5], [6, 6]])], dim=-1)).all())\nassert(SOLUTION_SIGNATURE([torch.tensor([[1, 1, 1], [2, 2, 2]]), torch.tensor([[3, 3, 3], [4, 4, 4], [5, 5, 5]])]) == broadcat([torch.tensor([[1, 1, 1], [2, 2, 2]]), torch.tensor([[3, 3, 3], [4, 4, 4], [5, 5, 5]])]))\n"
 },
 {
  "task_id": "RWPB/91",
  "prompt": "import math\nimport numpy as np\n\n\ndef r_sqrd(x, y):\n    \"\"\"\n    Calculate R^2 of best fit simple least squares regression line.\n    If denominator is zero, the code should return KeyError\n    Args:\n        - x: numpy array, shape (n,), x coordinates of (x,y) points.\n        - y: numpy array, shape (n,), y coordinates of (x,y) points.\n\n    Return:\n        r_squared: float, R^2 of the best fit simple least squares regression line.\n\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without unit test cases:\n\n```python\nimport math\nimport numpy as np\n\n\ndef r_sqrd(x, y):\n    \"\"\"\n    Calculate R^2 of best fit simple least squares regression line.\n    If denominator is zero, the code should return KeyError\n    Args:\n        - x: numpy array, shape (n,), x coordinates of (x,y) points.\n        - y: numpy array, shape (n,), y coordinates of (x,y) points.\n\n    Return:\n        r_squared: float, R^2 of the best fit simple least squares regression line.\n\n    \"\"\"\n    n = len(x)\n    \n    # Calculate means of x and y\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    \n    # Calculate sum of squares\n    ss_xx = np.sum((x - x_mean)**2)\n    ss_yy = np.sum((y - y_mean)**2)\n    ss_xy = np.sum((x - x_mean) * (y - y_mean))\n    \n    # Check if denominator is zero\n    if ss_xx == 0 or ss_yy == 0:\n        raise KeyError(\"Denominator is zero. Cannot calculate R^2.\")\n    \n    # Calculate R^2\n    r_squared = (ss_xy**2) / (ss_xx * ss_yy)\n    \n    return r_squared\n```\n\nExplanation:\n\n1. We define the function `r_sqrd` that takes two numpy arrays `x` and `y` as input.\n\n2. We calculate the number of data points `n` using `len(x)`.\n\n3. We calculate the means of `x` and `y` using `np.mean(x)` and `np.mean(y)`, respectively.\n\n4. We calculate the sum of squares:\n   - `ss_xx` is the sum of squares of the differences between each `x` value and the mean of `x`.\n   - `ss_yy` is the sum of squares of the differences between each `y` value and the mean of `y`.\n   - `ss_xy` is the sum of the products of the differences between each `x` value and the mean of `x`, and each `y` value and the mean of `y`.\n\n5. We check if either `ss_xx` or `ss_yy` is zero. If either of them is zero, we raise a `KeyError` with an appropriate error message since the denominator would be zero and we cannot calculate R^2.\n\n6. We calculate R^2 using the formula: R^2 = (ss_xy^2) / (ss_xx * ss_yy).\n\n7. Finally, we return the calculated R^2 value.\n\nThis code calculates the R^2 value of the best fit simple least squares regression line for the given `x` and `y` coordinates. If the denominator is zero, it raises a `KeyError` instead of returning a value.",
  "canonical_solution": "    \n    x_dot_x = x.dot(x)\n    x_dot_y = x.dot(y)\n    x_mean = x.mean()\n    x_sum = x.sum()\n    y_mean = y.mean()\n    denominator = x_dot_x - x_mean * x_sum\n    if denominator == 0:\n        return KeyError\n    \n    m = (x_dot_y - y_mean * x_sum) / denominator\n    b = (y_mean * x_dot_x - x_mean * x_dot_y) / denominator\n    y_pred = m * x + b\n    residuals = y - y_pred\n    tot = y - y_mean\n\n    if int(tot.dot(tot)) == 0:\n        return KeyError\n\n    return 1.0 - residuals.dot(residuals) / tot.dot(tot)\n\n    \n\n",
  "unprocess_testcases": "x = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 4, 6, 8, 10])\nassert(SOLUTION_SIGNATURE(x, y) == r_sqrd(x, y))\n\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 2, 2, 2, 2])\nassert(SOLUTION_SIGNATURE(x, y) == r_sqrd(x, y))\n\nx = np.array([1, 1, 1, 1, 1])\ny = np.array([1, 2, 3, 4, 5])\nassert(SOLUTION_SIGNATURE(x, y) == r_sqrd(x, y))\n\nx = np.array([1, 1, 3, 1, 5])\ny = np.array([1, 0, 3, 1, 5])\nassert(SOLUTION_SIGNATURE(x, y) == r_sqrd(x, y))\n\nx = np.array([6, 1, 5, 7, 5])\ny = np.array([0, -1, 3, 2, 3])\nassert(SOLUTION_SIGNATURE(x, y) == r_sqrd(x, y))\n"
 },
 {
  "task_id": "RWPB/81",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\ndef remove_outliers(arr, num_std_devs=2):\n    \"\"\"\n    Remove outliers from an array based on a specified number of standard deviations from the mean.\n    Args:\n        - arr: numpy array, the input array\n        - num_std_devs: int, number of standard deviations from the mean to consider as outliers (default is 2)\n    Returns:\n        filtered_arr: numpy array, the array with outliers removed\n    \"\"\"\n",
  "solution": "Here's the completed code to remove outliers from an array based on a specified number of standard deviations from the mean:\n\n```python\nimport math\nimport numpy as np\n\ndef remove_outliers(arr, num_std_devs=2):\n    \"\"\"\n    Remove outliers from an array based on a specified number of standard deviations from the mean.\n    Args:\n        - arr: numpy array, the input array\n        - num_std_devs: int, number of standard deviations from the mean to consider as outliers (default is 2)\n    Returns:\n        filtered_arr: numpy array, the array with outliers removed\n    \"\"\"\n    # Calculate the mean and standard deviation of the array\n    mean = np.mean(arr)\n    std_dev = np.std(arr)\n    \n    # Define the lower and upper bounds for outliers\n    lower_bound = mean - num_std_devs * std_dev\n    upper_bound = mean + num_std_devs * std_dev\n    \n    # Create a boolean mask for values within the bounds\n    mask = (arr >= lower_bound) & (arr <= upper_bound)\n    \n    # Apply the mask to the array to remove outliers\n    filtered_arr = arr[mask]\n    \n    return filtered_arr\n```\n\nExplanation:\n1. The function takes two arguments: `arr` (the input array) and `num_std_devs` (the number of standard deviations from the mean to consider as outliers, default is 2).\n\n2. It calculates the mean and standard deviation of the input array using `np.mean()` and `np.std()`, respectively.\n\n3. The lower and upper bounds for outliers are defined by subtracting and adding the product of `num_std_devs` and the standard deviation to the mean, respectively.\n\n4. A boolean mask is created using the conditions `(arr >= lower_bound) & (arr <= upper_bound)`. This mask will have `True` values for elements within the bounds and `False` values for elements outside the bounds (outliers).\n\n5. The boolean mask is applied to the input array using `arr[mask]`, which selects only the elements that satisfy the mask condition (i.e., elements within the bounds).\n\n6. The filtered array with outliers removed is returned.\n\nThis function removes outliers from the input array based on the specified number of standard deviations from the mean. Elements that fall outside the range of `mean - num_std_devs * std_dev` to `mean + num_std_devs * std_dev` are considered outliers and are excluded from the resulting filtered array.",
  "canonical_solution": "    \n    if len(arr) < 2:\n        return arr\n    mean = np.mean(arr)\n    std_dev = np.std(arr)\n    return arr[np.abs(arr - mean) <= num_std_devs * std_dev]\n\n\n",
  "unprocess_testcases": "a = np.random.normal(50, 10, size=100)\nb = np.linspace(0, 100, 100)\nc = np.append(np.random.normal(50, 10, size=95), [150, 200, -50, -100])\n\nassert((SOLUTION_SIGNATURE(a) == remove_outliers(a)).all())\nassert((SOLUTION_SIGNATURE(b) == remove_outliers(b)).all())\nassert((SOLUTION_SIGNATURE(c) == remove_outliers(c)).all())\n"
 },
 {
  "task_id": "RWPB/72",
  "prompt": "import numpy as np\n\ndef deriv_fitting_matrix(degree, t_end=1.0):\n  \"\"\"\n    Returns A s.t. that the vector x that satisfies\n    If degree % 2 != 0, return AssertionError\n\n    Ax = b\n\n    contains polynomial coefficients\n\n      p(t) = x_1 + x_2 t + x_3 t^2 ...\n\n    so that\n\n    p(0) = b_1\n    p^(i)(0) = b_{i+1}\n    p(t_end) = b_{degree / 2}\n    p^(i)(t_end) = b_{degree / 2 + i + 1}\n\n    i.e. the first degree / 2 derivatives of p at 0 match the first degree / 2\n         entries of b and the first degree / 2 derivatives of p at 1, match the last\n         degree / 2 entries of b\n    Args:\n        - degree: int, degree of the polynomial\n        - t_end: float, end point for the polynomial (default is 1.0)\n\n    Returns:\n        - A: numpy.ndarray, fitting matrix A\n  \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport numpy as np\n\ndef deriv_fitting_matrix(degree, t_end=1.0):\n    \"\"\"\n    Returns A s.t. that the vector x that satisfies\n    If degree % 2 != 0, return AssertionError\n\n    Ax = b\n\n    contains polynomial coefficients\n\n      p(t) = x_1 + x_2 t + x_3 t^2 ...\n\n    so that\n\n    p(0) = b_1\n    p^(i)(0) = b_{i+1}\n    p(t_end) = b_{degree / 2}\n    p^(i)(t_end) = b_{degree / 2 + i + 1}\n\n    i.e. the first degree / 2 derivatives of p at 0 match the first degree / 2\n         entries of b and the first degree / 2 derivatives of p at 1, match the last\n         degree / 2 entries of b\n    Args:\n        - degree: int, degree of the polynomial\n        - t_end: float, end point for the polynomial (default is 1.0)\n\n    Returns:\n        - A: numpy.ndarray, fitting matrix A\n    \"\"\"\n    if degree % 2 != 0:\n        raise AssertionError(\"Degree must be even\")\n\n    half_degree = degree // 2\n    A = np.zeros((degree, degree))\n\n    # Set the first half_degree rows for p(0) and its derivatives\n    for i in range(half_degree):\n        for j in range(i, degree):\n            if j == i:\n                A[i, j] = np.math.factorial(i)\n            else:\n                A[i, j] = 0\n\n    # Set the last half_degree rows for p(t_end) and its derivatives\n    for i in range(half_degree):\n        for j in range(degree):\n            A[half_degree + i, j] = (t_end ** j) * np.math.factorial(j) / np.math.factorial(j - i)\n\n    return A\n```\n\nExplanation:\n1. We first check if the degree is even. If it's odd, we raise an `AssertionError` as per the problem statement.\n\n2. We calculate `half_degree` as `degree // 2` to get the integer division result.\n\n3. We create a zero matrix `A` of size `(degree, degree)` to store the fitting matrix.\n\n4. We set the first `half_degree` rows of `A` to represent the constraints for `p(0)` and its derivatives up to `p^(half_degree-1)(0)`. We set the diagonal elements to the factorial of the corresponding derivative order and keep the rest as zeros.\n\n5. We set the last `half_degree` rows of `A` to represent the constraints for `p(t_end)` and its derivatives up to `p^(half_degree-1)(t_end)`. We calculate the values using the formula `(t_end ** j) * np.math.factorial(j) / np.math.factorial(j - i)`, where `i` represents the derivative order and `j` represents the polynomial term.\n\n6. Finally, we return the fitting matrix `A`.\n\nThis code implements the desired functionality of generating the fitting matrix `A` based on the given degree and end point `t_end`.",
  "canonical_solution": "\n  if degree % 2 == 0:\n    return AssertionError\n\n  A = np.zeros((degree, degree))\n\n  ts = t_end ** np.array(range(degree))\n\n  constant_term = 1\n  poly = np.ones(degree)\n  for i in range(degree // 2):\n    A[i, i] = constant_term\n    A[degree // 2 + i, :] = np.hstack((np.zeros(i), poly * ts[:degree - i]))\n    poly = np.polynomial.polynomial.polyder(poly)\n    constant_term *= (i + 1)\n\n  return A\n\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(2, 1.0) == deriv_fitting_matrix(2, 1.0))\nassert(SOLUTION_SIGNATURE(6, 1.0) == deriv_fitting_matrix(6, 1.0))\nassert((SOLUTION_SIGNATURE(5, 0.5) == deriv_fitting_matrix(5, 0.5)).all())\nassert((SOLUTION_SIGNATURE(31, 0.9) == deriv_fitting_matrix(31, 0.9)).all())\nassert((SOLUTION_SIGNATURE(73, 0.1) == deriv_fitting_matrix(73, 0.1)).all())\nassert((SOLUTION_SIGNATURE(9, 3) == deriv_fitting_matrix(9, 3)).all())\n"
 },
 {
  "task_id": "RWPB/23",
  "prompt": "def merge(ids, pair, idx):\n    \"\"\"\n    Merges consecutive occurrences of a pair of integers in a list with a new integer.\n\n    Parameters\n    ----------\n    ids : list[int]\n        The list of integers where the merging will occur.\n    pair : tuple[int, int]\n        The pair of integers to be merged.\n    idx : int\n        The new integer that will replace the consecutive occurrences of 'pair'.\n\n    Returns\n    -------\n    list[int]\n        A new list of integers with all occurrences of 'pair' merged into 'idx'.\n\n    Examples\n    --------\n    >>> merge([1, 2, 3, 1, 2], (1, 2), 4)\n    [4, 3, 4]\n\n    The function iterates through the list 'ids' and checks for consecutive occurrences\n    of the integers in 'pair'. When such a pair is found, it is replaced by 'idx' in the\n    new list 'newids'. If the pair is not found, the current integer is simply appended to\n    'newids'. The function returns the modified list with the pairs merged.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `merge` function based on the given problem description:\n\n```python\ndef merge(ids, pair, idx):\n    \"\"\"\n    Merges consecutive occurrences of a pair of integers in a list with a new integer.\n\n    Parameters\n    ----------\n    ids : list[int]\n        The list of integers where the merging will occur.\n    pair : tuple[int, int]\n        The pair of integers to be merged.\n    idx : int\n        The new integer that will replace the consecutive occurrences of 'pair'.\n\n    Returns\n    -------\n    list[int]\n        A new list of integers with all occurrences of 'pair' merged into 'idx'.\n\n    Examples\n    --------\n    >>> merge([1, 2, 3, 1, 2], (1, 2), 4)\n    [4, 3, 4]\n\n    The function iterates through the list 'ids' and checks for consecutive occurrences\n    of the integers in 'pair'. When such a pair is found, it is replaced by 'idx' in the\n    new list 'newids'. If the pair is not found, the current integer is simply appended to\n    'newids'. The function returns the modified list with the pairs merged.\n    \"\"\"\n    newids = []\n    i = 0\n    while i < len(ids):\n        if i < len(ids) - 1 and (ids[i], ids[i+1]) == pair:\n            newids.append(idx)\n            i += 2\n        else:\n            newids.append(ids[i])\n            i += 1\n    return newids\n```\n\nExplanation:\n1. We initialize an empty list called `newids` to store the modified list of integers.\n2. We start a loop with index `i` initialized to 0. The loop continues as long as `i` is less than the length of `ids`.\n3. Inside the loop, we check if the current index `i` is less than the second-to-last index of `ids` (to avoid going out of bounds) and if the pair of integers at indices `i` and `i+1` matches the given `pair`.\n   - If the condition is true, we append `idx` to `newids` and increment `i` by 2 to skip the next integer.\n   - If the condition is false, we simply append the integer at index `i` to `newids` and increment `i` by 1.\n4. After the loop ends, we return the modified list `newids`.\n\nThis implementation iterates through the list `ids` and checks for consecutive occurrences of the given `pair`. When such a pair is found, it is replaced by `idx` in the new list `newids`. If the pair is not found, the current integer is simply appended to `newids`. Finally, the function returns the modified list with the pairs merged.",
  "canonical_solution": "\n    newids = []\n    i = 0\n    while i < len(ids):\n        # Check if the current and next item form the pair and are not the last element\n        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n            # Append the idx to the new list and skip the next item\n            newids.append(idx)\n            i += 2\n        else:\n            # Append the current item to the new list\n            newids.append(ids[i])\n            i += 1\n    return newids\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(ids = [1, 2, 1, 2, 1, 2, 3], pair = (1, 2), idx = 4) == merge(ids = [1, 2, 1, 2, 1, 2, 3], pair = (1, 2), idx = 4))\nassert(SOLUTION_SIGNATURE(ids = [1, 3, 4, 5, 6], pair = (1, 2), idx = 4) == merge(ids = [1, 3, 4, 5, 6], pair = (1, 2), idx = 4))\nassert(SOLUTION_SIGNATURE(ids = [1, 2, 3, 4, 1, 2], pair = (1, 2), idx = 4) == merge(ids = [1, 2, 3, 4, 1, 2], pair = (1, 2), idx = 4))\n"
 },
 {
  "task_id": "RWPB/17",
  "prompt": "import numpy as np\n\ndef generate_linear_parameters(start, stop, num):\n    \"\"\"\n    Generates a list of linearly spaced parameters between start and stop.\n\n    Parameters\n    ----------\n    start : float\n        The starting value of the sequence.\n    stop : float\n        The ending value of the sequence.\n    num : int\n        The number of values to generate.\n\n    Returns\n    -------\n    List[float]\n        A list containing 'num' values, linearly spaced between 'start' and 'stop'.\n\n    Notes\n    -----\n    This function uses numpy's linspace function to create an array of 'num' values evenly\n    distributed over the interval [start, stop]. The resulting array is then converted to a list.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport numpy as np\n\ndef generate_linear_parameters(start, stop, num):\n    \"\"\"\n    Generates a list of linearly spaced parameters between start and stop.\n\n    Parameters\n    ----------\n    start : float\n        The starting value of the sequence.\n    stop : float\n        The ending value of the sequence.\n    num : int\n        The number of values to generate.\n\n    Returns\n    -------\n    List[float]\n        A list containing 'num' values, linearly spaced between 'start' and 'stop'.\n\n    Notes\n    -----\n    This function uses numpy's linspace function to create an array of 'num' values evenly\n    distributed over the interval [start, stop]. The resulting array is then converted to a list.\n    \"\"\"\n    return np.linspace(start, stop, num).tolist()\n```\n\nIn this code, we use numpy's `linspace` function to generate an array of `num` values evenly distributed over the interval `[start, stop]`. The `linspace` function takes three arguments:\n\n1. `start`: The starting value of the sequence.\n2. `stop`: The ending value of the sequence.\n3. `num`: The number of values to generate.\n\nThe `linspace` function returns a numpy array containing the generated values. We then convert the array to a list using the `tolist()` method and return it.\n\nThe resulting list will contain `num` values, linearly spaced between `start` and `stop`.\n\nFor example, if we call the function with the following arguments:\n```python\ngenerate_linear_parameters(1, 10, 5)\n```\n\nIt will return the following list:\n```\n[1.0, 3.25, 5.5, 7.75, 10.0]\n```\n\nThis list contains 5 values, linearly spaced between 1 and 10.",
  "canonical_solution": "\n    # Generate linearly spaced values between 'start' and 'stop'\n    parames = list(\n        np.linspace(\n            start=start,\n            stop=stop,\n            num=num,\n        )\n    )\n    return parames\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(start=0, stop=10, num=5) == generate_linear_parameters(start=0, stop=10, num=5))\nassert(SOLUTION_SIGNATURE(start=0, stop=10, num=1) == generate_linear_parameters(start=0, stop=10, num=1))\nassert(SOLUTION_SIGNATURE(start=0, stop=10, num=0) == generate_linear_parameters(start=0, stop=10, num=0))\n"
 },
 {
  "task_id": "RWPB/46",
  "prompt": "from typing import List, Union\n\nimport numpy as np\ndef cosine_similarity(X: Union[List[List[float]], List[np.ndarray], np.ndarray], Y: Union[List[List[float]], List[np.ndarray], np.ndarray]) -> np.ndarray:\n    \"\"\"\n    Compute the row-wise cosine similarity between two matrices of equal width.\n    If the length of x is not equal the length of Y, return ValueError\n    \n    Parameters:\n    X (Union[List[List[float]], List[np.ndarray], np.ndarray]): A matrix (or a list of lists) where each row represents a vector.\n    Y (Union[List[List[float]], List[np.ndarray], np.ndarray]): Another matrix (or a list of lists) where each row represents a vector. Must have the same number of columns as X.\n\n    Returns:\n    np.ndarray: A matrix of cosine similarities. Each element at position (i, j) represents the cosine similarity between the i-th row of X and the j-th row of Y.\n\n    The process involves:\n    1. Converting the inputs X and Y to numpy arrays if they are not already in that format.\n    2. Checking that X and Y have the same number of columns to ensure they are comparable.\n    3. Calculating the L2 norm (Euclidean norm) of each row in X and Y.\n    4. Computing the cosine similarity using the dot product of X and Y, normalized by the outer product of the row norms.\n    5. Handling cases where division by zero or invalid operations might occur by setting such results to zero.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nfrom typing import List, Union\n\nimport numpy as np\n\ndef cosine_similarity(X: Union[List[List[float]], List[np.ndarray], np.ndarray], Y: Union[List[List[float]], List[np.ndarray], np.ndarray]) -> np.ndarray:\n    \"\"\"\n    Compute the row-wise cosine similarity between two matrices of equal width.\n    If the length of x is not equal the length of Y, return ValueError\n    \n    Parameters:\n    X (Union[List[List[float]], List[np.ndarray], np.ndarray]): A matrix (or a list of lists) where each row represents a vector.\n    Y (Union[List[List[float]], List[np.ndarray], np.ndarray]): Another matrix (or a list of lists) where each row represents a vector. Must have the same number of columns as X.\n\n    Returns:\n    np.ndarray: A matrix of cosine similarities. Each element at position (i, j) represents the cosine similarity between the i-th row of X and the j-th row of Y.\n\n    The process involves:\n    1. Converting the inputs X and Y to numpy arrays if they are not already in that format.\n    2. Checking that X and Y have the same number of columns to ensure they are comparable.\n    3. Calculating the L2 norm (Euclidean norm) of each row in X and Y.\n    4. Computing the cosine similarity using the dot product of X and Y, normalized by the outer product of the row norms.\n    5. Handling cases where division by zero or invalid operations might occur by setting such results to zero.\n    \"\"\"\n    # Convert X and Y to numpy arrays\n    X = np.array(X)\n    Y = np.array(Y)\n\n    # Check if X and Y have the same number of columns\n    if X.shape[1] != Y.shape[1]:\n        raise ValueError(\"X and Y must have the same number of columns\")\n\n    # Calculate the L2 norm of each row in X and Y\n    X_norm = np.linalg.norm(X, axis=1)\n    Y_norm = np.linalg.norm(Y, axis=1)\n\n    # Compute the cosine similarity\n    similarity = np.dot(X, Y.T) / np.outer(X_norm, Y_norm)\n\n    # Handle division by zero and invalid operations\n    similarity[np.isnan(similarity) | np.isinf(similarity)] = 0\n\n    return similarity\n```\n\nExplanation:\n1. The function first converts the inputs `X` and `Y` to numpy arrays using `np.array()` to ensure they are in the correct format.\n\n2. It then checks if `X` and `Y` have the same number of columns by comparing their shapes. If they don't match, a `ValueError` is raised.\n\n3. The L2 norm (Euclidean norm) of each row in `X` and `Y` is calculated using `np.linalg.norm()` with `axis=1` to perform the calculation along each row.\n\n4. The cosine similarity is computed using the dot product of `X` and the transpose of `Y` (`Y.T`), normalized by the outer product of the row norms (`np.outer(X_norm, Y_norm)`). This calculates the cosine similarity between each pair of rows from `X` and `Y`.\n\n5. To handle cases where division by zero or invalid operations (such as NaN or infinity) might occur, the code uses boolean indexing to identify such elements in the `similarity` matrix and sets them to zero.\n\n6. Finally, the function returns the computed cosine similarity matrix.\n\nThis implementation assumes that `X` and `Y` are either lists of lists, lists of numpy arrays, or numpy arrays, and it handles the conversion to numpy arrays internally. The resulting cosine similarity matrix has shape `(len(X), len(Y))`, where each element represents the cosine similarity between the corresponding rows of `X` and `Y`.",
  "canonical_solution": "\n    if len(X) == 0 or len(Y) == 0:\n        return np.array([])\n\n    X = np.array(X)\n    Y = np.array(Y)\n    if X.shape[1] != Y.shape[1]:\n        return ValueError\n    \n    X_norm = np.linalg.norm(X, axis=1)\n    Y_norm = np.linalg.norm(Y, axis=1)\n    # Ignore divide by zero errors run time warnings as those are handled below.\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        similarity = np.dot(X, Y.T) / np.outer(X_norm, Y_norm)\n    similarity[np.isnan(similarity) | np.isinf(similarity)] = 0.0\n    return similarity\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE([[1, 2], [3, 4]], [[5, 6], [7, 8]]) == cosine_similarity([[1, 2], [3, 4]], [[5, 6], [7, 8]])).all())\nassert((SOLUTION_SIGNATURE([[0, 0], [1, 1]], [[1, 1], [0, 0]]) == cosine_similarity([[0, 0], [1, 1]], [[1, 1], [0, 0]])).all())\nassert(SOLUTION_SIGNATURE([[1, 2, 3], [4, 5, 6]], [[1, 2], [3, 4]]) == cosine_similarity([[1, 2, 3], [4, 5, 6]], [[1, 2], [3, 4]]))\nassert((SOLUTION_SIGNATURE(np.array([[1, 2], [3, 4]]), [[5, 6], [7, 8]]) == cosine_similarity(np.array([[1, 2], [3, 4]]), [[5, 6], [7, 8]])).all())\nassert((SOLUTION_SIGNATURE(np.array([[0, 0], [1, 1]]), np.array([[1, 1], [0, 0]])) == cosine_similarity(np.array([[0, 0], [1, 1]]), np.array([[1, 1], [0, 0]]))).all())\n"
 },
 {
  "task_id": "RWPB/57",
  "prompt": "def clean_folder_name(folder_name: str) -> str:\n    \"\"\"\n    Sanitizes a folder name by replacing characters that are invalid in file paths with underscores.\n\n    Arguments:\n    folder_name : str\n        The original folder name that may contain invalid characters.\n\n    Returns:\n    str\n        A sanitized version of the folder name where all invalid characters have been replaced with underscores.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport re\n\ndef clean_folder_name(folder_name: str) -> str:\n    \"\"\"\n    Sanitizes a folder name by replacing characters that are invalid in file paths with underscores.\n\n    Arguments:\n    folder_name : str\n        The original folder name that may contain invalid characters.\n\n    Returns:\n    str\n        A sanitized version of the folder name where all invalid characters have been replaced with underscores.\n    \"\"\"\n    # Define a regular expression pattern for invalid characters\n    invalid_chars_pattern = r'[<>:\"/\\\\|?*]'\n    \n    # Replace invalid characters with underscores\n    sanitized_folder_name = re.sub(invalid_chars_pattern, '_', folder_name)\n    \n    return sanitized_folder_name\n```\n\nExplanation:\n\n1. We import the `re` module to use regular expressions for pattern matching and replacement.\n\n2. Inside the `clean_folder_name` function, we define a regular expression pattern `invalid_chars_pattern` that matches characters that are invalid in file paths. These characters include `<`, `>`, `:`, `\"`, `/`, `\\`, `|`, `?`, and `*`.\n\n3. We use the `re.sub()` function to replace all occurrences of invalid characters in the `folder_name` string with underscores (`'_'`). The `re.sub()` function takes three arguments:\n   - The regular expression pattern to match (`invalid_chars_pattern`)\n   - The replacement string (`'_'`)\n   - The input string (`folder_name`)\n\n4. The sanitized folder name, with all invalid characters replaced by underscores, is stored in the `sanitized_folder_name` variable.\n\n5. Finally, we return the `sanitized_folder_name`.\n\nThis function ensures that the folder name is sanitized by replacing any invalid characters with underscores, making it safe to use as a file path.",
  "canonical_solution": "\n    cleaned_name = folder_name  # Start with the original folder name\n    invalid_chars = '<>:\"/\\\\|?*.'  # Define a string of characters that are invalid in file names\n\n    # Replace each invalid character with an underscore\n    for char in invalid_chars:\n        cleaned_name = cleaned_name.replace(char, \"_\")\n\n    return cleaned_name\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(\"Project<Name>:Version/1.2\\3|4*5?6\") == clean_folder_name(\"Project<Name>:Version/1.2\\3|4*5?6\"))\nassert(SOLUTION_SIGNATURE(\"Regular_Folder_Name\") == clean_folder_name(\"Regular_Folder_Name\"))\nassert(SOLUTION_SIGNATURE(\"<>>:|?**....\") == clean_folder_name(\"<>>:|?**....\"))\n"
 },
 {
  "task_id": "RWPB/32",
  "prompt": "import math\nimport torch\n\ndef repeat_to_batch_size(tensor, batch_size):\n    \"\"\"\n    Repeats the elements of the tensor to match the specified batch size or truncates it if larger.\n\n    Parameters\n    ----------\n    tensor : torch.Tensor\n        The input tensor which may need to be repeated or truncated.\n    batch_size : int\n        The target batch size to match the tensor's size.\n\n    Returns\n    -------\n    torch.Tensor\n        The tensor with its first dimension adjusted to match the batch size, either by\n        repeating elements or truncating the tensor.\n\n    Notes\n    -----\n    This function checks the size of the input tensor along its first dimension and performs\n    one of the following operations:\n    - If the tensor's size is greater than the batch size, it truncates the tensor to the batch size.\n    - If the tensor's size is smaller than the batch size, it repeats the elements of the tensor\n      to match the batch size, rounding up the number of repetitions if necessary.\n    - If the tensor's size is equal to the batch size, it returns the tensor unchanged.\n\n    The function uses PyTorch's repeat method to duplicate the tensor elements along the first\n    dimension and math.ceil to ensure that the tensor is repeated enough times to reach at least\n    the batch size.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\nimport math\nimport torch\n\ndef repeat_to_batch_size(tensor, batch_size):\n    \"\"\"\n    Repeats the elements of the tensor to match the specified batch size or truncates it if larger.\n\n    Parameters\n    ----------\n    tensor : torch.Tensor\n        The input tensor which may need to be repeated or truncated.\n    batch_size : int\n        The target batch size to match the tensor's size.\n\n    Returns\n    -------\n    torch.Tensor\n        The tensor with its first dimension adjusted to match the batch size, either by\n        repeating elements or truncating the tensor.\n\n    Notes\n    -----\n    This function checks the size of the input tensor along its first dimension and performs\n    one of the following operations:\n    - If the tensor's size is greater than the batch size, it truncates the tensor to the batch size.\n    - If the tensor's size is smaller than the batch size, it repeats the elements of the tensor\n      to match the batch size, rounding up the number of repetitions if necessary.\n    - If the tensor's size is equal to the batch size, it returns the tensor unchanged.\n\n    The function uses PyTorch's repeat method to duplicate the tensor elements along the first\n    dimension and math.ceil to ensure that the tensor is repeated enough times to reach at least\n    the batch size.\n    \"\"\"\n    tensor_size = tensor.size(0)\n\n    if tensor_size > batch_size:\n        # Truncate the tensor to match the batch size\n        return tensor[:batch_size]\n    elif tensor_size < batch_size:\n        # Calculate the number of repetitions needed to match the batch size\n        num_repeats = math.ceil(batch_size / tensor_size)\n        \n        # Repeat the tensor along the first dimension\n        repeated_tensor = tensor.repeat(num_repeats, *[1] * (tensor.dim() - 1))\n        \n        # Truncate the repeated tensor to match the batch size\n        return repeated_tensor[:batch_size]\n    else:\n        # Tensor size matches the batch size, return it unchanged\n        return tensor\n```\n\nThis code completes the `repeat_to_batch_size` function based on the provided problem description and comments. Here's how it works:\n\n1. The function takes two parameters: `tensor` (the input tensor) and `batch_size` (the target batch size).\n\n2. It first checks the size of the tensor along its first dimension using `tensor.size(0)`.\n\n3. If the tensor size is greater than the batch size, it truncates the tensor to match the batch size using slicing: `tensor[:batch_size]`.\n\n4. If the tensor size is smaller than the batch size:\n   - It calculates the number of repetitions needed to match the batch size using `math.ceil(batch_size / tensor_size)`. This ensures that the tensor is repeated enough times to reach at least the batch size.\n   - It repeats the tensor along the first dimension using `tensor.repeat(num_repeats, *[1] * (tensor.dim() - 1))`. The `*[1] * (tensor.dim() - 1)` part ensures that the tensor is repeated only along the first dimension while keeping the other dimensions unchanged.\n   - It truncates the repeated tensor to match the batch size using slicing: `repeated_tensor[:batch_size]`.\n\n5. If the tensor size matches the batch size, it returns the tensor unchanged.\n\n6. Finally, the function returns the adjusted tensor with its first dimension matching the specified batch size.\n\nThis code handles all the cases mentioned in the problem description and adjusts the tensor accordingly to match the target batch size.",
  "canonical_solution": "\n    if tensor.shape[0] > batch_size:\n        # If the tensor is larger than the batch size, truncate it\n        return tensor[:batch_size]\n    elif tensor.shape[0] < batch_size:\n        # If the tensor is smaller than the batch size, calculate the number of repetitions\n        repeat_times = math.ceil(batch_size / tensor.shape[0])\n        # Repeat the tensor along the first dimension and then truncate to batch size\n        return tensor.repeat([repeat_times] + [1] * (len(tensor.shape) - 1))[:batch_size]\n    # If the tensor's size is equal to the batch size, return it as is\n    return tensor\n\n\n",
  "unprocess_testcases": "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\nb = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15], [16, 17, 18]])\nc = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n\nassert((SOLUTION_SIGNATURE(a, 4) == repeat_to_batch_size(a, 4)).all())\nassert((SOLUTION_SIGNATURE(b, 4) == repeat_to_batch_size(b, 4)).all())\nassert((SOLUTION_SIGNATURE(c, 4) == repeat_to_batch_size(c, 4)).all())\n"
 },
 {
  "task_id": "RWPB/63",
  "prompt": "import torch\nfrom einops import rearrange\n\n\n\ndef rotate_half(x):\n    \"\"\"\n    Rotates the last dimension of a tensor by 90 degrees counterclockwise,\n    assuming the last dimension represents 2D coordinates or complex numbers.\n\n    Arguments:\n    x : torch.Tensor\n        Input tensor with its last dimension size as a multiple of 2, representing 2D vectors.\n\n    Returns:\n    torch.Tensor\n        A tensor with the same shape as the input, where each 2D vector has been rotated.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport torch\nfrom einops import rearrange\n\ndef rotate_half(x):\n    \"\"\"\n    Rotates the last dimension of a tensor by 90 degrees counterclockwise,\n    assuming the last dimension represents 2D coordinates or complex numbers.\n\n    Arguments:\n    x : torch.Tensor\n        Input tensor with its last dimension size as a multiple of 2, representing 2D vectors.\n\n    Returns:\n    torch.Tensor\n        A tensor with the same shape as the input, where each 2D vector has been rotated.\n    \"\"\"\n    # Ensure the last dimension is a multiple of 2\n    assert x.shape[-1] % 2 == 0, \"Last dimension must be a multiple of 2\"\n\n    # Reshape the tensor to separate the last dimension into two dimensions\n    x_reshaped = rearrange(x, \"... (a b) -> ... a b\", b=2)\n\n    # Perform the rotation by swapping and negating elements\n    x_rotated = torch.stack([-x_reshaped[..., 1], x_reshaped[..., 0]], dim=-1)\n\n    # Reshape the tensor back to the original shape\n    x_rotated = rearrange(x_rotated, \"... a b -> ... (a b)\")\n\n    return x_rotated\n```\n\nExplanation:\n\n1. We first assert that the last dimension of the input tensor `x` is a multiple of 2, ensuring that it represents 2D vectors.\n\n2. We use the `rearrange` function from the `einops` library to reshape the tensor. The last dimension is split into two dimensions, effectively separating the x and y components (or real and imaginary parts) of the 2D vectors.\n\n3. We perform the rotation by creating a new tensor using `torch.stack`. We swap the order of the elements and negate the first element (which corresponds to the y component or imaginary part). This achieves a 90-degree counterclockwise rotation.\n\n4. We use `rearrange` again to reshape the rotated tensor back to the original shape, combining the two dimensions into a single dimension.\n\n5. Finally, we return the rotated tensor.\n\nThis implementation assumes that the last dimension of the input tensor represents 2D coordinates or complex numbers, and it rotates each 2D vector by 90 degrees counterclockwise.",
  "canonical_solution": "    \n    # Reshape x assuming the last dimension can be split into 2 parts\n    x = rearrange(x, '... (d r) -> ... d r', r=2)\n\n    # Unbind the last dimension into two separate tensors\n    x1, x2 = x.unbind(dim=-1)\n\n    # Rotate each vector by 90 degrees counterclockwise\n    x = torch.stack((-x2, x1), dim=-1)\n\n    # Rearrange back to the original dimension structure\n    return rearrange(x, '... d r -> ... (d r)')\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([[1, 0], [0, 1]])) == rotate_half(torch.tensor([[1, 0], [0, 1]]))).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[[1, 0, 0, 1], [0, 1, -1, 0]]])) == rotate_half(torch.tensor([[[1, 0, 0, 1], [0, 1, -1, 0]]]))).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[1, 0]])) == rotate_half(torch.tensor([[1, 0]]))).all())\n"
 },
 {
  "task_id": "RWPB/90",
  "prompt": "import torch\n\ndef scale(s, device=None):\n    \"\"\"\n    Creates a 4x4 homogeneous transformation matrix for uniform scaling in 3D space.\n\n    Parameters:\n    s (float): Scaling factor applied equally to the x, y, and z axes.\n    device (torch.device, optional): The device on which to create the tensor (e.g., 'cpu' or 'cuda'). Default is None, which means the tensor will be created on the default device.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the scaling transformation matrix.\n\n    Process:\n    1. Construct a 4x4 identity matrix.\n    2. Replace the diagonal elements with the scaling factor for the x, y, and z coordinates.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport torch\n\ndef scale(s, device=None):\n    \"\"\"\n    Creates a 4x4 homogeneous transformation matrix for uniform scaling in 3D space.\n\n    Parameters:\n    s (float): Scaling factor applied equally to the x, y, and z axes.\n    device (torch.device, optional): The device on which to create the tensor (e.g., 'cpu' or 'cuda'). Default is None, which means the tensor will be created on the default device.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the scaling transformation matrix.\n\n    Process:\n    1. Construct a 4x4 identity matrix.\n    2. Replace the diagonal elements with the scaling factor for the x, y, and z coordinates.\n    \"\"\"\n    # Create a 4x4 identity matrix\n    scaling_matrix = torch.eye(4, device=device)\n\n    # Replace the diagonal elements with the scaling factor\n    scaling_matrix[0, 0] = s\n    scaling_matrix[1, 1] = s\n    scaling_matrix[2, 2] = s\n\n    return scaling_matrix\n```\n\nExplanation:\n1. We create a 4x4 identity matrix using `torch.eye(4, device=device)`. The `device` parameter is used to specify the device on which the tensor should be created.\n\n2. We replace the diagonal elements of the identity matrix with the scaling factor `s` for the x, y, and z coordinates. The elements at indices `[0, 0]`, `[1, 1]`, and `[2, 2]` correspond to the scaling factors for the x, y, and z axes, respectively.\n\n3. Finally, we return the modified `scaling_matrix`, which represents the scaling transformation matrix.\n\nThis function takes the scaling factor `s` and an optional `device` parameter, and returns a 4x4 tensor representing the scaling transformation matrix. The scaling factor is applied equally to the x, y, and z axes, resulting in uniform scaling in 3D space.",
  "canonical_solution": "    \n    return torch.tensor([[ s, 0, 0, 0], \n                         [ 0, s, 0, 0], \n                         [ 0, 0, s, 0], \n                         [ 0, 0, 0, 1]], dtype=torch.float32, device=device)\n\n\n",
  "unprocess_testcases": "a = torch.pi / 4\nassert((SOLUTION_SIGNATURE(a) == scale(a)).all())\n\na = 0\nassert((SOLUTION_SIGNATURE(a) == scale(a)).all())\n\na = 2 * torch.pi\nassert((SOLUTION_SIGNATURE(a) == scale(a)).all())\n"
 },
 {
  "task_id": "RWPB/80",
  "prompt": "import pandas as pd\n\ndef non_numeric_columns_and_head(df, num_rows=20):\n    \"\"\"\n    Identify non-numeric columns in a DataFrame and return their names and head.\n\n    :param df: Pandas DataFrame to be examined.\n    :param num_rows: Number of rows to include in the head (default is 20).\n    :return: A tuple with two elements:\n             1. List of column names that are not numeric (integer or float).\n             2. DataFrame containing the head of the non-numeric columns.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport pandas as pd\n\ndef non_numeric_columns_and_head(df, num_rows=20):\n    \"\"\"\n    Identify non-numeric columns in a DataFrame and return their names and head.\n\n    :param df: Pandas DataFrame to be examined.\n    :param num_rows: Number of rows to include in the head (default is 20).\n    :return: A tuple with two elements:\n             1. List of column names that are not numeric (integer or float).\n             2. DataFrame containing the head of the non-numeric columns.\n    \"\"\"\n    # Identify non-numeric columns\n    non_numeric_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]\n\n    # Get the head of the non-numeric columns\n    non_numeric_head = df[non_numeric_cols].head(num_rows)\n\n    return non_numeric_cols, non_numeric_head\n```\n\nExplanation:\n1. The function takes a Pandas DataFrame `df` and an optional parameter `num_rows` (default is 20) as input.\n2. It uses a list comprehension to iterate over the columns of the DataFrame and checks if each column is not of a numeric data type (integer or float) using the `pd.api.types.is_numeric_dtype()` function. The column names that satisfy this condition are stored in the `non_numeric_cols` list.\n3. It then selects the non-numeric columns from the DataFrame using `df[non_numeric_cols]` and retrieves the head (first `num_rows`) of those columns using the `head()` function. The result is stored in the `non_numeric_head` variable.\n4. Finally, the function returns a tuple containing the list of non-numeric column names (`non_numeric_cols`) and the DataFrame with the head of the non-numeric columns (`non_numeric_head`).\n\nYou can call this function by passing a Pandas DataFrame and optionally specifying the number of rows to include in the head. The function will return the list of non-numeric column names and the head of those columns as a DataFrame.",
  "canonical_solution": "    \n    # Identify columns that are not of numeric data type\n    non_numeric_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]\n    \n    # Get the head of the non-numeric columns\n    non_numeric_head = df[non_numeric_cols].head(num_rows).to_csv()\n    \n    return non_numeric_cols, non_numeric_head\n\n",
  "unprocess_testcases": "data = {\n  'id': [1, 2, 3, 4, 5],\n  'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n  'date_of_birth': pd.to_datetime(['1990-01-01', '1992-05-15', '1988-08-21', '1990-07-30', '1985-01-01']),\n  'salary': [50000.00, 60000.00, 55000.00, 45000.00, 70000.00],\n  'is_active': [True, False, True, False, True]\n}\ndf = pd.DataFrame(data)\nassert(SOLUTION_SIGNATURE(df)[0] == non_numeric_columns_and_head(df)[0])\nassert(SOLUTION_SIGNATURE(df)[1] == non_numeric_columns_and_head(df)[1])\n\ndata = {\n  'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n  'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'],\n  'email': ['alice@example.com', 'bob@example.com', 'charlie@example.com', 'david@example.com', 'eva@example.com']\n}\ndf = pd.DataFrame(data)\nassert(SOLUTION_SIGNATURE(df)[0] == non_numeric_columns_and_head(df)[0])\nassert(SOLUTION_SIGNATURE(df)[1] == non_numeric_columns_and_head(df)[1])\n\ndata = {\n  'id': [1, 2, 3],\n  'name': ['Alice', 'Bob', 'Charlie'],\n  'temperature': [98.6, 99.1, 98.7]\n}\ndf = pd.DataFrame(data)\nnum_rows = 10\nassert(SOLUTION_SIGNATURE(df)[0] == non_numeric_columns_and_head(df)[0])\nassert(SOLUTION_SIGNATURE(df)[1] == non_numeric_columns_and_head(df)[1])\n"
 },
 {
  "task_id": "RWPB/73",
  "prompt": "import numpy as np\n\n\ndef mse_to_psnr(mse):\n  \"\"\"\n  Compute the Peak Signal-to-Noise Ratio (PSNR) from the Mean Squared Error (MSE).\n  If mse is equal zero, the code need to return ValueError\n\n  Parameters:\n  mse (float): Mean Squared Error value. This value should be a non-negative float, \n               representing the average squared difference between the reference \n               and the distorted image.\n\n  Returns:\n  float: The PSNR value in decibels (dB).\n\n  Process:\n  - The function assumes that the maximum possible pixel value (I_max) in the image \n    is 1.\n  - The formula to convert MSE to PSNR is given by:\n      PSNR = -10 * log10(MSE)\n  - Use np.log to calculate.\n  \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nimport numpy as np\n\n\ndef mse_to_psnr(mse):\n    \"\"\"\n    Compute the Peak Signal-to-Noise Ratio (PSNR) from the Mean Squared Error (MSE).\n    If mse is equal zero, the code need to return ValueError\n\n    Parameters:\n    mse (float): Mean Squared Error value. This value should be a non-negative float, \n                 representing the average squared difference between the reference \n                 and the distorted image.\n\n    Returns:\n    float: The PSNR value in decibels (dB).\n\n    Process:\n    - The function assumes that the maximum possible pixel value (I_max) in the image \n      is 1.\n    - The formula to convert MSE to PSNR is given by:\n        PSNR = -10 * log10(MSE)\n    - Use np.log to calculate.\n    \"\"\"\n    if mse < 0:\n        raise ValueError(\"Mean Squared Error (MSE) should be a non-negative float.\")\n    \n    if mse == 0:\n        raise ValueError(\"Mean Squared Error (MSE) should not be equal to zero.\")\n    \n    psnr = -10 * np.log10(mse)\n    return psnr\n```\n\nIn this completed code:\n\n1. We first check if the `mse` value is negative. If it is, we raise a `ValueError` with an appropriate error message indicating that the MSE should be a non-negative float.\n\n2. Next, we check if the `mse` value is equal to zero. If it is, we raise a `ValueError` with an appropriate error message indicating that the MSE should not be equal to zero.\n\n3. If the `mse` value is valid (non-negative and not equal to zero), we proceed with the calculation of PSNR using the formula: `PSNR = -10 * log10(MSE)`. We use `np.log10()` to calculate the logarithm with base 10.\n\n4. Finally, we return the calculated `psnr` value.\n\nThis code assumes that the maximum possible pixel value (I_max) in the image is 1, as mentioned in the comments. The function takes the Mean Squared Error (MSE) as input and returns the corresponding Peak Signal-to-Noise Ratio (PSNR) value in decibels (dB).",
  "canonical_solution": "  \n  if mse == 0:\n    return ValueError\n  \n  return -10. / np.log(10.) * np.log(mse)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(0.01) == mse_to_psnr(0.01))\nassert(SOLUTION_SIGNATURE(1e-10) == mse_to_psnr(1e-10))\nassert(SOLUTION_SIGNATURE(0) == mse_to_psnr(0))\nassert(SOLUTION_SIGNATURE(1) == mse_to_psnr(1))\nassert(SOLUTION_SIGNATURE(0.2312) == mse_to_psnr(0.2312))\n"
 },
 {
  "task_id": "RWPB/22",
  "prompt": "def get_stats(ids, counts=None):\n    \"\"\"\n    Computes the frequency of consecutive integer pairs in a list.\n\n    Args:\n        ids (list of int): List of integers whose consecutive pairs are to be counted.\n        counts (dict, optional): Existing dictionary to update with new counts. If None, a new dictionary is created.\n\n    Returns:\n        dict: A dictionary where each key is a tuple representing a pair of consecutive integers,\n              and the value is the count of how often each pair appears.\n\n    Example:\n        >>> get_stats([1, 2, 3, 1, 2])\n        {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n\n    This function iterates over consecutive elements in the provided list, creating pairs,\n    and either updates an existing dictionary or creates a new one to record the frequency of each pair.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `get_stats` function:\n\n```python\ndef get_stats(ids, counts=None):\n    \"\"\"\n    Computes the frequency of consecutive integer pairs in a list.\n\n    Args:\n        ids (list of int): List of integers whose consecutive pairs are to be counted.\n        counts (dict, optional): Existing dictionary to update with new counts. If None, a new dictionary is created.\n\n    Returns:\n        dict: A dictionary where each key is a tuple representing a pair of consecutive integers,\n              and the value is the count of how often each pair appears.\n\n    Example:\n        >>> get_stats([1, 2, 3, 1, 2])\n        {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n\n    This function iterates over consecutive elements in the provided list, creating pairs,\n    and either updates an existing dictionary or creates a new one to record the frequency of each pair.\n    \"\"\"\n    if counts is None:\n        counts = {}\n\n    for i in range(len(ids) - 1):\n        pair = (ids[i], ids[i + 1])\n        counts[pair] = counts.get(pair, 0) + 1\n\n    return counts\n```\n\nExplanation:\n\n1. The function takes two parameters: `ids`, which is a list of integers, and `counts`, which is an optional dictionary to update with new counts. If `counts` is `None`, a new dictionary is created.\n\n2. We first check if `counts` is `None`. If it is, we initialize it as an empty dictionary.\n\n3. We iterate over the elements in the `ids` list using a `for` loop that goes up to the second-to-last element (excluding the last element).\n\n4. Inside the loop, we create a tuple `pair` consisting of the current element (`ids[i]`) and the next element (`ids[i + 1]`).\n\n5. We update the `counts` dictionary using the `get` method. If the `pair` already exists as a key in the dictionary, we retrieve its current count using `counts.get(pair, 0)` (which returns 0 if the key doesn't exist) and increment it by 1. If the `pair` doesn't exist as a key, it is added to the dictionary with a count of 1.\n\n6. After the loop finishes, we return the updated `counts` dictionary.\n\nThe function takes the list of integers `ids` and computes the frequency of consecutive integer pairs. It returns a dictionary where each key is a tuple representing a pair of consecutive integers, and the value is the count of how often each pair appears. If an existing dictionary is provided through the `counts` parameter, it is updated with the new counts; otherwise, a new dictionary is created.",
  "canonical_solution": "\n    counts = {} if counts is None else counts  # Initialize the dictionary if not provided\n    for pair in zip(ids, ids[1:]):  # Create pairs from consecutive elements\n        counts[pair] = counts.get(pair, 0) + 1  # Increment the count for each pair in the dictionary\n    return counts\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE([]) == get_stats([]))\nassert(SOLUTION_SIGNATURE([1, 2, 1, 2, 1, 2, 3, 4, 3, 4]) == get_stats([1, 2, 1, 2, 1, 2, 3, 4, 3, 4]))\nassert(SOLUTION_SIGNATURE([5, 6, 7, 5, 6], counts={(5, 6): 1, (6, 7): 2}) == get_stats([5, 6, 7, 5, 6], counts={(5, 6): 1, (6, 7): 2}))\n"
 },
 {
  "task_id": "RWPB/16",
  "prompt": "import numpy as np\n\ndef his_match(src, dst):\n    \"\"\"\n    Performs histogram matching between two color images src and dst.\n\n    Parameters\n    ----------\n    src : numpy.ndarray\n        Source image array of shape (H, W, 3) and type float64.\n    dst : numpy.ndarray\n        Destination image array of shape (H, W, 3) and type float64.\n\n    Returns\n    -------\n    numpy.ndarray\n        The matched image array of the same shape and type as src and dst.\n\n    Notes\n    -----\n    This function converts the images to uint8 type, calculates the cumulative\n    distribution functions (CDF) for each channel of the source and destination\n    images, and then finds the best match for each pixel in the destination image\n    in the source image's CDF. The matched values are then used to create a new\n    image that has a similar color distribution to the source image.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the histogram matching problem:\n\n```python\nimport numpy as np\n\ndef his_match(src, dst):\n    \"\"\"\n    Performs histogram matching between two color images src and dst.\n\n    Parameters\n    ----------\n    src : numpy.ndarray\n        Source image array of shape (H, W, 3) and type float64.\n    dst : numpy.ndarray\n        Destination image array of shape (H, W, 3) and type float64.\n\n    Returns\n    -------\n    numpy.ndarray\n        The matched image array of the same shape and type as src and dst.\n\n    Notes\n    -----\n    This function converts the images to uint8 type, calculates the cumulative\n    distribution functions (CDF) for each channel of the source and destination\n    images, and then finds the best match for each pixel in the destination image\n    in the source image's CDF. The matched values are then used to create a new\n    image that has a similar color distribution to the source image.\n    \"\"\"\n    # Convert images to uint8 type\n    src_uint8 = (src * 255).astype(np.uint8)\n    dst_uint8 = (dst * 255).astype(np.uint8)\n\n    # Calculate the cumulative distribution functions (CDF) for each channel\n    src_cdf = []\n    dst_cdf = []\n    for i in range(3):\n        src_hist, _ = np.histogram(src_uint8[:, :, i], bins=256, range=(0, 255))\n        dst_hist, _ = np.histogram(dst_uint8[:, :, i], bins=256, range=(0, 255))\n        src_cdf.append(np.cumsum(src_hist))\n        dst_cdf.append(np.cumsum(dst_hist))\n\n    # Find the best match for each pixel in the destination image\n    matched = np.zeros_like(dst_uint8)\n    for i in range(3):\n        lookup_table = np.zeros(256, dtype=np.uint8)\n        for j in range(256):\n            lookup_table[j] = np.argmin(np.abs(src_cdf[i] - dst_cdf[i][j]))\n        matched[:, :, i] = lookup_table[dst_uint8[:, :, i]]\n\n    # Convert the matched image back to float64 type\n    matched_float = matched.astype(np.float64) / 255.0\n\n    return matched_float\n```\n\nExplanation:\n\n1. The function takes two input images, `src` and `dst`, both of shape (H, W, 3) and type float64.\n\n2. The images are converted from float64 to uint8 type by multiplying by 255 and using `astype(np.uint8)`. This is done to work with integer pixel values in the range [0, 255].\n\n3. For each color channel (RGB), the cumulative distribution functions (CDF) are calculated using `np.histogram()` and `np.cumsum()`. The CDFs represent the cumulative sum of the pixel intensities in each channel.\n\n4. A lookup table is created for each color channel to find the best match for each pixel in the destination image. The lookup table is initialized with zeros and has a size of 256 (corresponding to the possible pixel values).\n\n5. For each pixel value in the destination image, the function finds the closest matching pixel value in the source image's CDF using `np.argmin(np.abs(src_cdf[i] - dst_cdf[i][j]))`. The matched pixel values are stored in the lookup table.\n\n6. The matched pixel values are then used to create a new image, `matched`, by applying the lookup table to each color channel of the destination image using `lookup_table[dst_uint8[:, :, i]]`.\n\n7. Finally, the matched image is converted back to float64 type by dividing by 255 and returned as the output.\n\nThis function performs histogram matching between the source and destination images, resulting in a new image that has a similar color distribution to the source image.",
  "canonical_solution": "\n    # Scale images to [0, 255] range and convert to uint8 type\n    src = src * 255.0\n    dst = dst * 255.0\n    src = src.astype(np.uint8)\n    dst = dst.astype(np.uint8)\n    res = np.zeros_like(dst)\n\n    # Initialize CDF arrays for each channel\n    cdf_src = np.zeros((3, 256))\n    cdf_dst = np.zeros((3, 256))\n    cdf_res = np.zeros((3, 256))\n\n    # Keyword arguments for histogram calculation\n    kw = dict(bins=256, range=(0, 256), density=True)\n\n    # Calculate histograms and CDFs for each channel\n    for ch in range(3):\n        his_src, _ = np.histogram(src[:, :, ch], **kw)\n        hist_dst, _ = np.histogram(dst[:, :, ch], **kw)\n        cdf_src[ch] = np.cumsum(his_src)\n        cdf_dst[ch] = np.cumsum(hist_dst)\n\n        # Find the best match index for each pixel in the destination image\n        index = np.searchsorted(cdf_src[ch], cdf_dst[ch], side=\"left\")\n        np.clip(index, 0, 255, out=index)\n\n        # Assign the matched values to the result image\n        res[:, :, ch] = index[dst[:, :, ch]]\n\n        # Update histogram and CDF for the result image\n        his_res, _ = np.histogram(res[:, :, ch], **kw)\n        cdf_res[ch] = np.cumsum(his_res)\n\n    # Scale the result back to [0, 1] range and return\n    return res / 255.0\n\n\n\n",
  "unprocess_testcases": "a = np.random.rand(100, 100, 3)\nb = np.full((50, 50, 3), 0.5)\nc = np.full((50, 50, 3), 0.7)\nd = np.random.rand(64, 64, 3)\ne = np.random.rand(128, 128, 3)\n\nassert((SOLUTION_SIGNATURE(src = a, dst = a) == his_match(src = a, dst = a)).all())\nassert((SOLUTION_SIGNATURE(src = b, dst = c) == his_match(src = b, dst = c)).all())\nassert((SOLUTION_SIGNATURE(src = d, dst = e) == his_match(src = d, dst = e)).all())\n\n"
 },
 {
  "task_id": "RWPB/47",
  "prompt": "def disjoint_cycles(perm):\n    \"\"\"\n    Find the disjoint cycles of a permutation.\n\n    Args:\n        perm (list): The permutation as a list.\n\n    Returns:\n        list: A list of lists, where each inner list represents a disjoint cycle.\n    \"\"\"\n",
  "solution": "Certainly! Here's the completed code to find the disjoint cycles of a permutation:\n\n```python\ndef disjoint_cycles(perm):\n    \"\"\"\n    Find the disjoint cycles of a permutation.\n\n    Args:\n        perm (list): The permutation as a list.\n\n    Returns:\n        list: A list of lists, where each inner list represents a disjoint cycle.\n    \"\"\"\n    n = len(perm)\n    visited = [False] * n\n    cycles = []\n\n    for i in range(n):\n        if not visited[i]:\n            cycle = []\n            j = i\n            while not visited[j]:\n                visited[j] = True\n                cycle.append(j)\n                j = perm[j]\n            cycles.append(cycle)\n\n    return cycles\n```\n\nExplanation:\n\n1. We initialize a list `visited` of length `n` (the size of the permutation) to keep track of the elements that have been visited during the cycle finding process. Initially, all elements are marked as `False` (not visited).\n\n2. We also initialize an empty list `cycles` to store the disjoint cycles.\n\n3. We iterate over each index `i` in the range `[0, n-1]`:\n   - If the element at index `i` has not been visited (`visited[i] == False`), we start a new cycle.\n   - We initialize an empty list `cycle` to store the elements of the current cycle.\n   - We start traversing the cycle by assigning `j = i` and continue until we reach an already visited element.\n   - Inside the inner loop, we mark the current element as visited by setting `visited[j] = True`, append the element to the `cycle` list, and update `j` to the value at index `j` in the permutation (`perm[j]`).\n   - Once the inner loop ends, we have found a complete cycle, so we append it to the `cycles` list.\n\n4. After iterating over all indices, we have found all the disjoint cycles of the permutation, and we return the `cycles` list.\n\nThe time complexity of this algorithm is O(n), where n is the size of the permutation, as we visit each element exactly once. The space complexity is also O(n) to store the `visited` list and the `cycles` list.\n\nI hope this helps! Let me know if you have any further questions.",
  "canonical_solution": "\n    used = set()\n    cycles = []\n    minimum = 1\n    while len(used) < len(perm):\n        cycle = [minimum]\n        used.add(minimum)\n        elt = perm[minimum - 1]\n        while elt not in cycle:\n            cycle.append(elt)\n            used.add(elt)\n            elt = perm[elt - 1]\n        if len(set(perm).symmetric_difference(used)) != 0:\n            minimum = min(set(perm).symmetric_difference(used))\n        cycles.append(cycle)\n    return cycles\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE([2, 3, 4, 5, 1]) == disjoint_cycles([2, 3, 4, 5, 1]))\nassert(SOLUTION_SIGNATURE([2, 1, 4, 3, 6, 5]) == disjoint_cycles([2, 1, 4, 3, 6, 5]))\nassert(SOLUTION_SIGNATURE([1, 3, 2, 5, 4]) == disjoint_cycles([1, 3, 2, 5, 4]))\n"
 },
 {
  "task_id": "RWPB/67",
  "prompt": "MAX_VAL = (1 << 256) - 1\n\n# does not revert on overflow\ndef unsafeMul(x, y):\n    \"\"\"\n    Multiply two integers, ignoring overflow.\n    Args:\n        - x: int, first operand\n        - y: int, second operand\n    Returns:\n        result: int, the product of x and y, with overflow ignored\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without providing unit test cases:\n\n```python\nMAX_VAL = (1 << 256) - 1\n\n# does not revert on overflow\ndef unsafeMul(x, y):\n    \"\"\"\n    Multiply two integers, ignoring overflow.\n    Args:\n        - x: int, first operand\n        - y: int, second operand\n    Returns:\n        result: int, the product of x and y, with overflow ignored\n    \"\"\"\n    result = x * y\n    return result & MAX_VAL\n```\n\nExplanation:\n\n1. The function `unsafeMul` takes two integers `x` and `y` as input.\n\n2. Inside the function, we multiply `x` and `y` using the `*` operator and store the result in the `result` variable.\n\n3. To ignore overflow, we perform a bitwise AND operation (`&`) between the `result` and `MAX_VAL`. This operation effectively truncates the result to the lower 256 bits, discarding any overflow that may have occurred during the multiplication.\n\n4. Finally, we return the truncated `result`.\n\nThe `MAX_VAL` constant is set to `(1 << 256) - 1`, which represents the maximum value that can be represented in 256 bits. By performing the bitwise AND operation with `MAX_VAL`, we ensure that the result is within the valid range of a 256-bit integer, effectively ignoring any overflow that may have occurred.\n\nThis implementation assumes that the input integers `x` and `y` are within the valid range of integers in Python. If the multiplication of `x` and `y` exceeds the maximum value representable by an integer in Python, the result will be truncated to fit within the 256-bit range.",
  "canonical_solution": "    \n    return (x * y) & MAX_VAL\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(15, 10) == unsafeMul(15, 10))\nassert(SOLUTION_SIGNATURE((1 << 256) - 1, 1) == unsafeMul((1 << 256) - 1, 1))\nassert(SOLUTION_SIGNATURE(pow(2, 128), pow(2, 129)) == unsafeMul(pow(2, 128), pow(2, 129)))\n"
 },
 {
  "task_id": "RWPB/36",
  "prompt": "import numpy as np\n\ndef smooth_pose_seq(pose_seq, window_size=5):\n    \"\"\"\n    Smooths a sequence of poses using a moving average filter.\n\n    Args:\n        pose_seq (np.array): Array of pose data, where each pose is expected to be an array of coordinates.\n        window_size (int, optional): The number of frames to include in the moving average window. Default is 5.\n\n    Returns:\n        np.array: An array of smoothed poses with the same shape as the input.\n\n    This function applies a simple moving average smoothing technique to a sequence of poses, which is often useful\n    in motion analysis or animation to reduce noise or fluctuations between frames. The function handles edge cases by\n    adjusting the window size at the start and end of the sequence, ensuring all poses are smoothed.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem without unit test cases:\n\n```python\nimport numpy as np\n\ndef smooth_pose_seq(pose_seq, window_size=5):\n    \"\"\"\n    Smooths a sequence of poses using a moving average filter.\n\n    Args:\n        pose_seq (np.array): Array of pose data, where each pose is expected to be an array of coordinates.\n        window_size (int, optional): The number of frames to include in the moving average window. Default is 5.\n\n    Returns:\n        np.array: An array of smoothed poses with the same shape as the input.\n\n    This function applies a simple moving average smoothing technique to a sequence of poses, which is often useful\n    in motion analysis or animation to reduce noise or fluctuations between frames. The function handles edge cases by\n    adjusting the window size at the start and end of the sequence, ensuring all poses are smoothed.\n    \"\"\"\n    num_poses = len(pose_seq)\n    smoothed_poses = np.zeros_like(pose_seq)\n\n    for i in range(num_poses):\n        start = max(0, i - window_size // 2)\n        end = min(num_poses, i + window_size // 2 + 1)\n        window = pose_seq[start:end]\n        smoothed_poses[i] = np.mean(window, axis=0)\n\n    return smoothed_poses\n```\n\nExplanation:\n1. We define the `smooth_pose_seq` function that takes in `pose_seq` (an array of pose data) and an optional `window_size` parameter (default is 5).\n\n2. We calculate the number of poses in the sequence using `len(pose_seq)` and store it in the `num_poses` variable.\n\n3. We create a new array called `smoothed_poses` with the same shape as `pose_seq` using `np.zeros_like(pose_seq)`. This array will store the smoothed poses.\n\n4. We iterate over each pose in the sequence using a `for` loop from 0 to `num_poses`.\n\n5. For each pose index `i`, we calculate the start and end indices of the window:\n   - `start` is set to the maximum of 0 and `i - window_size // 2` to ensure the window doesn't go out of bounds at the start of the sequence.\n   - `end` is set to the minimum of `num_poses` and `i + window_size // 2 + 1` to ensure the window doesn't go out of bounds at the end of the sequence.\n\n6. We extract the window of poses using `pose_seq[start:end]` and store it in the `window` variable.\n\n7. We calculate the mean of the poses in the window using `np.mean(window, axis=0)` and assign the result to `smoothed_poses[i]`. The `axis=0` argument ensures that the mean is calculated along the first axis (i.e., across poses).\n\n8. After the loop, we return the `smoothed_poses` array containing the smoothed poses.\n\nThis function applies a moving average smoothing technique to the input sequence of poses, reducing noise and fluctuations between frames. The window size determines the number of poses to include in the averaging window. The function handles edge cases by adjusting the window size at the start and end of the sequence to ensure all poses are smoothed.",
  "canonical_solution": "\n    # Initialize an array of zeros with the same shape as the input sequence to store the smoothed poses\n    smoothed_pose_seq = np.zeros_like(pose_seq)\n\n    # Iterate over each pose in the sequence\n    for i in range(len(pose_seq)):\n        # Calculate the start and end indices of the window, ensuring they are within the bounds of the sequence\n        start = max(0, i - window_size // 2)\n        end = min(len(pose_seq), i + window_size // 2 + 1)\n\n        # Compute the mean of the poses within the window and assign it to the smoothed sequence\n        smoothed_pose_seq[i] = np.mean(pose_seq[start:end], axis=0)\n\n    return smoothed_pose_seq\n\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([[1, 2], [2, 3], [3, 4]])) == smooth_pose_seq(np.array([[1, 2], [2, 3], [3, 4]]))).all())\nassert((SOLUTION_SIGNATURE(np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6], [7, 7], [8, 8]])) == smooth_pose_seq(np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6], [7, 7], [8, 8]]))).all())\nassert((SOLUTION_SIGNATURE(np.array([[1e10, 1e10], [1e-10, 1e-10], [1.5, 2.5], [0.001, 0.002], [5000, 5000]])) == smooth_pose_seq(np.array([[1e10, 1e10], [1e-10, 1e-10], [1.5, 2.5], [0.001, 0.002], [5000, 5000]]))).all())\n"
 },
 {
  "task_id": "RWPB/94",
  "prompt": "import torch\n\ndef lookAt(eye, at, up):\n    \"\"\"\n    Creates a view transformation matrix using the \"look-at\" method, which is commonly used in 3D graphics to create a camera view matrix.\n\n    Parameters:\n    eye (torch.Tensor): A 1D tensor with 3 elements representing the position of the camera in world coordinates.\n    at (torch.Tensor): A 1D tensor with 3 elements representing the point in world coordinates that the camera is looking at.\n    up (torch.Tensor): A 1D tensor with 3 elements representing the up direction of the camera.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the view transformation matrix.\n\n    Process:\n    1. Compute the forward vector `w` as the normalized vector pointing from `at` to `eye`.\n    2. Compute the right vector `u` as the normalized cross product of `up` and `w`.\n    3. Compute the true up vector `v` as the cross product of `w` and `u`.\n    4. Construct the translation matrix to move the scene so that the camera is at the origin.\n    5. Construct the rotation matrix to align the camera's orientation with the computed `u`, `v`, and `w` vectors.\n    6. Combine the translation and rotation matrices by matrix multiplication to get the final view transformation matrix.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `lookAt` function:\n\n```python\nimport torch\n\ndef lookAt(eye, at, up):\n    \"\"\"\n    Creates a view transformation matrix using the \"look-at\" method, which is commonly used in 3D graphics to create a camera view matrix.\n\n    Parameters:\n    eye (torch.Tensor): A 1D tensor with 3 elements representing the position of the camera in world coordinates.\n    at (torch.Tensor): A 1D tensor with 3 elements representing the point in world coordinates that the camera is looking at.\n    up (torch.Tensor): A 1D tensor with 3 elements representing the up direction of the camera.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the view transformation matrix.\n\n    Process:\n    1. Compute the forward vector `w` as the normalized vector pointing from `at` to `eye`.\n    2. Compute the right vector `u` as the normalized cross product of `up` and `w`.\n    3. Compute the true up vector `v` as the cross product of `w` and `u`.\n    4. Construct the translation matrix to move the scene so that the camera is at the origin.\n    5. Construct the rotation matrix to align the camera's orientation with the computed `u`, `v`, and `w` vectors.\n    6. Combine the translation and rotation matrices by matrix multiplication to get the final view transformation matrix.\n    \"\"\"\n    # Step 1: Compute the forward vector `w`\n    w = eye - at\n    w = w / torch.norm(w)\n\n    # Step 2: Compute the right vector `u`\n    u = torch.cross(up, w)\n    u = u / torch.norm(u)\n\n    # Step 3: Compute the true up vector `v`\n    v = torch.cross(w, u)\n\n    # Step 4: Construct the translation matrix\n    translation_matrix = torch.eye(4)\n    translation_matrix[:3, 3] = -eye\n\n    # Step 5: Construct the rotation matrix\n    rotation_matrix = torch.eye(4)\n    rotation_matrix[:3, 0] = u\n    rotation_matrix[:3, 1] = v\n    rotation_matrix[:3, 2] = w\n\n    # Step 6: Combine the translation and rotation matrices\n    view_matrix = rotation_matrix @ translation_matrix\n\n    return view_matrix\n```\n\nThis implementation follows the process described in the docstring:\n\n1. The forward vector `w` is computed by subtracting `at` from `eye` and normalizing the result.\n2. The right vector `u` is computed by taking the cross product of `up` and `w` and normalizing the result.\n3. The true up vector `v` is computed by taking the cross product of `w` and `u`.\n4. The translation matrix is constructed using `torch.eye(4)` to create an identity matrix, and the camera position `eye` is negated and assigned to the last column of the matrix.\n5. The rotation matrix is constructed using `torch.eye(4)` to create an identity matrix, and the computed `u`, `v`, and `w` vectors are assigned to the first three columns of the matrix.\n6. The final view transformation matrix is obtained by matrix multiplication of the rotation matrix and the translation matrix.\n\nThe function returns the resulting 4x4 view transformation matrix as a `torch.Tensor`.",
  "canonical_solution": "    \n    a = eye - at\n    w = a / torch.linalg.norm(a)\n    u = torch.cross(up, w)\n    u = u / torch.linalg.norm(u)\n    v = torch.cross(w, u)\n    translate = torch.tensor([[1, 0, 0, -eye[0]], \n                              [0, 1, 0, -eye[1]], \n                              [0, 0, 1, -eye[2]], \n                              [0, 0, 0, 1]], dtype=eye.dtype, device=eye.device)\n    rotate = torch.tensor([[u[0], u[1], u[2], 0], \n                           [v[0], v[1], v[2], 0], \n                           [w[0], w[1], w[2], 0], \n                           [0, 0, 0, 1]], dtype=eye.dtype, device=eye.device)\n    return rotate @ translate\n\n\n",
  "unprocess_testcases": "\neye = torch.tensor([1.0, 1, 1])\nat = torch.tensor([0.0, 0, 0])\nup = torch.tensor([0.0, 1, 0])\nassert((SOLUTION_SIGNATURE(eye, at, up) == lookAt(eye, at, up)).all())\n\neye = torch.tensor([0.0, 0, 1])\nat = torch.tensor([0.0, 0, 0])\nup = torch.tensor([0.0, 1, 0])\nassert((SOLUTION_SIGNATURE(eye, at, up) == lookAt(eye, at, up)).all())\n\neye = torch.tensor([-3.0, 2, 5])\nat = torch.tensor([4.0, -1, 0])\nup = torch.tensor([1.0, 1, 1])\nassert((SOLUTION_SIGNATURE(eye, at, up) == lookAt(eye, at, up)).all())\n"
 },
 {
  "task_id": "RWPB/53",
  "prompt": "import re\n\ndef general_detokenize(string):\n    \"\"\"\n    Detokenizes a string by fixing spacing around punctuation and contractions.\n\n    Arguments:\n    string : str\n        The input string that needs to be detokenized.\n\n    Returns:\n    str\n        The detokenized string with corrected spacing.\n    \"\"\"\n",
  "solution": "Certainly! Here's the completed code to solve the problem without unit test cases:\n\n```python\nimport re\n\ndef general_detokenize(string):\n    \"\"\"\n    Detokenizes a string by fixing spacing around punctuation and contractions.\n\n    Arguments:\n    string : str\n        The input string that needs to be detokenized.\n\n    Returns:\n    str\n        The detokenized string with corrected spacing.\n    \"\"\"\n    # Remove spaces before periods, commas, colons, semicolons, question marks, and exclamation points\n    string = re.sub(r'\\s+([.,;:?!])', r'\\1', string)\n\n    # Remove spaces before and after quotation marks, parentheses, and dashes\n    string = re.sub(r'\\s+([\"\\(\\)\\-])', r'\\1', string)\n    string = re.sub(r'([\"\\(\\)\\-])\\s+', r'\\1', string)\n\n    # Remove spaces before and after apostrophes in contractions\n    string = re.sub(r\"\\s+(['](?:s|m|d|re|ve|ll))\\s+\", r\"\\1 \", string)\n    string = re.sub(r\"\\s+(['](?:t|s))\\s+\", r\" \\1 \", string)\n\n    # Remove spaces before and after possessive apostrophes\n    string = re.sub(r\"\\s+([']s)\\s+\", r\"\\1 \", string)\n\n    # Add space after periods, commas, colons, semicolons, question marks, and exclamation points\n    string = re.sub(r'([.,;:?!])(\\w)', r'\\1 \\2', string)\n\n    # Add space after closing quotation mark if followed by a word character\n    string = re.sub(r'([\"\\)])(\\w)', r'\\1 \\2', string)\n\n    # Add space before opening quotation mark if preceded by a word character\n    string = re.sub(r'(\\w)([\"\"\\(])', r'\\1 \\2', string)\n\n    # Remove leading and trailing whitespace\n    string = string.strip()\n\n    return string\n```\n\nThis implementation of `general_detokenize` uses regular expressions to handle various cases of spacing around punctuation and contractions. Here's how it works:\n\n1. It removes spaces before periods, commas, colons, semicolons, question marks, and exclamation points using `re.sub()`.\n\n2. It removes spaces before and after quotation marks, parentheses, and dashes.\n\n3. It handles contractions by removing spaces before and after apostrophes in common contractions like \"it's,\" \"I'm,\" \"they're,\" etc.\n\n4. It removes spaces before and after possessive apostrophes, such as \"John's.\"\n\n5. It adds a space after periods, commas, colons, semicolons, question marks, and exclamation points if they are followed by a word character.\n\n6. It adds a space after a closing quotation mark if it is followed by a word character.\n\n7. It adds a space before an opening quotation mark if it is preceded by a word character.\n\n8. Finally, it removes any leading or trailing whitespace using `string.strip()`.\n\nThe function takes a string as input and returns the detokenized string with corrected spacing.\n\nNote that this implementation assumes a general case and may not cover all possible scenarios or variations in punctuation and contractions. Depending on the specific requirements of your project, you may need to adjust the regular expressions or add additional rules to handle edge cases.",
  "canonical_solution": "\n    # Fix the space before contractions like \"n't\" (e.g., \" ca n't\" -> \"can't\")\n    string = string.replace(\" n't\", \"n't\")\n\n    # Remove space before closing parentheses and after opening parentheses\n    string = string.replace(\" )\", \")\")\n    string = string.replace(\"( \", \"(\")\n\n    # Correct spacing issues with quotation marks\n    string = string.replace('\" ', '\"')\n    string = string.replace(' \"', '\"')\n\n    # Use regular expressions to remove space before commas and periods, and other single character punctuations\n    string = re.sub(r\" (['.,])\", r\"\\1\", string)\n\n    return string\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE('She said , \" Hello world ! \" I can not believe it ; can you ? Yes , I ca n\\'t .') == general_detokenize('She said , \" Hello world ! \" I can not believe it ; can you ? Yes , I ca n\\'t .'))\nassert(SOLUTION_SIGNATURE('We need ( in an ideal world ) a kind of setup that works ( efficiently and effectively ).') == general_detokenize('We need ( in an ideal world ) a kind of setup that works ( efficiently and effectively ).'))\nassert(SOLUTION_SIGNATURE('This is a test . What do you think ? Isn\\'t it interesting ?') == general_detokenize('This is a test . What do you think ? Isn\\'t it interesting ?'))\n"
 },
 {
  "task_id": "RWPB/43",
  "prompt": "import torch\nimport numpy as np\n\n\ndef inv(mat):\n    \"\"\"\n    Inverts a matrix that can be either a torch.Tensor or a numpy.ndarray.\n\n    Parameters\n    ----------\n    mat : torch.Tensor or np.ndarray\n        A square matrix to be inverted.\n\n    Returns\n    -------\n    torch.Tensor or np.ndarray\n        The inverse of the input matrix, with the same type as the input.\n\n    Raises\n    ------\n    ValueError\n        If the input is not a torch.Tensor or np.ndarray.\n\n    Notes\n    -----\n    This function checks the type of the input matrix and applies the appropriate inverse\n    operation using either PyTorch's torch.linalg.inv for tensor objects or NumPy's\n    np.linalg.inv for ndarray objects. It is important that the input matrix is square\n    and non-singular (i.e., it has an inverse), otherwise the inversion will fail.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `inv` function:\n\n```python\nimport torch\nimport numpy as np\n\n\ndef inv(mat):\n    \"\"\"\n    Inverts a matrix that can be either a torch.Tensor or a numpy.ndarray.\n\n    Parameters\n    ----------\n    mat : torch.Tensor or np.ndarray\n        A square matrix to be inverted.\n\n    Returns\n    -------\n    torch.Tensor or np.ndarray\n        The inverse of the input matrix, with the same type as the input.\n\n    Raises\n    ------\n    ValueError\n        If the input is not a torch.Tensor or np.ndarray.\n\n    Notes\n    -----\n    This function checks the type of the input matrix and applies the appropriate inverse\n    operation using either PyTorch's torch.linalg.inv for tensor objects or NumPy's\n    np.linalg.inv for ndarray objects. It is important that the input matrix is square\n    and non-singular (i.e., it has an inverse), otherwise the inversion will fail.\n    \"\"\"\n    if isinstance(mat, torch.Tensor):\n        return torch.linalg.inv(mat)\n    elif isinstance(mat, np.ndarray):\n        return np.linalg.inv(mat)\n    else:\n        raise ValueError(\"Input must be a torch.Tensor or np.ndarray.\")\n```\n\nThe completed code for the `inv` function does the following:\n\n1. It checks the type of the input `mat` using `isinstance`.\n\n2. If `mat` is a `torch.Tensor`, it uses PyTorch's `torch.linalg.inv` function to compute the inverse of the matrix and returns the result.\n\n3. If `mat` is a `np.ndarray`, it uses NumPy's `np.linalg.inv` function to compute the inverse of the matrix and returns the result.\n\n4. If `mat` is neither a `torch.Tensor` nor a `np.ndarray`, it raises a `ValueError` with an appropriate error message.\n\nThe function assumes that the input matrix is square and non-singular (i.e., it has an inverse). If the matrix is not square or is singular, the respective inversion function (`torch.linalg.inv` or `np.linalg.inv`) will raise an error.\n\nThis implementation allows the `inv` function to handle both PyTorch tensor objects and NumPy ndarray objects, providing flexibility in the input type while maintaining the appropriate inversion operation for each type.",
  "canonical_solution": "    \n    # Check if the input matrix is a torch.Tensor and invert it using torch.linalg.inv\n    if isinstance(mat, torch.Tensor):\n        return torch.linalg.inv(mat)\n\n    # Check if the input matrix is a np.ndarray and invert it using np.linalg.inv\n    if isinstance(mat, np.ndarray):\n        return np.linalg.inv(mat)\n\n    # If the input is neither a torch.Tensor nor a np.ndarray, raise a ValueError\n    return ValueError\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([[4.0, 7.0], [2.0, 6.0]])) == inv(torch.tensor([[4.0, 7.0], [2.0, 6.0]]))).all())\nassert((SOLUTION_SIGNATURE(np.array([[1, 2, 3], [0, 1, 4], [5, 6, 0]])) == inv(np.array([[1, 2, 3], [0, 1, 4], [5, 6, 0]]))).all())\nassert(SOLUTION_SIGNATURE([[1, 2], [3, 4]]) == inv([[1, 2], [3, 4]]))\n"
 },
 {
  "task_id": "RWPB/12",
  "prompt": "import torch\n\n\ndef normalize_activation(x, eps=1e-10):\n    \"\"\"\n    Normalize activation values along the channel dimension.\n    Args:\n        - x: Tensor, the input activation tensor.\n        - eps: float, optional, small value to avoid division by zero.\n    Return:\n        normalized_activation: Tensor, the normalized activation tensor.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of normalizing activation values along the channel dimension:\n\n```python\nimport torch\n\n\ndef normalize_activation(x, eps=1e-10):\n    \"\"\"\n    Normalize activation values along the channel dimension.\n    Args:\n        - x: Tensor, the input activation tensor.\n        - eps: float, optional, small value to avoid division by zero.\n    Return:\n        normalized_activation: Tensor, the normalized activation tensor.\n    \"\"\"\n    # Compute the mean of the activation values along the channel dimension\n    mean = torch.mean(x, dim=1, keepdim=True)\n\n    # Compute the standard deviation of the activation values along the channel dimension\n    std = torch.std(x, dim=1, keepdim=True)\n\n    # Normalize the activation values\n    normalized_activation = (x - mean) / (std + eps)\n\n    return normalized_activation\n```\n\nExplanation:\n\n1. The function `normalize_activation` takes two arguments:\n   - `x`: The input activation tensor.\n   - `eps`: An optional small value to avoid division by zero (default is 1e-10).\n\n2. Inside the function, we first compute the mean of the activation values along the channel dimension using `torch.mean()`. The `dim=1` argument specifies that we want to compute the mean along the second dimension (assuming the tensor has shape [batch_size, num_channels, ...]), and `keepdim=True` ensures that the resulting tensor maintains the same number of dimensions as the input tensor.\n\n3. Next, we compute the standard deviation of the activation values along the channel dimension using `torch.std()`. Similar to the mean calculation, we use `dim=1` and `keepdim=True`.\n\n4. Finally, we normalize the activation values by subtracting the mean and dividing by the standard deviation plus the small value `eps`. This ensures that the normalized activation values have zero mean and unit variance along the channel dimension.\n\n5. The function returns the normalized activation tensor.\n\nThis code normalizes the activation values along the channel dimension, which can be useful in various deep learning tasks to improve the stability and convergence of the training process.",
  "canonical_solution": "    \n    norm_factor = torch.sqrt(torch.sum(x ** 2, dim=1, keepdim=True))\n    return x / (norm_factor + eps)\n\n\n\n",
  "unprocess_testcases": "a = torch.rand([1, 3, 5, 5])\nb = torch.rand([2, 3, 4, 4])\nc = torch.rand([1, 1, 1, 1])\n\nassert((SOLUTION_SIGNATURE(a) == normalize_activation(a)).all())\nassert((SOLUTION_SIGNATURE(b) == normalize_activation(b)).all())\nassert((SOLUTION_SIGNATURE(c) == normalize_activation(c)).all())\n"
 },
 {
  "task_id": "RWPB/84",
  "prompt": "import torch\n\ndef build_homog_matrix(R, t=None):\n    \"\"\"\n    Create a batch of homogeneous transformation matrices from given rotation matrices and translation vectors.\n    If the shape of R do not match the shape of t, please return ValueError\n\n    Args:\n    R (torch.Tensor): Rotation matrices of shape (B, Nj, 3, 3), where B is the batch size, and Nj is the number of joints.\n    t (torch.Tensor, optional): Translation vectors of shape (B, Nj, 3, 1). If None, a zero translation vector is used.\n    \n    Returns:\n    torch.Tensor: Homogeneous transformation matrices of shape (B, Nj, 4, 4).\n    \n    Process:\n    1. If the translation vector `t` is not provided, initialize it to zero vectors of the appropriate shape.\n    2. If the rotation matrix `R` is not provided, initialize it to identity matrices of the appropriate shape.\n    3. Ensure that the shapes of `R` and `t` match the expected dimensions.\n    4. Concatenate the rotation matrix `R` and translation vector `t` along the last dimension to form a (B, Nj, 3, 4) local transformation matrix.\n    5. Create a padding row [0, 0, 0, 1] and expand it to match the batch and joint dimensions, forming a (B, Nj, 1, 4) tensor.\n    6. Concatenate the padded row to the local transformation matrix to create a final homogeneous matrix of shape (B, Nj, 4, 4).\n    7. Return the resulting homogeneous transformation matrix.\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem of building homogeneous transformation matrices from given rotation matrices and translation vectors:\n\n```python\nimport torch\n\ndef build_homog_matrix(R, t=None):\n    \"\"\"\n    Create a batch of homogeneous transformation matrices from given rotation matrices and translation vectors.\n    If the shape of R do not match the shape of t, please return ValueError\n\n    Args:\n    R (torch.Tensor): Rotation matrices of shape (B, Nj, 3, 3), where B is the batch size, and Nj is the number of joints.\n    t (torch.Tensor, optional): Translation vectors of shape (B, Nj, 3, 1). If None, a zero translation vector is used.\n    \n    Returns:\n    torch.Tensor: Homogeneous transformation matrices of shape (B, Nj, 4, 4).\n    \n    Process:\n    1. If the translation vector `t` is not provided, initialize it to zero vectors of the appropriate shape.\n    2. If the rotation matrix `R` is not provided, initialize it to identity matrices of the appropriate shape.\n    3. Ensure that the shapes of `R` and `t` match the expected dimensions.\n    4. Concatenate the rotation matrix `R` and translation vector `t` along the last dimension to form a (B, Nj, 3, 4) local transformation matrix.\n    5. Create a padding row [0, 0, 0, 1] and expand it to match the batch and joint dimensions, forming a (B, Nj, 1, 4) tensor.\n    6. Concatenate the padded row to the local transformation matrix to create a final homogeneous matrix of shape (B, Nj, 4, 4).\n    7. Return the resulting homogeneous transformation matrix.\n    \"\"\"\n    # Check if rotation matrix R is provided\n    if R is None:\n        raise ValueError(\"Rotation matrix R must be provided.\")\n\n    # Get the batch size and number of joints from the shape of R\n    B, Nj, _, _ = R.shape\n\n    # If translation vector t is not provided, initialize it to zero vectors\n    if t is None:\n        t = torch.zeros(B, Nj, 3, 1, device=R.device)\n    else:\n        # Ensure that the shape of t matches the expected dimensions\n        if t.shape != (B, Nj, 3, 1):\n            raise ValueError(f\"Translation vector t has an invalid shape. Expected: {(B, Nj, 3, 1)}, Actual: {t.shape}\")\n\n    # Concatenate the rotation matrix R and translation vector t along the last dimension\n    local_transform = torch.cat([R, t], dim=-1)\n\n    # Create a padding row [0, 0, 0, 1] and expand it to match the batch and joint dimensions\n    padding_row = torch.tensor([0, 0, 0, 1], dtype=R.dtype, device=R.device)\n    padding_row = padding_row.view(1, 1, 1, 4).expand(B, Nj, 1, 4)\n\n    # Concatenate the padded row to the local transformation matrix\n    homog_matrix = torch.cat([local_transform, padding_row], dim=-2)\n\n    return homog_matrix\n```\n\nThis code follows the steps outlined in the problem description to build homogeneous transformation matrices from given rotation matrices and translation vectors.\n\n1. It checks if the rotation matrix `R` is provided. If not, it raises a `ValueError`.\n2. It retrieves the batch size `B` and number of joints `Nj` from the shape of `R`.\n3. If the translation vector `t` is not provided, it initializes `t` to zero vectors of shape `(B, Nj, 3, 1)`.\n4. It ensures that the shape of `t` matches the expected dimensions `(B, Nj, 3, 1)`. If not, it raises a `ValueError`.\n5. It concatenates the rotation matrix `R` and translation vector `t` along the last dimension to form a local transformation matrix of shape `(B, Nj, 3, 4)`.\n6. It creates a padding row `[0, 0, 0, 1]` and expands it to match the batch and joint dimensions, forming a tensor of shape `(B, Nj, 1, 4)`.\n7. It concatenates the padded row to the local transformation matrix to create the final homogeneous matrix of shape `(B, Nj, 4, 4)`.\n8. Finally, it returns the resulting homogeneous transformation matrix.\n\nThis code should solve the problem of building homogeneous transformation matrices as described.",
  "canonical_solution": "    \n    \n    if t is None:\n        B = R.shape[0]\n        Nj = R.shape[1]\n        t = torch.zeros(B, Nj, 3, 1).to(R.device)\n    \n    if R is None:\n        B = t.shape[0]\n        Nj = t.shape[1]\n        R = torch.eye(3).unsqueeze(0).unsqueeze(0).repeat(B, Nj, 1, 1).to(t.device)\n    \n    B = t.shape[0]\n    Nj = t.shape[1]\n        \n    # import ipdb; ipdb.set_trace()\n    if R.shape != (B, Nj, 3, 3) or t.shape != (B, Nj, 3, 1):\n        return KeyError\n    \n    G = torch.cat([R, t], dim=-1) # BxJx3x4 local transformation matrix\n    pad_row = torch.FloatTensor([0, 0, 0, 1]).to(R.device).view(1, 1, 1, 4).expand(B, Nj, -1, -1) # BxJx1x4\n    G = torch.cat([G, pad_row], dim=2) # BxJx4x4 padded to be 4x4 matrix an enable multiplication for the kinematic chain\n\n    return G\n\n\n",
  "unprocess_testcases": "R = torch.eye(3).unsqueeze(0).unsqueeze(0).repeat(2, 3, 1, 1)\nt = torch.tensor([[[[1.0], [2.0], [3.0]], [[4.0], [5.0], [6.0]], [[7.0], [8.0], [9.0]]],\n                  [[[1.0], [0.0], [1.0]], [[2.0], [2.0], [2.0]], [[3.0], [3.0], [3.0]]]])\nassert(((SOLUTION_SIGNATURE(R=R, t=t) == build_homog_matrix(R=R, t=t))).all())\n\nR = torch.tensor([[[[0.0, -1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],\n                   [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]]])\nassert(SOLUTION_SIGNATURE(R=R, t=t) == build_homog_matrix(R=R, t=t))\n\nR = torch.eye(3).unsqueeze(0).unsqueeze(0).repeat(3, 2, 1, 1)\nt = torch.zeros(2, 3, 3, 1)  # Intentional shape mismatch to trigger an assertion error\nassert(SOLUTION_SIGNATURE(R=R, t=t) == build_homog_matrix(R=R, t=t))\n"
 },
 {
  "task_id": "RWPB/26",
  "prompt": "import torch\n\ndef to_one_hot(tensor, num_cls, dim=1, ignore_index=255):\n    \"\"\"\n    Converts a given tensor to a one-hot encoded tensor.\n\n    Args:\n        tensor (torch.Tensor): A tensor containing class indices.\n        num_cls (int): The total number of classes.\n        dim (int, optional): The dimension along which to apply one-hot encoding. Default is 1.\n        ignore_index (int, optional): A specific index that should be ignored during the conversion. Default is 255.\n\n    Returns:\n        torch.Tensor: A one-hot encoded tensor.\n\n    This function takes a tensor with class indices (usually integers) and converts it into a one-hot\n    encoded format. Entries with the 'ignore_index' are treated as background and set to zero before encoding.\n    The function uses the scatter_ method to expand the tensor along the specified 'dim' dimension\n    and sets the corresponding index to 1, indicating class presence.\n    \"\"\"\n",
  "solution": "Here's the completed code for the `to_one_hot` function:\n\n```python\nimport torch\n\ndef to_one_hot(tensor, num_cls, dim=1, ignore_index=255):\n    \"\"\"\n    Converts a given tensor to a one-hot encoded tensor.\n\n    Args:\n        tensor (torch.Tensor): A tensor containing class indices.\n        num_cls (int): The total number of classes.\n        dim (int, optional): The dimension along which to apply one-hot encoding. Default is 1.\n        ignore_index (int, optional): A specific index that should be ignored during the conversion. Default is 255.\n\n    Returns:\n        torch.Tensor: A one-hot encoded tensor.\n\n    This function takes a tensor with class indices (usually integers) and converts it into a one-hot\n    encoded format. Entries with the 'ignore_index' are treated as background and set to zero before encoding.\n    The function uses the scatter_ method to expand the tensor along the specified 'dim' dimension\n    and sets the corresponding index to 1, indicating class presence.\n    \"\"\"\n    # Create a tensor of zeros with the same shape as the input tensor, but with an additional dimension for classes\n    one_hot = torch.zeros((*tensor.shape, num_cls), dtype=torch.float32, device=tensor.device)\n\n    # Set entries with the ignore_index to zero\n    valid_mask = (tensor != ignore_index).long()\n    tensor = tensor * valid_mask\n\n    # Use scatter_ to set the corresponding class index to 1 in the one-hot tensor\n    one_hot.scatter_(dim, tensor.unsqueeze(dim), 1)\n\n    # Multiply the one-hot tensor by the valid_mask to handle ignored entries\n    one_hot = one_hot * valid_mask.unsqueeze(dim)\n\n    return one_hot\n```\n\nExplanation:\n\n1. We create a tensor of zeros with the same shape as the input tensor, but with an additional dimension for classes. This tensor will store the one-hot encoded representation.\n\n2. We create a `valid_mask` tensor that has a value of 1 for valid entries (not equal to `ignore_index`) and 0 for entries that should be ignored. We multiply the input tensor by this mask to set ignored entries to zero.\n\n3. We use the `scatter_` method to expand the tensor along the specified `dim` dimension. The `scatter_` method takes three arguments: the dimension along which to scatter, the indices where the values should be scattered, and the values to be scattered. In this case, we scatter a value of 1 at the corresponding class index in the one-hot tensor.\n\n4. We multiply the one-hot tensor by the `valid_mask` tensor (with an additional dimension) to handle ignored entries. This step ensures that the ignored entries remain zero in the one-hot representation.\n\n5. Finally, we return the one-hot encoded tensor.\n\nThis function efficiently converts a tensor with class indices into a one-hot encoded format, handling ignored entries and allowing for customization of the dimension along which the encoding is applied.",
  "canonical_solution": "\n    # Get dimensions of the input tensor\n    b, h, w = tensor.shape\n\n    # Set ignore_index to 0 to treat it as background\n    tensor[tensor == ignore_index] = 0\n\n    # Initialize a new tensor of zeros with dimensions for batch, classes, height, and width\n    onehot_tensor = torch.zeros(b, num_cls, h, w)\n\n    # Perform the one-hot encoding using scatter_\n    onehot_tensor.scatter_(dim, tensor.unsqueeze(dim), 1)\n\n    return onehot_tensor\n\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([[[1, 2], [0, 1]]]), num_cls = 3) == to_one_hot(torch.tensor([[[1, 2], [0, 1]]]), num_cls = 3)).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[[255, 1], [2, 255]]]), num_cls = 4, ignore_index = 255) == to_one_hot(torch.tensor([[[255, 1], [2, 255]]]), num_cls = 4, ignore_index = 255)).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[[0, 1, 2, 3, 4, 5]]]), num_cls = 1000) == to_one_hot(torch.tensor([[[0, 1, 2, 3, 4, 5]]]), num_cls = 1000)).all())\n"
 },
 {
  "task_id": "RWPB/77",
  "prompt": "def is_power_of_two(n):\n    \"\"\"\n    chat.openai.com/chat\n    Return True if n is a power of 2, otherwise return False.\n\n    The function is_power_of_two takes an integer n as input and returns True if n is a power of 2, otherwise it returns False.\n    The function works by first checking if n is less than or equal to 0. If n is less than or equal to 0, it can't be a power of 2, so the function returns False.\n    If n is greater than 0, the function checks whether n is a power of 2 by using a bitwise AND operation between n and n-1. If n is a power of 2, then it will have only one bit set to 1 in its binary representation. When we subtract 1 from a power of 2, all the bits to the right of that bit become 1, and the bit itself becomes 0. So, when we perform a bitwise AND between n and n-1, we get 0 if n is a power of 2, and a non-zero value otherwise.\n    Thus, if the result of the bitwise AND operation is 0, then n is a power of 2 and the function returns True. Otherwise, the function returns False.\n\n    \"\"\"\n",
  "solution": "Here's the completed code to solve the problem:\n\n```python\ndef is_power_of_two(n):\n    \"\"\"\n    Return True if n is a power of 2, otherwise return False.\n\n    The function is_power_of_two takes an integer n as input and returns True if n is a power of 2, otherwise it returns False.\n    The function works by first checking if n is less than or equal to 0. If n is less than or equal to 0, it can't be a power of 2, so the function returns False.\n    If n is greater than 0, the function checks whether n is a power of 2 by using a bitwise AND operation between n and n-1. If n is a power of 2, then it will have only one bit set to 1 in its binary representation. When we subtract 1 from a power of 2, all the bits to the right of that bit become 1, and the bit itself becomes 0. So, when we perform a bitwise AND between n and n-1, we get 0 if n is a power of 2, and a non-zero value otherwise.\n    Thus, if the result of the bitwise AND operation is 0, then n is a power of 2 and the function returns True. Otherwise, the function returns False.\n\n    \"\"\"\n    if n <= 0:\n        return False\n    return (n & (n - 1)) == 0\n```\n\nExplanation:\n1. The function first checks if `n` is less than or equal to 0. If it is, the function returns `False` because a power of 2 must be a positive integer.\n2. If `n` is greater than 0, the function performs a bitwise AND operation between `n` and `n-1` using the `&` operator.\n3. If the result of the bitwise AND operation is 0, it means that `n` is a power of 2, and the function returns `True`.\n4. If the result of the bitwise AND operation is non-zero, it means that `n` is not a power of 2, and the function returns `False`.\n\nThis implementation efficiently checks whether a given integer is a power of 2 using bitwise operations.",
  "canonical_solution": "    \n    if n <= 0:\n        return False\n    return (n & (n - 1)) == 0\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(0) == is_power_of_two(0))\nassert(SOLUTION_SIGNATURE(-16) == is_power_of_two(-16))\nassert(SOLUTION_SIGNATURE(16) == is_power_of_two(16))\nassert(SOLUTION_SIGNATURE(1024) == is_power_of_two(1024))\nassert(SOLUTION_SIGNATURE(18) == is_power_of_two(18))\nassert(SOLUTION_SIGNATURE(999) == is_power_of_two(999))\n"
 }
]