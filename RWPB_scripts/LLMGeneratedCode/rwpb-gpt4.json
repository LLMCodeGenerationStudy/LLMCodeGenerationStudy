[
 {
  "task_id": "RWPB/109",
  "prompt": "import torch\n\ndef rot_trans_geo(geometry, rot, trans):\n    \"\"\"\n    Applies rotation and translation to geometry.\n\n    Args:\n    geometry (torch.Tensor): Tensor of shape [batch_size, num_points, 3] representing the geometry points.\n    rot (torch.Tensor): Rotation matrix of shape [batch_size, 3, 3].\n    trans (torch.Tensor): Translation vector of shape [batch_size, 3].\n\n    Returns:\n    torch.Tensor: Transformed geometry of shape [batch_size, num_points, 3].\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the function `rot_trans_geo` which applies a rotation and translation to a set of geometric points represented as tensors:\n\n```python\nimport torch\n\ndef rot_trans_geo(geometry, rot, trans):\n    \"\"\"\n    Applies rotation and translation to geometry.\n\n    Args:\n    geometry (torch.Tensor): Tensor of shape [batch_size, num_points, 3] representing the geometry points.\n    rot (torch.Tensor): Rotation matrix of shape [batch_size, 3, 3].\n    trans (torch.Tensor): Translation vector of shape [batch_size, 3].\n\n    Returns:\n    torch.Tensor: Transformed geometry of shape [batch_size, num_points, 3].\n    \"\"\"\n    # Apply rotation: matrix multiplication between rotation matrices and geometry points\n    rotated_geometry = torch.matmul(geometry, rot.transpose(-1, -2))\n    \n    # Apply translation: add the translation vector to the rotated points\n    # trans[:, None, :] reshapes trans to [batch_size, 1, 3] for broadcasting\n    transformed_geometry = rotated, geometry + trans[:, None, :]\n    \n    return transformed_geometry\n```\n\nThis function first performs a matrix multiplication between the geometry points and the transpose of the rotation matrices to apply the rotation. The transpose is taken on the rotation matrix to align the dimensions correctly for the multiplication. After rotation, the translation is applied by adding the translation vector, which is reshaped for broadcasting to match the dimensions of the geometry tensor. The result is a new tensor of the same shape as the input geometry tensor, containing the transformed points.",
  "canonical_solution": "    \n    rott_geo = torch.bmm(rot, geometry.permute(0, 2, 1)) + trans.view(-1, 3, 1)\n    return rott_geo.permute(0, 2, 1)\n\n\n",
  "unprocess_testcases": "geometry = torch.tensor([[[1., 2., 3.],\n                          [4., 5., 6.]]])\nrot = torch.eye(3).unsqueeze(0)  # Identity rotation for 1 batch\ntrans = torch.zeros(1, 3)        # Zero translation for 1 batch\nassert((SOLUTION_SIGNATURE(geometry, rot, trans) == rot_trans_geo(geometry, rot, trans)).all())\n\ngeometry = torch.tensor([[[1., 0., 0.],\n                          [0., 1., 0.]],\n                         [[0., 0., 1.],\n                          [1., 1., 1.]]])\nrot = torch.tensor([[[0., -1., 0.], [1., 0., 0.], [0., 0., 1.]],  # 90 degrees on Z-axis\n                    [[-1., 0., 0.], [0., 1., 0.], [0., 0., -1.]]]) # 180 degrees on Y-axis\ntrans = torch.tensor([[1., 2., 3.],\n                      [0., -1., 2.]])\nassert((SOLUTION_SIGNATURE(geometry, rot, trans) == rot_trans_geo(geometry, rot, trans)).all())\n\ngeometry = torch.tensor([[[0., 0., 0.],\n                          [999., 999., 999.]]])\nrot = torch.tensor([[[0., 1., 0.], [-1., 0., 0.], [0., 0., 1.]]])  # 90 degrees on X-axis\ntrans = torch.tensor([[1000., 2000., 3000.]])\nassert((SOLUTION_SIGNATURE(geometry, rot, trans) == rot_trans_geo(geometry, rot, trans)).all())\n"
 },
 {
  "task_id": "RWPB/54",
  "prompt": "def get_crop_box(box, expand):\n    \"\"\"\n    Expands a given bounding box by a specified factor, keeping the center of the box the same.\n\n    Arguments:\n    box : list or tuple\n        The original bounding box specified as (x, y, x1, y1) where (x, y) is the top-left corner and\n        (x1, y1) is the bottom-right corner.\n    expand : float\n        The factor by which to expand the bounding box. For example, a factor of 1.5 will increase\n        the size of the box by 50%.\n\n    Returns:\n    tuple\n        A tuple containing the new expanded bounding box as a list [new_x, new_y, new_x1, new_y1] and\n        the half-size of the new box `s`.\n    \"\"\"\n",
  "solution": "Here's the completed function `get_crop_box` that expands a given bounding box by a specified factor while keeping the center of the box the same:\n\n```python\ndef get_crop_box(box, expand):\n    \"\"\"\n    Expands a given bounding box by a specified factor, keeping the center of the box the same.\n\n    Arguments:\n    box : list or tuple\n        The original bounding box specified as (x, y, x1, y1) where (x, y) is the top-left corner and\n        (x1, y1) is the bottom-right corner.\n    expand : float\n        The factor by which to expand the bounding box. For example, a factor of 1.5 will increase\n        the size of the box by 50%.\n\n    Returns:\n    tuple\n        A tuple containing the new expanded bounding box as a list [new_x, new_y, new_x1, new_y1] and\n        the half-size of the new box `s`.\n    \"\"\"\n    # Calculate the center of the original box\n    center_x = (box[0] + box[2]) / 2\n    center_y = (box[1] + box[3]) / 2\n\n    # Calculate the width and height of the original box\n    width = box[2] - box[0]\n    height = box[3] - box[1]\n\n    # Calculate the new width and height based on the expansion factor\n    new_width = width * expand\n    new_height = height * expand\n\n    # Calculate the new top-left and bottom-right corners\n    new_x = center_x - new_width / 2\n    new_y = center_y - new_height / 2\n    new_x1 = center_x + new_width / 2\n    new_y1 = center_y + new_height / 2\n\n    # Calculate the half-size of the new box\n    s = max(new_width, new_height) / 2\n\n    # Return the new bounding box and the half-size\n    return ([new_x, new_y, new_x1, new_y1], s)\n```\n\nThis function first calculates the center of the original bounding box. It then computes the new dimensions based on the expansion factor and uses these dimensions to determine the new corners of the box. The function returns the new bounding box coordinates and the half-size of the new box.",
  "canonical_solution": "    \n    # Extract the coordinates from the original bounding box\n    x, y, x1, y1 = box\n    # Calculate the center of the original box\n    x_c, y_c = (x + x1) // 2, (y + y1) // 2\n    # Calculate width and height of the original box\n    w, h = x1 - x, y1 - y\n    # Calculate half of the size of the expanded box\n    s = int(max(w, h) // 2 * expand)\n    # Define the new expanded bounding box based on the center and size\n    crop_box = [x_c - s, y_c - s, x_c + s, y_c + s]\n\n    return crop_box, s\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(box = (100, 100, 200, 200), expand = 1.5)[0] == get_crop_box(box = (100, 100, 200, 200), expand = 1.5)[0])\nassert(SOLUTION_SIGNATURE(box = (50, 50, 150, 150), expand = 1.0)[0] == get_crop_box(box = (50, 50, 150, 150), expand = 1.0)[0])\nassert(SOLUTION_SIGNATURE(box = (-100, -100, 100, 100), expand = 2.0)[0] == get_crop_box(box = (-100, -100, 100, 100), expand = 2.0)[0])\nassert(SOLUTION_SIGNATURE(box = (100, 100, 200, 200), expand = 1.5)[1] == get_crop_box(box = (100, 100, 200, 200), expand = 1.5)[1])\nassert(SOLUTION_SIGNATURE(box = (50, 50, 150, 150), expand = 1.0)[1] == get_crop_box(box = (50, 50, 150, 150), expand = 1.0)[1])\nassert(SOLUTION_SIGNATURE(box = (-100, -100, 100, 100), expand = 2.0)[1] == get_crop_box(box = (-100, -100, 100, 100), expand = 2.0)[1])\n"
 },
 {
  "task_id": "RWPB/93",
  "prompt": "MAX_VAL = (1 << 256) - 1\n\n# does not revert on overflow\ndef unsafeSub(x, y):\n    \"\"\"\n    Subtract two integers, ignoring overflow.\n    Args:\n        - x: int, the minuend\n        - y: int, the subtrahend\n    Returns:\n        result: int, the difference of x and y, with overflow ignored\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `unsafeSub` function which performs subtraction and ignores overflow by using modulo arithmetic to wrap around the result based on a maximum value defined by `MAX_VAL`.\n\n```python\nMAX_VAL = (1 << 256) - 1\n\ndef unsafeSub(x, y):\n    \"\"\"\n    Subtract two integers, ignoring overflow.\n    Args:\n        - x: int, the minuend\n        - y: int, the subtrahend\n    Returns:\n        result: int, the difference of x and y, with overflow ignored\n    \"\"\"\n    # Calculate the difference\n    result = x - y\n    \n    # Apply modulo operation to simulate 256-bit overflow behavior\n    result = result % (MAX_VAL + 1)\n    \n    return result\n```\n\nThis function calculates the difference between `x` and `y`, then uses modulo arithmetic to ensure the result fits within a 256-bit integer range, effectively simulating the behavior of a 256-bit unsigned integer overflow.",
  "canonical_solution": "    \n    return (x - y) & MAX_VAL\n\n\n",
  "unprocess_testcases": "\nx = 100\ny = 50\nassert(SOLUTION_SIGNATURE(x, y) == unsafeSub(x, y))\n\nx = 0\ny = 100\nassert(SOLUTION_SIGNATURE(x, y) == unsafeSub(x, y))\n\nx = (1 << 256) - 1\ny = 1\nassert(SOLUTION_SIGNATURE(x, y) == unsafeSub(x, y))\n"
 },
 {
  "task_id": "RWPB/60",
  "prompt": "import torch\n\n\ndef rms_norm_ref(x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False):\n    \"\"\"\n    Applies Residual Multi-scale Normalization (RMSNorm) to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        The input tensor.\n    weight : torch.Tensor\n        The weight tensor for scaling the normalized input.\n    bias : torch.Tensor or None\n        The bias tensor for shifting the normalized input. If None, no bias is added.\n    residual : torch.Tensor or None\n        The residual tensor to be added to the input. If None, no residual is added.\n    eps : float, optional\n        A small constant added for numerical stability.\n    prenorm : bool, optional\n        If True, the function returns both the normalized output and the original input.\n    upcast : bool, optional\n        If True, upcasts all tensors to float type for computation.\n\n    Returns\n    -------\n    torch.Tensor or tuple of torch.Tensor\n        The RMS normalized output tensor. If `prenorm` is True, returns a tuple\n        containing the output tensor and the original input tensor.\n\n    Notes\n    -----\n    RMSNorm is a normalization technique that scales the input by the inverse square root\n    of the mean of squares. This function performs the normalization, applies the weight\n    and bias, and optionally adds a residual connection. If `upcast` is True, the\n    function upcasts all tensors to float type before performing the operations to\n    maintain precision.\n    \"\"\"\n",
  "solution": "    # Upcast tensors to float if required\n    if upcast:\n        x = x.float()\n        weight = weight.float()\n        if bias is not None:\n            bias = bias.float()\n        if residual is not None:\n            residual = residual.float()\n\n    # Compute the mean of the squares along the last dimension\n    mean_square = torch.mean(x ** 2, dim=-1, keepdim=True)\n\n    # Add a small epsilon for numerical stability and compute the root\n    norm = torch.sqrt(mean_square + eps)\n\n    # Normalize the input tensor\n    normalized_x = x / norm\n\n    # Apply weight and bias\n    output = normalized_x * weight\n    if bias is not None:\n        output += bias\n\n    # Add residual if provided\n    if residual is not None:\n        output += residual\n\n    # Return the output and optionally the original input\n    if prenorm:\n        return output, x\n    else:\n        return output\n\n# Example usage:\n# Assuming x, weight, bias, and residual are properly defined torch.Tensor objects,\n# you can call the function like this:\n# output = rms_norm_ref(x, weight, bias, residual)",
  "canonical_solution": "\n    # Preserve the original dtype\n    dtype = x.dtype\n\n    # Upcast weight, bias, and x to float if necessary\n    if upcast:\n        weight = weight.float()\n        bias = bias.float() if bias is not None else None\n        x = x.float()\n        residual = residual.float() if residual is not None else residual\n\n    # Add residual to input if provided\n    if residual is not None:\n        x = (x + residual).to(x.dtype)\n\n    # Compute the reciprocal of the standard deviation of the input\n    rstd = 1 / torch.sqrt((x.square()).mean(dim=-1, keepdim=True) + eps)\n\n    # Apply normalization, weight, and bias\n    out = (x * rstd * weight) + bias if bias is not None else (x * rstd * weight)\n\n    # Cast the output back to the original dtype\n    out = out.to(dtype)\n\n    # Return the output only or the output with the original input if prenorm is True\n    return out if not prenorm else (out, x)\n\n\n",
  "unprocess_testcases": "a = torch.rand(10, 5)\nb = torch.rand(5,)\n\nassert((SOLUTION_SIGNATURE(a, b, b, None) == rms_norm_ref(a, b, b, None)).all())\nassert((SOLUTION_SIGNATURE(a, b, b, None, 1e-5, False, True) == rms_norm_ref(a, b, b, None, 1e-5, False, True)).all())\nassert((SOLUTION_SIGNATURE(a, b, b, a) == rms_norm_ref(a, b, b, a)).all())\nassert((SOLUTION_SIGNATURE(a, b, None, b, 1e-6, True, False)[0] == rms_norm_ref(a, b, None, b, 1e-6, True, False)[0]).all())\nassert((SOLUTION_SIGNATURE(a, b, None, b, 1e-6, True, False)[1] == rms_norm_ref(a, b, None, b, 1e-6, True, False)[1]).all())\nassert((SOLUTION_SIGNATURE(a, b, None) == rms_norm_ref(a, b, None)).all())\n"
 },
 {
  "task_id": "RWPB/31",
  "prompt": "import torch\nimport math\n\ndef resize_to_batch_size(tensor, batch_size):\n    \"\"\"\n    Adjusts the batch size of a tensor to the specified size, interpolating if necessary.\n\n    Args:\n        tensor (torch.Tensor): Input tensor whose batch size needs to be adjusted.\n        batch_size (int): The desired batch size after adjustment.\n\n    Returns:\n        torch.Tensor: The tensor with its batch size adjusted to the specified size.\n\n    This function handles resizing of the batch dimension of a tensor. If the current batch size matches\n    the desired batch size, the tensor is returned unchanged. For reducing the batch size, it uses linear\n    interpolation to select indices, ensuring that the resized tensor covers the whole original batch as\n    evenly as possible. For increasing the batch size, it applies a different interpolation method to avoid\n    extrapolation and to ensure that every element from the original batch is represented.\n    \"\"\"\n",
  "solution": "    current_batch_size = tensor.size(0)\n\n    # If the current batch size is already the desired size, return the tensor as is\n    if current_batch_size == batch_size:\n        return tensor\n\n    # Create a range of indices to interpolate along the batch dimension\n    indices = torch.linspace(0, current_batch_size - 1, batch_size)\n\n    # Round indices to nearest integer to use for indexing\n    indices = torch.round(indices).long()\n\n    # Use indexing to create the new tensor with the desired batch size\n    resized_tensor = tensor[indices]\n\n    return resized_tensor\n\n# Example usage:\n# tensor = torch.randn(10, 3, 64, 64)  # Example tensor with batch size of 10\n# new_batch_size = 5\n# resized_tensor = resize_to_batch_size(tensor, new_batch_size)\n# print(resized_tensor.shape)  # Should print torch.Size([5, 3, 64, 64])",
  "canonical_solution": "    \n    # Retrieve the initial batch size from the tensor\n    in_batch_size = tensor.shape[0]\n\n    # Return the tensor unchanged if the current batch size is already the desired size\n    if in_batch_size == batch_size:\n        return tensor\n\n    # Return only the required portion of the tensor if the desired batch size is less than or equal to 1\n    if batch_size <= 1:\n        return tensor[:batch_size]\n\n    # Create an empty tensor to store the output, maintaining the original data type and device\n    output = torch.empty([batch_size] + list(tensor.shape[1:]), dtype=tensor.dtype, device=tensor.device)\n\n    # Handle reduction of batch size\n    if batch_size < in_batch_size:\n        scale = (in_batch_size - 1) / (batch_size - 1)\n        for i in range(batch_size):\n            output[i] = tensor[min(round(i * scale), in_batch_size - 1)]\n\n    # Handle increase in batch size\n    else:\n        scale = in_batch_size / batch_size\n        for i in range(batch_size):\n            index = min(math.floor((i + 0.5) * scale), in_batch_size - 1)\n            output[i] = tensor[index]\n\n    return output\n\n\n",
  "unprocess_testcases": "a = torch.randn(5, 3, 3)\nb = torch.randn(10, 4, 4)\nc = torch.randn(3, 2, 2)\n\nassert((SOLUTION_SIGNATURE(a, 5) == resize_to_batch_size(a, 5)).all())\nassert((SOLUTION_SIGNATURE(b, 3) == resize_to_batch_size(b, 3)).all())\nassert((SOLUTION_SIGNATURE(c, 8) == resize_to_batch_size(c, 8)).all())\n"
 },
 {
  "task_id": "RWPB/21",
  "prompt": "import re\n\ndef validate_user_id(user_id):\n    \"\"\"\n    Validates a user ID against a specific pattern.\n\n    Parameters\n    ----------\n    user_id : str\n        The user ID to validate.\n\n    Returns\n    -------\n    bool\n        True if the user ID is valid, False otherwise.\n\n    Notes\n    -----\n    The function checks if the user ID is a non-empty string that:\n    - Starts with a letter (uppercase or lowercase).\n    - Followed by any number of alphanumeric characters or underscores.\n\n    The validation is performed using a regular expression (regex) pattern.\n    \"\"\"\n",
  "solution": "Here's the completion of the `validate_user_id` function using a regular expression to match the specified pattern:\n\n```python\nimport re\n\ndef validate_user_id(user_id):\n    \"\"\"\n    Validates a user ID against a specific pattern.\n\n    Parameters\n    ----------\n    user_id : str\n        The user ID to validate.\n\n    Returns\n    -------\n    bool\n        True if the user ID is valid, False otherwise.\n\n    Notes\n    -----\n    The function checks if the user ID is a non-empty string that:\n    - Starts with a letter (uppercase or lowercase).\n    - Followed by any number of alphanumeric characters or underscores.\n\n    The validation is performed using a regular expression (regex) pattern.\n    \"\"\"\n    # Define the regex pattern\n    pattern = r'^[A-Za-z][A-Za-z0-9_]*$'\n    \n    # Use the re.match function to check if the pattern matches the entire string\n    if re.match(pattern, user_id):\n        return True\n    else:\n        return False\n```\n\nThis function uses the `re.match` method to check if the `user_id` matches the pattern defined. The pattern `^[A-Za-z][A-Za-z0-9_]*$` ensures that the string starts with a letter and is followed by any number of alphanumeric characters or underscores. The `^` and `$` are anchors that match the start and end of the string, respectively, ensuring that the entire string conforms to the pattern.",
  "canonical_solution": "    \n    # Define the regex pattern for a valid user ID\n    pattern = r'^[A-Za-z][A-Za-z0-9_]*$'\n\n    # Check if the user_id matches the pattern and is an instance of str\n    if isinstance(user_id, str) and re.match(pattern, user_id):\n        return True\n    else:\n        return False\n    \n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(\"A1_bcd\") == validate_user_id(\"A1_bcd\"))\nassert(SOLUTION_SIGNATURE(\"Z\") == validate_user_id(\"Z\"))\nassert(SOLUTION_SIGNATURE(\"123abc\") == validate_user_id(\"123abc\"))\n"
 },
 {
  "task_id": "RWPB/70",
  "prompt": "import numpy as np\n\ndef psnr_to_mse(psnr):\n  \"\"\"\n  Compute the Mean Squared Error (MSE) given the Peak Signal-to-Noise Ratio (PSNR).\n\n  Parameters:\n  psnr (float): The Peak Signal-to-Noise Ratio, typically in decibels (dB).\n\n  Returns:\n  float: The computed Mean Squared Error (MSE).\n\n  Process:\n  - The function assumes the maximum pixel value is 1.\n  - PSNR is converted to a linear scale using the formula:\n    MSE = exp(-0.1 * ln(10) * PSNR)\n  - This conversion is based on the logarithmic relationship between PSNR and MSE.\n  - The constant -0.1 * ln(10) transforms the PSNR value into a scale where \n    the exponentiation gives the MSE.\n  \"\"\"\n",
  "solution": "  import numpy as np\n\n  def psnr_to_mse(psnr):\n    \"\"\"\n    Compute the Mean Squared Error (MSE) given the Peak Signal-to-Noise Ratio (PSNR).\n\n    Parameters:\n    psnr (float): The Peak Signal-to-Noise Ratio, typically in decibels (dB).\n\n    Returns:\n    float: The computed Mean Squared Error (MSE).\n\n    Process:\n    - The function assumes the maximum pixel value is 1.\n    - PSNR is converted to a linear scale using the formula:\n      MSE = exp(-0.1 * ln(10) * PSNR)\n    - This conversion is based on the logarithmic relationship between PSNR and MSE.\n    - The constant -0.1 * ln(10) transforms the PSNR value into a scale where \n      the exponentiation gives the MSE.\n    \"\"\"\n    return np.exp(-0.1 * np.log(10) * psnr)\n\n  # Example usage:\n  psnr_value = 30  # Example PSNR value in dB\n  mse = psnr_to_mse(psnr_value)\n  print(\"MSE for PSNR\", psnr_value, \"dB is:\", mse)",
  "canonical_solution": "  \n  return np.exp(-0.1 * np.log(10.) * psnr)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(1) == psnr_to_mse(1))\nassert(SOLUTION_SIGNATURE(0) == psnr_to_mse(0))\nassert(SOLUTION_SIGNATURE(0.2) == psnr_to_mse(0.2))\nassert(SOLUTION_SIGNATURE(0.31423) == psnr_to_mse(0.31423))\nassert(SOLUTION_SIGNATURE(1657546223) == psnr_to_mse(1657546223))\n"
 },
 {
  "task_id": "RWPB/83",
  "prompt": "import math\nimport cv2\nimport numpy as np\n\ndef apply_min_size(sample, size, image_interpolation_method=cv2.INTER_AREA):\n    \"\"\"\n    Resize the images in the sample to ensure they meet the minimum specified size while maintaining the aspect ratio.\n\n    Args:\n        sample (dict): A dictionary containing the images to be resized. The keys are:\n            - \"image\": The main image to be resized.\n            - \"disparity\": The disparity map associated with the image.\n            - \"mask\": A binary mask associated with the image.\n        size (tuple): The minimum desired size (height, width) for the images.\n        image_interpolation_method (int, optional): Interpolation method to be used for resizing the main image. \n                                                    Defaults to cv2.INTER_AREA.\n\n    Returns:\n        tuple: The new size (height, width) of the resized images.\n    \"\"\"\n",
  "solution": "    # Extract the images from the sample dictionary\n    image = sample['image']\n    disparity = sample['disparity']\n    mask = sample['mask']\n\n    # Get the current dimensions of the image\n    current_height, current_width = image.shape[:2]\n\n    # Calculate the scaling factors needed to meet the minimum size\n    height_scale = size[0] / current_height if current_height < size[0] else 1\n    width_scale = size[1] / current_width if current_width < size[1] else 1\n\n    # Choose the larger scale to maintain the aspect ratio\n    scale_factor = max(height_scale, width_scale)\n\n    # Calculate the new dimensions\n    new_height = int(current_height * scale_factor)\n    new_width = int(current_width * scale_factor)\n\n    # Resize the images using the calculated dimensions\n    if scale_factor > 1:\n        # Resize the main image using the specified interpolation method\n        sample['image'] = cv2.resize(image, (new_width, new_height), interpolation=image_interpolation_method)\n        # Resize the disparity map and mask using nearest neighbor interpolation to avoid introducing new values\n        sample['disparity'] = cv2.resize(disparity, (new_width, new_height), interpolation=cv2.INTER_NEAREST)\n        sample['mask'] = cv2.resize(mask, (new_width, new_height), interpolation=cv2.INTER_NEAREST)\n\n    # Return the new size of the images\n    return (new_height, new_width)",
  "canonical_solution": "    \n    shape = list(sample[\"disparity\"].shape)\n\n    if shape[0] >= size[0] and shape[1] >= size[1]:\n        return sample\n\n    scale = [0, 0]\n    scale[0] = size[0] / shape[0]\n    scale[1] = size[1] / shape[1]\n\n    scale = max(scale)\n\n    shape[0] = math.ceil(scale * shape[0])\n    shape[1] = math.ceil(scale * shape[1])\n\n    # resize\n    sample[\"image\"] = cv2.resize(\n        sample[\"image\"], tuple(shape[::-1]), interpolation=image_interpolation_method\n    )\n\n    sample[\"disparity\"] = cv2.resize(\n        sample[\"disparity\"], tuple(shape[::-1]), interpolation=cv2.INTER_NEAREST\n    )\n    sample[\"mask\"] = cv2.resize(\n        sample[\"mask\"].astype(np.float32),\n        tuple(shape[::-1]),\n        interpolation=cv2.INTER_NEAREST,\n    )\n    sample[\"mask\"] = sample[\"mask\"].astype(bool)\n\n    return tuple(shape)\n\n\n",
  "unprocess_testcases": "sample = {\n    \"image\": np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8),\n    \"disparity\": np.random.randint(0, 256, (100, 100), dtype=np.uint8),\n    \"mask\": np.random.randint(0, 2, (100, 100), dtype=np.bool_)\n}\nsize = (200, 300)\nassert((SOLUTION_SIGNATURE(sample, size) == apply_min_size(sample, size)).all())\n\nsample = {\n    \"image\": np.random.randint(0, 256, (300, 300, 3), dtype=np.uint8),\n    \"disparity\": np.random.randint(0, 256, (300, 300), dtype=np.uint8),\n    \"mask\": np.random.randint(0, 2, (300, 300), dtype=np.bool_)\n}\nsize = (200, 200)\nassert((SOLUTION_SIGNATURE(sample, size) == apply_min_size(sample, size)).all())\n\nsample = {\n    \"image\": np.random.randint(0, 256, (150, 100, 3), dtype=np.uint8),\n    \"disparity\": np.random.randint(0, 256, (150, 100), dtype=np.uint8),\n    \"mask\": np.random.randint(0, 2, (150, 100), dtype=np.bool_)\n}\nsize = (300, 450)\nassert((SOLUTION_SIGNATURE(sample, size) == apply_min_size(sample, size)).all())\n"
 },
 {
  "task_id": "RWPB/44",
  "prompt": "import torch\n\ndef interleave(tensor1, tensor2):\n    \"\"\"\n    Interleaves two tensors along a new dimension, creating two interleaved sequences.\n    If the \n\n    Args:\n        tensor1 (torch.Tensor): The first tensor to interleave.\n        tensor2 (torch.Tensor): The second tensor to interleave.\n\n    Returns:\n        tuple: A tuple containing two tensors. The first tensor is interleaved starting with tensor1,\n               and the second tensor is interleaved starting with tensor2.\n\n    This function interleaves two input tensors element-wise along a newly created dimension and then flattens\n    the interleaved tensor back into the original dimension. It produces two outputs: one starting with the first tensor\n    and the other starting with the second tensor. This can be particularly useful for tasks where data from two different\n    sources need to be alternately processed or analyzed.\n\n    Example Usage:\n        tensor1 = torch.tensor([1, 3, 5])\n        tensor2 = torch.tensor([2, 4, 6])\n        res1, res2 = interleave(tensor1, tensor2)\n        print(res1)  # Outputs: tensor([1, 2, 3, 4, 5, 6])\n        print(res2)  # Outputs: tensor([2, 1, 4, 3, 6, 5])\n    \"\"\"\n",
  "solution": "    # Ensure both tensors have the same shape\n    if tensor1.shape != tensor2.shape:\n        raise ValueError(\"Both tensors must have the same shape\")\n\n    # Stack tensors along a new dimension\n    stacked = torch.stack((tensor1, tensor2), dim=0)\n\n    # Interleave the tensors by reshaping\n    interleaved_shape = (-1,) + tensor1.shape\n    interleaved = stacked.permute(1, 0, *range(2, stacked.dim())).reshape(interleaved_shape)\n\n    # Create two interleaved tensors, one starting with tensor1, the other with tensor2\n    result1 = interleaved\n    result2 = interleaved.roll(shifts=-1, dims=0)\n\n    return result1, result2\n\n# Example usage:\n# tensor1 = torch.tensor([1, 3, 5])\n# tensor2 = torch.tensor([2, 4, 6])\n# res1, res2 = interleave(tensor1, tensor2)\n# print(res1)  # Outputs: tensor([1, 2, 3, 4, 5, 6])\n# print(res2)  # Outputs: tensor([2, 1, 4, 3, 6, 5])",
  "canonical_solution": "\n    # Stack the tensors alternately along a new dimension (dim=1)\n    res1 = torch.stack((tensor1, tensor2), dim=1).flatten(0, 1)  # Flatten across original and new dimension\n    res2 = torch.stack((tensor2, tensor1), dim=1).flatten(0, 1)  # Start with the second tensor\n\n    return res1, res2\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([1, 3, 5]), torch.tensor([2, 4, 6]))[0] == interleave(torch.tensor([1, 3, 5]), torch.tensor([2, 4, 6]))[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 3, 5]), torch.tensor([2, 4, 6]))[1] == interleave(torch.tensor([1, 3, 5]), torch.tensor([2, 4, 6]))[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 3, 5, 7]), torch.tensor([2, 4, 5, 0]))[0] == interleave(torch.tensor([1, 3, 5, 7]), torch.tensor([2, 4, 5, 0]))[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 3, 5, 7]), torch.tensor([2, 4, 5, 0]))[1] == interleave(torch.tensor([1, 3, 5, 7]), torch.tensor([2, 4, 5, 0]))[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))[0] == interleave(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))[1] == interleave(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))[1]).all())\n"
 },
 {
  "task_id": "RWPB/15",
  "prompt": "import numpy as np\n\n\ndef cosine_distance(a, b):\n    \"\"\"\n    Compute the consine distance between vector a and vector b. Only support `a` is an embedding vector, `b` is a vector or matrix.\n    if the length of a is not equal the length of b, please return KeyError\n    Input: \n    a: numpy vector\n    b: numpy vector\n    return:\n    the cosine distance between vector a and vector b: int.\n    \"\"\"\n",
  "solution": "Certainly! Below is the completed function `cosine_distance` that computes the cosine distance between two vectors or between a vector and each row of a matrix. The function checks if the dimensions of the input vectors or matrices are compatible and raises a `KeyError` if they are not.\n\n```python\nimport numpy as np\n\ndef cosine_distance(a, b):\n    \"\"\"\n    Compute the cosine distance between vector a and vector b. Only support `a` is an embedding vector, `b` is a vector or matrix.\n    If the length of a is not equal to the length of b, please return KeyError.\n    Input: \n    a: numpy vector\n    b: numpy vector or matrix\n    return:\n    the cosine distance between vector a and vector b: float or numpy array.\n    \"\"\"\n    # Ensure a is a 1D vector\n    if a.ndim != 1:\n        raise ValueError(\"Vector a must be a 1-dimensional array.\")\n    \n    # Check if b is a vector or a matrix\n    if b.ndim == 1:\n        # b is a vector\n        if a.shape[0] != b.shape[0]:\n            raise KeyError(\"The dimensions of vector a and vector b do not match.\")\n        # Normalize vectors\n        a_norm = np.linalg.norm(a)\n        b_norm = np.linalg.norm(b)\n        if a_norm == 0 or b_norm == 0:\n            return 1.0  # Return maximum distance if one vector is zero\n        cosine_similarity = np.dot(a, b) / (a_norm * b_norm)\n        return 1 - cosine_similarity  # Cosine distance is 1 - cosine similarity\n    elif b.ndim == 2:\n        # b is a matrix\n        if a.shape[0] != b.shape[1]:\n            raise KeyError(\"The dimension of vector a does not match the number of columns in matrix b.\")\n        # Normalize vector a\n        a_norm = np.linalg.norm(a)\n        if a_norm == 0:\n            return np.ones(b.shape[0])  # Return maximum distance if vector a is zero\n        # Normalize each row in matrix b\n        b_norms = np.linalg.norm(b, axis=1)\n        # Compute dot product\n        dot_products = np.dot(b, a)\n        # Avoid division by zero\n        valid_norms = b_norms != 0\n        cosine_similarities = np.zeros(b.shape[0])\n        cosine_similarities[valid_norms] = dot_products[valid_norms] / (a_norm * b_norms[valid_norms])\n        return 1 - cosine_similarities  # Cosine distance\n    else:\n        raise ValueError(\"Array b must be either a 1-dimensional or 2-dimensional array.\")\n```\n\nThis function handles both cases where `b` is a single vector or a matrix of vectors. It computes the cosine similarity and then converts it to cosine distance. The function also includes error handling for mismatched dimensions and zero-length vectors.",
  "canonical_solution": "\n    if a.shape[-1] != b.shape[-1]:\n        return ValueError\n    dist = np.dot(a, b.T) / (np.linalg.norm(a, axis=-1) * np.linalg.norm(b, axis=-1))\n    return dist\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(a = np.array([1, 1]), b = np.array([0, 1])) == cosine_distance(a = np.array([1, 1]), b = np.array([0, 1]))).all())\nassert(SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([0, 1])) == cosine_distance(a = np.array([1, 0, 0]), b = np.array([0, 1])))\nassert((SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])) == cosine_distance(a = np.array([1, 0, 0]), b = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))).all())\n"
 },
 {
  "task_id": "RWPB/119",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\ndef rotate_about_x_axis(vector, theta):\n    \"\"\"\n    Rotate a 3D vector about the x-axis by a specified angle.\n    Args:\n        - vector: numpy array of shape (3,), the 3D vector to be rotated\n        - theta: float, angle of rotation in radians\n    Returns:\n        rotated_vector: numpy array of shape (3,), the rotated 3D vector\n    \"\"\"\n",
  "solution": "    # Define the rotation matrix for rotation about the x-axis\n    rotation_matrix = np.array([\n        [1, 0, 0],\n        [0, math.cos(theta), -math.sin(theta)],\n        [0, math.sin(theta), math.cos(theta)]\n    ])\n    \n    # Perform the matrix multiplication to get the rotated vector\n    rotated_vector = np.dot(rotation_matrix, vector)\n    \n    return rotated_vector\n\n# Example usage:\n# vector = np.array([1, 0, 0])\n# theta = math.radians(90)  # Convert degrees to radians\n# rotated_vector = rotate_about_x_axis(vector, theta)\n# print(rotated_vector)  # Output should be [1, 0, 0] after a 90 degree rotation about the x-axis",
  "canonical_solution": "    \n    return np.dot(np.array([\n[1, 0, 0],\n[0, np.cos(theta), -np.sin(theta)],\n[0, np.sin(theta), np.cos(theta)]        \n]), vector)\n\n\n",
  "unprocess_testcases": "vector = np.array([0, 1, 0])\ntheta = np.pi / 2\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_x_axis(vector, theta)).all())\n\nvector = np.array([1, 1, 1])\ntheta = 0\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_x_axis(vector, theta)).all())\n\nvector = np.array([1, 0, 0])\ntheta = -np.pi / 4\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_x_axis(vector, theta)).all())\n"
 },
 {
  "task_id": "RWPB/139",
  "prompt": "import torch\nimport numpy as np\n\ndef rotate_normalmap_by_angle_torch(normal_map, angle):\n    \"\"\"\n    Rotates the normals in a normal map along the y-axis by a specified angle using PyTorch.\n\n    Args:\n        normal_map (torch.Tensor): A tensor of shape (H, W, 3) representing the normal map,\n                                   where H is the height, W is the width, and each normal is a 3D vector with components in [-1, 1].\n                                   This tensor should be on a CUDA device for GPU acceleration.\n        angle (float): The rotation angle in degrees.\n\n    Returns:\n        torch.Tensor: The rotated normal map with the same shape as the input.\n    \"\"\"\n",
  "solution": "```python\n    # Convert the angle from degrees to radians\n    angle_rad = torch.tensor(angle * np.pi / 180, device=normal_map.device)\n\n    # Define the rotation matrix for rotation around the y-axis\n    cos_a = torch.cos(angle_rad)\n    sin_a = torch.sin(angle_rad)\n    rotation_matrix = torch.tensor([\n        [cos_a, 0, sin_a],\n        [0, 1, 0],\n        [-sin_a, 0, cos_a]\n    ], device=normal_map.device)\n\n    # Reshape the normal map to (H*W, 3) to multiply with the rotation matrix\n    original_shape = normal_map.shape\n    normal_map_flat = normal_map.view(-1, 3)\n\n    # Rotate the normals\n    rotated_normals = torch.matmul(normal_map_flat, rotation_matrix)\n\n    # Reshape back to the original shape\n    rotated_normal_map = rotated_normals.view(original_shape)\n\n    return rotated_normal_map\n```\n\nThis function first converts the input angle from degrees to radians. It then constructs a rotation matrix for rotation around the y-axis using the cosine and sine of the angle. The normal map is reshaped to a 2D tensor of shape (H*W, 3) to facilitate matrix multiplication with the rotation matrix. After applying the rotation, the tensor is reshaped back to its original shape and returned.",
  "canonical_solution": "    \n    # Convert the angle from degrees to radians and move it to the same device as the normal_map\n    angle = torch.tensor(angle / 180 * np.pi).to(normal_map.device)\n\n    # Define the rotation matrix for rotating around the y-axis\n    R = torch.tensor([\n        [torch.cos(angle), 0, torch.sin(angle)],  # X' = X*cos(theta) + Z*sin(theta)\n        [0, 1, 0],                                # Y' = Y (no change)\n        [-torch.sin(angle), 0, torch.cos(angle)]  # Z' = -X*sin(theta) + Z*cos(theta)\n    ]).to(normal_map.device)\n\n    # Apply the rotation matrix to each normal vector in the normal map\n    # Reshape the normal map from (H, W, 3) to (H*W, 3) for matrix multiplication, then reshape back to (H, W, 3)\n    return torch.matmul(normal_map.view(-1, 3), R.T).view(normal_map.shape)\n\n\n",
  "unprocess_testcases": "normal_map = torch.rand(10, 10, 3) * 2 - 1\nangle = 0\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle_torch(normal_map, angle)).all())\n\nnormal_map = torch.tensor([[[0, 0, 1]]], dtype=torch.float32).repeat(5, 5, 1)\nangle = 90\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle_torch(normal_map, angle)).all())\n\nnormal_map = torch.rand(50, 50, 3) * 2 - 1\nangle = 360\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle_torch(normal_map, angle)).all())\n"
 },
 {
  "task_id": "RWPB/97",
  "prompt": "import numpy as np\nimport torch\n\ndef numpy_image_to_torch(image: np.ndarray) -> torch.Tensor:\n    \"\"\"\n    Converts a NumPy image array to a normalized PyTorch tensor and reorders the dimensions.\n    If the img is invalid, please return ValueError\n    \n    Parameters:\n    image (np.ndarray): The input image in NumPy array format. \n                        - For color images, the shape should be (H, W, C) where H is height, W is width, and C is the number of channels (typically 3 for RGB).\n                        - For grayscale images, the shape should be (H, W).\n\n    Returns:\n    torch.Tensor: The converted image as a PyTorch tensor with normalized values in the range [0, 1].\n                  - For color images, the shape will be (C, H, W).\n                  - For grayscale images, the shape will be (1, H, W).\n\n    Process:\n    1. Check the number of dimensions of the input image:\n       - If the image is a 3D array (color image), transpose the dimensions from (H, W, C) to (C, H, W).\n       - If the image is a 2D array (grayscale image), add a new axis to create a channel dimension, resulting in a shape of (1, H, W).\n       - If the image does not match these conditions, raise a ValueError indicating it is not a valid image.\n    2. Normalize the pixel values by dividing by 255.0 to convert the range from [0, 255] to [0, 1].\n    3. Convert the resulting NumPy array to a PyTorch tensor with dtype=torch.float.\n    \"\"\"\n",
  "solution": "```python\nimport numpy as np\nimport torch\n\ndef numpy_image_to_torch(image: np.ndarray) -> torch.Tensor:\n    \"\"\"\n    Converts a NumPy image array to a normalized PyTorch tensor and reorders the dimensions.\n    If the img is invalid, please return ValueError\n    \n    Parameters:\n    image (np.ndarray): The input image in NumPy array format. \n                        - For color images, the shape should be (H, W, C) where H is height, W is width, and C is the number of channels (typically 3 for RGB).\n                        - For grayscale images, the shape should be (H, W).\n\n    Returns:\n    torch.Tensor: The converted image as a PyTorch tensor with normalized values in the range [0, 1].\n                  - For color images, the shape will be (C, H, W).\n                  - For grayscale images, the shape will be (1, H, W).\n\n    Process:\n    1. Check the number of dimensions of the input image:\n       - If the image is a 3D array (color image), transpose the dimensions from (H, W, C) to (C, H, W).\n       - If the image is a 2D array (grayscale image), add a new axis to create a channel dimension, resulting in a shape of (1, H, W).\n       - If the image does not match these conditions, raise a ValueError indicating it is not a valid image.\n    2. Normalize the pixel values by dividing by 255.0 to convert the range from [0, 255] to [0, 1].\n    3. Convert the resulting NumPy array to a PyTorch tensor with dtype=torch.float.\n    \"\"\"\n    if image.ndim == 3:\n        if image.shape[2] not in [1, 3, 4]:  # Typically 1 (grayscale), 3 (RGB), or 4 (RGBA)\n            raise ValueError(\"Invalid image format: Expected channel dimension to be 1, 3, or 4\")\n        # Transpose the dimensions from (H, W, C) to (C, H, W)\n        image = image.transpose(2, 0, 1)\n    elif image.ndim == 2:\n        # Add a new axis to create a channel dimension, (H, W) to (1, H, W)\n        image = image[np.newaxis, :, :]\n    else:\n        raise ValueError(\"Invalid image format: Image must be either 2D or 3D array\")\n\n    # Normalize the pixel values\n    image = image.astype(np.float32) / 255.0\n\n    # Convert the NumPy array to a PyTorch tensor\n    return torch.from_numpy(image)\n```",
  "canonical_solution": "    \n    if image.ndim == 3:\n        image = image.transpose((2, 0, 1))  # HxWxC to CxHxW\n    elif image.ndim == 2:\n        image = image[None]  # add channel axis\n    else:\n        return ValueError\n    return torch.tensor(image / 255.0, dtype=torch.float)\n\n\n",
  "unprocess_testcases": "\n\ncolor_image = np.random.randint(0, 256, size=(100, 200, 3), dtype=np.uint8)\nassert((SOLUTION_SIGNATURE(color_image) == numpy_image_to_torch(color_image)).all())\n\ngrayscale_image = np.random.randint(0, 256, size=(50, 50), dtype=np.uint8)\nassert((SOLUTION_SIGNATURE(grayscale_image) == numpy_image_to_torch(grayscale_image)).all())\n\ninvalid_image = np.random.randint(0, 256, size=(100,), dtype=np.uint8)\nassert(SOLUTION_SIGNATURE(invalid_image) == numpy_image_to_torch(invalid_image))\n"
 },
 {
  "task_id": "RWPB/8",
  "prompt": "import numpy as np\nimport torch\n\ndef cvimg2torch(img):\n    '''Convert a img to tensor\n    input:\n        im -> ndarray uint8 HxWxC \n    return\n        tensor -> torch.tensor BxCxHxW \n    '''\n",
  "solution": "Certainly! Below is the completion of the function `cvimg2torch` which converts an image from a NumPy array (commonly used with OpenCV) to a PyTorch tensor with the appropriate dimensions for deep learning models (batch, channels, height, width). The function assumes the input image is in HxWxC format (height, width, channels) and outputs a tensor in BxCxHxW format (batch, channels, height, width).\n\n```python\nimport numpy as np\nimport torch\n\ndef cvimg2torch(img):\n    '''Convert an image to a PyTorch tensor.\n    Input:\n        img: ndarray uint8 HxWxC \n    Return:\n        tensor: torch.tensor BxCxHxW \n    '''\n    # Check if the input image is a single image by checking its dimension\n    if img.ndim == 3:\n        # Add a new axis at the beginning to represent the batch size: 1 x H x W x C\n        img = np.expand_dims(img, axis=0)\n    \n    # Convert the numpy array (BxHxWxC) to a PyTorch tensor\n    tensor = torch.from_numpy(img)\n    \n    # Rearrange the dimensions from BxHxWxC to BxCxHxW\n    tensor = tensor.permute(0, 3, 1, 2).contiguous()\n    \n    # Convert the data type from uint8 to float, and normalize the pixel values\n    tensor = tensor.float() / 255.0\n    \n    return tensor\n```\n\n### Explanation:\n1. **Dimension Check and Expansion**: The function first checks if the input image `img` has three dimensions (indicating a single image). If so, it uses `np.expand_dims` to add a batch dimension at the start, making it a four-dimensional array.\n\n2. **Conversion to Tensor**: The NumPy array is then converted to a PyTorch tensor using `torch.from_numpy`.\n\n3. **Permute Dimensions**: The tensor is rearranged from BxHxWxC (batch, height, width, channels) to BxCxHxW using `permute`. This is the standard format for PyTorch tensors in many computer vision tasks.\n\n4. **Normalization**: Finally, the tensor's data type is converted to float, and pixel values are normalized to the range [0, 1] by dividing by 255. This step is common practice in deep learning as it helps in faster convergence during training.\n\nThis function now can be used to convert images loaded with OpenCV (or similar libraries that use NumPy arrays) into tensors suitable for input into PyTorch models.",
  "canonical_solution": "\n    img = img.astype(float) / 255.0\n    img = img.transpose(2, 0, 1) # NHWC -> NCHW\n    img = np.expand_dims(img, 0)\n    img = torch.from_numpy(img).float()\n    return img\n\n\n\n",
  "unprocess_testcases": "a = (np.random.rand(256, 256, 3) * 255).astype(np.uint8)\nb = np.repeat((np.random.rand(10, 10) * 255).astype(np.uint8)[:, :, np.newaxis], 3, axis=2)\nc = np.array([[[300, -10, 500], [256, 255, 0]]], dtype=np.int32)\n\nassert((SOLUTION_SIGNATURE(a) == cvimg2torch(a)).all())\nassert((SOLUTION_SIGNATURE(b) == cvimg2torch(b)).all())\nassert((SOLUTION_SIGNATURE(c) == cvimg2torch(c)).all())"
 },
 {
  "task_id": "RWPB/35",
  "prompt": "def flatten_dict(d: dict, parent_key: str = \"\", sep: str = \"/\") -> dict:\n    \"\"\"\n    Flattens a nested dictionary by collapsing its nested structure into a single layer dictionary with keys\n    representing paths, separated by a specified separator.\n\n    Args:\n        d (dict): The dictionary to flatten.\n        parent_key (str): The base path for keys in the nested dictionary. Initially left empty.\n        sep (str): The separator to use when concatenating nested keys.\n\n    Returns:\n        dict: A flattened version of the dictionary where all nested keys are transformed into single-layered keys.\n\n    Example:\n        >>> dct = {\"a\": {\"b\": 1, \"c\": {\"d\": 2}}, \"e\": 3}\n        >>> print(flatten_dict(dct))\n        {\"a/b\": 1, \"a/c/d\": 2, \"e\": 3}\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `flatten_dict` function which recursively flattens a nested dictionary:\n\n```python\ndef flatten_dict(d: dict, parent_key: str = \"\", sep: str = \"/\") -> dict:\n    items = {}\n    for k, v in d.items():\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            items.update(flatten_dict(v, new_key, sep))\n        else:\n            items[new_key] = v\n    return items\n```\n\nThis function works by iterating over each key-value pair in the dictionary. If the value is another dictionary (indicating a nested structure), it recursively calls itself with the updated `parent_key`. If the value is not a dictionary, it adds the value to the `items` dictionary with the constructed `new_key`. The `new_key` is formed by concatenating the `parent_key` with the current key, separated by the specified separator `sep`. If `parent_key` is empty (which it is initially), it simply uses the current key as the `new_key`. This ensures that the top-level keys remain unchanged.",
  "canonical_solution": "\n    items = []\n    for k, v in d.items():\n        # Generate the new key by concatenating the current key with the parent key using the separator\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            # Recursively flatten the dictionary and extend the list items\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            # Add the key-value pair to the items list\n            items.append((new_key, v))\n    return dict(items)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE({\"a\": {\"b\": {\"c\": {\"d\": 1}}}, \"e\": {\"f\": 2, \"g\": {\"h\": 3}}, \"i\": 4}) == flatten_dict({\"a\": {\"b\": {\"c\": {\"d\": 1}}}, \"e\": {\"f\": 2, \"g\": {\"h\": 3}}, \"i\": 4}))\nassert(SOLUTION_SIGNATURE({\"a\": {\"b\": 1, \"c\": {\"d\": 2}}, \"e\": 3}, sep='-') == flatten_dict({\"a\": {\"b\": 1, \"c\": {\"d\": 2}}, \"e\": 3}, sep='-'))\nassert(SOLUTION_SIGNATURE({\"a\": {}, \"b\": {\"c\": {}}, \"d\": 5}) == flatten_dict({\"a\": {}, \"b\": {\"c\": {}}, \"d\": 5}))\n"
 },
 {
  "task_id": "RWPB/64",
  "prompt": "import numpy as np\n\ndef moving_average(arr, window_size):\n    \"\"\"\n    Calculates the moving average of an array with a specified window size and returns both\n    the smoothed values and the residuals.\n\n    Arguments:\n    arr : numpy.ndarray\n        The input array over which the moving average is computed.\n    window_size : int\n        The number of elements in the moving average window.\n\n    Returns:\n    list of numpy.ndarray\n        The first element is an array of the moving averages, and the second element is an array of residuals.\n    \"\"\"\n",
  "solution": "```python\n    # Ensure the input is a numpy array\n    arr = np.asarray(arr)\n\n    # Check if the window size is valid\n    if window_size < 1:\n        raise ValueError(\"Window size must be at least 1\")\n    if window_size > len(arr):\n        raise ValueError(\"Window size cannot be greater than the length of the array\")\n\n    # Compute the moving averages using a convolution approach\n    weights = np.ones(window_size) / window_size\n    smoothed = np.convolve(arr, weights, mode='valid')\n\n    # Calculate residuals (difference between original and smoothed values)\n    # Residuals are only computed for the range where the moving average is defined\n    residuals = arr[window_size - 1:] - smoothed\n\n    return [smoothed, residuals]\n```",
  "canonical_solution": "\n    # Pad the input array with zeros at the beginning to handle the boundary effect\n    arr_padded = np.pad(arr, (window_size - 1, 0), \"constant\")\n\n    # Calculate the moving average using a convolution with a uniform kernel (i.e., np.ones(window_size))\n    smoothed_arr = np.convolve(arr_padded, np.ones(window_size), \"valid\") / window_size\n\n    # Calculate residuals by subtracting the smoothed array from the original array\n    residuals = arr - smoothed_arr\n\n    return [smoothed_arr, residuals]\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5, 6]), 3)[0] == moving_average(np.array([1, 2, 3, 4, 5, 6]), 3)[0]).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5, 6]), 3)[1] == moving_average(np.array([1, 2, 3, 4, 5, 6]), 3)[1]).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5, 6]), 6)[0] == moving_average(np.array([1, 2, 3, 4, 5, 6]), 6)[0]).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5, 6]), 6)[1] == moving_average(np.array([1, 2, 3, 4, 5, 6]), 6)[1]).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3]), 5)[0] == moving_average(np.array([1, 2, 3]), 5)[0]).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3]), 5)[1] == moving_average(np.array([1, 2, 3]), 5)[1]).all())\n"
 },
 {
  "task_id": "RWPB/50",
  "prompt": "import torch\n\ndef unpad_image(tensor: torch.Tensor, original_size: tuple) -> torch.Tensor:\n    \"\"\"\n    Removes padding from an image tensor to restore it to its original aspect ratio.\n    The function assumes the tensor is in the CxHxW format and has been padded symmetrically.\n\n    Args:\n        tensor (torch.Tensor): The image tensor, assumed to be in CxHxW format.\n        original_size (tuple): The original dimensions of the image as (height, width).\n\n    Returns:\n        torch.Tensor: The unpadded image tensor resized back to the original aspect ratio.\n\n    The function calculates the aspect ratio of the original image and the current tensor,\n    determining the necessary adjustments to either height or width. It then crops the tensor\n    to match the original aspect ratio, ensuring that the central part of the image is retained.\n    \"\"\"\n",
  "solution": "    # Extract the original dimensions\n    original_height, original_width = original_size\n\n    # Get the current dimensions of the tensor\n    _, current_height, current_width = tensor.size()\n\n    # Calculate the aspect ratio of the original and current image\n    original_aspect_ratio = original_height / original_width\n    current_aspect_ratio = current_height / current_width\n\n    # Determine if the padding was added to the height or the width\n    if current_aspect_ratio > original_aspect_ratio:\n        # More padding was added to the height, calculate the new height to match the original aspect ratio\n        new_height = int(original_aspect, ratio * current_width)\n        start_height = (current_height - new_height) // 2\n        end_height = start_height + new_height\n        # Crop the height\n        tensor = tensor[:, start_height:end_height, :]\n    else:\n        # More padding was added to the width, calculate the new width to match the original aspect ratio\n        new_width = int(current_height / original_aspect_ratio)\n        start_width = (current_width - new_width) // 2\n        end_width = start_width + new_width\n        # Crop the width\n        tensor = tensor[:, :, start_width:end_width]\n\n    return tensor\n\n# Example usage:\n# Assuming `padded_tensor` is a CxHxW tensor that has been symmetrically padded\n# and `original_size` is a tuple containing the original height and width of the image\n# unpadded_tensor = unpad_image(padded_tensor, original_size)",
  "canonical_solution": "\n    # Extract original and current dimensions\n    original_height, original_width = original_size\n    _, current_height, current_width = tensor.shape\n\n    # Calculate aspect ratios\n    original_aspect_ratio = original_width / original_height\n    current_aspect_ratio = current_width / current_height\n\n    # Determine how to adjust tensor to match original aspect ratio\n    if original_aspect_ratio > current_aspect_ratio:\n        # Original is wider than current: adjust height\n        scale_factor = current_width / original_width\n        new_height = int(original_height * scale_factor)\n        vertical_padding = (current_height - new_height) // 2\n        # Crop vertically\n        unpadded_tensor = tensor[:, vertical_padding:vertical_padding + new_height, :]\n    else:\n        # Original is taller than current: adjust width\n        scale_factor = current_height / original_height\n        new_width = int(original_width * scale_factor)\n        horizontal_padding = (current_width - new_width) // 2\n        # Crop horizontally\n        unpadded_tensor = tensor[:, :, horizontal_padding:horizontal_padding + new_width]\n\n    return unpadded_tensor\n\n\n",
  "unprocess_testcases": "a = torch.rand(3, 10, 20)\nb = torch.rand(3, 20, 10)\nc = torch.rand(3, 15, 30)\n\nassert((SOLUTION_SIGNATURE(a, (5, 10)) == unpad_image(a, (5, 10))).all())\nassert((SOLUTION_SIGNATURE(b, (10, 5)) == unpad_image(b, (10, 5))).all())\nassert((SOLUTION_SIGNATURE(c, (10, 20)) == unpad_image(c, (10, 20))).all())\n"
 },
 {
  "task_id": "RWPB/11",
  "prompt": "from typing import List\n\ndef pad_to_max_seq_length(ls: List[int], max_seq_length: int, pad_idx: int=0, pad_right: bool=True, check: bool=True):\n    \"\"\"Apply padding to an input sequence.\n\n    Args:\n        ls: List[int]: sequence to pad.\n        max_seq_length: int: max length up to which to apply padding.\n        pad_idx :int: element to use for padding.\n        pad_right: bool: True if padding is applied to right side of sequence, False to pad on left side.\n        check: bool: True if result length should be checked as under the max sequence length.\n        if result length is not equal the max sequence length, please return ValueError\n\n    Returns:\n        Sequence with specified padding applied.\n\n    \"\"\"\n",
  "solution": "```python\nfrom typing import List\n\ndef pad_to_max_seq_length(ls: List[int], max_seq_length: int, pad_idx: int = 0, pad_right: bool = True, check: bool = True) -> List[int]:\n    \"\"\"Apply padding to an input sequence.\n\n    Args:\n        ls (List[int]): Sequence to pad.\n        max_seq_length (int): Max length up to which to apply padding.\n        pad_idx (int): Element to use for padding.\n        pad_right (bool): True if padding is applied to the right side of sequence, False to pad on left side.\n        check (bool): True if result length should be checked as under the max sequence length.\n\n    Returns:\n        List[int]: Sequence with specified padding applied.\n\n    Raises:\n        ValueError: If the length of the result is not equal to max_seq_length when check is True.\n    \"\"\"\n    current_length = len(ls)\n    if current_length > max_seq_length:\n        raise ValueError(\"The current sequence length is greater than the maximum sequence length.\")\n\n    # Calculate the number of padding elements needed\n    num_padding = max_seq_length - current_length\n\n    # Create padding\n    padding = [pad_idx] * num_padding\n\n    # Apply padding\n    if pad_right:\n        padded_sequence = ls + padding\n    else:\n        padded_sequence = padding + ls\n\n    # Check the length of the result if required\n    if check and len(padded_sequence) != max_seq_length:\n        raise ValueError(\"Padded sequence length does not match the specified maximum sequence length.\")\n\n    return padded_sequence\n```",
  "canonical_solution": "\n    padding = [pad_idx] * (max_seq_length - len(ls))\n    if pad_right:\n        result = ls + padding\n    else:\n        result = padding + ls\n\n    if check:\n        if len(result) != max_seq_length:\n            return ValueError\n    return result\n\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE([1, 2, 3], 5) == pad_to_max_seq_length([1, 2, 3], 5))\nassert(SOLUTION_SIGNATURE([1, 2, 3], 5, pad_idx=-1) == pad_to_max_seq_length([1, 2, 3], 5, pad_idx=-1))\nassert(SOLUTION_SIGNATURE([1, 2, 3], 5, pad_right=False) == pad_to_max_seq_length([1, 2, 3], 5, pad_right=False))\nassert(SOLUTION_SIGNATURE([1, 2, 3], 3) == pad_to_max_seq_length([1, 2, 3], 3))\nassert(SOLUTION_SIGNATURE([1, 2, 3], 2) == pad_to_max_seq_length([1, 2, 3], 2))\n"
 },
 {
  "task_id": "RWPB/40",
  "prompt": "import inspect\nfrom typing import Callable, List\n\ndef extract_code_from_funct(funct: Callable) -> List[str]:\n    \"\"\"\n    Extracts the body of a Python function, removing the function definition and return statement, and adjusting indentation.\n\n    Args:\n        funct (Callable): The function from which code is extracted.\n\n    Returns:\n        List[str]: A list of strings, where each string is a line of code from the function's body, adjusted for indentation.\n\n    This function utilizes Python's `inspect` module to retrieve the source code of a given function. It then processes\n    this code to remove the function's signature and any return statement at the end, adjusting the indentation of the\n    remaining lines to align with the leftmost edge.\n\n    Example:\n        def example_function(x):\n            result = x + 1\n            return result\n\n        >>> extract_code_from_funct(example_function)\n        ['result = x + 1']\n    \"\"\"\n",
  "solution": "    # Get the source code of the function as a list of lines\n    source_lines = inspect.getsource(funct).splitlines()\n\n    # Find the start of the function body (after the signature)\n    for i, line in enumerate(source_lines):\n        if line.strip().startswith('def '):\n            start_index = i + 1\n            break\n\n    # Find the end of the function body (before the return statement, if any)\n    end_index = len(source_lines)\n    for i in range(start_index, len(source_lines)):\n        if source_lines[i].strip().startswith('return '):\n            end_index = i\n            break\n\n    # Extract the body of the function, adjusting indentation\n    body_lines = source_lines[start_index:end_index]\n    if body_lines:\n        # Determine the minimum indentation level (excluding empty lines)\n        min_indentation = min(len(line) - len(line.lstrip()) for line in body_lines if line.strip())\n        adjusted_lines = [line[min_indentation:] for line in body_lines if line.strip()]  # Remove the leading spaces\n    else:\n        adjusted_lines = []\n\n    return adjusted_lines\n\n# Example usage:\n# def example_function(x):\n#     result = x + 1\n#     return result\n#\n# print(extract_code_from_funct(example_function))  # Output: ['result = x + 1']",
  "canonical_solution": "\n    # Retrieve the source code of the function\n    source_code = inspect.getsource(funct)\n\n    # Split the source code into individual lines and remove the first line (function definition)\n    source_code_lines = source_code.splitlines()[1:]\n\n    # Calculate the number of characters of indentation in the first line of the function body\n    nident = len(source_code_lines[0]) - len(source_code_lines[0].lstrip())\n\n    # Adjust each line to remove the initial indentation and exclude the last line (usually a return statement)\n    return [line[nident:] for line in source_code_lines[:-1]]\n\n\n",
  "unprocess_testcases": "def sample_function(x):\n    y = x + 10\n    z = y * 2\n    return z\n\nassert(SOLUTION_SIGNATURE(sample_function) == extract_code_from_funct(sample_function))\n\ndef empty_function():\n    pass\nassert(SOLUTION_SIGNATURE(empty_function) == extract_code_from_funct(empty_function))\n\ndef complex_function(x):\n    if x > 0:\n        for i in range(x):\n            print(i)\n    else:\n        while x < 0:\n            x += 1\n            print(x)\n    return x\nassert(SOLUTION_SIGNATURE(complex_function) == extract_code_from_funct(complex_function))\n"
 },
 {
  "task_id": "RWPB/74",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\ndef arccos2(vector, value):\n    \"\"\"\n    Calculate the inverse cosine of a value with consideration of the vector direction.\n    Args:\n        - vector: numpy array of shape (2,), the 2D vector to consider for direction\n        - value: float, the value for which to calculate the inverse cosine\n    Returns:\n        angle: float, the inverse cosine with consideration of vector direction\n    \"\"\"\n",
  "solution": "```python\n    # Ensure the value is within the valid range for arccos\n    if value < -1 or value > 1:\n        raise ValueError(\"Value must be within the range [-1, 1] for arccos.\")\n\n    # Calculate the angle using arccos\n    angle = math.acos(value)\n\n    # Determine the direction based on the vector\n    if vector[1] < 0:\n        angle = 2 * math.pi - angle\n\n    return angle\n```",
  "canonical_solution": "    \n    if vector[0] == 0:\n        if vector[1] >= 0:\n            return 0\n        else:\n            return np.pi\n    return -np.sign(vector[0]) * np.arccos(value)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(np.array([-1, 0]), value = 0.5) == arccos2(np.array([-1, 0]), value = 0.5))\nassert(SOLUTION_SIGNATURE(np.array([1, 0]), value = 0.5) == arccos2(np.array([1, 0]), value = 0.5))\nassert(SOLUTION_SIGNATURE(np.array([0, 1]), value = -1) == arccos2(np.array([0, 1]), value = -1))\nassert(SOLUTION_SIGNATURE(np.array([0, -1]), value = -1) == arccos2(np.array([0, -1]), value = -1))\n"
 },
 {
  "task_id": "RWPB/25",
  "prompt": "import numpy as np\nimport cv2\n\ndef hole_fill(img):\n    \"\"\"\n    Performs hole filling on an image using morphological operations.\n\n    Args:\n        img (np.array): A grayscale image array where holes are to be filled.\n\n    Returns:\n        np.array: An image array with holes filled.\n\n    This function applies the flood fill algorithm to identify and fill holes in the input image.\n    The process involves padding the image, copying it for result calculation, and applying\n    bitwise operations to fill the holes based on the flood filled mask.\n    \"\"\"\n",
  "solution": "```python\n    # Ensure the input image is binary\n    _, img_bin = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n\n    # Create a mask for floodFill function, which is larger than the original image by 2 pixels in each direction\n    h, w = img_bin.shape\n    mask = np.zeros((h + 2, w + 2), np.uint8)\n\n    # Use floodFill to change the background to 1\n    # The new value will be applied to the areas which are connected to the seed point (0,0)\n    cv2.floodFill(img_bin, mask, (0, 0), 255)\n\n    # Invert the image to make holes as white\n    img_inv = cv2.bitwise_not(img_bin)\n\n    # Combine the inverted image with the original image using bitwise OR\n    # This step fills the holes with white\n    img_filled = cv2.bitwise_or(img, img_inv)\n\n    return img_filled\n```",
  "canonical_solution": "\n    # Pad the image to avoid boundary issues during flood fill\n    img = np.pad(img[1:-1, 1:-1], pad_width=1, mode='constant', constant_values=0)\n    img_copy = img.copy()  # Create a copy of the image for later use\n\n    # Create a mask for flood fill operation, size needs to be 2 pixels bigger than the image\n    mask = np.zeros((img.shape[0] + 2, img.shape[1] + 2), dtype=np.uint8)\n\n    # Perform flood fill from the top-left corner (0,0) with the new value 255\n    cv2.floodFill(img, mask, (0, 0), 255)\n\n    # Create inverse of the flood filled image\n    img_inverse = cv2.bitwise_not(img)\n\n    # Combine the original copy and the inverse to fill the holes\n    dst = cv2.bitwise_or(img_copy, img_inverse)\n\n    return dst\n\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(np.zeros((10, 10), dtype=np.uint8)) == hole_fill(np.zeros((10, 10), dtype=np.uint8)))\n\nimg_large_hole = np.ones((10, 10), dtype=np.uint8) * 255\nimg_large_hole[3:7, 3:7] = 0\nassert(SOLUTION_SIGNATURE(img_large_hole) == hole_fill(img_large_hole))\n\n# Create an image with both small holes on the edges and small holes scattered in the middle of the image\nimg_multiple_holes = np.ones((10, 10), dtype=np.uint8) * 255\nimg_multiple_holes[1, 1] = 0 # small holes in corners\nimg_multiple_holes[1, 8] = 0 # small holes in the other corner\nimg_multiple_holes[5, 5] = 0 # small holes in the centre\nimg_multiple_holes[8, 1] = 0 # edge holes\nimg_multiple_holes[8, 8] = 0 # holes at the edges\n\nassert(SOLUTION_SIGNATURE(img_multiple_holes) == hole_fill(img_multiple_holes))\n"
 },
 {
  "task_id": "RWPB/87",
  "prompt": "import torch\nimport numpy as np\n\n\ndef rotate_x(a, device=None):\n    \"\"\"\n    Creates a 4x4 homogeneous transformation matrix for rotation around the x-axis in 3D space.\n\n    Parameters:\n    a (float): Rotation angle in radians.\n    device (torch.device, optional): The device on which to create the tensor (e.g., 'cpu' or 'cuda'). Default is None, which means the tensor will be created on the default device.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the rotation transformation matrix around the x-axis.\n\n    Process:\n    1. Calculate the sine and cosine of the rotation angle.\n    2. Construct a 4x4 matrix where the rotation is applied to the y and z coordinates, leaving the x coordinate unchanged.\n    \"\"\"\n",
  "solution": "    # Calculate the sine and cosine of the rotation angle\n    cos_a = torch.cos(a)\n    sin_a = torch.sin(a)\n\n    # Create the rotation matrix as a 4x4 tensor\n    # The matrix is:\n    # [[1,     0,      0, 0],\n    #  [0, cos_a, -sin_a, 0],\n    #  [0, sin_a,  cos_a, 0],\n    #  [0,     0,      0, 1]]\n    rotation_matrix = torch.tensor([\n        [1.0, 0.0, 0.0, 0.0],\n        [0.0, cos_a, -sin_a, 0.0],\n        [0.0, sin_a, cos_a, 0.0],\n        [0.0, 0.0, 0.0, 1.0]\n    ], dtype=torch.float32, device=device)\n\n    return rotation_matrix\n\n# Example usage:\n# angle_in_radians = np.pi / 4  # 45 degrees\n# rotation_matrix = rotate_x(angle_in_radians)\n# print(rotation_matrix)",
  "canonical_solution": "    \n    s, c = np.sin(a), np.cos(a)\n    return torch.tensor([[1,  0, 0, 0], \n                         [0,  c, s, 0], \n                         [0, -s, c, 0], \n                         [0,  0, 0, 1]], dtype=torch.float32, device=device)\n\n\n",
  "unprocess_testcases": "a = torch.pi / 4\nassert((SOLUTION_SIGNATURE(a) == rotate_x(a)).all())\n\na = 0\nassert((SOLUTION_SIGNATURE(a) == rotate_x(a)).all())\n\na = 2 * torch.pi\nassert((SOLUTION_SIGNATURE(a) == rotate_x(a)).all())\n"
 },
 {
  "task_id": "RWPB/129",
  "prompt": "import torch\nimport torch.nn.functional as F\n\n\ndef scale_masks(masks, shape, padding=True):\n    \"\"\"\n    Rescale segment masks to shape.\n\n    Args:\n        masks (torch.Tensor): (N, C, H, W).\n        shape (tuple): Height and width.\n        padding (bool): If True, assuming the boxes is based on image augmented by yolo style. If False then do regular\n            rescaling.\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `scale_masks` function which rescales the given masks to a specified shape. The function handles two cases based on the `padding` parameter: if `padding` is `True`, it assumes that the masks need to be adjusted for padding as used in YOLO-style augmentation; if `padding` is `False`, it performs a straightforward rescaling of the masks.\n\n```python\nimport torch\nimport torch.nn.functional as F\n\ndef scale_masks(masks, shape, padding=True):\n    \"\"\"\n    Rescale segment masks to shape.\n\n    Args:\n        masks (torch.Tensor): (N, C, H, W).\n        shape (tuple): Height and width.\n        padding (bool): If True, assuming the boxes is based on image augmented by yolo style. If False then do regular\n            rescaling.\n    \"\"\"\n    N, C, H, W = masks.size()\n    new_height, new_width = shape\n\n    if padding:\n        # Calculate scaling factor for each dimension\n        scale_x = new_width / W\n        scale_y = new_height / H\n        scale = min(scale_x, scale_y)\n\n        # Calculate padding\n        new_scaled_width = int(W * scale)\n        new_scaled_height = int(H * scale)\n        pad_width = (new_width - new_scaled_width) // 2\n        pad_height = (new_height - new_scaled_height) // 2\n\n        # Resize masks\n        resized_masks = F.interpolate(masks, size=(new_scaled_height, new_scaled_width), mode='nearest')\n\n        # Pad masks to the target size\n        padded_masks = F.pad(resized_masks, pad=(pad_width, new_width - new_scaled_width - pad_width,\n                                                 pad_height, new_height - new_scaled_height - pad_height))\n        return padded_masks\n    else:\n        # Directly resize masks to the target size\n        resized_masks = F.interpolate(masks, size=(new_height, new_width), mode='nearest')\n        return resized_masks\n```\n\nThis function first checks the `padding` flag. If `padding` is `True`, it calculates the scaling factor for each dimension to maintain the aspect ratio, scales the masks, and then adds padding to center the scaled masks within the new dimensions. If `padding` is `False`, it directly resizes the masks to the new dimensions using nearest neighbor interpolation, which is typically suitable for mask data to avoid introducing non-existent classes.",
  "canonical_solution": "    \n    mh, mw = masks.shape[2:]\n    gain = min(mh / shape[0], mw / shape[1])  # gain  = old / new\n    pad = [mw - shape[1] * gain, mh - shape[0] * gain]  # wh padding\n    if padding:\n        pad[0] /= 2\n        pad[1] /= 2\n    top, left = (int(pad[1]), int(pad[0])) if padding else (0, 0)  # y, x\n    bottom, right = (int(mh - pad[1]), int(mw - pad[0]))\n    masks = masks[..., top:bottom, left:right]\n\n    masks = F.interpolate(masks, shape, mode=\"bilinear\", align_corners=False)  # NCHW\n    return masks\n\n\n",
  "unprocess_testcases": "masks = torch.rand(2, 3, 100, 200)\ntarget_shape = (50, 100)\nassert((SOLUTION_SIGNATURE(masks, target_shape) == scale_masks(masks, target_shape)).all())\n\nmasks = torch.rand(1, 1, 150, 300)\ntarget_shape = (100, 100)\nassert((SOLUTION_SIGNATURE(masks, target_shape) == scale_masks(masks, target_shape)).all())\n\nmasks = torch.rand(3, 3, 200, 200)\ntarget_shape = (210, 210)\nassert((SOLUTION_SIGNATURE(masks, target_shape) == scale_masks(masks, target_shape)).all())\n"
 },
 {
  "task_id": "RWPB/138",
  "prompt": "import numpy as np\n\ndef rotate_normalmap_by_angle(normal_map: np.ndarray, angle: float):\n    \"\"\"\n    Rotates the normals in a normal map along the y-axis by a specified angle.\n\n    Args:\n        normal_map (np.ndarray): An array of shape (H, W, 3) representing the normal map,\n                                 where H is the height, W is the width, and each normal is a 3D vector with components in [-1, 1].\n        angle (float): The rotation angle in degrees.\n\n    Returns:\n        np.ndarray: The rotated normal map with the same shape as the input.\n    \"\"\"\n",
  "solution": "To complete the function `rotate_normalmap_by_angle`, we need to apply a rotation transformation to each normal vector in the normal map. Since the rotation is specified to be around the y-axis, we can use the corresponding rotation matrix for y-axis rotation. The rotation matrix \\( R_y(\\theta) \\) for an angle \\( \\theta \\) in radians is given by:\n\n\\[\nR_y(\\theta) = \\begin{bmatrix}\n\\cos(\\theta) & 0 & \\sin(\\theta) \\\\\n0 & 1 & 0 \\\\\n-\\sin(\\theta) & 0 & \\cos(\\theta)\n\\end{bmatrix}\n\\]\n\nHere's how you can implement this:\n\n```python\nimport numpy as np\n\ndef rotate_normalmap_by_angle(normal_map: np.ndarray, angle: float) -> np.ndarray:\n    \"\"\"\n    Rotates the normals in a normal map along the y-axis by a specified angle.\n\n    Args:\n        normal_map (np.ndarray): An array of shape (H, W, 3) representing the normal map,\n                                 where H is the height, W is the width, and each normal is a 3D vector with components in [-1, 1].\n        angle (float): The rotation angle in degrees.\n\n    Returns:\n        np.ndarray: The rotated normal map with the same shape as the input.\n    \"\"\"\n    # Convert angle from degrees to radians\n    angle_rad = np.radians(angle)\n\n    # Define the rotation matrix for rotation around the y-axis\n    cos_angle = np.cos(angle_rad)\n    sin_angle = np.sin(angle_rad)\n    rotation_matrix = np.array([\n        [cos_angle, 0, sin_angle],\n        [0, 1, 0],\n        [-sin_angle, 0, cos_angle]\n    ])\n\n    # Apply the rotation matrix to each normal vector\n    # Reshape the normal map to (H*W, 3) to multiply with the rotation matrix\n    H, W, _ = normal_map.shape\n    normal_map_flat = normal_map.reshape(-1, 3)\n    rotated_normals_flat = normal_map_flat.dot(rotation_matrix.T)\n\n    # Reshape back to the original shape (H, W, 3)\n    rotated_normal_map = rotated_normals_flat.reshape(H, W, 3)\n\n    return rotated_normal_map\n```\n\nThis function first converts the angle from degrees to radians. It then constructs the rotation matrix for a rotation around the y-axis. The normal map is reshaped into a two-dimensional array where each row represents a normal vector. This array is then multiplied by the transpose of the rotation matrix to apply the rotation. Finally, the array is reshaped back to its original shape.",
  "canonical_solution": "    \n    # Convert the angle from degrees to radians\n    angle = angle / 180 * np.pi\n    # Define the rotation matrix for rotating around the y-axis\n    R = np.array([\n        [np.cos(angle), 0, np.sin(angle)],  # X' = X*cos(theta) + Z*sin(theta)\n        [0, 1, 0],                          # Y' = Y (no change)\n        [-np.sin(angle), 0, np.cos(angle)]  # Z' = -X*sin(theta) + Z*cos(theta)\n    ])\n    # Apply the rotation matrix to each normal vector in the normal map\n    # Reshape the normal map from (H, W, 3) to (H*W, 3) for matrix multiplication, then reshape back to (H, W, 3)\n    return np.dot(normal_map.reshape(-1, 3), R.T).reshape(normal_map.shape)\n\n\n",
  "unprocess_testcases": "normal_map = np.array([[[0, 0, 1]]])\nangle = 90\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle(normal_map, angle)).all())\n\nnormal_map = np.array([[[1, 0, 0], [0, 1, 0], [0, 0, 1]]])\nangle = 0\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle(normal_map, angle)).all())\n\nnormal_map = np.array([[[1, 0, 0], [0, 1, 0], [0, 0, 1]]])\nangle = 720  # Two full rotations\nassert((SOLUTION_SIGNATURE(normal_map, angle) == rotate_normalmap_by_angle(normal_map, angle)).all())\n"
 },
 {
  "task_id": "RWPB/96",
  "prompt": "import numpy as np\n\ndef movmean(arr, n_avg):\n  \"\"\"\n  Compute the moving average of an array.\n  Args:\n      - arr: numpy.ndarray, the input array\n      - n_avg: int, the number of elements to include in the moving average\n  Returns:\n      mov_avg: numpy.ndarray, the moving average of the input array\n\n  If the windows size is bigger than the length of array, the code should return a empty array.\n  \"\"\"\n",
  "solution": "```python\nimport numpy as np\n\ndef movmean(arr, n_avg):\n    \"\"\"\n    Compute the moving average of an array.\n    Args:\n        - arr: numpy.ndarray, the input array\n        - n_avg: int, the number of elements to include in the moving average\n    Returns:\n        mov_avg: numpy.ndarray, the moving average of the input array\n\n    If the windows size is bigger than the length of array, the code should return an empty array.\n    \"\"\"\n    # Check if the window size is larger than the array length\n    if n_avg > len(arr):\n        return np.array([])  # Return an empty array if window size is too large\n\n    # Initialize an empty list to store the moving averages\n    mov_avg = []\n\n    # Calculate the moving average using a sliding window approach\n    for i in range(len(arr) - n_avg + 1):\n        # Calculate the average of the current window\n        window_avg = np.mean(arr[i:i + n_avg])\n        # Append the average to the list of moving averages\n        mov_avg.append(window_avg)\n\n    # Convert the list of moving averages to a numpy array\n    return np.array(mov_avg)\n```",
  "canonical_solution": "  \n  sums = np.cumsum(arr, dtype=float)\n  sums[n_avg:] -= sums[:-n_avg]\n  return sums[n_avg - 1 :] / n_avg\n\n\n",
  "unprocess_testcases": "arr = [1, 2, 3, 4, 5]\nn_avg = 3\nassert((SOLUTION_SIGNATURE(arr, n_avg) == movmean(arr, n_avg)).all())\n\narr = [1, 2, 3, 4, 5]\nn_avg = 5\nassert((SOLUTION_SIGNATURE(arr, n_avg) == movmean(arr, n_avg)).all())\n\narr = [1, 2, 3]\nn_avg = 4\nassert((SOLUTION_SIGNATURE(arr, n_avg) == movmean(arr, n_avg)).all())\n"
 },
 {
  "task_id": "RWPB/9",
  "prompt": "def is_byte_token(s: str) -> bool:\n  \"\"\"Returns True if s is a byte string like \"<0xAB>\".\n    input:\n        s: string\n    return\n        bool: Ture or False\n  \"\"\"\n  # Bytes look like \"<0xAB>\".\n",
  "solution": "Certainly! To determine if a string `s` is a byte token formatted like \"<0xAB>\", we need to check several conditions:\n\n1. The string should start with '<' and end with '>'.\n2. The string should contain a hexadecimal number prefixed with \"0x\".\n3. The hexadecimal number should consist of exactly two hexadecimal digits (A-F, a-f, 0-9).\n\nHere's the complete function to check these conditions:\n\n```python\ndef is_byte_token(s: str) -> bool:\n    \"\"\"Returns True if s is a byte string like \"<0xAB>\".\n    input:\n        s: string\n    return:\n        bool: True or False\n    \"\"\"\n    # Check if the string starts with '<' and ends with '>'\n    if not (s.startswith('<') and s.endswith('>')):\n        return False\n    \n    # Remove the '<' and '>' to examine the inner content\n    inner_content = s[1:-1]\n    \n    # Check if the inner content starts with '0x'\n    if not inner_content.startswith('0x'):\n        return False\n    \n    # Check if the rest of the string after '0x' is exactly two hexadecimal digits\n    hex_digits = inner_content[2:]\n    if len(hex_digits) != 2:\n        return False\n    \n    # Check if all characters in hex_digits are valid hexadecimal characters\n    valid_hex_chars = set('0123456789abcdefABCDEF')\n    if all(char in valid_hex_chars for char in hex_digits):\n        return True\n    \n    return False\n```\n\nThis function first checks the format of the string to ensure it starts and ends with the correct characters. It then verifies the presence of the '0x' prefix and checks that exactly two hexadecimal characters follow. If all these conditions are met, the function returns `True`; otherwise, it returns `False`.",
  "canonical_solution": "  \n  if len(s) != 6 or s[0:3] != \"<0x\" or s[-1] != \">\":\n    return False\n  return True\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(\"<0xAB>\") == is_byte_token(\"<0xAB>\"))\nassert(SOLUTION_SIGNATURE(\"<0xG1>\") == is_byte_token(\"<0xG1>\"))\nassert(SOLUTION_SIGNATURE(\"0xAB>\") == is_byte_token(\"0xAB>\"))\nassert(SOLUTION_SIGNATURE(\"<0xAB\") == is_byte_token(\"<0xAB\"))\n"
 },
 {
  "task_id": "RWPB/34",
  "prompt": "import numpy as np\n\ndef pad_tokens(tokens: np.ndarray, context_window: int, pad_token: int) -> np.ndarray:\n    \"\"\"\n    Pads or truncates a sequence of tokens to a specified length determined by the context window.\n\n    Args:\n        tokens (np.ndarray): An array of tokenized data, possibly with multiple sequences.\n                             The shape should typically be (batch_size, sequence_length) or (sequence_length,).\n        context_window (int): The target number of tokens in a sequence after padding or truncation.\n                              The final sequence length will be `context_window + 1`.\n        pad_token (int): The token used to pad shorter sequences.\n\n    Returns:\n        np.ndarray: An array where each sequence is exactly `context_window + 1` tokens long.\n\n    This function modifies the length of sequences in a batch of tokenized data:\n    - If a sequence is longer than `context_window + 1`, it gets truncated to this length.\n    - If a sequence is shorter, it gets padded with `pad_token` until it reaches the length of `context_window + 1`.\n    \"\"\"\n",
  "solution": "    # Calculate the target length for the sequences\n    target_length = context_window + 1\n\n    # Check if the input tokens array is 1D or 2D\n    if tokens.ndim == 1:\n        # Single sequence case\n        if len(tokens) > target_length:\n            # Truncate the sequence if it's longer than the target length\n            tokens = tokens[:target_length]\n        elif len(tokens) < target_length:\n            # Pad the sequence if it's shorter than the target length\n            padding = np.full(target_length - len(tokens), pad_token, dtype=tokens.dtype)\n            tokens = np.concatenate([tokens, padding])\n    elif tokens.ndim == 2:\n        # Batch of sequences case\n        padded_tokens = np.empty((tokens.shape[0], target_length), dtype=tokens.dtype)\n        for i, seq in enumerate(tokens):\n            if len(seq) > target_length:\n                # Truncate the sequence\n                padded_tokens[i] = seq[:target_length]\n            elif len(seq) < target_length:\n                # Pad the sequence\n                padding = np.full(target_length - len(seq), pad_token, dtype=seq.dtype)\n                padded_tokens[i] = np.concatenate([seq, padding])\n            else:\n                # If the sequence is already the target length, just copy it\n                padded_tokens[i] = seq\n        tokens = padded_tokens\n    else:\n        raise ValueError(\"Unsupported number of dimensions for tokens array. Expected 1D or 2D array.\")\n\n    return tokens\n\n# Example usage:\n# tokens = np.array([[1, 2, 3], [4, 5]])\n# padded_tokens = pad_tokens(tokens, context_window=4, pad_token=0)\n# print(padded_tokens)",
  "canonical_solution": "\n    # Determine the target length for each sequence\n    target_length = context_window + 1\n    current_length = tokens.shape[-1]\n\n    # Truncate if the current length is greater than the target length\n    if current_length > target_length:\n        tokens = tokens[..., :target_length]\n    # Pad if the current length is less than the target length\n    elif current_length < target_length:\n        # Calculate the required number of padding elements\n        padding_size = target_length - current_length\n        # Create a padding array filled with the pad_token\n        padding = np.full(tokens.shape[:-1] + (padding_size,), pad_token)\n        # Concatenate the original tokens with the padding array\n        tokens = np.concatenate([tokens, padding], axis=-1)\n\n    # Ensure the final tokens array is of the correct shape\n    assert tokens.shape[-1] == target_length, \"Final token sequence length does not match the target length.\"\n\n    return tokens\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5, 6, 7]), 4, 0) == pad_tokens(np.array([1, 2, 3, 4, 5, 6, 7]), 4, 0)).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3]), 5, 0) == pad_tokens(np.array([1, 2, 3]), 5, 0)).all())\nassert((SOLUTION_SIGNATURE(np.array([1, 2, 3, 4, 5]), 4, 0) == pad_tokens(np.array([1, 2, 3, 4, 5]), 4, 0)).all())\n"
 },
 {
  "task_id": "RWPB/65",
  "prompt": "import itertools as it\nimport numpy as np\n\n\ndef merge_dicts_recursively(*dicts):\n    \"\"\"\n    Create a dictionary whose keys are the union of all input dictionaries. \n    The value for each key is based on the first dictionary in the list with that key.\n    Dictionaries later in the list have higher priority.\n    When values are dictionaries, it is applied recursively.\n    Args:\n        - *dicts: tuple of dicts, the dictionaries to merge\n    Returns:\n        result: dict, the merged dictionary\n    \"\"\"\n",
  "solution": "```python\ndef merge_dicts_recursively(*dicts):\n    # Initialize the result dictionary\n    result = {}\n\n    # Iterate over each dictionary in reverse order to prioritize later dictionaries\n    for d in reversed(dicts):\n        for key, value in d.items():\n            # If the key is already in the result and both values are dictionaries, merge them recursively\n            if key in result and isinstance(value, dict) and isinstance(result[key], dict):\n                result[key] = merge_dicts_recursively(result[key], value)\n            else:\n                # Otherwise, set/overwrite the value in the result dictionary\n                result[key] = value\n\n    return result\n```\n\nThis function works by iterating over each dictionary in the input from last to first. For each key-value pair, it checks if the key already exists in the result dictionary and if both the existing value and the new value are dictionaries. If they are, it merges them recursively. If not, it simply sets or overwrites the value in the result dictionary. This ensures that values from dictionaries later in the input list take precedence, and that merging of nested dictionaries is handled correctly.",
  "canonical_solution": "    \n    result = dict()\n    all_items = it.chain(*[d.items() for d in dicts])\n    for key, value in all_items:\n        if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n            result[key] = merge_dicts_recursively(result[key], value)\n        else:\n            result[key] = value\n    return result\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE({'a': 1, 'b': 2}, {'b': 3, 'c': 4}) == merge_dicts_recursively({'a': 1, 'b': 2}, {'b': 3, 'c': 4}))\nassert(SOLUTION_SIGNATURE({'a': 1, 'nested': {'x': 10, 'y': 20}}, {'b': 2, 'nested': {'y': 30, 'z': 40}}) == merge_dicts_recursively({'a': 1, 'nested': {'x': 10, 'y': 20}}, {'b': 2, 'nested': {'y': 30, 'z': 40}}))\nassert(SOLUTION_SIGNATURE({'a': [1, 2, 3], 'b': 'hello'}, {'a': 'not a list', 'b': {'nested': 'dict'}}) == merge_dicts_recursively({'a': [1, 2, 3], 'b': 'hello'}, {'a': 'not a list', 'b': {'nested': 'dict'}}))\n"
 },
 {
  "task_id": "RWPB/51",
  "prompt": "import torch\nimport torch.nn.functional as F\n\n\ndef _ssim(img1, img2, window, window_size, channel, size_average=True):\n    \"\"\"\n    Calculate the Structural Similarity Index (SSIM) between two images.\n\n    Arguments:\n    img1, img2 : torch.Tensor\n        The two images between which SSIM is to be computed. They should have the same dimensions.\n    window : torch.Tensor\n        The window tensor used for mean and variance calculation, typically a Gaussian window.\n    window_size : int\n        The size of the Gaussian window, used here for padding calculations.\n    channel : int\n        The number of channels in img1 and img2 (e.g., 3 for RGB, 1 for grayscale).\n    size_average : bool, optional\n        If True (default), returns the mean SSIM over all elements. If False, returns a tensor of SSIM values for each channel.\n\n    Returns:\n    torch.Tensor\n        SSIM index as a float or as a tensor based on 'size_average' parameter.\n    \"\"\"\n",
  "solution": "    mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)\n    mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)\n\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1 * mu2\n\n    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size//2, groups=channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size//2, groups=channel) - mu2_sq\n    sigma12 = F.conv2d(img1 * img2, window, padding=window_size//2, groups=channel) - mu1_mu2\n\n    C1 = 0.01 ** 2\n    C2 = 0.03 ** 2\n\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n\n    if size_average:\n        return ssim_map.mean()\n    else:\n        return ssim_map\n\n# Example usage:\n# Assuming img1 and img2 are torch tensors of the same shape [batch_size, channel, height, width]\n# and window is a Gaussian window tensor of shape [1, 1, window_size, window_size] repeated 'channel' times\n# window = torch.tensor(...)  # Define or load your Gaussian window here\n# ssim_index = _ssim(img1, img2, window, window_size, channel)",
  "canonical_solution": "\n    # Apply convolution to calculate the mean of img1 and img2 using the specified window\n    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n\n    # Compute the squares of means\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    # Compute the mean of products\n    mu1_mu2 = mu1 * mu2\n\n    # Calculate variance of img1 and img2, and the covariance between them\n    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n\n    # Constants for stability (avoid division by zero)\n    C1 = 0.01 ** 2\n    C2 = 0.03 ** 2\n\n    # Compute the SSIM index\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n\n    # Return the average SSIM over all pixels if size_average is True, else return SSIM map\n    if size_average:\n        return ssim_map.mean()  # average over all channels and pixels\n    else:\n        return ssim_map.mean(1).mean(1).mean(1)  # average over each channel separately\n\n\n",
  "unprocess_testcases": "def gaussian_window(channel, window_size):\n    \"\"\" Creates a Gaussian window tensor for given channel and window size. \"\"\"\n    # Ensure the operation is done with tensors\n    _1D_window = torch.exp(-torch.arange(window_size).float()**2 / (2 * window_size**2))\n    _1D_window = _1D_window / _1D_window.sum()  # Normalize the window\n    _2D_window = _1D_window[:, None] * _1D_window[None, :]  # Create 2D Gaussian window\n    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n    return window\n\n\nimg = torch.rand(1, 3, 256, 256)\nassert((SOLUTION_SIGNATURE(img, img.clone(), gaussian_window(3, 11), 11, 3) == _ssim(img, img.clone(), gaussian_window(3, 11), 11, 3)).all())\n\nimg1 = torch.zeros(1, 1, 512, 512)  # Black image\nimg2 = torch.ones(1, 1, 512, 512)   # White image\nwindow = gaussian_window(1, 25)\nassert((SOLUTION_SIGNATURE(img1, img2, window, 25, 1, False) == _ssim(img1, img2, window, 25, 1, False)).all())\n\nimg1 = torch.rand(1, 1, 300, 300)\nimg2 = torch.rand(1, 1, 300, 300)\nwindow = gaussian_window(1, 25)\nassert((SOLUTION_SIGNATURE(img1, img2, window, 11, 1) == _ssim(img1, img2, window, 11, 1)).all())\n"
 },
 {
  "task_id": "RWPB/10",
  "prompt": "import torch\n\ndef euler2rot(euler_angle):\n    \"\"\"\n    Converts Euler angles to a rotation matrix.\n\n    Args:\n    euler_angle (torch.Tensor): Tensor of shape [batch_size, 3] containing Euler angles.\n\n    Returns:\n    torch.Tensor: Rotation matrix of shape [batch_size, 3, 3].\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `euler2rot` function that converts Euler angles to a rotation matrix using PyTorch. The Euler angles are assumed to be in the order of rotation around the z-axis, y-axis, and x-axis (yaw, pitch, and roll).\n\n```python\nimport torch\n\ndef euler2rot(euler_angle):\n    \"\"\"\n    Converts Euler angles to a rotation matrix.\n\n    Args:\n    euler_angle (torch.Tensor): Tensor of shape [batch_size, 3] containing Euler angles.\n\n    Returns:\n    torch.Tensor: Rotation matrix of shape [batch_size, 3, 3].\n    \"\"\"\n    # Extract individual angles\n    yaw = euler_angle[:, 0]\n    pitch = euler_angle[:, 1]\n    roll = euler_angle[:, 2]\n\n    # Precompute cosines and sines of the angles\n    cy = torch.cos(yaw)\n    sy = torch.sin(yaw)\n    cp = torch.cos(pitch)\n    sp = torch.sin(pitch)\n    cr = torch.cos(roll)\n    sr = torch.sin(roll)\n\n    # Compute elements of the rotation matrix\n    # Row 1\n    r00 = cy * cp\n    r01 = cy * sp * sr - sy * cr\n    r02 = cy * sp * cr + sy * sr\n\n    # Row 2\n    r10 = sy * cp\n    r11 = sy * sp * sr + cy * cr\n    r12 = sy * sp * cr - cy * sr\n\n    # Row 3\n    r20 = -sp\n    r21 = cp * sr\n    r22 = cp * cr\n\n    # Stack the rows to form the rotation matrices for each sample in the batch\n    rotation_matrices = torch.stack([\n        torch.stack([r00, r01, r02], dim=-1),\n        torch.stack([r10, r11, r12], dim=-1),\n        torch.stack([r20, r21, r22], dim=-1)\n    ], dim=-2)\n\n    return rotation_matrices\n\n# Example usage:\n# euler_angles = torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n# rotation_matrices = euler2rot(euler_angles)\n# print(rotation_matrices)\n```\n\nThis function first computes the cosine and sine values for each of the Euler angles. It then constructs the rotation matrix for each set of angles by calculating the individual elements of the matrix based on the rotation formulas. Finally, it stacks these elements to form the 3x3 rotation matrices for each sample in the batch. The rotation matrices are returned as a tensor of shape `[batch_size, 3, 3]`.",
  "canonical_solution": "\n    batch_size = euler_angle.shape[0]\n    theta = euler_angle[:, 0].reshape(-1, 1, 1)\n    phi = euler_angle[:, 1].reshape(-1, 1, 1)\n    psi = euler_angle[:, 2].reshape(-1, 1, 1)\n    one = torch.ones((batch_size, 1, 1), dtype=torch.float32, device=euler_angle.device)\n    zero = torch.zeros((batch_size, 1, 1), dtype=torch.float32, device=euler_angle.device)\n    rot_x = torch.cat((\n        torch.cat((one, zero, zero), 1),\n        torch.cat((zero, theta.cos(), theta.sin()), 1),\n        torch.cat((zero, -theta.sin(), theta.cos()), 1),\n    ), 2)\n    rot_y = torch.cat((\n        torch.cat((phi.cos(), zero, -phi.sin()), 1),\n        torch.cat((zero, one, zero), 1),\n        torch.cat((phi.sin(), zero, phi.cos()), 1),\n    ), 2)\n    rot_z = torch.cat((\n        torch.cat((psi.cos(), -psi.sin(), zero), 1),\n        torch.cat((psi.sin(), psi.cos(), zero), 1),\n        torch.cat((zero, zero, one), 1)\n    ), 2)\n    return torch.bmm(rot_x, torch.bmm(rot_y, rot_z))\n\n\n",
  "unprocess_testcases": "a = torch.tensor([[0.0, 0.0, 0.0]])\nb = torch.tensor([[0.0, 0.0, torch.pi / 4]])\nc = torch.tensor([[torch.pi / 2, torch.pi / 4, torch.pi / 6]])\n\nassert((SOLUTION_SIGNATURE(a) == euler2rot(a)).all())\nassert((SOLUTION_SIGNATURE(b) == euler2rot(b)).all())\nassert((SOLUTION_SIGNATURE(c) == euler2rot(c)).all())\n"
 },
 {
  "task_id": "RWPB/41",
  "prompt": "def merge_short_sentences_en(sens):\n    \"\"\"Avoid short sentences by merging them with the following sentence.\n\n    Args:\n        List[str]: list of input sentences.\n\n    Returns:\n        List[str]: list of output sentences.\n    \"\"\"\n",
  "solution": "Certainly! Below is the Python function `merge_short_sentences_en` that merges short sentences (less than a specified threshold) with the following sentence. The function assumes that each sentence in the input list ends with a period, and that the sentences are stripped of leading and trailing spaces.\n\n```python\ndef merge_short_sentences_en(sens, threshold=10):\n    \"\"\"Avoid short sentences by merging them with the following sentence.\n\n    Args:\n        sens (List[str]): list of input sentences.\n        threshold (int): minimum length of a sentence to stand alone.\n\n    Returns:\n        List[str]: list of output sentences.\n    \"\"\"\n    if not sens:\n        return []\n\n    merged_sentences = []\n    i = 0\n    while i < len(sens):\n        current_sentence = sens[i].strip()\n        \n        # Check if the current sentence is shorter than the threshold and not the last sentence\n        if len(current_sentence) < threshold and i + 1 < len(sens):\n            # Merge with the next sentence\n            next_sentence = sens[i + 1].strip()\n            merged_sentence = current_sentence.rstrip('.') + ' ' + next_sentence.lstrip().capitalize()\n            sens[i + 1] = merged_sentence  # Update the next sentence in the list\n            i += 1  # Skip the next sentence as it is now merged\n        else:\n            # If the sentence is long enough or it is the last one, add it to the result\n            merged_sentences.append(current_sentence)\n        i += 1\n\n    return merged_sentences\n```\n\n### Explanation:\n1. **Input and Output**: The function takes a list of sentences and a threshold length. Sentences shorter than the threshold are merged with the following sentence.\n2. **Processing**: The function iterates through the list of sentences. If a sentence is shorter than the threshold and it is not the last sentence in the list, it is merged with the next sentence. The merged sentence replaces the next sentence in the list, and the current index skips to the next one to avoid processing the merged sentence again.\n3. **Edge Cases**: The function handles cases where the list might be empty. It also ensures that the last sentence is added to the result as there is no subsequent sentence to merge with.\n4. **Capitalization and Punctuation**: The function removes the period at the end of the current short sentence before merging and ensures the next sentence starts with a capital letter.\n\nThis function should work well for basic sentence merging tasks where simple rules based on sentence length are sufficient.",
  "canonical_solution": "\n    sens_out = []\n    for s in sens:\n        # If the previous sentense is too short, merge them with\n        # the current sentence.\n        if len(sens_out) > 0 and len(sens_out[-1].split(\" \")) <= 2:\n            sens_out[-1] = sens_out[-1] + \" \" + s\n        else:\n            sens_out.append(s)\n    try:\n        if len(sens_out[-1].split(\" \")) <= 2:\n            sens_out[-2] = sens_out[-2] + \" \" + sens_out[-1]\n            sens_out.pop(-1)\n    except:\n        pass\n    return sens_out\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE([\"I am.\", \"Here now.\", \"John is going to the store.\", \"Oh.\", \"Really?\", \"That's interesting.\"]) == merge_short_sentences_en([\"I am.\", \"Here now.\", \"John is going to the store.\", \"Oh.\", \"Really?\", \"That's interesting.\"]))\nassert(SOLUTION_SIGNATURE([\"This is a proper sentence.\", \"Here is another example.\", \"Each of these sentences is long enough.\"]) == merge_short_sentences_en([\"This is a proper sentence.\", \"Here is another example.\", \"Each of these sentences is long enough.\"]))\nassert(SOLUTION_SIGNATURE([\"Weather is bad.\", \"Indeed.\", \"Very cold.\"]) == merge_short_sentences_en([\"Weather is bad.\", \"Indeed.\", \"Very cold.\"]))\n"
 },
 {
  "task_id": "RWPB/75",
  "prompt": "import numpy as np\n\ndef align_vector_to_another(a=np.array([0, 0, 1]), b=np.array([1, 0, 0])):\n    \"\"\"\n    Aligns vector 'a' to vector 'b' using an axis-angle rotation.\n\n    Parameters:\n    a (np.array): The initial vector to be aligned. Default is np.array([0, 0, 1]).\n    b (np.array): The target vector to align with. Default is np.array([1, 0, 0]).\n\n    Returns:\n    tuple: A tuple containing:\n        - axis_ (np.array): The normalized axis of rotation.\n        - angle (float): The angle of rotation in radians.\n\n    Process:\n    1. Check if vectors 'a' and 'b' are already aligned. If they are, return (None, None).\n    2. Compute the cross product of 'a' and 'b' to find the rotation axis.\n    3. Normalize the rotation axis to get a unit vector.\n    4. Calculate the angle between 'a' and 'b' using the arccosine of their dot product.\n\n    If 'a' and 'b' are not aligned, the function returns the necessary rotation axis and angle to align 'a' with 'b'.\n    \"\"\"\n",
  "solution": "    # Step 1: Check if vectors 'a' and 'b' are already aligned\n    if np.allclose(a, b):\n        return (None, None)\n\n    # Step 2: Compute the cross product of 'a' and 'b' to find the rotation axis\n    axis_ = np.cross(a, b)\n\n    # Step 3: Normalize the rotation axis to get a unit vector\n    norm = np.linalg.norm(axis_)\n    if norm == 0:\n        return (None, None)  # This means 'a' and 'b' are parallel but opposite\n    axis_ = axis_ / norm\n\n    # Step 4: Calculate the angle between 'a' and 'b'\n    a_norm = np.linalg.norm(a)\n    b_norm = np.linalg.norm(b)\n    cos_theta = np.dot(a, b) / (a_norm * b_norm)\n    angle = np.arccos(np.clip(cos_theta, -1.0, 1.0))  # Clip to handle numerical errors\n\n    return (axis_, angle)",
  "canonical_solution": "    \n    if np.array_equal(a, b):\n        return None, None\n    axis_ = np.cross(a, b)\n    axis_ = axis_ / np.linalg.norm(axis_)\n    angle = np.arccos(np.dot(a, b))\n\n    return axis_, angle\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([1, 0, 0]))[0] == align_vector_to_another(a = np.array([1, 0, 0]), b = np.array([1, 0, 0]))[0]))\nassert((SOLUTION_SIGNATURE(a = np.array([0, 0, 1]), b = np.array([1, 0, 0]))[0] == align_vector_to_another(a = np.array([0, 0, 1]), b = np.array([1, 0, 0]))[0]).all())\nassert((SOLUTION_SIGNATURE(a = np.array([1, 0, 1]), b = np.array([-1, 0, 0]))[0] == align_vector_to_another(a = np.array([1, 0, 1]), b = np.array([-1, 0, 0]))[0]).all())\nassert((SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([1, 0, 1]))[0] == align_vector_to_another(a = np.array([1, 0, 0]), b = np.array([1, 0, 1]))[0]).all())\nassert((SOLUTION_SIGNATURE(a = np.array([1, 1, 1]), b = np.array([-1, 1, -1]))[0] == align_vector_to_another(a = np.array([1, 1, 1]), b = np.array([-1, 1, -1]))[0]).all())\nassert(SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([1, 0, 0]))[1] == align_vector_to_another(a = np.array([1, 0, 0]), b = np.array([1, 0, 0]))[1])\nassert(SOLUTION_SIGNATURE(a = np.array([0, 0, 1]), b = np.array([1, 0, 0]))[1] == align_vector_to_another(a = np.array([0, 0, 1]), b = np.array([1, 0, 0]))[1])\nassert(SOLUTION_SIGNATURE(a = np.array([1, 0, 1]), b = np.array([-1, 0, 0]))[1] == align_vector_to_another(a = np.array([1, 0, 1]), b = np.array([-1, 0, 0]))[1])\nassert(SOLUTION_SIGNATURE(a = np.array([1, 0, 0]), b = np.array([1, 0, 1]))[1] == align_vector_to_another(a = np.array([1, 0, 0]), b = np.array([1, 0, 1]))[1])\nassert(SOLUTION_SIGNATURE(a = np.array([1, 1, 1]), b = np.array([-1, 1, -1]))[1] == align_vector_to_another(a = np.array([1, 1, 1]), b = np.array([-1, 1, -1]))[1])\n"
 },
 {
  "task_id": "RWPB/24",
  "prompt": "import re\nimport hashlib\n\n\ndef check(x, checksum_token):\n    \"\"\"\n    Verifies if the provided checksum in a given string matches the SHA-256 hash of the string with a replaced checksum.\n\n    Parameters\n    ----------\n    x : str\n        The input string that contains a checksum to be verified.\n    checksum_token : str\n        A placeholder token that will replace the current checksum in the string for hashing.\n\n    Returns\n    -------\n    bool\n        True if the checksum is correct, False otherwise.\n\n    Notes\n    -----\n    This function uses a regular expression to find the checksum in the input string `x`.\n    It then replaces the checksum with `checksum_token`, hashes the resulting string using SHA-256,\n    and compares the hash with the original checksum. If they match, the function returns True,\n    indicating that the checksum is valid. If they do not match or if no checksum is found,\n    the function returns False.\n    \"\"\"\n",
  "solution": "    # Use regex to find the checksum in the string\n    match = re.search(r'\\bchecksum=([a-fA-F0-9]{64})\\b', x)\n    if not match:\n        return False\n\n    # Extract the original checksum from the string\n    original_checksum = match.group(1)\n\n    # Replace the original checksum with the checksum_token\n    modified_string = x.replace(f'checksum={original_checksum}', f'checksum={checksum_token}')\n\n    # Compute the SHA-256 hash of the modified string\n    hash_object = hashlib.sha256(modified_string.encode())\n    computed_checksum = hash_object.hexdigest()\n\n    # Compare the computed checksum with the original checksum\n    return computed_checksum == original_checksum\n\n# Example usage:\n# result = check('data=hello&checksum=abcdef1234567890abcdef1234567890abcdef1234567890abcdef1234567890', 'TOKEN')\n# print(result)  # Output will depend on the actual checksum and data.",
  "canonical_solution": "\n\n    # Pre-compiled regular expression for matching and replacing checksums\n    re_checksum = re.compile(r'\"Checksum\": \"([0-9a-fA-F]{64})\"')\n\n    # Search for the checksum in the input string\n    m = re.search(re_checksum, x)\n    if not m:\n        return False\n\n    # Replace the checksum with the provided token\n    replaced = re.sub(re_checksum, f'\"Checksum\": \"{checksum_token}\"', x)\n\n    # Calculate the SHA-256 hash of the modified string\n    h = hashlib.sha256(replaced.encode(\"utf8\"))\n\n    # Compare the calculated hash with the original checksum\n    return h.hexdigest() == m.group(1)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE('{\"Data\": \"Example data\", \"Checksum\": \"0b3ad42a156a99abcce760eee811fcfcbb806126697b77c5eb9638e20c455439\"}', checksum_token = \"TOKEN\") == check('{\"Data\": \"Example data\", \"Checksum\": \"0b3ad42a156a99abcce760eee811fcfcbb806126697b77c5eb9638e20c455439\"}', checksum_token = \"TOKEN\"))\nassert(SOLUTION_SIGNATURE('{\"Data\": \"Example data\", \"Checksum\": \"f03a9dcfea7e161acf569d267c75034617de5779b416ea8e4aea18d52318dd4b\"}', checksum_token = \"Apple\") == check('{\"Data\": \"Example data\", \"Checksum\": \"f03a9dcfea7e161acf569d267c75034617de5779b416ea8e4aea18d52318dd4b\"}', checksum_token = \"Apple\"))\nassert(SOLUTION_SIGNATURE('{\"Data\": \"Example data\", \"Checksum\": \"f03a9dcfea7e161acf569d267c75034617de5779b416ea8e4aea18d52318dd4b\"}', checksum_token = \"LLMs\") == check('{\"Data\": \"Example data\", \"Checksum\": \"f03a9dcfea7e161acf569d267c75034617de5779b416ea8e4aea18d52318dd4b\"}', checksum_token = \"LLMs\"))\n"
 },
 {
  "task_id": "RWPB/86",
  "prompt": "import torch\n\ndef inverse_sigmoid(x):\n    \"\"\"\n    Computes the inverse sigmoid of a tensor.\n\n    Args:\n    x (torch.Tensor): Input tensor.\n\n    Returns:\n    torch.Tensor: Tensor with the inverse sigmoid applied.\n    \"\"\"\n",
  "solution": "Certainly! To compute the inverse of the sigmoid function, also known as the logit function, you can use the formula:\n\n\\[ \\text{logit}(x) = \\log\\left(\\frac{x}{1 - x}\\right) \\]\n\nHere's how you can implement this in the function:\n\n```python\nimport torch\n\ndef inverse_sigmoid(x):\n    \"\"\"\n    Computes the inverse sigmoid (logit) of a tensor.\n\n    Args:\n    x (torch.Tensor): Input tensor.\n\n    Returns:\n    torch.Tensor: Tensor with the inverse sigmoid applied.\n    \"\"\"\n    if torch.any(x <= 0) or torch.any(x >= 1):\n        raise ValueError(\"Input tensor elements should be in the range (0, 1) exclusive.\")\n    \n    return torch.log(x / (1 - x))\n```\n\nThis function first checks if all elements of the input tensor `x` are strictly between 0 and 1, as the logit function is undefined for values outside this range. It then computes the logit using the formula provided.",
  "canonical_solution": "    \n    return torch.log(x / (1 - x))\n\n",
  "unprocess_testcases": "x = torch.tensor([0.2, 0.5, 0.8])\nassert((SOLUTION_SIGNATURE(x) == inverse_sigmoid(x)).all())\n\nx = torch.tensor([0.001, 0.999])\nassert((SOLUTION_SIGNATURE(x) == inverse_sigmoid(x)).all())\n\nx = torch.tensor([0.0, 1.0])\nassert((SOLUTION_SIGNATURE(x) == inverse_sigmoid(x)).all())\n"
 },
 {
  "task_id": "RWPB/128",
  "prompt": "import numpy as np\nimport torch\n\n\ndef xywh2xyxy(x):\n    \"\"\"\n    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n    top-left corner and (x2, y2) is the bottom-right corner.\n\n    Args:\n        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.\n\n    Returns:\n        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.\n    \"\"\"\n",
  "solution": "    # Check if the input is a NumPy array or a PyTorch tensor\n    if isinstance(x, np.ndarray):\n        # Calculate x1, y1, x2, y2 using NumPy operations\n        x1 = x[..., 0]\n        y1 = x[..., 1]\n        x2 = x[..., 0] + x[..., 2]\n        y2 = x[..., 1] + x[..., 3]\n        y = np.stack([x1, y1, x2, y2], axis=-1)\n    elif isinstance(x, torch.Tensor):\n        # Calculate x1, y1, x2, y2 using PyTorch operations\n        x1 = x[..., 0]\n        y1 = x[..., 1]\n        x2 = x[..., 0] + x[..., 2]\n        y2 = x[..., 1] + x[..., 3]\n        y = torch.stack([x1, y1, x2, y2], dim=-1)\n    else:\n        raise TypeError(\"Input must be a NumPy array or a PyTorch tensor\")\n\n    return y\n\n# Example usage:\n# x_np = np.array([10, 20, 50, 40])\n# print(xywh2xyxy(x_np))\n# x_torch = torch.tensor([10, 20, 50, 40])\n# print(xywh2xyxy(x_torch))",
  "canonical_solution": "    \n    assert x.shape[-1] == 4, f\"input shape last dimension expected 4 but input shape is {x.shape}\"\n    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\n    dw = x[..., 2] / 2  # half-width\n    dh = x[..., 3] / 2  # half-height\n    y[..., 0] = x[..., 0] - dw  # top left x\n    y[..., 1] = x[..., 1] - dh  # top left y\n    y[..., 2] = x[..., 0] + dw  # bottom right x\n    y[..., 3] = x[..., 1] + dh  # bottom right y\n    return y\n\n",
  "unprocess_testcases": "x = np.array([[10, 20, 30, 40]])\nassert((SOLUTION_SIGNATURE(x) == xywh2xyxy(x)).all())\n\nx = np.array([[10, 10, 20, 20], [30, 40, 10, 10], [50, 60, 30, 40]])\nassert((SOLUTION_SIGNATURE(x) == xywh2xyxy(x)).all())\n\nx = torch.tensor([[15, 25, 0, 35]])\nassert((SOLUTION_SIGNATURE(x) == xywh2xyxy(x)).all())\n"
 },
 {
  "task_id": "RWPB/108",
  "prompt": "import torch\n\ndef norm_grid(v_grid):\n    \"\"\"Normalize the grid values to the range [-1, 1].\n\n    Args:\n        v_grid (torch.Tensor): The input grid of shape (B, 2, H, W).\n\n    Returns:\n        torch.Tensor: The normalized grid of shape (B, H, W, 2).\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `norm_grid` function that normalizes the grid values to the range [-1, 1] and reshapes the tensor as specified:\n\n```python\nimport torch\n\ndef norm_grid(v_grid):\n    \"\"\"Normalize the grid values to the range [-1, 1].\n\n    Args:\n        v_grid (torch.Tensor): The input grid of shape (B, 2, H, W).\n\n    Returns:\n        torch.Tensor: The normalized grid of shape (B, H, W, 2).\n    \"\"\"\n    B, _, H, W = v_grid.shape\n    # Create tensors representing the maximum index in each dimension\n    max_h = H - 1\n    max_w = W - 1\n\n    # Normalize the grid by scaling the indices from [0, max] to [-1, 1]\n    # For x coordinates (width dimension)\n    v_grid[:, 0, :, :] = 2.0 * v_grid[:, 0, :, :] / max_w - 1.0\n    # For y coordinates (height dimension)\n    v_grid[:, 1, :, :] = 2.0 * v_grid[:, 1, :, :] / max_h - 1.0\n\n    # Permute the dimensions from (B, 2, H, W) to (B, H, W, 2) for expected output shape\n    normalized_grid = v_grid.permute(0, 2, 3, 1)\n\n    return normalized_grid\n```\n\nThis function first calculates the maximum indices for height and width. It then normalizes the grid values by scaling the indices from their original range `[0, max_h]` or `[0, max_w]` to the target range `[-1, 1]`. Finally, it permutes the dimensions of the tensor to match the required output shape `(B, H, W, 2)`.",
  "canonical_solution": "    \n    _, _, H, W = v_grid.size()\n\n    # scale grid to [-1,1]\n    v_grid_norm = torch.zeros_like(v_grid)\n    v_grid_norm[:, 0, :, :] = 2.0 * v_grid[:, 0, :, :] / (W - 1) - 1.0\n    v_grid_norm[:, 1, :, :] = 2.0 * v_grid[:, 1, :, :] / (H - 1) - 1.0\n    return v_grid_norm.permute(0, 2, 3, 1)  # BHW2\n\n\n",
  "unprocess_testcases": "v_grid = torch.rand((2, 2, 4, 4))\nassert((SOLUTION_SIGNATURE(v_grid) == norm_grid(v_grid)).all())\n\nv_grid = torch.tensor([[[[0, 1], [0, 1]], [[0, 0], [1, 1]]]])\nassert((SOLUTION_SIGNATURE(v_grid) == norm_grid(v_grid)).all())\n\nv_grid = torch.rand((1, 2, 3, 5))\nassert((SOLUTION_SIGNATURE(v_grid) == norm_grid(v_grid)).all())\n"
 },
 {
  "task_id": "RWPB/55",
  "prompt": "import numpy as np\nimport torch\n\n\ndef numpy_to_pytorch(x):\n    \"\"\"\n    Converts a numpy array to a PyTorch tensor with specific preprocessing.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        A numpy array to be converted to a PyTorch tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        A PyTorch tensor with the same underlying data as the input numpy array,\n        normalized and reshaped to have a batch dimension.\n\n    Notes\n    -----\n    This function performs the following operations on the input numpy array:\n    - Converts the data type to np.float32.\n    - Normalizes the values by dividing by 255.0, which is often used for image pixel values.\n    - Adds a batch dimension by prepending a singleton dimension (None).\n    - Creates a contiguous array to ensure the data layout is compatible with PyTorch.\n    - Converts the numpy array to a PyTorch tensor using torch.from_numpy.\n    - Casts the resulting tensor to a float type.\n\n    This is a common preprocessing step when preparing numpy arrays for use with PyTorch models,\n    especially in computer vision tasks where image data is involved.\n    \"\"\"\n",
  "solution": "    # Convert the data type to np.float32\n    x = x.astype(np.float32)\n    \n    # Normalize the values by dividing by 255.0\n    x /= 255.0\n    \n    # Add a batch dimension by prepending a singleton dimension\n    x = x[None, ...]\n    \n    # Ensure the array is contiguous\n    x = np.ascontiguousarray(x)\n    \n    # Convert the numpy array to a PyTorch tensor\n    x_tensor = torch.from_numpy(x)\n    \n    # Cast the tensor to float type (redundant here since it's already float32, but included for completeness)\n    x_tensor = x_tensor.float()\n    \n    return x_tensor\n\n# Example usage:\n# x_np = np.random.randint(0, 256, (28, 28), dtype=np.uint8)\n# x_tensor = numpy_to_pytorch(x_np)\n# print(x_tensor.shape)  # Should print: torch.Size([1, 28, 28])",
  "canonical_solution": "    \n    # Convert numpy array to float32 and normalize by dividing by 255.0\n    y = x.astype(np.float32) / 255.0\n\n    # Add a batch dimension by prepending a singleton dimension\n    y = y[None]\n\n    # Ensure the array is contiguous in memory for compatibility with PyTorch\n    y = np.ascontiguousarray(y.copy())\n\n    # Convert the numpy array to a PyTorch tensor and cast it to float\n    y = torch.from_numpy(y).float()\n\n    return y\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([[10, 20, 30], [40, 50, 60]])) == numpy_to_pytorch(np.array([[10, 20, 30], [40, 50, 60]]))).all())\nassert((SOLUTION_SIGNATURE(np.array([[255]])) == numpy_to_pytorch(np.array([[255]]))).all())\nassert((SOLUTION_SIGNATURE(np.array([[[255, 0, 0], [0, 255, 0], [0, 0, 255]], [[100, 100, 100], [150, 150, 150], [200, 200, 200]], [[50, 25, 75], [125, 75, 175], [225, 125, 25]]])) == numpy_to_pytorch(np.array([[[255, 0, 0], [0, 255, 0], [0, 0, 255]], [[100, 100, 100], [150, 150, 150], [200, 200, 200]], [[50, 25, 75], [125, 75, 175], [225, 125, 25]]]))).all())\n"
 },
 {
  "task_id": "RWPB/92",
  "prompt": "import math\nimport numpy as np\nfrom decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\ndef round(number, num_decimal_places, leave_int=False):\n    '''\n    Purpose: round a number to the specified number of decimal places. Existing \n        round functions may not round correctly so that's why I built my own.\n    Args:\n        - number: float, number to round\n        - num_decimal_places: int, number of decimal places to round number to\n        - leave_int: bool, whether to leave integers unchanged or convert them to floats\n\n    Returns:\n        - float: rounded number\n    '''\n",
  "solution": "    # Convert the number to a Decimal for high precision rounding\n    decimal_number = Decimal(str(number))\n    \n    # Define the rounding context based on the number of decimal places\n    rounding_context = Decimal('1.' + '0' * num_decimal_places)\n    \n    # Perform rounding\n    rounded_number = decimal_number.quantize(rounding_context, rounding=ROUND_HALF_UP)\n    \n    # If the number is an integer and leave_int is True, return it as an integer\n    if leave_int and rounded_number == rounded_number.to_integral():\n        return int(rounded_number)\n    \n    # Otherwise, return as float\n    return float(rounded_number)\n\n# Example usage:\n# result = round(3.14159, 2)  # Should return 3.14\n# result = round(3.14159, 2, True)  # Should return 3.14\n# result = round(3.0, 2, True)  # Should return 3 (as an integer)\n# result = round(3.0, 2)  # Should return 3.0 (as a float)",
  "canonical_solution": "    \n    if leave_int and int(number) == number:\n        return number\n    decimal_str = '1.'\n    for decimal_place in range(num_decimal_places):\n        decimal_str += '1'\n    return float(Decimal(str(number)).quantize(Decimal(decimal_str), rounding=ROUND_HALF_UP))\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(3.14159265, 4) == round(3.14159265, 4))\nassert(SOLUTION_SIGNATURE(200, 2, leave_int=True) == round(200, 2, leave_int=True))\nassert(SOLUTION_SIGNATURE(2.789, 0) == round(2.789, 0))\n"
 },
 {
  "task_id": "RWPB/61",
  "prompt": "import torch\nimport torch.nn.functional as F\n\n\ndef layer_norm_ref(x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False):\n    \"\"\"\n    Applies Layer Normalization to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        The input tensor.\n    weight : torch.Tensor\n        The weight tensor for scaling the normalized input.\n    bias : torch.Tensor or None\n        The bias tensor for shifting the normalized input. If None, no bias is added.\n    residual : torch.Tensor or None\n        The residual tensor to be added to the normalized input. If None, no residual is added.\n    eps : float, optional\n        A small constant added for numerical stability.\n    prenorm : bool, optional\n        If True, the function returns both the normalized output and the original input.\n    upcast : bool, optional\n        If True, upcasts all tensors to float type for computation.\n\n    Returns\n    -------\n    torch.Tensor or tuple of torch.Tensor\n        The layer-normalized output tensor. If `prenorm` is True, returns a tuple\n        containing the output tensor and the original input tensor.\n\n    Notes\n    -----\n    Layer Normalization normalizes the input tensor by subtracting the mean and dividing by\n    the standard deviation of each individual layer. This function performs the normalization,\n    applies the weight and bias, and optionally adds a residual connection. If `upcast` is True,\n    all tensors are upcast to float type before the operations for precision.\n    \"\"\"\n",
  "solution": "    # Upcast tensors to float if required\n    if upcast:\n        x = x.float()\n        weight = weight.float()\n        if bias is not None:\n            bias = bias.float()\n        if residual is not None:\n            residual = residual.float()\n\n    # Compute the mean and standard deviation along the last dimension\n    mean = x.mean(dim=-1, keepdim=True)\n    std = x.std(dim=-1, keepdim=True, unbiased=False)\n\n    # Normalize the input tensor\n    normalized_x = (x - mean) / (std + eps)\n\n    # Apply weight and bias\n    output = weight * normalized_x\n    if bias is not None:\n        output += bias\n\n    # Add residual if provided\n    if residual is not None:\n        output += residual\n\n    # Return the output and optionally the original input\n    if prenorm:\n        return output, x\n    else:\n        return output\n\n# Example usage:\n# x = torch.randn(10, 20)\n# weight = torch.randn(20)\n# bias = torch.randn(20)\n# residual = torch.randn(10, 20)\n# output = layer_norm_ref(x, weight, bias, residual)",
  "canonical_solution": "\n    # Preserve the original dtype\n    dtype = x.dtype\n\n    # Upcast weight, bias, and x to float if necessary\n    if upcast:\n        weight = weight.float()\n        bias = bias.float() if bias is not None else None\n        x = x.float()\n        residual = residual.float() if residual is not None else residual\n\n    # Add residual to input if provided\n    if residual is not None:\n        x = (x + residual).to(dtype)\n\n    # Apply layer normalization\n    # Note: It was x.to(weight.dtype) but it should be x because F.layer_norm expects the input tensor x.\n    out = F.layer_norm(x, x.shape[-1:], weight=weight, bias=bias, eps=eps).to(dtype)\n\n    # Return the output only or the output with the original input if prenorm is True\n    return out if not prenorm else (out, x)\n\n\n",
  "unprocess_testcases": "a = torch.rand(10, 5)\nb = torch.rand(5,)\n\nassert((SOLUTION_SIGNATURE(a, b, b) == layer_norm_ref(a, b, b)).all())\nassert((SOLUTION_SIGNATURE(a, b, b, a, 1e-5, False, True) == layer_norm_ref(a, b, b, a, 1e-5, False, True)).all())\nassert((SOLUTION_SIGNATURE(a, b, None, None, 1e-6, True, False)[0] == layer_norm_ref(a, b, None, None, 1e-6, True, False)[0]).all())\nassert((SOLUTION_SIGNATURE(a, b, None, None, 1e-6, True, False)[1] == layer_norm_ref(a, b, None, None, 1e-6, True, False)[1]).all())\nassert((SOLUTION_SIGNATURE(a, b, None, b, 1e-6, True, False)[0] == layer_norm_ref(a, b, None, b, 1e-6, True, False)[0]).all())\nassert((SOLUTION_SIGNATURE(a, b, None, b, 1e-6, True, False)[1] == layer_norm_ref(a, b, None, b, 1e-6, True, False)[1]).all())\nassert((SOLUTION_SIGNATURE(a, b, None) == layer_norm_ref(a, b, None)).all())\n"
 },
 {
  "task_id": "RWPB/30",
  "prompt": "import numpy as np\n\ndef get_confusion_matrix(gt_label, pred_label, num_classes):\n    \"\"\"\n    Calculates the confusion matrix from the ground truth and predicted labels.\n    If the length of gt_label is not equal to the length of pred_label, please return ValueError \n\n    Args:\n        gt_label (np.array): An array of ground truth labels.\n        pred_label (np.array): An array of predicted labels from the model.\n        num_classes (int): The number of classes in the dataset.\n\n    Returns:\n        np.array: A 2D array (matrix) of shape (num_classes, num_classes) representing the confusion matrix,\n                  where each row corresponds to the true classes and each column to the predicted classes.\n\n    The function calculates a confusion matrix which helps in evaluating the accuracy of a classification model.\n    It does so by cross-tabulating each class label with its corresponding prediction, thus providing insight\n    into misclassifications.\n    \"\"\"\n",
  "solution": "    # Check if the lengths of the ground truth and predicted labels are the same\n    if len(gt_label) != len(pred_label):\n        raise ValueError(\"The length of ground truth labels and predicted labels must be the same.\")\n\n    # Initialize the confusion matrix with zeros\n    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n\n    # Populate the confusion matrix\n    for true_label, pred_label in zip(gt_label, pred_label):\n        confusion_matrix[true_label, pred_label] += 1\n\n    return confusion_matrix\n\n# Example usage:\n# gt_labels = np.array([0, 1, 2, 1, 0])\n# pred_labels = np.array([0, 2, 2, 1, 0])\n# num_classes = 3\n# cm = get_confusion_matrix(gt_labels, pred_labels, num_classes)\n# print(cm)",
  "canonical_solution": "\n    if gt_label.shape[-1] != pred_label.shape[-1]:\n        return ValueError\n    \n    # Compute the linear index for each element of the confusion matrix\n    index = (gt_label * num_classes + pred_label).astype('int32')\n\n    # Count the occurrence of each index\n    label_count = np.bincount(index, minlength=num_classes*num_classes)\n\n    # Initialize the confusion matrix with zeros\n    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n\n    # Fill the confusion matrix with counts\n    for i_label in range(num_classes):\n        for i_pred_label in range(num_classes):\n            cur_index = i_label * num_classes + i_pred_label\n            confusion_matrix[i_label, i_pred_label] = label_count[cur_index]\n\n    return confusion_matrix\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([0, 1, 2, 1, 0]), np.array([0, 2, 2, 1, 0]), 3) == get_confusion_matrix(np.array([0, 1, 2, 1, 0]), np.array([0, 2, 2, 1, 0]), 3)).all())\nassert((SOLUTION_SIGNATURE(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0]), 1) == get_confusion_matrix(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0]), 1)).all())\nassert(SOLUTION_SIGNATURE(np.array([0, 1, 2, 3]), np.array([0, 1, 2]), 4) == get_confusion_matrix(np.array([0, 1, 2, 3]), np.array([0, 1, 2]), 4))\n"
 },
 {
  "task_id": "RWPB/20",
  "prompt": "import math\n\n\ndef get_line_angle(x1, y1, x2, y2):\n    '''\n    Calculates the angle of a line segment relative to the horizontal axis.\n\n    Parameters\n    ----------\n    x1, y1 : float\n        Coordinates of the first point (x1, y1).\n    x2, y2 : float\n        Coordinates of the second point (x2, y2).\n\n    Return\n    ------\n    float\n        The angle of the line segment from point (x1, y1) to (x2, y2) in degrees.\n\n    The function computes the slope of the line segment and then uses the arctangent\n    function to determine the angle in radians. This angle is converted to degrees.\n    The function includes handling for vertical lines where the slope would be undefined.\n    '''\n",
  "solution": "    # Calculate the difference in coordinates\n    dx = x2 - x1\n    dy = y2 - y1\n\n    # Calculate the angle in radians using arctan2 which handles the quadrant correctly\n    angle_radians = math.atan2(dy, dx)\n\n    # Convert radians to degrees\n    angle_degrees = math.degrees(angle_radians)\n\n    # Normalize the angle to be within the range [0, 360)\n    if angle_degrees < 0:\n        angle_degrees += 360\n\n    return angle_degrees\n\n# Example usage:\n# angle = get_line_angle(0, 0, 1, 1)\n# print(angle)  # Output will be 45.0 for this input",
  "canonical_solution": "\n    # Check for a vertical line to avoid division by zero\n    if x2 == x1:\n        return 90 if y2 > y1 else -90  # Directly return 90 or -90 degrees depending on the direction\n\n    # Calculate the slope of the line\n    slope = (y2 - y1) / (x2 - x1)\n\n    # Calculate the angle in radians\n    angle_radians = math.atan(slope)\n\n    # Convert the angle to degrees\n    angle_degrees = math.degrees(angle_radians)\n\n    return angle_degrees\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(0, 10, 10, 0) == get_line_angle(0, 10, 10, 0))\nassert(SOLUTION_SIGNATURE(5, 5, 5, 10) == get_line_angle(5, 5, 5, 10))\nassert(SOLUTION_SIGNATURE(0, 0, 10, 0) == get_line_angle(0, 0, 10, 0))\n"
 },
 {
  "task_id": "RWPB/71",
  "prompt": "from math import factorial\n\n\ndef getLambertCoefs(numOfLambertCoefs):\n    \"\"\"\n    Compute the Lambert coefficients for a given number of coefficients.\n    Args:\n        - numOfLambertCoefs: int, the number of coefficients\n    Returns:\n        lambert_coefs: list, the computed Lambert coefficients\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the Python function to compute the Lambert coefficients. The Lambert coefficients are related to the Lambert series and can be computed using the formula involving factorials. Here, we'll assume the coefficients \\( c_n \\) are given by:\n\n\\[ c_n = \\frac{(-1)^{n-1}}{(n-1)!} \\]\n\nThis formula is one interpretation based on the generating function for the sequence of Lambert coefficients. Let's complete the function based on this formula:\n\n```python\nfrom math import factorial\n\ndef getLambertCoefs(numOfLambertCoefs):\n    \"\"\"\n    Compute the Lambert coefficients for a given number of coefficients.\n    Args:\n        - numOfLambertCoefs: int, the number of coefficients\n    Returns:\n        lambert_coefs: list, the computed Lambert coefficients\n    \"\"\"\n    lambert_coefs = []\n    for n in range(1, numOfLambertCoefs + 1):\n        coef = (-1)**(n-1) / factorial(n-1)\n        lambert_coefs.append(coef)\n    return lambert_coefs\n```\n\nThis function initializes an empty list `lambert_coefs`, then iterates from 1 to `numOfLambertCoefs` (inclusive). For each `n`, it computes the coefficient using the given formula and appends it to the list. Finally, it returns the list of coefficients. This function will provide the first `numOfLambertCoefs` of the series based on the specified formula.",
  "canonical_solution": "    \n    maxFactorial = factorial(numOfLambertCoefs-1)\n    return [maxFactorial*i**(i-1)//factorial(i) for i in range(1,numOfLambertCoefs)]\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(5) == getLambertCoefs(5))\nassert(SOLUTION_SIGNATURE(1) == getLambertCoefs(1))\nassert(SOLUTION_SIGNATURE(20) == getLambertCoefs(20))\nassert(SOLUTION_SIGNATURE(12) == getLambertCoefs(12))\nassert(SOLUTION_SIGNATURE(15) == getLambertCoefs(15))\n"
 },
 {
  "task_id": "RWPB/82",
  "prompt": "import importlib.util\nimport numpy as np\nimport torch\n\ndef is_tensor(x):\n    \"\"\"\n    Determines if the input `x` is a `torch.Tensor` or a `np.ndarray`.\n\n    Parameters:\n    x : any type\n        The input variable to be checked.\n\n    Returns:\n    bool\n        Returns True if `x` is either a `torch.Tensor` (if the PyTorch library is available) \n        or a `np.ndarray`. Otherwise, returns False.\n\n    Process:\n    1. Checks if the PyTorch library is available using `importlib.util.find_spec(\"torch\")`.\n    2. If PyTorch is available, imports the library and checks if `x` is an instance of `torch.Tensor`.\n    3. If `x` is not a `torch.Tensor` or if PyTorch is not available, it then checks if `x` is an instance of `np.ndarray`.\n    \"\"\"\n",
  "solution": "    # Step 1: Check if the PyTorch library is available\n    if importlib.util.find_spec(\"torch\") is not None:\n        # Step 2: Check if `x` is an instance of `torch.Tensor`\n        if isinstance(x, torch.Tensor):\n            return True\n\n    # Step 3: Check if `x` is an instance of `np.ndarray`\n    if isinstance(x, np.ndarray):\n        return True\n\n    # If none of the conditions are met, return False\n    return False\n\n# Example usage:\n# x = np.array([1, 2, 3])\n# print(is_tensor(x))  # Should print True if numpy is correctly imported\n# y = torch.tensor([1, 2, 3])\n# print(is_tensor(y))  # Should print True if torch is correctly imported and available",
  "canonical_solution": "    \n    if importlib.util.find_spec(\"torch\") is not None:\n        import torch\n\n        if isinstance(x, torch.Tensor):\n            return True\n\n    return isinstance(x, np.ndarray)\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(np.array([1, 2, 3])) == is_tensor(np.array([1, 2, 3])))\nassert(SOLUTION_SIGNATURE([1, 2, 3]) == is_tensor([1, 2, 3]))\nassert(SOLUTION_SIGNATURE(torch.tensor([1, 2, 3])) == is_tensor(torch.tensor([1, 2, 3])))\nassert(SOLUTION_SIGNATURE(0) == is_tensor(0))\n"
 },
 {
  "task_id": "RWPB/45",
  "prompt": "import torch\n\ndef invalid_to_zeros(arr, valid_mask, ndim=999):\n    \"\"\"\n    Replaces invalid entries in an array with zeros, based on a validity mask.\n\n    Parameters\n    ----------\n    arr : torch.Tensor\n        The input tensor with potentially invalid entries.\n    valid_mask : torch.Tensor (boolean)\n        A boolean mask with the same number of elements as 'arr', where True indicates\n        a valid entry and False indicates an invalid entry that should be replaced with zero.\n    ndim : int, optional\n        The desired number of dimensions for the output tensor. If 'arr' has more than\n        'ndim' dimensions, it will be flattened accordingly. Default is 999, meaning no flattening.\n\n    Returns\n    -------\n    torch.Tensor\n        The input tensor 'arr' with invalid entries replaced by zeros.\n    int or torch.Tensor\n        The number of non-zero entries in the validity mask (nnz), which corresponds to the\n        number of valid points per image.\n\n    Notes\n    -----\n    This function is particularly useful in the context of point cloud processing or similar\n    applications where some data points may be invalid or missing. By setting these invalid\n    points to zero, the resulting tensor can be used for further processing, such as neural\n    network input.\n\n    If 'valid_mask' is provided, the function updates 'arr' in-place to zero out the invalid\n    entries. It also calculates the number of non-zero entries in the mask (nnz), which is\n    returned separately. If 'valid_mask' is None, it assumes all entries are valid and calculates\n    nnz based on the total number of elements in 'arr' divided by the first dimension size.\n\n    If 'arr' has more dimensions than specified by 'ndim', the function flattens it to the\n    desired number of dimensions.\n    \"\"\"\n",
  "solution": "    # Check if the validity mask is provided, if not, assume all entries are valid\n    if valid_mask is None:\n        valid_mask = torch.ones_like(arr, dtype=torch.bool)\n\n    # Replace invalid entries with zeros\n    arr = torch.where(valid_mask, arr, torch.zeros_like(arr))\n\n    # Calculate the number of non-zero entries in the validity mask\n    nnz = valid_mask.sum()\n\n    # Flatten the array if it has more dimensions than specified by 'ndim'\n    if arr.dim() > ndim:\n        arr = arr.view(-1, *arr.shape[-ndim+1:])\n\n    return arr, nnz\n\n# Example usage:\n# arr = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n# valid_mask = torch.tensor([[True, False], [True, True]], dtype=torch.bool)\n# result, nnz = invalid_to_zeros(arr, valid_mask)\n# print(result)\n# print(nnz)",
  "canonical_solution": "\n    if valid_mask is not None:\n        arr = arr.clone()  # Create a copy of arr to avoid modifying the original tensor\n        arr[~valid_mask] = 0  # Set invalid entries (where mask is False) to zero\n        nnz = valid_mask.view(len(valid_mask), -1).sum(1)  # Calculate the number of valid points per image\n    else:\n        # If no valid_mask is provided, calculate nnz based on total elements\n        nnz = arr.numel() // len(arr) if len(arr) else 0\n\n    # If the number of dimensions in arr is greater than ndim, flatten it\n    if arr.ndim > ndim:\n        if ndim > 0:\n            arr = arr.reshape([arr.shape[0], -1])[:ndim]\n        else:\n            arr = arr.flatten()\n\n    return arr, nnz\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([1, 2, 3, 4]), valid_mask = torch.tensor([True, False, True, True]))[0] == invalid_to_zeros(torch.tensor([1, 2, 3, 4]), valid_mask = torch.tensor([True, False, True, True]))[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 2, 3, 4]), valid_mask = torch.tensor([True, False, True, True]))[1] == invalid_to_zeros(torch.tensor([1, 2, 3, 4]), valid_mask = torch.tensor([True, False, True, True]))[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 2, 3, 4]), torch.tensor([False, False, False, False]))[0] == invalid_to_zeros(torch.tensor([1, 2, 3, 4]), torch.tensor([False, False, False, False]))[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([1, 2, 3, 4]), torch.tensor([False, False, False, False]))[1] == invalid_to_zeros(torch.tensor([1, 2, 3, 4]), torch.tensor([False, False, False, False]))[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]), torch.tensor([[[True, True], [False, True]], [[True, False], [True, True]]]), 1)[0] == invalid_to_zeros(torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]), torch.tensor([[[True, True], [False, True]], [[True, False], [True, True]]]), 1)[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]), torch.tensor([[[True, True], [False, True]], [[True, False], [True, True]]]), 1)[1] == invalid_to_zeros(torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]), torch.tensor([[[True, True], [False, True]], [[True, False], [True, True]]]), 1)[1]).all())\n"
 },
 {
  "task_id": "RWPB/14",
  "prompt": "import numpy as np\n\ndef align_vector_to_another(a=np.array([0, 0, 1]), b=np.array([1, 0, 0])):\n    '''Aligns vector a to vector b with axis angle rotation.\n    If array a is equal array b, please return (None, None)\n    Args:\n        - a: numpy.array. the first vector. \n        - b: numpy.array. the second vector.\n    Return:\n        - axis: numpy.array. the axis of vector\n        - angle: numpy.array. the angle of vector\n    '''\n",
  "solution": "Certainly! Below is the completion of the function `align_vector_to_another` which calculates the rotation axis and angle required to align vector `a` to vector `b` using the axis-angle rotation method. The function uses the cross product to find the rotation axis and the dot product to find the cosine of the angle between the two vectors.\n\n```python\nimport numpy as np\n\ndef align_vector_to_another(a=np.array([0, 0, 1]), b=np.array([1, 0, 0])):\n    '''Aligns vector a to vector b with axis angle rotation.\n    If array a is equal array b, please return (None, None)\n    Args:\n        - a: numpy.array. the first vector. \n        - b: numpy.array. the second vector.\n    Return:\n        - axis: numpy.array. the axis of rotation\n        - angle: float. the angle of rotation in radians\n    '''\n    # Normalize vectors\n    a = a / np.linalg.norm(a)\n    b = b / np.linalg.norm(b)\n    \n    # Check if vectors are the same or opposite\n    if np.allclose(a, b):\n        return (None, None)\n    if np.allclose(a, -b):\n        # Find a vector orthogonal to a (or b) to use as the rotation axis\n        # This is necessary because the cross product of parallel or anti-parallel vectors is zero\n        if a[0] != 0 or a[1] != 0:\n            orthogonal = np.array([-a[1], a[0], 0])\n        else:\n            orthogonal = np.array([1, 0, 0])\n        return (orthogonal, np.pi)\n    \n    # Compute the axis of rotation (cross product of a and b)\n    axis = np.cross(a, b)\n    \n    # Compute the angle of rotation (arccos of the dot product of a and b)\n    angle = np.arccos(np.clip(np.dot(a, b), -1.0, 1.0))\n    \n    return (axis, angle)\n```\n\nThis function first normalizes the vectors `a` and `b` to ensure they are unit vectors. It then checks if the vectors are identical or exactly opposite. If they are identical, it returns `(None, None)` as no rotation is needed. If they are opposite, it finds an orthogonal vector to use as the rotation axis and sets the rotation angle to π (180 degrees).\n\nFor other cases, the function calculates the rotation axis using the cross product of `a` and `b`, and the rotation angle using the arccosine of the dot product of `a` and `b`. The angle is clipped between -1 and 1 to avoid numerical issues with the arccos function.",
  "canonical_solution": "\n    if np.array_equal(a, b):\n        return None, None\n    axis_ = np.cross(a, b)\n    axis_ = axis_ / np.linalg.norm(axis_)\n    angle = np.arccos(np.dot(a, b))\n\n    return axis_, angle\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(a = [1, 0, 0], b = [1, 0, 0])[0] == align_vector_to_another(a = [1, 0, 0], b = [1, 0, 0])[0])\nassert((SOLUTION_SIGNATURE(a = [0, 0, 1], b = [0, 1, 0])[0] == align_vector_to_another(a = [0, 0, 1], b = [0, 1, 0])[0]).all())\nassert((SOLUTION_SIGNATURE(a = [1, 1, 0], b = [-1, 0, 0])[0] == align_vector_to_another(a = [1, 1, 0], b = [-1, 0, 0])[0]).all())\nassert(SOLUTION_SIGNATURE(a = [1, 0, 0], b = [1, 0, 0])[1] == align_vector_to_another(a = [1, 0, 0], b = [1, 0, 0])[1])\nassert((SOLUTION_SIGNATURE(a = [0, 0, 1], b = [0, 1, 0])[1] == align_vector_to_another(a = [0, 0, 1], b = [0, 1, 0])[1]).all())\nassert((SOLUTION_SIGNATURE(a = [1, 1, 0], b = [-1, 0, 0])[1] == align_vector_to_another(a = [1, 1, 0], b = [-1, 0, 0])[1]).all())\n"
 },
 {
  "task_id": "RWPB/118",
  "prompt": "import numpy as np\n\ndef filter_nan(ts, vs):\n  \"\"\"\n  Filter NaN values from a time series.\n  Args:\n      - ts: numpy.ndarray, the time series\n      - vs: numpy.ndarray, the values corresponding to the time series\n  Returns:\n      filtered_ts: numpy.ndarray, the time series with NaN values removed\n      filtered_vs: numpy.ndarray, the corresponding values with NaN values removed\n  \"\"\"\n",
  "solution": "Certainly! Below is the completion of the function `filter_nan` which filters out NaN values from the given time series and its corresponding values:\n\n```python\nimport numpy as np\n\ndef filter_nan(ts, vs):\n    \"\"\"\n    Filter NaN values from a time series.\n    Args:\n        - ts: numpy.ndarray, the time series\n        - vs: numpy.ndarray, the values corresponding to the time series\n    Returns:\n        filtered_ts: numpy.ndarray, the time series with NaN values removed\n        filtered_vs: numpy.ndarray, the corresponding values with NaN values removed\n    \"\"\"\n    # Check for NaN values in the values array\n    valid_indices = ~np.isnan(vs)\n    \n    # Filter both time series and values where values are not NaN\n    filtered_ts = ts[valid_indices]\n    filtered_vs = vs[valid_indices]\n    \n    return filtered_ts, filtered_vs\n```\n\nThis function uses NumPy's capabilities to identify NaN values in the `vs` array and filters out these indices from both `ts` and `vs`. The `~` operator is used to invert the boolean array produced by `np.isnan(vs)`, thus selecting only the indices where `vs` is not NaN. The filtered arrays are then returned.",
  "canonical_solution": "  \n  good_vs = np.logical_not(np.isnan(vs))\n  return ts[good_vs], vs[good_vs]\n\n\n",
  "unprocess_testcases": "ts = np.array([1, 2, 3, 4, 5])\nvs = np.array([1.1, 2.2, 3.3, 4.4, 5.5])\nassert((SOLUTION_SIGNATURE(ts, vs)[0] == filter_nan(ts, vs)[0]).all())\nassert((SOLUTION_SIGNATURE(ts, vs)[1] == filter_nan(ts, vs)[1]).all())\n\nts = np.array([1, 2, 3, 4, 5])\nvs = np.array([np.nan, np.nan, np.nan, np.nan, np.nan])\nassert((SOLUTION_SIGNATURE(ts, vs)[0] == filter_nan(ts, vs)[0]).all())\nassert((SOLUTION_SIGNATURE(ts, vs)[1] == filter_nan(ts, vs)[1]).all())\n\nts = np.array([1, 2, 3, 4, 5])\nvs = np.array([1.1, np.nan, 3.3, np.nan, 5.5])\nassert((SOLUTION_SIGNATURE(ts, vs)[0] == filter_nan(ts, vs)[0]).all())\nassert((SOLUTION_SIGNATURE(ts, vs)[1] == filter_nan(ts, vs)[1]).all())\n"
 },
 {
  "task_id": "RWPB/6",
  "prompt": "def format_seconds(seconds: float) -> str:\n    \"\"\"Converts a float number (seconds) to a string in the format:\n\n    - HH:MM:SS.SSS when greater than or equal to one hour\n    - MM:SS.SSS when less than one hour\n\n    Args:\n        seconds: The float number to convert (in seconds).\n\n    Returns:\n        str: The formatted string representation of the time.\n    \"\"\"\n",
  "solution": "Certainly! Here's the completed function `format_seconds` that formats the given number of seconds into the specified string format:\n\n```python\ndef format_seconds(seconds: float) -> str:\n    \"\"\"Converts a float number (seconds) to a string in the format:\n\n    - HH:MM:SS.SSS when greater than or equal to one hour\n    - MM:SS.SSS when less than one hour\n\n    Args:\n        seconds: The float number to convert (in seconds).\n\n    Returns:\n        str: The formatted string representation of the time.\n    \"\"\"\n    hours = int(seconds // 3600)\n    minutes = int((seconds % 3600) // 60)\n    remaining_seconds = seconds % 60\n\n    if hours > 0:\n        return f\"{hours:02}:{minutes:02}:{remaining_seconds:06.3f}\"\n    else:\n        return f\"{minutes:02}:{remaining_seconds:06.3f}\"\n```\n\nThis function first calculates the number of hours, minutes, and the remaining seconds. It then formats the time accordingly based on whether the total time is greater than or equal to one hour. The format strings ensure that hours and minutes are always displayed as two digits, and seconds are displayed with three decimal places, padded as necessary.",
  "canonical_solution": "    \n    hours = int(seconds // 3600)  # Extract whole hours\n    minutes = int((seconds % 3600) // 60)  # Extract whole minutes\n    seconds = seconds % 60  # Extract remaining seconds\n\n    # Use f-strings for clean formatting\n    if hours > 0:\n        return f\"{hours:02d}:{minutes:02d}:{seconds:06.3f}\"\n    else:\n        return f\"{minutes:02d}:{seconds:06.3f}\"\n    \n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(90.123) == format_seconds(90.123))\nassert(SOLUTION_SIGNATURE(3600.0) == format_seconds(3600.0))\nassert(SOLUTION_SIGNATURE(5436.789) == format_seconds(5436.789))"
 },
 {
  "task_id": "RWPB/99",
  "prompt": "import math\n\ndef focal2fov(focal, pixels):\n    \"\"\"\n    Converts focal length to field of view (FOV).\n    If focal is zero, the code should return KeyError\n\n    Args:\n    focal (float): Focal length.\n    pixels (int): Number of pixels.\n\n    Returns:\n    float: Field of view in radians.\n    \"\"\"\n",
  "solution": "Certainly! Below is the completed function that converts the focal length to the field of view (FOV) in radians. The function uses the formula for calculating the FOV from the focal length and sensor size (in pixels), assuming a standard sensor width per pixel. The function raises a `KeyError` if the focal length is zero, as specified.\n\n```python\nimport math\n\ndef focal2fov(focal, pixels):\n    \"\"\"\n    Converts focal length to field of view (FOV).\n    If focal is zero, the code should return KeyError\n\n    Args:\n    focal (float): Focal length.\n    pixels (int): Number of pixels.\n\n    Returns:\n    float: Field of view in radians.\n    \"\"\"\n    if focal == 0:\n        raise KeyError(\"Focal length cannot be zero.\")\n    \n    # Assuming a standard sensor width per pixel (e.g., 0.01mm per pixel)\n    # This is a simplification and may vary depending on the actual camera sensor specifications.\n    pixel_size = 0.01  # mm per pixel\n    sensor_width = pixels * pixel_size  # total width of the sensor in mm\n\n    # Calculate the field of view using the formula: 2 * atan((sensor width / 2) / focal length)\n    fov = 2 * math.atan((sensor_width / 2) / focal)\n    \n    return fov\n```\n\nThis function assumes a pixel size, which is a simplification. In real-world applications, you would need the actual sensor dimensions or pixel pitch to accurately compute the FOV. The `atan` function from the `math` module is used to compute the angle in radians.",
  "canonical_solution": "    \n    if int(focal) == 0:\n        return KeyError\n    return 2 * math.atan(pixels / (2 * focal))\n\n",
  "unprocess_testcases": "\nfocal = 49.1\npixels = 50000\nassert(SOLUTION_SIGNATURE(focal, pixels) == focal2fov(focal, pixels))\n\nfocal = 50.0\npixels = 1920\nassert(SOLUTION_SIGNATURE(focal, pixels) == focal2fov(focal, pixels))\n\nfocal = 1.0\npixels = 1920\nassert(SOLUTION_SIGNATURE(focal, pixels) == focal2fov(focal, pixels))\n\nfocal = 0\npixels = 2048\nassert(SOLUTION_SIGNATURE(focal, pixels) == focal2fov(focal, pixels))\n"
 },
 {
  "task_id": "RWPB/137",
  "prompt": "import numpy as np\nimport torch\nfrom sklearn.linear_model import LinearRegression\n\ndef fit_params(\n    x,\n    y,\n    fun,\n    a_range=(-10, 10),\n    b_range=(-10, 10),\n    grid_number=101,\n    iteration=3,\n    verbose=True,\n):\n    \"\"\"\n    fit a, b, c, d such that\n\n    .. math::\n        |y-(cf(ax+b)+d)|^2\n\n    is minimized. Both x and y are 1D array. Sweep a and b, find the best fitted model.\n\n    Args:\n    -----\n        x : 1D array\n            x values\n        y : 1D array\n            y values\n        fun : function\n            symbolic function\n        a_range : tuple\n            sweeping range of a\n        b_range : tuple\n            sweeping range of b\n        grid_num : int\n            number of steps along a and b\n        iteration : int\n            number of zooming in\n        verbose : bool\n            print extra information if True\n\n    Returns:\n    --------\n        a_best : float\n            best fitted a\n        b_best : float\n            best fitted b\n        c_best : float\n            best fitted c\n        d_best : float\n            best fitted d\n        r2_best : float\n            best r2 (coefficient of determination)\n\n    Example\n    -------\n    >>> num = 100\n    >>> x = torch.linspace(-1,1,steps=num)\n    >>> noises = torch.normal(0,1,(num,)) * 0.02\n    >>> y = 5.0*torch.sin(3.0*x + 2.0) + 0.7 + noises\n    >>> fit_params(x, y, torch.sin)\n    r2 is 0.9999727010726929\n    (tensor([2.9982, 1.9996, 5.0053, 0.7011]), tensor(1.0000))\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the provided Python function `fit_params` which fits parameters `a`, `b`, `c`, and `d` to minimize the squared error between `y` and the model `cf(ax+b)+d` using a grid search approach followed by iterative refinement:\n\n```python\nimport numpy as np\nimport torch\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ndef fit_params(\n    x,\n    y,\n    fun,\n    a_range=(-10, 10),\n    b_range=(-10, 10),\n    grid_number=101,\n    iteration=3,\n    verbose=True,\n):\n    x = x.numpy()  # Ensure x is a numpy array for compatibility with sklearn\n    y = y.numpy()  # Ensure y is a numpy array for compatibility with sklearn\n\n    a_values = np.linspace(a_range[0], a_range[1], grid_number)\n    b_values = np.linspace(b_range[0], b_range[1], grid_number)\n\n    best_r2 = -np.inf\n    best_params = (0, 0, 0, 0)\n\n    for a in a_values:\n        for b in b_values:\n            transformed_x = fun(a * x + b)\n\n            # Fit linear model to transformed_x\n            model = LinearRegression().fit(transformed_x.reshape(-1, 1), y)\n            c = model.coef_[0]\n            d = model.intercept_\n\n            # Calculate predictions and r2 score\n            predictions = model.predict(transformed_x.reshape(-1, 1))\n            r2 = r2_score(y, predictions)\n\n            # Update best parameters if current r2 is better\n            if r2 > best_r2:\n                best_r2 = r2\n                best_params = (a, b, c, d)\n\n                if verbose:\n                    print(f\"New best r2: {best_r2} with params a: {a}, b: {b}, c: {c}, d: {d}\")\n\n    a_best, b_best, c_best, d_best = best_params\n\n    # Refinement loop\n    for _ in range(iteration):\n        a_range = (a_best - (a_range[1] - a_range[0]) / grid_number, a_best + (a_range[1] - a_range[0]) / grid_number)\n        b_range = (b_best - (b_range[1] - b_range[0]) / grid_number, b_best + (b_range[1] - b_range[0]) / grid_number)\n        a_values = np.linspace(a_range[0], a_range[1], grid_number)\n        b_values = np.linspace(b_range[0], b_range[1], grid_number)\n\n        for a in a_values:\n            for b in b_values:\n                transformed_x = fun(a * x + b)\n                model = LinearRegression().fit(transformed_x.reshape(-1, 1), y)\n                c = model.coef_[0]\n                d = model.intercept_\n                predictions = model.predict(transformed_x.reshape(-1, 1))\n                r2 = r2_score(y, predictions)\n\n                if r2 > best_r2:\n                    best_r2 = r2\n                    best_params = (a, b, c, d)\n\n                    if verbose:\n                        print(f\"Refined best r2: {best_r2} with params a: {a}, b: {b}, c: {c}, d: {d}\")\n\n        a_best, b_best, c_best, d_best = best_params\n\n    return a_best, b_best, c_best, d_best, best_r2\n\n# Example usage:\n# num = 100\n# x = torch.linspace(-1, 1, steps=num)\n# noises = torch.normal(0, 1, (num,)) * 0.02\n# y = 5.0 * torch.sin(3.0 * x + 2.0) + 0.7 + noises\n# fit_params(x, y, torch.sin)\n```\n\nThis function performs a grid search over the parameters `a` and `b`, fitting a linear model to the transformed data `cf(ax+b)+d` and then iteratively refines the search around the best parameters found. It uses the `LinearRegression` model from `sklearn` and computes the coefficient of determination (R²) to evaluate the fit. The function returns the best parameters and the best R² score.",
  "canonical_solution": "    \n    # fit a, b, c, d such that y=c*fun(a*x+b)+d; both x and y are 1D array.\n    # sweep a and b, choose the best fitted model\n    for _ in range(iteration):\n        a_ = torch.linspace(a_range[0], a_range[1], steps=grid_number)\n        b_ = torch.linspace(b_range[0], b_range[1], steps=grid_number)\n        a_grid, b_grid = torch.meshgrid(a_, b_, indexing=\"ij\")\n        post_fun = fun(\n            a_grid[None, :, :] * x[:, None, None] + b_grid[None, :, :]\n        )\n        x_mean = torch.mean(post_fun, dim=[0], keepdim=True)\n        y_mean = torch.mean(y, dim=[0], keepdim=True)\n        numerator = (\n            torch.sum((post_fun - x_mean) * (y - y_mean)[:, None, None], dim=0)\n            ** 2\n        )\n        denominator = torch.sum((post_fun - x_mean) ** 2, dim=0) * torch.sum(\n            (y - y_mean)[:, None, None] ** 2, dim=0\n        )\n        r2 = numerator / (denominator + 1e-4)\n        r2 = torch.nan_to_num(r2)\n\n        best_id = torch.argmax(r2)\n        a_id, b_id = (\n            torch.div(best_id, grid_number, rounding_mode=\"floor\"),\n            best_id % grid_number,\n        )\n\n        if (\n            a_id == 0\n            or a_id == grid_number - 1\n            or b_id == 0\n            or b_id == grid_number - 1\n        ):\n            if _ == 0 and verbose:\n                pass\n            if a_id == 0:\n                a_arange = [a_[0], a_[1]]  # noqa\n            if a_id == grid_number - 1:\n                a_arange = [a_[-2], a_[-1]]  # noqa\n            if b_id == 0:\n                b_arange = [b_[0], b_[1]]  # noqa\n            if b_id == grid_number - 1:\n                b_arange = [b_[-2], b_[-1]]  # noqa\n\n        else:\n            a_range = [a_[a_id - 1], a_[a_id + 1]]\n            b_range = [b_[b_id - 1], b_[b_id + 1]]\n\n    a_best = a_[a_id]\n    b_best = b_[b_id]\n    post_fun = fun(a_best * x + b_best)\n    r2_best = r2[a_id, b_id]\n\n\n    post_fun = torch.nan_to_num(post_fun)\n    reg = LinearRegression().fit(\n        post_fun[:, None].detach().numpy(), y.detach().numpy()\n    )\n    c_best = torch.from_numpy(reg.coef_)[0]\n    d_best = torch.from_numpy(np.array(reg.intercept_))\n    return torch.stack([a_best, b_best, c_best, d_best]), r2_best\n\n\n\n\n",
  "unprocess_testcases": "x = torch.linspace(-1, 1, steps=100)\ny = 5.0 * torch.sin(3.0 * x + 2.0) + 0.7 + torch.normal(0, 1, (100,)) * 0.02\nassert((SOLUTION_SIGNATURE(x, y, torch.sin)[0] == fit_params(x, y, torch.sin)[0]).all())\nassert((SOLUTION_SIGNATURE(x, y, torch.sin)[1] == fit_params(x, y, torch.sin)[1]).all())\n\n\nx = torch.linspace(-2, 2, steps=50)\ny = 4.0 * torch.cos(2.0 * x + 1.0) + 1.0 + torch.normal(0, 1, (50,)) * 0.05\nassert((SOLUTION_SIGNATURE(x, y, torch.cos, a_range=(-2, 2), b_range=(-2, 2), grid_number=5)[0] == fit_params(x, y, torch.cos, a_range=(-2, 2), b_range=(-2, 2), grid_number=5)[0]).all())\nassert((SOLUTION_SIGNATURE(x, y, torch.cos, a_range=(-2, 2), b_range=(-2, 2), grid_number=5)[1] == fit_params(x, y, torch.cos, a_range=(-2, 2), b_range=(-2, 2), grid_number=5)[1]).all())\n\nx = torch.linspace(-3, 3, steps=150)\ny = 2.0 * torch.tanh(0.5 * x + 3.0) + 0.5 + torch.normal(0, 1, (150,)) * 0.1\nassert((SOLUTION_SIGNATURE(x, y, torch.tanh, iteration=5)[0] == fit_params(x, y, torch.tanh, iteration=5)[0]).all())\nassert((SOLUTION_SIGNATURE(x, y, torch.tanh, iteration=5)[1] == fit_params(x, y, torch.tanh, iteration=5)[1]).all())\n"
 },
 {
  "task_id": "RWPB/103",
  "prompt": "MAX_VAL = (1 << 256) - 1\n\n\n\n# does not revert on overflow\ndef unsafeAdd(x, y):\n    \"\"\"\n    Add two integers, ignoring overflow.\n    Args:\n        - x: int, first operand\n        - y: int, second operand\n    Returns:\n        result: int, the sum of x and y, with overflow ignored\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `unsafeAdd` function which adds two integers and ignores overflow by using Python's inherent ability to handle arbitrarily large integers. However, to simulate overflow as per typical 256-bit arithmetic, we'll use modulo operation with `MAX_VAL + 1`:\n\n```python\nMAX_VAL = (1 << 256) - 1\n\ndef unsafeAdd(x, y):\n    \"\"\"\n    Add two integers, ignoring overflow.\n    Args:\n        - x: int, first operand\n        - y: int, second operand\n    Returns:\n        result: int, the sum of x and y, with overflow ignored\n    \"\"\"\n    result = (x + y) % (MAX_VAL + 1)\n    return result\n```\n\nThis function will add `x` and `y`, and then use the modulo operation to wrap around the result if it exceeds the maximum value for a 256-bit integer, effectively simulating an overflow scenario typical in fixed-size integer arithmetic.",
  "canonical_solution": "    \n    return (x + y) & MAX_VAL\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(10, 20) == unsafeAdd(10, 20))\nassert(SOLUTION_SIGNATURE(MAX_VAL - 5, 10) == unsafeAdd(MAX_VAL - 5, 10))\nassert(SOLUTION_SIGNATURE(MAX_VAL // 2, (MAX_VAL // 2) + 1) == unsafeAdd(MAX_VAL // 2, (MAX_VAL // 2) + 1))\n"
 },
 {
  "task_id": "RWPB/113",
  "prompt": "import torch\n\ndef _linspace(start: torch.Tensor, stop: torch.Tensor, num: int):\n    \"\"\"\n    Generate a sequence of evenly spaced values between start and stop tensors.\n\n    Args:\n        start (torch.Tensor): The starting values of the sequence.\n        stop (torch.Tensor): The stopping values of the sequence.\n        num (int): The number of values to generate.\n\n    Returns:\n        torch.Tensor: A tensor containing `num` evenly spaced values between `start` and `stop`.\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `_linspace` function using PyTorch:\n\n```python\nimport torch\n\ndef _linspace(start: torch.Tensor, stop: torch.Tensor, num: int) -> torch.Tensor:\n    \"\"\"\n    Generate a sequence of evenly spaced values between start and stop tensors.\n\n    Args:\n        start (torch.Tensor): The starting values of the sequence.\n        stop (torch.Tensor): The stopping values of the sequence.\n        num (int): The number of values to generate.\n\n    Returns:\n        torch.Tensor: A tensor containing `num` evenly spaced values between `start` and `stop`.\n    \"\"\"\n    # Ensure that start and stop are tensors of the same shape\n    if start.shape != stop.shape:\n        raise ValueError(\"Start and stop tensors must have the same shape\")\n\n    # Create a tensor of steps from 0 to 1 with `num` elements\n    steps = torch.linspace(0, 1, num, dtype=start.dtype, device=start.device)\n\n    # Broadcast the steps to match the shape of start and stop\n    # This involves adding new axes to steps so it can be broadcasted correctly\n    for _ in range(len(start.shape)):\n        steps = steps.unsqueeze(-1)\n\n    # Linear interpolation between start and stop for each element\n    result = start + steps * (stop - start)\n\n    return result\n```\n\nThis function first checks if the `start` and `stop` tensors have the same shape. It then generates a linear space from 0 to 1 with `num` points using `torch.linspace`. The steps are reshaped to match the dimensions of the `start` and `stop` tensors by adding new axes, allowing for broadcasting during the computation. Finally, it computes the linear interpolation between `start` and `stop` for each element, resulting in a tensor of shape matching `start` and `stop` but with an additional dimension for the `num` points.",
  "canonical_solution": "    \n    if num == 1:\n        return start.clone()\n\n    steps = torch.linspace(0, 1, num, dtype=start.dtype, device=start.device)\n\n    steps = steps.view(*([1] * (start.dim() - 1)), num)\n    out = start.unsqueeze(-1) + (stop - start).unsqueeze(-1) * steps\n    return out\n\n\n",
  "unprocess_testcases": "start = torch.tensor([0.])\nstop = torch.tensor([10.])\nnum = 5\nassert((SOLUTION_SIGNATURE(start, stop, num) == _linspace(start, stop, num)).all())\n\nstart = torch.tensor([1., 2., 3.])\nstop = torch.tensor([1., 2., 3.])\nnum = 1\nassert((SOLUTION_SIGNATURE(start, stop, num) == _linspace(start, stop, num)).all())\n\nstart = torch.tensor([0, 10], dtype=torch.int32)\nstop = torch.tensor([5, 15], dtype=torch.int32)\nnum = 3\nassert((SOLUTION_SIGNATURE(start, stop, num) == _linspace(start, stop, num)).all())\n\nstart = torch.tensor([5., 5., 5.])\nstop = torch.tensor([5., 5., 5.])\nnum = 3\nassert((SOLUTION_SIGNATURE(start, stop, num) == _linspace(start, stop, num)).all())\n\nstart = torch.tensor([[0., 0.], [10., 10.]])\nstop = torch.tensor([[5., 10.], [15., 20.]])\nnum = 4\nassert((SOLUTION_SIGNATURE(start, stop, num) == _linspace(start, stop, num)).all())\n"
 },
 {
  "task_id": "RWPB/127",
  "prompt": "import torch\n\ndef shift_dim(x, src_dim=-1, dest_dim=-1, make_contiguous=True):\n    \"\"\"\n    Shifts a specified dimension of a tensor to a new position.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        src_dim (int): The original position of the dimension to move. Negative values count from the end.\n        dest_dim (int): The destination position for the dimension. Negative values count from the end.\n        make_contiguous (bool): If True, returns a contiguous tensor. This can be necessary after a permute\n                                operation because permute can lead to non-contiguous memory layout, potentially\n                                affecting performance.\n\n    Returns:\n        torch.Tensor: A tensor with the specified dimension shifted to the new position.\n\n    Example:\n        # Assuming x is a tensor with shape (batch, channels, time, height, width)\n        # and we want to move the 'channels' dimension to the end:\n        shifted_tensor = shift_dim(x, 1, -1)\n        # shifted_tensor will have the shape (batch, time, height, width, channels)\n    \"\"\"\n",
  "solution": "    # Calculate the actual positive indices based on the input dimensions\n    num_dims = len(x.shape)\n    src_dim = src_dim % num_dims\n    dest_dim = dest_dim % num_dims\n\n    # Create a list of the new order of dimensions\n    dim_order = list(range(num_dims))\n    element = dim_order.pop(src_dim)\n    dim_order.insert(dest_dim, element)\n\n    # Permute the tensor to the new dimension order\n    x_permuted = x.permute(*dim_order)\n\n    # Make the tensor contiguous if required\n    if make_contiguous:\n        x_permuted = x_permuted.contiguous()\n\n    return x_permuted\n\n# Example usage:\n# x = torch.randn(10, 3, 100, 28, 28)  # Example tensor\n# shifted_x = shift_dim(x, 1, -1)  # Move the 'channels' dimension to the end\n# print(shifted_x.shape)  # Output should be: torch.Size([10, 100, 28, 28, 3])",
  "canonical_solution": "    \n    n_dims = len(x.shape)  # Number of dimensions in the input tensor\n\n    # Adjust src_dim and dest_dim to be positive if they are given as negative\n    if src_dim < 0:\n        src_dim += n_dims\n    if dest_dim < 0:\n        dest_dim += n_dims\n\n    # Ensure the source and destination dimensions are within the valid range\n    assert 0 <= src_dim < n_dims and 0 <= dest_dim < n_dims, \"Source and destination dimensions must be within the valid range\"\n\n    # Create a list of dimensions except for the source dimension\n    dims = list(range(n_dims))\n    del dims[src_dim]  # Remove the source dimension from the list\n\n    # Build a new permutation of dimensions\n    permutation = []\n    ctr = 0\n    for i in range(n_dims):\n        if i == dest_dim:\n            permutation.append(src_dim)  # Insert the source dimension at the destination index\n        else:\n            permutation.append(dims[ctr])  # Fill in the rest of dimensions\n            ctr += 1\n\n    # Permute the dimensions of the tensor according to the new order\n    x = x.permute(permutation)\n\n    # Make the tensor contiguous if required\n    if make_contiguous:\n        x = x.contiguous()\n\n    return x\n\n\n",
  "unprocess_testcases": "tensor = torch.randn(2, 3, 4, 5)\nassert((SOLUTION_SIGNATURE(tensor, src_dim=1, dest_dim=-1) == shift_dim(tensor, src_dim=1, dest_dim=-1)).all())\n\ntensor = torch.randn(3, 4, 5)\nassert((SOLUTION_SIGNATURE(tensor, src_dim=-3, dest_dim=0, make_contiguous=False) == shift_dim(tensor, src_dim=-3, dest_dim=0, make_contiguous=False)).all())\n\n\ntensor = torch.randn(1, 2, 3, 4, 5)\nassert((SOLUTION_SIGNATURE(tensor, src_dim=0, dest_dim=4, make_contiguous=True) == shift_dim(tensor, src_dim=0, dest_dim=4, make_contiguous=True)).all())\n"
 },
 {
  "task_id": "RWPB/89",
  "prompt": "import math\n\n\ndef format_ratio(ratio):\n    \"\"\"\n    Formats a ratio into a percentage string\n\n    Args:\n        - ratio: float, ratio to be formatted\n    Returns:\n        formatted_ratio: str, formatted ratio in percentage\n\n    Examples:\n        - input: 3.1245133412\n        - output: percentage:3.124\n    \"\"\"\n",
  "solution": "Certainly! Below is the completed function `format_ratio` that takes a float ratio and formats it into a percentage string with three decimal places:\n\n```python\nimport math\n\ndef format_ratio(ratio):\n    \"\"\"\n    Formats a ratio into a percentage string\n\n    Args:\n        - ratio: float, ratio to be formatted\n    Returns:\n        formatted_ratio: str, formatted ratio in percentage\n\n    Examples:\n        - input: 3.1245133412\n        - output: \"percentage:3.124\"\n    \"\"\"\n    formatted_ratio = f\"percentage:{ratio:.3f}\"\n    return formatted_ratio\n```\n\nThis function uses Python's string formatting to limit the decimal places to three. The `:.3f` in the format string specifies that the floating-point number should be formatted with three digits after the decimal point.",
  "canonical_solution": "    \n    percentage = ratio * 100\n    return f\"{percentage:3.3f}%\"\n\n\n",
  "unprocess_testcases": "assert(format(3.14) == format(3.14))\nassert(format(6.29) == format(6.29))\nassert(format(0) == format(0))\n"
 },
 {
  "task_id": "RWPB/2",
  "prompt": "import torch\nfrom scipy import special, pi\n\ndef logbessel_I_scipy(nu, z, check = True):\n\t'''\n\tPytorch version of scipy computation of modified Bessel functions \n\tof the 1st kind I(nu,z).\n\tParameters\n\t----------\n\tnu: positive int, float\n\t\tOrder of modified Bessel function of 1st kind.\n\tz: int/float or tensor, shape (N,) \n\t\tArgument of Bessel function.\n\tcheck: bool\n\t\tIf True, check if argument of log is non zero.\n\t\n\tReturn\n\t------\n\tresult: tensor, shape (N,)\n\t'''\n",
  "solution": "Certainly! Below is the completion of the provided Python function `logbessel_I_scipy` which computes the logarithm of the modified Bessel function of the first kind using `scipy.special.ive`, which is the exponentially scaled modified Bessel function. This function is particularly useful for large values of `z` to avoid numerical overflow. The function is designed to work with PyTorch tensors.\n\n```python\nimport torch\nfrom scipy import special, pi\n\ndef logbessel_I_scipy(nu, z, check=True):\n    '''\n    Pytorch version of scipy computation of modified Bessel functions \n    of the 1st kind I(nu,z).\n    Parameters\n    ----------\n    nu: positive int, float\n        Order of modified Bessel function of 1st kind.\n    z: int/float or tensor, shape (N,) \n        Argument of Bessel function.\n    check: bool\n        If True, check if argument of log is non zero.\n    \n    Return\n    ------\n    result: tensor, shape (N,)\n    '''\n    # Convert input to numpy if it is a tensor\n    if isinstance(z, torch.Tensor):\n        z = z.numpy()\n    \n    # Compute the exponentially scaled modified Bessel function\n    scaled_bessel = special.ive(nu, z)\n    \n    # Compute the log of the Bessel function\n    log_bessel = np.log(scaled_bessel) + z\n    \n    # Convert result back to PyTorch tensor\n    result = torch.from_numpy(log_bessel)\n    \n    # Check if the result contains any inf or -inf if required\n    if check:\n        if torch.any(torch.isinf(result)) or torch.any(torch.isnan(result)):\n            raise ValueError(\"Result contains inf or NaN.\")\n    \n    return result\n```\n\n### Explanation:\n1. **Input Conversion**: If the input `z` is a PyTorch tensor, it is converted to a NumPy array to be compatible with SciPy functions.\n2. **Bessel Function Computation**: The function `special.ive(nu, z)` computes the exponentially scaled modified Bessel function of the first kind. This helps in avoiding overflow issues for large values of `z`.\n3. **Logarithm Calculation**: The logarithm of the Bessel function is calculated by adding `z` to the logarithm of the scaled Bessel function. This step reverses the exponential scaling applied by `special.ive`.\n4. **Output Conversion**: The result is converted back to a PyTorch tensor.\n5. **Check for Validity**: Optionally, the function checks if the result contains infinite or NaN values, raising an error if such values are found.\n\nThis function now allows for the computation of the logarithm of the modified Bessel function of the first kind using PyTorch tensors, leveraging the stability of SciPy's `special.ive` for numerical computations.",
  "canonical_solution": "\n\tif not isinstance(z, torch.Tensor):\n\t\tz = torch.tensor(z)\n\tz = z.reshape(-1)\n\n\tresult = special.ive(nu, z)\n\tif check:\n\t\tassert len(result[ result == 0]) == 0\n\tresult = torch.log(result) + z\n\treturn result\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(nu=1, z=5.0) == logbessel_I_scipy(nu=1, z=5.0)).all())\nassert((SOLUTION_SIGNATURE(nu=0.5, z=torch.tensor([0.1, 1.0, 10.0])) == logbessel_I_scipy(nu=0.5, z=torch.tensor([0.1, 1.0, 10.0]))).all())\nassert((SOLUTION_SIGNATURE(nu=-0.0001, z=0.0001) == logbessel_I_scipy(nu=-0.0001, z=0.0001)).all())"
 },
 {
  "task_id": "RWPB/107",
  "prompt": "import torch\n\ndef pad_camera_extrinsics_4x4(extrinsics):\n    \"\"\"\n    Pads a given extrinsics matrix to a 4x4 matrix if it is not already.\n\n    Args:\n        extrinsics (torch.Tensor): Input extrinsics matrix of shape (..., 3, 4).\n\n    Returns:\n        torch.Tensor: Padded extrinsics matrix of shape (..., 4, 4).\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the function `pad_camera_extrinsics_4x4` that pads a given extrinsics matrix to a 4x4 matrix:\n\n```python\nimport torch\n\ndef pad_camera_extrinsics_4x4(extrinsics):\n    \"\"\"\n    Pads a given extrinsics matrix to a 4x4 matrix if it is not already.\n\n    Args:\n        extrinsics (torch.Tensor): Input extrinsics matrix of shape (..., 3, 4).\n\n    Returns:\n        torch.Tensor: Padded extrinsics matrix of shape (..., 4, 4).\n    \"\"\"\n    # Check if the last two dimensions are 3 and 4 respectively\n    if extrinsics.shape[-2:] != (3, 4):\n        raise ValueError(\"Input extrinsics matrix must have shape (..., 3, 4)\")\n\n    # Create a padding tensor of shape (..., 1, 4)\n    # The last row should be [0, 0, 0, 1] to represent an affine transformation\n    pad_row = torch.tensor([0, 0, 0, 1], dtype=extrinsics.dtype, device=extrinsics.device)\n    pad_row = pad_row.expand(*extrinsics.shape[:-2], 1, 4)  # Match the batch dimensions\n\n    # Concatenate the original extrinsics matrix with the padding row\n    padded_extrinsics = torch.cat([extrinsics, pad_row], dim=-2)\n\n    return padded_extrinsics\n```\n\nThis function first checks if the input tensor has the correct shape. It then creates a padding row `[0, 0, 0, 1]` which is expanded to match any batch dimensions present in the input tensor. Finally, it concatenates this row to the original extrinsics matrix along the appropriate dimension to form a 4x4 matrix.",
  "canonical_solution": "    \n    if extrinsics.shape[-2] == 4:\n        return extrinsics\n    padding = torch.tensor([[0, 0, 0, 1]]).to(extrinsics)\n    if extrinsics.ndim == 3:\n        padding = padding.unsqueeze(0).repeat(extrinsics.shape[0], 1, 1)\n    extrinsics = torch.cat([extrinsics, padding], dim=-2)\n    return extrinsics\n\n\n",
  "unprocess_testcases": "extrinsics = torch.rand((2, 4, 4))\nassert((SOLUTION_SIGNATURE(extrinsics) == pad_camera_extrinsics_4x4(extrinsics)).all())\n\nextrinsics = torch.rand((3, 3, 4))\nassert((SOLUTION_SIGNATURE(extrinsics) == pad_camera_extrinsics_4x4(extrinsics)).all())\n\nextrinsics = torch.rand((3, 4))\nassert((SOLUTION_SIGNATURE(extrinsics) == pad_camera_extrinsics_4x4(extrinsics)).all())\n\nextrinsics = torch.rand((2, 1, 4))\nassert((SOLUTION_SIGNATURE(extrinsics) == pad_camera_extrinsics_4x4(extrinsics)).all())\n"
 },
 {
  "task_id": "RWPB/133",
  "prompt": "from PIL import Image\n\ndef divide_to_patches(image, patch_size):\n    \"\"\"\n    Divides an image into patches of a specified size.\n\n    Args:\n        image (PIL.Image.Image): The input image.\n        patch_size (int): The size of each patch.\n\n    Returns:\n        list: A list of PIL.Image.Image objects representing the patches.\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `divide_to_patches` function that divides an image into square patches of a specified size. The function will handle cases where the image dimensions are not perfectly divisible by the patch size by ignoring the remaining border areas.\n\n```python\nfrom PIL import Image\n\ndef divide_to_patches(image, patch_size):\n    \"\"\"\n    Divides an image into patches of a specified size.\n\n    Args:\n        image (PIL.Image.Image): The input image.\n        patch_size (int): The size of each patch.\n\n    Returns:\n        list: A list of PIL.Image.Image objects representing the patches.\n    \"\"\"\n    width, height = image.size\n    patches = []\n\n    # Iterate over the image in increments of patch_size\n    for y in range(0, height, patch_size):\n        for x in range(0, width, patch_size):\n            # Define the box to crop the image\n            box = (x, y, x + patch_size, y + patch_size)\n            # Crop the image to the box\n            patch = image.crop(box)\n            # Append the patch to the list if it matches the patch size\n            if patch.size == (patch_size, patch_size):\n                patches.append(patch)\n\n    return patches\n```\n\nThis function uses the `crop` method of the PIL Image object to extract patches from the image. It iterates over the image in both horizontal and vertical directions in steps of `patch_size`. For each position, it defines a bounding box for the patch and crops the image accordingly. The cropped patch is then checked to ensure it matches the desired size before being added to the list of patches. This ensures that any incomplete patches at the edges of the image (if the dimensions are not perfectly divisible by `patch_size`) are not included in the result.",
  "canonical_solution": "    \n    patches = []\n    width, height = image.size\n    for i in range(0, height, patch_size):\n        for j in range(0, width, patch_size):\n            box = (j, i, j + patch_size, i + patch_size)\n            patch = image.crop(box)\n            patches.append(patch)\n\n    return patches\n\n\n",
  "unprocess_testcases": "img = Image.new('RGB', (256, 256))\npatch_size = 64\nassert(SOLUTION_SIGNATURE(img, patch_size) == divide_to_patches(img, patch_size))\n\nimg = Image.new('RGB', (300, 150))\npatch_size = 50\nassert(SOLUTION_SIGNATURE(img, patch_size) == divide_to_patches(img, patch_size))\n\nimg = Image.new('RGB', (30, 30))\npatch_size = 50\nassert(SOLUTION_SIGNATURE(img, patch_size) == divide_to_patches(img, patch_size))\n"
 },
 {
  "task_id": "RWPB/123",
  "prompt": "import torch\nimport torch.nn as nn\n\n\ndef fuse_deconv_and_bn(deconv, bn):\n    \"\"\"\n    Fuses a ConvTranspose2d layer with a BatchNorm2d layer into a single ConvTranspose2d layer.\n\n    Arguments:\n    deconv : torch.nn.ConvTranspose2d\n        The convolutional transpose layer to be fused.\n    bn : torch.nn.BatchNorm2d\n        The batch normalization layer to be fused.\n\n    Returns:\n    torch.nn.ConvTranspose2d\n        The resulting fused ConvTranspose2d layer.\n    \"\"\"\n",
  "solution": "```python\n    # Extract the parameters from the deconvolution layer\n    deconv_weight = deconv.weight\n    deconv_bias = deconv.bias if deconv.bias is not None else torch.zeros(bn.num_features)\n\n    # Extract the parameters from the batch normalization layer\n    bn_mean = bn.running_mean\n    bn_var = bn.running_var\n    bn_weight = bn.weight\n    bn_bias = bn.bias\n    bn_eps = bn.eps\n\n    # Calculate the scale and shift to fuse the batch normalization\n    bn_scale = bn_weight / torch.sqrt(bn_var + bn_eps)\n    bn_shift = bn_bias - bn_weight * bn_mean / torch.sqrt(bn_var + bn_eps)\n\n    # Adjust the deconvolution parameters using the batch normalization parameters\n    fused_weight = deconv_weight * bn_scale.reshape(-1, 1, 1, 1)\n    fused_bias = deconv_bias * bn_scale + bn_shift\n\n    # Create a new ConvTranspose2d layer with the fused parameters\n    fused_deconv = nn.ConvTranspose2d(\n        in_channels=deconv.in_channels,\n        out_channels=deconv.out_channels,\n        kernel_size=deconv.kernel_size,\n        stride=deconv.stride,\n        padding=deconv.padding,\n        output_padding=deconv.output_padding,\n        groups=deconv.groups,\n        bias=True,\n        dilation=deconv.dilation,\n        padding_mode=deconv.padding_mode\n    )\n\n    # Set the fused parameters to the new layer\n    fused_deconv.weight = nn.Parameter(fused_weight)\n    fused_deconv.bias = nn.Parameter(fused_bias)\n\n    return fused_deconv\n```",
  "canonical_solution": "    \n    # Create a new ConvTranspose2d layer with the same parameters as the original but with bias enabled\n    fuseddconv = nn.ConvTranspose2d(\n        in_channels=deconv.in_channels,\n        out_channels=deconv.out_channels,\n        kernel_size=deconv.kernel_size,\n        stride=deconv.stride,\n        padding=deconv.padding,\n        output_padding=deconv.output_padding,\n        dilation=deconv.dilation,\n        groups=deconv.groups,\n        bias=True  # Always true to accommodate the fused bias\n    ).requires_grad_(False).to(deconv.weight.device)  # Disable grad and move to the same device\n\n    # Prepare the weights for fusion\n    w_deconv = deconv.weight.clone().view(deconv.out_channels, -1)\n    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))\n    fuseddconv.weight.copy_(\n        torch.mm(w_bn, w_deconv).view(fuseddconv.weight.shape))  # Apply the BN transformation to weights\n\n    # Prepare and fuse the biases\n    b_conv = torch.zeros(deconv.weight.shape[1], device=deconv.weight.device) if deconv.bias is None else deconv.bias\n    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n    fuseddconv.bias.copy_(\n        torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)  # Combine biases from BN and deconv layers\n\n    return fuseddconv\n\n\n",
  "unprocess_testcases": "a = torch.randn(20, 3, 50, 100)\ndeconv = nn.ConvTranspose2d(3, 3, 3, stride=2, padding=1, output_padding=1, dilation=1, groups=1, bias=None)\nbn = nn.BatchNorm2d(3)\nassert((SOLUTION_SIGNATURE(deconv, bn)(a) == fuse_deconv_and_bn(deconv, bn)(a)).all())\n\ninitial_bias = torch.randn(3)\ndeconv = nn.ConvTranspose2d(3, 3, 3, stride=2, padding=1, output_padding=1, dilation=1, groups=1, bias=True)\ndeconv.bias.data = initial_bias.clone()\nbn = nn.BatchNorm2d(3)\nassert((SOLUTION_SIGNATURE(deconv, bn)(a) == fuse_deconv_and_bn(deconv, bn)(a)).all())\n\ndeconv = nn.ConvTranspose2d(3, 4, (5, 5), (2, 2), (1, 1), output_padding=(1, 1), dilation=(2, 2))\nbn = nn.BatchNorm2d(4)\nassert((SOLUTION_SIGNATURE(deconv, bn)(a) == fuse_deconv_and_bn(deconv, bn)(a)).all())\n\n"
 },
 {
  "task_id": "RWPB/117",
  "prompt": "import os.path\n\ndef path_spliter(path: str):\n    \"\"\"\n    Splits the given path into its components.\n\n    Args:\n        path (str): The path to split.\n\n    Returns:\n        list: A list of path components.\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `path_spliter` function which uses the `os.path` module to split the given path into its components:\n\n```python\nimport os.path\n\ndef path_spliter(path: str):\n    \"\"\"\n    Splits the given path into its components.\n\n    Args:\n        path (str): The path to split.\n\n    Returns:\n        list: A list of path components.\n    \"\"\"\n    components = []\n    while True:\n        parts = os.path.split(path)\n        if parts[0] == path:  # if split didn't do anything, we're done\n            if path:  # if there's something left in the path, add it\n                components.append(path)\n            break\n        else:\n            path = parts[0]\n            if parts[1]:  # if the right part of the split is not empty, add it to components\n                components.append(parts[1])\n\n    components.reverse()  # reverse to get the correct order\n    return components\n```\n\nThis function uses a loop to repeatedly call `os.path.split()` on the path. Each call to `os.path.split()` separates the path into two parts: everything before the last path separator, and everything after. The function collects the \"after\" parts into a list, and then continues to split the \"before\" part until it can't be split any further. Finally, it reverses the list of components to restore them to the original order from the input path.",
  "canonical_solution": "    \n    res = []\n    if path == '':\n        return []\n\n    while path != '' and path != '/':\n        res.append(os.path.split(path)[1])\n        path = os.path.split(path)[0]\n    if path != '':\n        res.append(path)\n    \n\n    return res[::-1]\n\n\n",
  "unprocess_testcases": "str = '/usr/bin/python'\nassert(SOLUTION_SIGNATURE(str) == path_spliter(str))\n\nstr = '../folder/subfolder/file.txt'\nassert(SOLUTION_SIGNATURE(str) == path_spliter(str))\n\nstr = './../xx/yy/./../yty/file.txt'\nassert(SOLUTION_SIGNATURE(str) == path_spliter(str))\n\nstr = ''\nassert(SOLUTION_SIGNATURE(str) == path_spliter(str))\n"
 },
 {
  "task_id": "RWPB/3",
  "prompt": "import numpy as np\n\ndef get_focalLength_from_fieldOfView(fov=60, img_size=512):\n    \"\"\"\n    Compute the focal length of the camera lens by assuming a certain FOV for the entire image\n    Args:\n        - fov: float, expressed in degree\n        - img_size: int\n    Return:\n        focal: float\n    \"\"\"\n",
  "solution": "Certainly! To compute the focal length from the field of view (FOV) and the image size, you can use the formula derived from the pinhole camera model. The formula relates the field of view, the image sensor size (or in this case, the size of the image in pixels), and the focal length. The formula is:\n\n\\[ \\text{focal\\_length} = \\frac{\\text{img\\_size}}{2 \\times \\tan(\\frac{\\text{fov}}{2} \\times \\frac{\\pi}{180})} \\]\n\nHere's the complete function:\n\n```python\nimport numpy as np\n\ndef get_focalLength_from_fieldOfView(fov=60, img_size=512):\n    \"\"\"\n    Compute the focal length of the camera lens by assuming a certain FOV for the entire image\n    Args:\n        - fov: float, expressed in degree\n        - img_size: int\n    Return:\n        focal: float\n    \"\"\"\n    # Convert FOV from degrees to radians\n    fov_rad = np.deg2rad(fov)\n    \n    # Calculate the focal length using the formula\n    focal_length = img_size / (2 * np.tan(fov_rad / 2))\n    \n    return focal_length\n\n# Example usage:\nfocal_length = get_focalLength_from_fieldOfView(60, 512)\nprint(f\"Calculated Focal Length: {focal_length}\")\n```\n\nThis function first converts the field of view from degrees to radians using `np.deg2rad`. It then calculates the focal length using the given formula. The result is returned as a floating-point number, representing the focal length in terms of pixel units.",
  "canonical_solution": "    \n    focal = img_size / (2 * np.tan(np.radians(fov) /2))\n    return focal\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(fov = 60, img_size = 512) == get_focalLength_from_fieldOfView(fov = 60, img_size = 512))\nassert(SOLUTION_SIGNATURE(fov = 1, img_size = 512) == get_focalLength_from_fieldOfView(fov = 1, img_size = 512))\nassert(SOLUTION_SIGNATURE(fov = 120, img_size = 2048) == get_focalLength_from_fieldOfView(fov = 120, img_size = 2048))"
 },
 {
  "task_id": "RWPB/106",
  "prompt": "import numpy as np\n\ndef interpolate_features(features,\n                         input_rate,\n                         output_rate,\n                         output_len):\n    \"\"\"\n    Interpolate DeepSpeech features.\n\n    Parameters\n    ----------\n    features : np.array\n        DeepSpeech features.\n    input_rate : int\n        input rate (FPS).\n    output_rate : int\n        Output rate (FPS).\n    output_len : int\n        Output data length.\n\n    Returns\n    -------\n    np.array\n        Interpolated data.\n    \"\"\"\n",
  "solution": "Certainly! To complete the function for interpolating features, we can use the `np.interp` function for 1D interpolation or a more general approach using `scipy.interpolate.interp1d` for handling potentially multi-dimensional feature arrays. Below, I'll provide a solution using `scipy.interpolate.interp1d` which can handle both 1D and multi-dimensional data:\n\n```python\nimport numpy as np\nfrom scipy.interpolate import interp1d\n\ndef interpolate_features(features, input_rate, output_rate, output_len):\n    \"\"\"\n    Interpolate DeepSpeech features.\n\n    Parameters\n    ----------\n    features : np.array\n        DeepSpeech features.\n    input_rate : int\n        input rate (FPS).\n    output_rate : int\n        Output rate (FPS).\n    output_len : int\n        Output data length.\n\n    Returns\n    -------\n    np.array\n        Interpolated data.\n    \"\"\"\n    # Calculate the total duration based on the input features and rate\n    total_duration = features.shape[0] / input_rate\n    \n    # Create a time array for the original data\n    original_times = np.linspace(0, total_duration, features.shape[0])\n    \n    # Create a new time array for the interpolated data\n    new_times = np.linspace(0, total_duration, int(output_rate * total_duration))\n    \n    # Check if the features are multi-dimensional\n    if features.ndim > 1:\n        # Interpolate each feature dimension separately\n        interpolated_features = np.zeros((new_times.size, features.shape[1]))\n        for i in range(features.shape[1]):\n            interp_func = interp1d(original_times, features[:, i], kind='linear', fill_value=\"extrapolate\")\n            interpolated_features[:, i] = interp_func(new_times)\n    else:\n        # Interpolate the 1D feature array\n        interp_func = interp1d(original_times, features, kind='linear', fill_value=\"extrapolate\")\n        interpolated_features = interp_func(new_times)\n    \n    # If the output length is specified and different from the calculated length, adjust the output\n    if output_len is not None and output_len != len(interpolated_features):\n        if output_len < len(interpolated_features):\n            interpolated_features = interpolated_features[:output_len]\n        else:\n            # Extend the array with the last value if output_len is greater\n            last_val = interpolated_features[-1]\n            extra_len = output_len - len(interpolated_features)\n            if features.ndim > 1:\n                extra_features = np.tile(last_val, (extra_len, 1))\n            else:\n                extra_features = np.full((extra_len,), last_val)\n            interpolated_features = np.concatenate((interpolated_features, extra_features))\n    \n    return interpolated_features\n```\n\nThis function first calculates the total duration of the input features based on the input rate. It then creates a time array for both the original and the new interpolated data. Depending on whether the features are multi-dimensional, it applies interpolation to each dimension separately. Finally, it adjusts the length of the output array to match the desired `output_len` if necessary.",
  "canonical_solution": "    \n    input_len = features.shape[0]\n    num_features = features.shape[1]\n    input_timestamps = np.arange(input_len) / float(input_rate)\n    output_timestamps = np.arange(output_len) / float(output_rate)\n    output_features = np.zeros((output_len, num_features))\n    for feature_idx in range(num_features):\n        output_features[:, feature_idx] = np.interp(\n            x=output_timestamps,\n            xp=input_timestamps,\n            fp=features[:, feature_idx])\n    return output_features\n\n\n",
  "unprocess_testcases": "features = np.random.rand(10, 2)\ninput_rate = 2\noutput_rate = 2\noutput_len = 5\nassert((SOLUTION_SIGNATURE(features, input_rate, output_rate, output_len) == interpolate_features(features, input_rate, output_rate, output_len)).all())\n\nfeatures = np.random.rand(10, 3)\ninput_rate = 5\noutput_rate = 5\noutput_len = 0\nassert((SOLUTION_SIGNATURE(features, input_rate, output_rate, output_len) == interpolate_features(features, input_rate, output_rate, output_len)).all())\n\nfeatures = np.random.rand(15, 4)\ninput_rate = 3\noutput_rate = 4\noutput_len = 10\nassert((SOLUTION_SIGNATURE(features, input_rate, output_rate, output_len) == interpolate_features(features, input_rate, output_rate, output_len)).all())\n"
 },
 {
  "task_id": "RWPB/132",
  "prompt": "import torch\n\n\ndef B_batch(x, grid, k=0, extend=True, device=\"cpu\"):\n    \"\"\"\n    evaludate x on B-spline bases\n\n    Args:\n    -----\n        x : 2D torch.tensor\n            inputs, shape (number of splines, number of samples)\n        grid : 2D torch.tensor\n            grids, shape (number of splines, number of grid points)\n        k : int\n            the piecewise polynomial order of splines.\n        extend : bool\n            If True, k points are extended on both ends. If False, no extension (zero boundary condition). Default: True\n        device : str\n            devicde\n\n    Returns:\n    --------\n        spline values : 3D torch.tensor\n            shape (number of splines, number of B-spline bases (coeffcients), number of samples). The numbef of B-spline bases = number of grid points + k - 1.\n\n    Example\n    -------\n    >>> num_spline = 5\n    >>> num_sample = 100\n    >>> num_grid_interval = 10\n    >>> k = 3\n    >>> x = torch.normal(0,1,size=(num_spline, num_sample))\n    >>> grids = torch.einsum('i,j->ij', torch.ones(num_spline,), torch.linspace(-1,1,steps=num_grid_interval+1))\n    >>> B_batch(x, grids, k=k).shape\n    torch.Size([5, 13, 100])\n    \"\"\"\n",
  "solution": "Certainly! Below is the completed Python function `B_batch` that evaluates input data `x` on B-spline bases using the provided grid and spline order `k`. The function uses PyTorch for computations and supports running on different devices (e.g., CPU or GPU).\n\n```python\nimport torch\n\ndef B_batch(x, grid, k=0, extend=True, device=\"cpu\"):\n    \"\"\"\n    Evaluate x on B-spline bases\n\n    Args:\n    -----\n        x : 2D torch.tensor\n            inputs, shape (number of splines, number of samples)\n        grid : 2D torch.tensor\n            grids, shape (number of splines, number of grid points)\n        k : int\n            the piecewise polynomial order of splines.\n        extend : bool\n            If True, k points are extended on both ends. If False, no extension (zero boundary condition). Default: True\n        device : str\n            device\n\n    Returns:\n    --------\n        spline values : 3D torch.tensor\n            shape (number of splines, number of B-spline bases (coefficients), number of samples). The number of B-spline bases = number of grid points + k - 1.\n    \"\"\"\n    num_splines, num_samples = x.shape\n    _, num_grid_points = grid.shape\n    num_bases = num_grid_points + k - 1\n\n    # Move data to the specified device\n    x = x.to(device)\n    grid = grid.to(device)\n\n    # Extend the grid if required\n    if extend:\n        grid_extension = torch.zeros(num_splines, k // 2, device=device)\n        grid = torch.cat([grid_extension, grid, grid_extension], dim=1)\n\n    # Prepare the output tensor\n    spline_values = torch.zeros(num_splines, num_bases, num_samples, device=device)\n\n    # Calculate the B-spline basis functions\n    for i in range(num_splines):\n        for j in range(num_bases):\n            # Define the local support of the B-spline basis\n            t_min = grid[i, max(j - k, 0)]\n            t_max = grid[i, min(j + 1, num_grid_points - 1)]\n            # Evaluate the B-spline basis function\n            spline_values[i, j, :] = ((x[i, :] >= t_min) & (x[i, :] < t_max)).float()\n\n    return spline_values\n\n# Example usage\nif __name__ == \"__main__\":\n    num_spline = 5\n    num_sample = 100\n    num_grid_interval = 10\n    k = 3\n    x = torch.normal(0, 1, size=(num_spline, num_sample))\n    grids = torch.einsum('i,j->ij', torch.ones(num_spline,), torch.linspace(-1, 1, steps=num_grid_interval + 1))\n    result = B_batch(x, grids, k=k)\n    print(result.shape)\n```\n\nThis function initializes a tensor for the spline values and iterates over each spline and each basis function to compute the B-spline basis functions within their local support. The example at the end demonstrates how to use the function and prints the expected shape of the output tensor. Adjustments might be needed based on the specific definition of B-splines and boundary conditions.",
  "canonical_solution": "\n    # x shape: (size, x); grid shape: (size, grid)\n    def extend_grid(grid, k_extend=0):\n        # pad k to left and right\n        # grid shape: (batch, grid)\n        h = (grid[:, [-1]] - grid[:, [0]]) / (grid.shape[1] - 1)\n\n        for i in range(k_extend):\n            grid = torch.cat([grid[:, [0]] - h, grid], dim=1)\n            grid = torch.cat([grid, grid[:, [-1]] + h], dim=1)\n        grid = grid.to(device)\n        return grid\n\n    if extend:\n        grid = extend_grid(grid, k_extend=k)\n\n    grid = grid.unsqueeze(dim=2).to(device)\n    x = x.unsqueeze(dim=1).to(device)\n\n    if k == 0:\n        value = (x >= grid[:, :-1]) * (x < grid[:, 1:])\n    else:\n        B_km1 = B_batch(\n            x[:, 0], grid=grid[:, :, 0], k=k - 1, extend=False, device=device\n        )\n        value = (x - grid[:, : -(k + 1)]) / (\n            grid[:, k:-1] - grid[:, : -(k + 1)]\n        ) * B_km1[:, :-1] + (grid[:, k + 1 :] - x) / (\n            grid[:, k + 1 :] - grid[:, 1:(-k)]\n        ) * B_km1[\n            :, 1:\n        ]\n    return value\n\n\n",
  "unprocess_testcases": "num_splines = 3\nnum_samples = 50\nnum_grid_points = 8\nk = 2\nx = torch.normal(0, 1, size=(num_splines, num_samples))\ngrids = torch.einsum('i,j->ij', torch.ones(num_splines,), torch.linspace(-1, 1, steps=num_grid_points))\nassert((SOLUTION_SIGNATURE(x, grids, k=k) == B_batch(x, grids, k=k)).all())\n\nnum_splines = 2\nnum_samples = 30\nnum_grid_points = 5\nk = 0\nx = torch.linspace(-1, 1, steps=num_samples).repeat(num_splines, 1)\ngrids = torch.einsum('i,j->ij', torch.ones(num_splines,), torch.linspace(-1, 1, steps=num_grid_points))\nassert((SOLUTION_SIGNATURE(x, grids, k=k, extend=False) == B_batch(x, grids, k=k, extend=False)).all())\n\n\nnum_splines = 4\nnum_samples = 100\nnum_grid_points = 10\nk = 4\nx = torch.cat([\n    torch.linspace(-1.5, -1, steps=num_samples // 2),\n    torch.linspace(1, 1.5, steps=num_samples // 2)\n]).repeat(num_splines, 1)\ngrids = torch.einsum('i,j->ij', torch.ones(num_splines,), torch.linspace(-1, 1, steps=num_grid_points))\nassert((SOLUTION_SIGNATURE(x, grids, k=k, extend=False) == B_batch(x, grids, k=k, extend=False)).all())\n"
 },
 {
  "task_id": "RWPB/122",
  "prompt": "import torch\n\ndef view_range(x, i, j, shape):\n    \"\"\"\n    Reshapes a portion of a tensor from dimension i (inclusive) to dimension j (exclusive) into the specified shape.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        i (int): The start dimension for reshaping, inclusive. Negative values count from the end.\n        j (int): The end dimension for reshaping, exclusive. If None, extends to the end of dimensions. Negative values count from the end.\n        shape (tuple): The target shape to apply between dimensions i and j.\n\n    Returns:\n        torch.Tensor: A tensor reshaped from dimension i to j according to the specified shape.\n\n    Example:\n        # Assuming x has shape (batch, thw, channels)\n        # To reshape 'thw' into (t, h, w):\n        reshaped_x = view_range(x, 1, 2, (t, h, w))\n        # reshaped_x will have the shape (batch, t, h, w, channels)\n    \"\"\"\n",
  "solution": "    # First, we need to handle the dimensions before i and after j\n    # We will slice the tensor into three parts: before i, between i and j, and after j\n    prefix_shape = x.shape[:i]\n    if j is None:\n        suffix_shape = ()\n        middle_shape = x.shape[i:]\n    else:\n        suffix_shape = x.shape[j:]\n        middle_shape = x.shape[i:j]\n\n    # Calculate the total number of elements in the middle part\n    middle_size = 1\n    for dim in middle_shape:\n        middle_size *= dim\n\n    # Check if the product of the dimensions in the new shape matches the number of elements in the middle part\n    new_shape_size = 1\n    for dim in shape:\n        new_shape_size *= dim\n\n    if middle_size != new_shape_size:\n        raise ValueError(\"The product of the new shape dimensions must match the product of the reshaped dimensions.\")\n\n    # Reshape the middle part of the tensor\n    reshaped_middle = x.view(*prefix_shape, *shape, *suffix_shape)\n\n    return reshaped_middle\n\n# Example usage:\n# x = torch.randn(10, 24, 3)  # Example tensor with shape (batch, thw, channels)\n# reshaped_x = view_range(x, 1, 2, (2, 3, 4))  # Reshape 'thw' into (2, 3, 4)\n# print(reshaped_x.shape)  # Output should be (10, 2, 3, 4, 3)",
  "canonical_solution": "    \n    shape = tuple(shape)  # Ensure the shape is a tuple, necessary for concatenation with other shapes\n\n    n_dims = len(x.shape)  # Total number of dimensions in the input tensor\n\n    # Adjust indices for negative values\n    if i < 0:\n        i += n_dims\n    if j is None:\n        j = n_dims\n    elif j < 0:\n        j += n_dims\n\n    # Ensure the indices are valid\n    assert 0 <= i < j <= n_dims, \"Indices i and j must define a valid range of dimensions\"\n\n    # Get the current shape of the tensor\n    x_shape = x.shape\n    # Construct the new shape by combining slices of the old shape with the new specified shape\n    target_shape = x_shape[:i] + shape + x_shape[j:]\n\n    # Reshape the tensor to the target shape\n    return x.view(target_shape)\n\n",
  "unprocess_testcases": "x = torch.randn(4, 3, 2, 5)\ni = 1\nj = 3\nshape = (3, 2)\nassert((SOLUTION_SIGNATURE(x, i, j, shape) == view_range(x, i, j, shape)).all())\n\nx = torch.randn(4, 3, 2, 5, 6)\ni = -4\nj = -2\nshape = (6,)\nassert((SOLUTION_SIGNATURE(x, i, j, shape) == view_range(x, i, j, shape)).all())\n\nx = torch.randn(4, 3, 2)\ni = 1\nj = None\nshape = (6,)\nassert((SOLUTION_SIGNATURE(x, i, j, shape) == view_range(x, i, j, shape)).all())\n"
 },
 {
  "task_id": "RWPB/116",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\n\ndef round_nearest_multiple(number, a, direction='standard'):\n    \"\"\"\n    Rounds a number to the nearest multiple of a. The returned number will have the\n    same precision as a.\n\n    Args:\n        - number: float, the number to be rounded.\n        - a: float, the multiple to round to.\n        - direction: str, optional, specifying the rounding direction ('standard', 'down', or 'up').\n                     Default is 'standard'.\n\n    Returns:\n        rounded_number: float, the rounded number to the nearest multiple of a.\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the provided function `round_nearest_multiple` which rounds a number to the nearest multiple of another number `a`, with options for different rounding directions.\n\n```python\nimport math\nimport numpy as np\n\ndef round_nearest_multiple(number, a, direction='standard'):\n    \"\"\"\n    Rounds a number to the nearest multiple of a. The returned number will have the\n    same precision as a.\n\n    Args:\n        - number: float, the number to be rounded.\n        - a: float, the multiple to round to.\n        - direction: str, optional, specifying the rounding direction ('standard', 'down', or 'up').\n                     Default is 'standard'.\n\n    Returns:\n        rounded_number: float, the rounded number to the nearest multiple of a.\n    \"\"\"\n    if a == 0:\n        raise ValueError(\"The multiple 'a' cannot be zero.\")\n    \n    # Calculate the nearest multiple depending on the direction\n    if direction == 'standard':\n        # Standard rounding to the nearest multiple\n        return round(number / a) * a\n    elif direction == 'down':\n        # Rounding down to the nearest multiple\n        return math.floor(number / a) * a\n    elif direction == 'up':\n        # Rounding up to the nearest multiple\n        return math.ceil(number / a) * a\n    else:\n        raise ValueError(\"Invalid direction. Choose 'standard', 'down', or 'up'.\")\n\n# Example usage:\n# result = round_nearest_multiple(123.456, 10)\n# print(result)\n```\n\nThis function first checks if `a` is zero, as rounding to a multiple of zero is not defined. It then proceeds to round the number based on the specified direction:\n- `'standard'`: Uses Python's built-in `round()` function to round to the nearest multiple.\n- `'down'`: Uses `math.floor()` to always round down to the nearest multiple.\n- `'up'`: Uses `math.ceil()` to always round up to the nearest multiple.\n\nThe function raises a `ValueError` if an invalid direction is specified.",
  "canonical_solution": "    \n    if direction == 'down':\n        return round(math.floor(number / a) * a, -int(math.floor(math.log10(a))))\n    elif direction == 'up':\n        return round(math.ceil(number / a) * a, -int(math.floor(math.log10(a))))\n    elif direction == 'standard':\n        return round(number, -int(math.floor(math.log10(a))))\n\n\n\n",
  "unprocess_testcases": "number = 14.7\na = 5\nassert(SOLUTION_SIGNATURE(number, a) == round_nearest_multiple(number, a))\n\nnumber = 9.8\na = 2\ndirection = 'down'\nassert(SOLUTION_SIGNATURE(number, a, direction) == round_nearest_multiple(number, a, direction))\n\nnumber = 7.1\na = 0.5\ndirection = 'up'\nassert(SOLUTION_SIGNATURE(number, a, direction) == round_nearest_multiple(number, a, direction))"
 },
 {
  "task_id": "RWPB/7",
  "prompt": "from importlib import import_module\n\ndef dynamic_import_function(function_path):\n    '''Dynamically import a function from a path string by using import_module() (e.g., \"module.submodule.my_function\")\n    if no match distribution found, please return ModuleNotFoundError\n    Args:\n        a function path string:string\n    Returns:\n        function (e.g., my_function)\n    '''\n",
  "solution": "Certainly! Below is the Python code that completes the `dynamic_import_function` function. This function takes a string representing the full path to a function (including the module and submodules) and dynamically imports that function using `import_module` from the `importlib` library. If the module or function is not found, it will handle the exception and return `None`.\n\n```python\nfrom importlib import import_module\n\ndef dynamic_import_function(function_path):\n    '''Dynamically import a function from a path string by using import_module() (e.g., \"module.submodule.my_function\")\n    if no match distribution found, please return ModuleNotFoundError\n    Args:\n        function_path (str): A function path string\n    Returns:\n        function or None if not found\n    '''\n    try:\n        # Split the function path to separate the module path and the function name\n        module_path, function_name = function_path.rsplit('.', 1)\n        \n        # Import the module\n        module = import_module(module_path)\n        \n        # Get the function from the module\n        func = getattr(module, function_name)\n        \n        return func\n    except (ImportError, AttributeError) as e:\n        # Return None or raise an appropriate exception if the module or function is not found\n        raise ModuleNotFoundError(f\"Module or function not found for path: {function_path}\") from e\n\n# Example usage:\n# Assuming there's a function `my_function` in `module.submodule`, you can import it like this:\n# imported_function = dynamic_import_function(\"module.submodule.my_function\")\n# result = imported_function()\n```\n\nThis function splits the input string to separate the module path from the function name, imports the module, and then retrieves the function using `getattr`. If any part of this process fails (either the module cannot be imported or the function does not exist in the module), it catches the exception and raises a `ModuleNotFoundError` with a message indicating the failure.",
  "canonical_solution": "    \n    try:\n        module_path, function_name = function_path.rsplit(\".\", 1)\n        module = import_module(module_path)\n        function = getattr(module, function_name)\n    except:\n        return ModuleNotFoundError\n    return function\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE('math.sqrt') == dynamic_import_function('math.sqrt'))\nassert(SOLUTION_SIGNATURE('os.path.join') == dynamic_import_function('os.path.join'))\nassert(SOLUTION_SIGNATURE('nonexistent.mod.func') == dynamic_import_function('nonexistent.mod.func'))\n"
 },
 {
  "task_id": "RWPB/98",
  "prompt": "import numpy as np\n\n\ndef exp_smooth(vals, alpha):\n  \"\"\"\n  Exponential smoothing function.\n  Smooths a sequence of values using exponential smoothing.\n  If input array is an empty, the code should return KeyError\n  Args:\n      - vals: list or numpy.ndarray, input sequence of values\n      - alpha: float, smoothing factor\n  Returns:\n      smoothed: numpy.ndarray, smoothed sequence of values\n  \"\"\"\n",
  "solution": "```python\nimport numpy as np\n\ndef exp_smooth(vals, alpha):\n    \"\"\"\n    Exponential smoothing function.\n    Smooths a sequence of values using exponential smoothing.\n    If input array is empty, the code should return KeyError\n    Args:\n        - vals: list or numpy.ndarray, input sequence of values\n        - alpha: float, smoothing factor\n    Returns:\n        smoothed: numpy.ndarray, smoothed sequence of values\n    \"\"\"\n    if len(vals) == 0:\n        raise KeyError(\"Input array is empty\")\n\n    vals = np.asarray(vals)\n    smoothed = np.zeros_like(vals)\n    smoothed[0] = vals[0]  # Initialize the first element\n\n    for i in range(1, len(vals)):\n        smoothed[i] = alpha * vals[i] + (1 - alpha) * smoothed[i - 1]\n\n    return smoothed\n```",
  "canonical_solution": "  \n  if len(vals) == 0:\n    return KeyError\n  smooth = np.array(vals[0], dtype=float)\n  smoothed = [smooth.copy()]\n\n  for i in range(1, len(vals)):\n    smooth += -alpha * (smooth - vals[i])\n    smoothed.append(smooth.copy())\n\n  return np.array(smoothed)\n\n\n",
  "unprocess_testcases": "vals = [1, 2, 3, 4, 5]\nalpha = 0.5\nassert((SOLUTION_SIGNATURE(vals, alpha) == exp_smooth(vals, alpha)).all())\n\nvals = [67, 576, 12, 4, 5, 34, 123, 67, 32, 34]\nalpha = 0.9\nassert((SOLUTION_SIGNATURE(vals, alpha) == exp_smooth(vals, alpha)).all())\n\nvals = [10, 10, 10, 10, 10]\nalpha = 0.2\nassert((SOLUTION_SIGNATURE(vals, alpha) == exp_smooth(vals, alpha)).all())\n\nvals = []\nalpha = 0.3\nassert(SOLUTION_SIGNATURE(vals, alpha) == exp_smooth(vals, alpha))\n"
 },
 {
  "task_id": "RWPB/136",
  "prompt": "import torch\n\ndef slerp(v0: torch.Tensor, v1: torch.Tensor, t: float, DOT_THRESHOLD: float = 0.9995) -> torch.Tensor:\n    \"\"\"\n    Performs spherical linear interpolation between two vectors.\n\n    Args:\n        v0 (torch.Tensor): The starting vector of the interpolation.\n        v1 (torch.Tensor): The ending vector of the interpolation.\n        t (float): The interpolation parameter, where 0 <= t <= 1.\n        DOT_THRESHOLD (float): A threshold for the cosine of the angle between v0 and v1 to handle numerical instabilities.\n\n    Returns:\n        torch.Tensor: The interpolated vector at parameter t along the great circle joining v0 and v1.\n    \"\"\"\n",
  "solution": "    # Normalize the input vectors\n    v0_normalized = v0 / torch.norm(v0)\n    v1_normalized = v1 / torch.norm(v1)\n\n    # Compute the cosine of the angle between the vectors\n    dot = torch.dot(v0_normalized, v1_normalized)\n\n    # If the dot product is very close to 1, the vectors are nearly parallel, so use linear interpolation instead\n    if dot > DOT_THRESHOLD:\n        return v0_normalized + t * (v1_normalized - v0_normalized)\n\n    # Clamp the dot product to be within the range of acos function to avoid numerical issues\n    dot = torch.clamp(dot, -1.0, 1.0)\n\n    # Compute the angle between the vectors\n    theta = torch.acos(dot)\n\n    # Compute the sin of the angle, which is needed for the slerp formula\n    sin_theta = torch.sin(theta)\n\n    # Compute the weights for each vector\n    s0 = torch.sin((1 - t) * theta) / sin_theta\n    s1 = torch.sin(t * theta) / sin_theta\n\n    # Compute the interpolated vector\n    return s0 * v0_normalized + s1 * v1_normalized\n\n# Example usage:\n# v0 = torch.tensor([1.0, 0.0, 0.0])\n# v1 = torch.tensor([0.0, 1.0, 0.0])\n# t = 0.5\n# interpolated_vector = slerp(v0, v1, t)\n# print(interpolated_vector)",
  "canonical_solution": "    \n    # Normalize the input vectors to unit vectors\n    u0 = v0 / v0.norm()\n    u1 = v1 / v1.norm()\n    # Compute the cosine of the angle between the vectors\n    dot = (u0 * u1).sum()\n    # Check if the vectors are almost parallel\n    if dot.abs() > DOT_THRESHOLD:\n        # If vectors are nearly parallel, fall back to linear interpolation\n        return (1.0 - t) * v0 + t * v1\n    # Compute the angle between the vectors using arccos\n    omega = dot.acos()\n    # Perform spherical linear interpolation\n    return (((1.0 - t) * omega).sin() * v0 + (t * omega).sin() * v1) / omega.sin()\n\n",
  "unprocess_testcases": "v0 = torch.tensor([1.0, 0.0, 0.0])\nv1 = torch.tensor([0.0, 1.0, 0.0])\nt = 0.5\nassert((SOLUTION_SIGNATURE(v0, v1, t) == slerp(v0, v1, t)).all())\n\nv0 = torch.tensor([1.0, 0.0, 0.0])\nv1 = torch.tensor([1.0, 0.0, 0.0])\nt = 0.3\nassert((SOLUTION_SIGNATURE(v0, v1, t) == slerp(v0, v1, t)).all())\n\nv0 = torch.tensor([0.9995, 0.01, 0.0])\nv1 = torch.tensor([1.0, 0.0, 0.0])\nt = 0.7\nassert((SOLUTION_SIGNATURE(v0, v1, t) == slerp(v0, v1, t)).all())\n"
 },
 {
  "task_id": "RWPB/102",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\n\ndef rotate_about_z_axis(vector, theta):\n    \"\"\"\n    Rotate a 3D vector about the z-axis by a specified angle.\n    Args:\n        - vector: numpy array of shape (3,), the 3D vector to be rotated\n        - theta: float, angle of rotation in radians\n    Returns:\n        rotated_vector: numpy array of shape (3,), the rotated 3D vector\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the function `rotate_about_z_axis` which rotates a 3D vector about the z-axis by a given angle `theta` in radians. The rotation matrix for a rotation about the z-axis is used to compute the rotated vector.\n\n```python\nimport numpy as np\n\ndef rotate_about_z_axis(vector, theta):\n    \"\"\"\n    Rotate a 3D vector about the z-axis by a specified angle.\n    Args:\n        - vector: numpy array of shape (3,), the 3D vector to be rotated\n        - theta: float, angle of rotation in radians\n    Returns:\n        rotated_vector: numpy array of shape (3,), the rotated 3D vector\n    \"\"\"\n    # Define the rotation matrix for rotation about the z-axis\n    rotation_matrix = np.array([\n        [np.cos(theta), -np.sin(theta), 0],\n        [np.sin(theta),  np.cos(theta), 0],\n        [0,             0,             1]\n    ])\n    \n    # Convert the input vector to a numpy array if it's not already\n    vector = np.asarray(vector)\n    \n    # Perform the matrix multiplication to get the rotated vector\n    rotated_vector = np.dot(rotation_matrix, vector)\n    \n    return rotated_vector\n```\n\nThis function constructs the rotation matrix for a rotation about the z-axis using the cosine and sine of the angle `theta`. It then multiplies this matrix by the input vector `vector` to produce the rotated vector. The function assumes that the input vector is a numpy array of shape (3,). If the input is not already a numpy array, it is converted to one to ensure compatibility with the matrix multiplication operation.",
  "canonical_solution": "\n    return np.dot(np.array([\n[np.cos(theta), -np.sin(theta), 0],\n[np.sin(theta), np.cos(theta), 0],\n[0, 0, 1]\n]), vector)\n\n",
  "unprocess_testcases": "vector = np.array([1, 0, 0])\ntheta = 0\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_z_axis(vector, theta)).all())\n\nvector = np.array([1, 0, 0])\ntheta = np.pi / 2\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_z_axis(vector, theta)).all())\n\nvector = np.array([1, 2, 3])\ntheta = np.pi / 4\nassert((SOLUTION_SIGNATURE(vector, theta) == rotate_about_z_axis(vector, theta)).all())\n\n"
 },
 {
  "task_id": "RWPB/112",
  "prompt": "def submatrix(matrix, col):\n    \"\"\"\n    Extracts the submatrix formed by deleting a specified column from the given matrix.\n\n    Args:\n        - matrix: list of lists, the input matrix.\n        - col: int, the index of the column to be removed.\n\n    Returns:\n        sub_matrix: list of lists, the submatrix obtained after removing the specified column.\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `submatrix` function that removes a specified column from a given matrix:\n\n```python\ndef submatrix(matrix, col):\n    \"\"\"\n    Extracts the submatrix formed by deleting a specified column from the given matrix.\n\n    Args:\n        - matrix: list of lists, the input matrix.\n        - col: int, the index of the column to be removed.\n\n    Returns:\n        sub_matrix: list of lists, the submatrix obtained after removing the specified column.\n    \"\"\"\n    # Initialize an empty list to store the submatrix\n    sub_matrix = []\n\n    # Iterate through each row in the original matrix\n    for row in matrix:\n        # Use list comprehension to build a new row excluding the specified column\n        new_row = [row[i] for i in range(len(row)) if i != col]\n        # Append the new row to the submatrix\n        sub_matrix.append(new_row)\n\n    return sub_matrix\n```\n\nThis function works by iterating through each row of the input matrix and constructing a new row that excludes the element at the index `col`. Each new row is then added to `sub_matrix`, which is returned at the end of the function.",
  "canonical_solution": "    \n    return [row[:col] + row[col + 1:] for row in matrix[1:]]\n\n\n\n",
  "unprocess_testcases": "matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ncol = 1\nassert(SOLUTION_SIGNATURE(matrix, col) == submatrix(matrix, col))\n\nmatrix = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]\ncol = 0\nassert(SOLUTION_SIGNATURE(matrix, col) == submatrix(matrix, col))\n\nmatrix = [[1, 2, 3], [4, 5], [6]]\ncol = 2\nassert(SOLUTION_SIGNATURE(matrix, col) == submatrix(matrix, col))\n\nmatrix = [[1, 5, 10], [4, 5], [6]]\ncol = 1\nassert(SOLUTION_SIGNATURE(matrix, col) == submatrix(matrix, col))\n"
 },
 {
  "task_id": "RWPB/126",
  "prompt": "import torch\n\ndef crop_mask(masks, boxes):\n    \"\"\"\n    Crops each mask in a batch of masks to the specified bounding boxes.\n\n    Args:\n        masks (torch.Tensor): A tensor of shape [n, h, w] where 'n' is the number of masks, and 'h' and 'w' are the height and width of the masks.\n        boxes (torch.Tensor): A tensor of shape [n, 4] representing the bounding boxes for each mask. The boxes are in the format [x1, y1, x2, y2].\n\n    Returns:\n        torch.Tensor: A tensor of the same shape as 'masks' but with each mask cropped to its corresponding bounding box.\n    \"\"\"\n",
  "solution": "```python\n    # Ensure that the input tensors are on the same device\n    device = masks.device\n\n    # Get the number of masks\n    n = masks.size(0)\n\n    # Initialize a tensor to hold the cropped masks\n    cropped_masks = torch.zeros_like(masks)\n\n    # Iterate over each mask and corresponding box\n    for i in range(n):\n        # Get the current box, ensuring integer coordinates\n        x1, y1, x2, y2 = boxes[i].int()\n\n        # Crop the mask according to the box coordinates\n        cropped_mask = masks[i, y1:y2, x1:x2]\n\n        # Create a new mask of the same size as the original\n        new_mask = torch.zeros(masks.size(1), masks.size(2), device=device)\n\n        # Place the cropped mask in the corresponding position in the new mask\n        new_mask[y1:y2, x1:x2] = cropped_mask\n\n        # Store the new mask in the output tensor\n        cropped_masks[i] = new_mask\n\n    return cropped_masks\n```\n\nThis function iterates over each mask and its corresponding bounding box, crops the mask according to the box coordinates, and places the cropped mask back into a new mask of the original size. This ensures that the output tensor has the same dimensions as the input masks tensor.",
  "canonical_solution": "    \n    _, h, w = masks.shape  # Get the dimensions of the masks\n    # Split the bounding box tensor into x1, y1, x2, y2 components. Each is expanded to have shape [n, 1, 1].\n    x1, y1, x2, y2 = torch.chunk(boxes[:, :, None], 4, dim=1)\n    # Generate tensors representing the indices of each column and each row in the image, and broadcast them to match dimensions\n    r = torch.arange(w, device=masks.device, dtype=x1.dtype)[None, None, :]  # row indices, shape [1, 1, w]\n    c = torch.arange(h, device=masks.device, dtype=y1.dtype)[None, :, None]  # column indices, shape [1, h, 1]\n\n    # Generate a mask by comparing indices with the bounding box coordinates\n    # The comparison results in a binary mask for each bounding box, which when multiplied with the original mask, crops it.\n    return masks * ((r >= x1) * (r < x2) * (c >= y1) * (c < y2))\n\n",
  "unprocess_testcases": "masks = torch.ones((3, 5, 5))\nboxes = torch.tensor([[1, 1, 4, 4], [0, 0, 3, 3], [2, 2, 5, 5]])\nassert((SOLUTION_SIGNATURE(masks, boxes) == crop_mask(masks, boxes)).all())\n\nmasks = torch.ones((2, 5, 5))\nboxes = torch.tensor([[0, 0, 5, 5], [-1, -1, 6, 6]])\nassert((SOLUTION_SIGNATURE(masks, boxes) == crop_mask(masks, boxes)).all())\n\nmasks = torch.tensor([\n    [[1, 0, 1, 0, 1, 0],\n     [0, 1, 0, 1, 0, 1],\n     [1, 0, 1, 0, 1, 0],\n     [0, 1, 0, 1, 0, 1],\n     [1, 0, 1, 0, 1, 0],\n     [0, 1, 0, 1, 0, 1]],\n    [[0, 1, 0, 1, 0, 1],\n     [1, 0, 1, 0, 1, 0],\n     [0, 1, 0, 1, 0, 1],\n     [1, 0, 1, 0, 1, 0],\n     [0, 1, 0, 1, 0, 1],\n     [1, 0, 1, 0, 1, 0]]\n])\nboxes = torch.tensor([[1, 1, 3, 3], [3, 3, 6, 6]])\nassert((SOLUTION_SIGNATURE(masks, boxes) == crop_mask(masks, boxes)).all())\n"
 },
 {
  "task_id": "RWPB/88",
  "prompt": "import torch\nimport numpy as np\n\ndef rotate_y(a, device=None):\n    \"\"\"\n    Creates a 4x4 homogeneous transformation matrix for rotation around the y-axis in 3D space.\n\n    Parameters:\n    a (float): Rotation angle in radians.\n    device (torch.device, optional): The device on which to create the tensor (e.g., 'cpu' or 'cuda'). Default is None, which means the tensor will be created on the default device.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the rotation transformation matrix around the y-axis.\n\n    Process:\n    1. Calculate the sine and cosine of the rotation angle.\n    2. Construct a 4x4 matrix where the rotation is applied to the x and z coordinates, leaving the y coordinate unchanged.\n    \"\"\"\n",
  "solution": "    # Calculate cosine and sine of the angle\n    cos_a = torch.cos(a)\n    sin_a = torch.sin(a)\n\n    # Create the rotation matrix\n    # The matrix layout for rotation around the y-axis is:\n    # [ cos(a)  0  sin(a)  0 ]\n    # [   0     1    0     0 ]\n    # [-sin(a)  0  cos(a)  0 ]\n    # [   0     0    0     1 ]\n    rotation_matrix = torch.tensor([\n        [cos_a, 0, sin_a, 0],\n        [0, 1, 0, 0],\n        [-sin_a, 0, cos_a, 0],\n        [0, 0, 0, 1]\n    ], dtype=torch.float32, device=device)\n\n    return rotation_matrix\n\n# Example usage:\n# angle_in_radians = np.radians(90)\n# rotation_matrix = rotate_y(angle_in_radians, device=torch.device('cpu'))\n# print(rotation_matrix)",
  "canonical_solution": "    \n    s, c = np.sin(a), np.cos(a)\n    return torch.tensor([[ c, 0, s, 0], \n                         [ 0, 1, 0, 0], \n                         [-s, 0, c, 0], \n                         [ 0, 0, 0, 1]], dtype=torch.float32, device=device)\n\n",
  "unprocess_testcases": "a = torch.pi / 4\nassert((SOLUTION_SIGNATURE(a) == rotate_y(a)).all())\n\na = 0\nassert((SOLUTION_SIGNATURE(a) == rotate_y(a)).all())\n\na = 2 * torch.pi\nassert((SOLUTION_SIGNATURE(a) == rotate_y(a)).all())\n"
 },
 {
  "task_id": "RWPB/58",
  "prompt": "def unpad_image_shape(current_height, current_width, original_size):\n    \"\"\"\n    Calculates the new shape of an image after unpading a resized and padded PyTorch tensor.\n\n    Parameters\n    ----------\n    current_height : int\n        The current height of the padded image.\n    current_width : int\n        The current width of the padded image.\n    original_size : tuple\n        A tuple representing the original size of the image in the format (original_width, original_height).\n\n    Returns\n    -------\n    tuple\n        The new shape of the image after unpadding, in the format (new_height, new_width).\n\n    Notes\n    -----\n    This function first calculates the aspect ratios of the original and current images.\n    It then determines how to unpad the image based on the aspect ratios to maintain the original aspect ratio.\n\n    If the original aspect ratio is greater than the current aspect ratio, it means the image has been made wider.\n    The function calculates the new height by scaling the original height with the same factor that the width has been scaled.\n    It then computes the padding needed to center the original aspect ratio within the current dimensions.\n\n    Conversely, if the original aspect ratio is less than the current aspect ratio, the image has been made taller.\n    The function calculates the new width and computes the necessary padding to center the image vertically.\n\n    The function returns the new dimensions of the image after unpadding, which can be used to trim the padded image back to its original shape.\n    \"\"\"\n",
  "solution": "    original_width, original_height = original_size\n    original_aspect_ratio = original_width / original_height\n    current_aspect_ratio = current_width / current_height\n\n    if original_aspect_ratio > current_aspect_ratio:\n        # Image has been made wider\n        scale_factor = current_height / original_height\n        new_width = int(original_width * scale_factor)\n        new_height = current_height\n    else:\n        # Image has been made taller\n        scale_factor = current_width / original_width\n        new_height = int(original_height * scale_factor)\n        new_width = current_width\n\n    return (new_height, new_width)",
  "canonical_solution": "\n    original_width, original_height = original_size\n\n    # Calculate aspect ratios\n    original_aspect_ratio = original_width / original_height\n    current_aspect_ratio = current_width / current_height\n\n    # Check which dimension has been altered more and unpad accordingly\n    if original_aspect_ratio > current_aspect_ratio:\n        # The image has been made wider, so calculate new height and padding\n        scale_factor = current_width / original_width\n        new_height = int(original_height * scale_factor)\n        padding = (current_height - new_height) // 2\n        new_shape = (current_height - 2 * padding, current_width)\n    else:\n        # The image has been made taller, so calculate new width and padding\n        scale_factor = current_height / original_height\n        new_width = int(original_width * scale_factor)\n        padding = (current_width - new_width) // 2\n        new_shape = (current_height, current_width - 2 * padding)\n\n    return new_shape\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(current_height=1000, current_width=2000, original_size=(1000, 500))[0] == unpad_image_shape(current_height=1000, current_width=2000, original_size=(1000, 500))[0])\nassert(SOLUTION_SIGNATURE(current_height=2000, current_width=1000, original_size=(500, 1000))[0] == unpad_image_shape(current_height=2000, current_width=1000, original_size=(500, 1000))[0])\nassert(SOLUTION_SIGNATURE(current_height=1200, current_width=1600, original_size=(600, 800))[0] == unpad_image_shape(current_height=1200, current_width=1600, original_size=(600, 800))[0])\nassert(SOLUTION_SIGNATURE(current_height=100, current_width=160, original_size=(5, 1))[0] == unpad_image_shape(current_height=100, current_width=160, original_size=(5, 1))[0])\nassert(SOLUTION_SIGNATURE(current_height=1231, current_width=4123, original_size=(76, 2))[0] == unpad_image_shape(current_height=1231, current_width=4123, original_size=(76, 2))[0])\nassert(SOLUTION_SIGNATURE(current_height=1000, current_width=2000, original_size=(1000, 500))[1] == unpad_image_shape(current_height=1000, current_width=2000, original_size=(1000, 500))[1])\nassert(SOLUTION_SIGNATURE(current_height=2000, current_width=1000, original_size=(500, 1000))[1] == unpad_image_shape(current_height=2000, current_width=1000, original_size=(500, 1000))[1])\nassert(SOLUTION_SIGNATURE(current_height=1200, current_width=1600, original_size=(600, 800))[1] == unpad_image_shape(current_height=1200, current_width=1600, original_size=(600, 800))[1])\nassert(SOLUTION_SIGNATURE(current_height=100, current_width=160, original_size=(5, 1))[1] == unpad_image_shape(current_height=100, current_width=160, original_size=(5, 1))[1])\nassert(SOLUTION_SIGNATURE(current_height=1231, current_width=4123, original_size=(76, 2))[1] == unpad_image_shape(current_height=1231, current_width=4123, original_size=(76, 2))[1])\n"
 },
 {
  "task_id": "RWPB/105",
  "prompt": "import torch\n\ndef sample_from_3dgrid(grid: torch.Tensor, coordinates: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Samples features from a 3D grid based on 3D coordinates.\n\n    Args:\n        grid (torch.Tensor): 3D grid with shape (1, channels, H, W, D).\n        coordinates (torch.Tensor): 3D coordinates to sample from with shape (batch_size, num_points_per_batch, 3).\n\n    Returns:\n        torch.Tensor: Sampled features with shape (batch_size, num_points_per_batch, feature_channels).\n    \"\"\"\n",
  "solution": "```python\n    # Ensure the coordinates are within the range [0, 1] for grid_sample\n    grid_size = torch.tensor(grid.size()[2:], dtype=torch.float32, device=grid.device)\n    scaled_coordinates = coordinates / (grid_size - 1) * 2.0 - 1.0\n\n    # Add batch dimension to coordinates and reshape for grid_sample\n    scaled_coordinates = scaled_coordinates.unsqueeze(0)\n\n    # Permute and reshape coordinates to match the grid_sample input requirements\n    # grid_sample expects coordinates in the shape (N, H_out, W_out, D_out, 3)\n    scaled_coordinates = scaled_coordinates.permute(0, 2, 1, 3).unsqueeze(2)\n\n    # Use grid_sample to sample from the grid\n    sampled_features = torch.nn.functional.grid_sample(grid, scaled_coordinates, mode='bilinear', padding_mode='zeros', align_corners=True)\n\n    # Remove unnecessary dimensions and permute to get the desired output shape\n    sampled_features = sampled_features.squeeze(4).permute(0, 2, 1, 3).squeeze(3)\n\n    return sampled_features\n```",
  "canonical_solution": "    \n    batch_size, n_coords, n_dims = coordinates.shape\n    sampled_features = torch.nn.functional.grid_sample(\n        grid.expand(batch_size, -1, -1, -1, -1),\n        coordinates.reshape(batch_size, 1, 1, -1, n_dims),\n        mode='bilinear', \n        padding_mode='zeros', \n        align_corners=False,\n    )\n    N, C, H, W, D = sampled_features.shape\n    sampled_features = sampled_features.permute(0, 4, 3, 2, 1).reshape(N, H*W*D, C)\n    return sampled_features\n\n\n",
  "unprocess_testcases": "grid1 = torch.rand((1, 3, 4, 4, 4))  # 3D grid with 3 channels, shape (1, 3, 4, 4, 4)\ncoordinates1 = torch.tensor([[[1.0, 1.0, 1.0], [2.5, 2.5, 2.5]]])  # 3D coordinates, shape (1, 2, 3)\nassert((SOLUTION_SIGNATURE(grid1, coordinates1) == sample_from_3dgrid(grid1, coordinates1)).all())\n\ngrid2 = torch.rand((1, 2, 5, 5, 5))  # 3D grid with 2 channels, shape (1, 2, 5, 5, 5)\ncoordinates2 = torch.tensor([[[6.0, 6.0, 6.0], [-1.0, -1.0, -1.0]]])  # Out of bounds coordinates, shape (1, 2, 3)\nassert((SOLUTION_SIGNATURE(grid2, coordinates2) == sample_from_3dgrid(grid2, coordinates2)).all())\n\ngrid3 = torch.rand((1, 4, 6, 6, 6))  # 3D grid with 4 channels, shape (1, 4, 6, 6, 6)\ncoordinates3 = torch.tensor([\n    [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0]],\n    [[0.5, 0.5, 0.5], [4.5, 4.5, 4.5], [5.5, 5.5, 5.5]]\n])  # 3D coordinates, shape (2, 3, 3)\nassert((SOLUTION_SIGNATURE(grid3, coordinates3) == sample_from_3dgrid(grid3, coordinates3)).all())\n"
 },
 {
  "task_id": "RWPB/131",
  "prompt": "import torch\nimport numpy as np\n\ndef box_iou(box1, box2, eps=1e-7):\n    \"\"\"\n    Calculate intersection-over-union (IoU) of boxes. Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n    Based on https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n\n    Args:\n        box1 (torch.Tensor): A tensor of shape (N, 4) representing N bounding boxes.\n        box2 (torch.Tensor): A tensor of shape (M, 4) representing M bounding boxes.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n\n    Returns:\n        (torch.Tensor): An NxM tensor containing the pairwise IoU values for every element in box1 and box2.\n    \"\"\"\n",
  "solution": "```python\n    # Ensure that the boxes are in the correct shape\n    assert box1.size(1) == 4 and box2.size(1) == 4, \"Boxes input should have size (*, 4)\"\n\n    # Unpack the coordinates for easier reference\n    x1_min, y1_min, x1_max, y1_max = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n    x2_min, y2_min, x2_max, y2_max = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n\n    # Calculate the coordinates of the intersection rectangle\n    inter_x_min = torch.max(x1_min[:, None], x2_min)  # Shape: (N, M)\n    inter_y_min = torch.max(y1_min[:, None], y2_min)  # Shape: (N, M)\n    inter_x_max = torch.min(x1_max[:, None], x2_max)  # Shape: (N, M)\n    inter_y_max = torch.min(y1_max[:, None], y2_max)  # Shape: (N, M)\n\n    # Calculate the area of the intersection rectangle\n    inter_area = torch.clamp(inter_x_max - inter_x_min, min=0) * torch.clamp(inter_y_max - inter_y_min, min=0)\n\n    # Calculate the area of both sets of boxes\n    box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n    box2_area = (x2_max - x2_min) * (y2_max - y2_min)\n\n    # Calculate the area of the union\n    union_area = box1_area[:, None] + box2_area - inter_area\n\n    # Calculate the IoU\n    iou = inter_area / (union_area + eps)\n\n    return iou\n```\n\nThis function computes the Intersection over Union (IoU) for each pair of boxes in `box1` and `box2`. It handles the computation in a vectorized manner, ensuring efficiency when dealing with large numbers of bounding boxes. The function uses broadcasting to compare each box in `box1` with each box in `box2`, calculating the intersection and union areas, and finally the IoU values. The result is an NxM tensor where each element represents the IoU between a box from `box1` and a box from `box2`.",
  "canonical_solution": "\n    # NOTE: need float32 to get accurate iou values\n    box1 = torch.as_tensor(box1, dtype=torch.float32)\n    box2 = torch.as_tensor(box2, dtype=torch.float32)\n    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n    (a1, a2), (b1, b2) = box1.unsqueeze(1).chunk(2, 2), box2.unsqueeze(0).chunk(2, 2)\n    inter = (torch.min(a2, b2) - torch.max(a1, b1)).clamp_(0).prod(2)\n\n    # IoU = inter / (area1 + area2 - inter)\n    return inter / ((a2 - a1).prod(2) + (b2 - b1).prod(2) - inter + eps)\n\n",
  "unprocess_testcases": "box1 = np.array([[0, 0, 2, 2]])\nbox2 = np.array([[1, 1, 3, 3]])\nassert((SOLUTION_SIGNATURE(box1, box2) == box_iou(box1, box2)).all())\n\nbox1 = np.array([[0, 0, 1, 1]])\nbox2 = np.array([[2, 2, 3, 3]])\nassert((SOLUTION_SIGNATURE(box1, box2) == box_iou(box1, box2)).all())\n\nbox1 = np.array([[0, 0, 3, 3], [4, 4, 5, 5]])\nbox2 = np.array([[1, 1, 4, 4], [0, 0, 2, 2]])\nassert((SOLUTION_SIGNATURE(box1, box2) == box_iou(box1, box2)).all())\n\nbox1 = np.array([[0, 0, 3, 3], [4, 4, 5, 5]])\nbox2 = np.array([[1, 1, 4, 4], [0, 0, 2, 2]])\nassert((SOLUTION_SIGNATURE(box1, box2, 2e-5) == box_iou(box1, box2, 2e-5)).all())\n\nbox1 = np.array([[1, 3, 3, 6], [7, 7, 2, 6]])\nbox2 = np.array([[9, 3, 5, 4], [0, 1, 3, 8]])\nassert((SOLUTION_SIGNATURE(box1, box2) == box_iou(box1, box2)).all())\n"
 },
 {
  "task_id": "RWPB/121",
  "prompt": "def isprime(n):\n    \"\"\"\n    Check if a number is a prime number.\n\n    Args:\n        - n: int, number to be checked for primality\n\n    Returns:\n        - bool: True if n is prime, False otherwise\n    \"\"\"\n",
  "solution": "Certainly! Here's the completion of the `isprime` function:\n\n```python\ndef isprime(n):\n    \"\"\"\n    Check if a number is a prime number.\n\n    Args:\n        - n: int, number to be checked for primality\n\n    Returns:\n        - bool: True if n is prime, False otherwise\n    \"\"\"\n    if n <= 1:\n        return False\n    if n <= 3:\n        return True\n    if n % 2 == 0 or n % 3 == 0:\n        return False\n    i = 5\n    while i * i <= n:\n        if n % i == 0 or n % (i + 2) == 0:\n            return False\n        i += 6\n    return True\n```\n\nThis function first handles the simple cases:\n- If `n` is less than or equal to 1, it is not prime.\n- If `n` is 2 or 3, it is prime.\n\nFor other numbers, it checks divisibility:\n- It first checks if `n` is divisible by 2 or 3.\n- Then it uses a loop to check divisibility by all integers of the form `6k ± 1` up to the square root of `n`. This is an efficient way to check for prime numbers because all primes greater than 3 are of the form `6k ± 1`.\n\nThis approach efficiently reduces the number of checks needed for larger numbers.",
  "canonical_solution": "    \n    if n == 2:\n        return True\n    if n % 2 == 0:\n        return False\n\n    max = n**0.5+1\n    i = 3\n    \n    while i <= max:\n        if n % i == 0:\n            return False\n        i+=2\n    return True\n\n\n",
  "unprocess_testcases": "n = 2\nassert(SOLUTION_SIGNATURE(n) == isprime(n))\n\nn = 4\nassert(SOLUTION_SIGNATURE(n) == isprime(n))\n\nn = 29\nassert(SOLUTION_SIGNATURE(n) == isprime(n))\n"
 },
 {
  "task_id": "RWPB/115",
  "prompt": "import torch\nimport torch.nn as nn\n\ndef compute_tri_normal(geometry, tris):\n    \"\"\"\n    Compute the normal vectors for triangles defined by vertex indices.\n\n    Args:\n        geometry (torch.Tensor): A tensor of shape (batch_size, num_points, 3) representing the 3D coordinates of points.\n        tris (torch.Tensor): A tensor of shape (num_tris, 3) representing the vertex indices that form triangles.\n\n    Returns:\n        torch.Tensor: A tensor of shape (batch_size, num_tris, 3) representing the normal vectors of the triangles.\n    \"\"\"\n",
  "solution": "```python\n    # Extract the vertices of each triangle\n    v0 = geometry[:, tris[:, 0], :]  # Shape: (batch_size, num_tris, 3)\n    v1 = geometry[:, tris[:, 1], :]  # Shape: (batch_size, num_tris, 3)\n    v2 = geometry[:, tris[:, 2], :]  # Shape: (batch_size, num_tris, 3)\n\n    # Compute the vectors along the sides of the triangles\n    edge1 = v1 - v0  # Shape: (batch_size, num_tris, 3)\n    edge2 = v2 - v0  # Shape: (batch_size, num_tris, 3)\n\n    # Compute the cross product of the vectors to get the normal\n    normals = torch.cross(edge1, edge2, dim=2)  # Shape: (batch_size, num_tris, 3)\n\n    # Optionally normalize the normals\n    norm = torch.linalg.norm(normals, dim=2, keepdim=True)\n    normals = normals / norm  # Avoid division by zero; handle with torch.clamp or similar if necessary\n\n    return normals\n```",
  "canonical_solution": "    \n    geometry = geometry.permute(0, 2, 1)\n    tri_1 = tris[:, 0]\n    tri_2 = tris[:, 1]\n    tri_3 = tris[:, 2]\n\n    vert_1 = torch.index_select(geometry, 2, tri_1)\n    vert_2 = torch.index_select(geometry, 2, tri_2)\n    vert_3 = torch.index_select(geometry, 2, tri_3)\n\n    nnorm = torch.cross(vert_2 - vert_1, vert_3 - vert_1, 1)\n    normal = nn.functional.normalize(nnorm).permute(0, 2, 1)\n    return normal\n\n\n",
  "unprocess_testcases": "geometry = torch.tensor([[[0., 0., 0.],\n                  [1., 0., 0.],\n                  [0., 1., 0.]]], dtype=torch.float32)\ntris = torch.tensor([[0, 1, 2]], dtype=torch.int64)\nassert((SOLUTION_SIGNATURE(geometry, tris) == compute_tri_normal(geometry, tris)).all())\n\ngeometry = torch.tensor([[[0., 0., 0.], [1., 0., 0.], [0., 1., 0.], [1., 1., 0.], [2., 0., 0.], [0., 2., 0.]],\n                 [[0., 0., 0.], [0., 1., 0.], [1., 0., 0.], [0., 0., 1.], [1., 1., 1.], [1., 0., 1.]]],\n                dtype=torch.float32)\ntris = torch.tensor([[0, 1, 2], [3, 4, 5]], dtype=torch.int64)\nassert((SOLUTION_SIGNATURE(geometry, tris) == compute_tri_normal(geometry, tris)).all())\n\ngeometry = torch.tensor([[[0., 0., 0.],\n                  [1., 1., 1.],\n                  [2., 2., 2.]]], dtype=torch.float32)\ntris = torch.tensor([[0, 1, 2]], dtype=torch.int64)\nassert((SOLUTION_SIGNATURE(geometry, tris) == compute_tri_normal(geometry, tris)).all())\n"
 },
 {
  "task_id": "RWPB/48",
  "prompt": "def select_best_resolution(original_size, possible_resolutions):\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        original_size (tuple): The original size of the image in the format (width, height).\n        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2), ...].\n\n    Returns:\n        tuple: The best fit resolution in the format (width, height).\n    \"\"\"\n",
  "solution": "Certainly! Here's the completion of the function `select_best_resolution`:\n\n```python\ndef select_best_resolution(original_size, possible_resolutions):\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        original_size (tuple): The original size of the image in the format (width, height).\n        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2), ...].\n\n    Returns:\n        tuple: The best fit resolution in the format (width, height).\n    \"\"\"\n    original_width, original_height = original_size\n    best_resolution = None\n    min_diff = float('inf')\n\n    for width, height in possible_resolutions:\n        # Calculate the difference in area between the original and the possible resolution\n        diff = abs((original_width * original_height) - (width * height))\n        \n        # Update the best resolution if the current one has a smaller difference\n        if diff < min_diff:\n            min_diff = diff\n            best_resolution = (width, height)\n\n    return best_resolution\n```\n\nThis function works by iterating through each possible resolution and calculating the difference in the total pixel count (area) between the original size and each possible resolution. It then selects the resolution with the smallest difference in area, assuming that a smaller difference in area will result in a better fit for resizing purposes.",
  "canonical_solution": "\n    original_width, original_height = original_size\n    best_fit = None\n    max_effective_resolution = 0\n    min_wasted_resolution = float('inf')\n\n    for width, height in possible_resolutions:\n        scale = min(width / original_width, height / original_height)\n        downscaled_width, downscaled_height = int(original_width * scale), int(original_height * scale)\n        effective_resolution = min(downscaled_width * downscaled_height, original_width * original_height)\n        wasted_resolution = (width * height) - effective_resolution\n\n        if effective_resolution > max_effective_resolution or (effective_resolution == max_effective_resolution and wasted_resolution < min_wasted_resolution):\n            max_effective_resolution = effective_resolution\n            min_wasted_resolution = wasted_resolution\n            best_fit = (width, height)\n\n    return best_fit\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE((1920, 1080), [(1280, 720), (1920, 1080), (2560, 1440)])[0] == select_best_resolution((1920, 1080), [(1280, 720), (1920, 1080), (2560, 1440)])[0])\nassert(SOLUTION_SIGNATURE((1920, 1080), [(1024, 768), (1280, 1024), (1600, 1200)])[0] == select_best_resolution((1920, 1080), [(1024, 768), (1280, 1024), (1600, 1200)])[0])\nassert(SOLUTION_SIGNATURE((800, 600), [(1024, 768), (1280, 960), (1920, 1440)])[0] == select_best_resolution((800, 600), [(1024, 768), (1280, 960), (1920, 1440)])[0])\nassert(SOLUTION_SIGNATURE((1920, 1080), [(1280, 720), (1920, 1080), (2560, 1440)])[1] == select_best_resolution((1920, 1080), [(1280, 720), (1920, 1080), (2560, 1440)])[1])\nassert(SOLUTION_SIGNATURE((1920, 1080), [(1024, 768), (1280, 1024), (1600, 1200)])[1] == select_best_resolution((1920, 1080), [(1024, 768), (1280, 1024), (1600, 1200)])[1])\nassert(SOLUTION_SIGNATURE((800, 600), [(1024, 768), (1280, 960), (1920, 1440)])[1] == select_best_resolution((800, 600), [(1024, 768), (1280, 960), (1920, 1440)])[1])\n"
 },
 {
  "task_id": "RWPB/19",
  "prompt": "def rank_accuracy(preds, references):\n    '''\n    Computes the accuracy of ranking predictions compared to reference rankings.\n\n    Parameters\n    ----------\n    preds : list\n        A list of numeric predictions, typically confidence scores, where higher\n        values represent stronger predictions.\n    references : list\n        A list of actual values or labels, with the same ordering principle as\n        'preds', used as a ground truth for accuracy calculation.\n\n    Return\n    ------\n    float\n        The proportion of correct ranking relations in 'preds' relative to 'references'.\n\n    This function first generates all possible pairs of different predictions and checks if\n    the ranking (greater or lesser) of the first element relative to the second in each pair\n    is correct based on the corresponding reference values.\n    '''\n",
  "solution": "    # Initialize a counter for correct rankings\n    correct_count = 0\n    total_pairs = 0\n\n    # Get the number of predictions\n    n = len(preds)\n\n    # Iterate over all pairs of predictions\n    for i in range(n):\n        for j in range(i + 1, n):\n            # Check if the ranking relation between preds[i] and preds[j] is the same as\n            # the ranking relation between references[i] and references[j]\n            if (preds[i] > preds[j] and references[i] > references[j]) or (preds[i] < preds[j] and references[i] < references[j]):\n                correct_count += 1\n            total_pairs += 1\n\n    # Calculate the accuracy as the ratio of correct rankings to total comparisons\n    if total_pairs == 0:\n        return 0.0\n    else:\n        return correct_count / total_pairs\n\n# Example usage:\n# preds = [0.9, 0.55, 0.8]\n# references = [3, 1, 2]\n# print(rank_accuracy(preds, references))  # Output should be the accuracy of the ranking based on the example lists provided.",
  "canonical_solution": "\n    # Initialize an empty list to store the pairs and their comparison results\n    pairs = []\n\n    # Generate all pairwise comparisons from preds except self-comparisons\n    for i, pred in enumerate(preds):\n        for j, pred2 in enumerate(preds):\n            if i == j:\n                continue  # Skip comparing the same elements\n            pairs.append((i, j, pred > pred2))  # Store index and comparison result\n\n    # Count how many of these predicted comparisons are correct according to the references\n    correct = 0\n    for i, ref in enumerate(references):\n        for j, ref2 in enumerate(references):\n            if i == j:\n                continue  # Skip self-comparisons\n            if (i, j, ref > ref2) in pairs:\n                correct += 1  # Increment if the prediction comparison is correct\n\n    # Calculate the accuracy as the ratio of correct predictions to total comparisons\n    return correct / len(pairs) if pairs else 0  # Handle the case where pairs might be empty\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(preds = [0.9, 0.1, 0.5], references = [3, 1, 2]) == rank_accuracy(preds = [0.9, 0.1, 0.5], references = [3, 1, 2]))\nassert(SOLUTION_SIGNATURE(preds = [0.5, 0.9, 0.1], references = [3, 1, 2]) == rank_accuracy(preds = [0.5, 0.9, 0.1], references = [3, 1, 2]))\nassert(SOLUTION_SIGNATURE(preds = [0.5, 0.5, 0.1], references = [2, 2, 1]) == rank_accuracy(preds = [0.5, 0.5, 0.1], references = [2, 2, 1]))\n"
 },
 {
  "task_id": "RWPB/4",
  "prompt": "import numpy as np\nimport torch.nn.functional as F\nimport torch\nimport roma\n\ndef rot6d_to_rotmat(x):\n    \"\"\"\n    6D rotation representation to 3x3 rotation matrix. \n    If the size of matrix is not (B,6), please return ValueError\n\n    Args:\n        x: (B,6) Batch of 6-D rotation representations.\n    Returns:\n        torch.Tensor: Batch of corresponding rotation matrices with shape (B,3,3).\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the function `rot6d_to_rotmat` which converts a batch of 6D rotation representations into 3x3 rotation matrices using PyTorch:\n\n```python\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\ndef rot6d_to_rotmat(x):\n    \"\"\"\n    6D rotation representation to 3x3 rotation matrix. \n    If the size of matrix is not (B,6), please return ValueError\n\n    Args:\n        x: (B,6) Batch of 6-D rotation representations.\n    Returns:\n        torch.Tensor: Batch of corresponding rotation matrices with shape (B,3,3).\n    \"\"\"\n    if x.shape[1] != 6:\n        raise ValueError(\"Input tensor must have shape (B, 6)\")\n\n    # Reshape the input to (B, 2, 3)\n    x = x.reshape(-1, 2, 3)\n\n    # Normalize the first two vectors to be of unit length\n    b1 = F.normalize(x[:, 0], dim=1)\n    b2 = F.normalize(x[:, 1], dim=1)\n\n    # Compute the third basis vector as the cross product of the first two\n    b3 = torch.cross(b1, b2, dim=1)\n    b3 = F.normalize(b3, dim=1)\n\n    # Recompute b2 to ensure the frame is orthonormal\n    # b2 should be orthogonal to both b1 and b3\n    b2 = torch.cross(b3, b1, dim=1)\n\n    # Stack the vectors to form the rotation matrices\n    rotmat = torch.stack((b1, b2, b3), dim=-1)\n\n    return rotmat\n```\n\nThis function first checks if the input tensor `x` has the correct shape `(B, 6)`. It then reshapes the tensor to `(B, 2, 3)` to separate it into two vectors per batch element. These vectors are normalized to ensure they are of unit length. The third basis vector is computed as the cross product of the first two, ensuring the resulting frame is orthogonal. The second vector `b2` is recomputed to ensure the frame remains orthonormal. Finally, the vectors are stacked to form the rotation matrices `(B, 3, 3)`.",
  "canonical_solution": "    \n    if x.size(1) != 6:\n        return ValueError\n    x = x.reshape(-1,2,3).permute(0, 2, 1).contiguous()\n    y = roma.special_gramschmidt(x)\n    return y\n\n\n",
  "unprocess_testcases": "a = torch.randn(3, 6)\nb = torch.randn(1, 6)\nc = torch.randn(3, 5)\n\nassert((SOLUTION_SIGNATURE(a) == rot6d_to_rotmat(a)).all())\nassert((SOLUTION_SIGNATURE(b) == rot6d_to_rotmat(b)).all())\nassert(SOLUTION_SIGNATURE(c) == rot6d_to_rotmat(c))"
 },
 {
  "task_id": "RWPB/39",
  "prompt": "import re\nfrom typing import List\n\n\ndef clean_html(\n        html_to_clean: str,\n        tags_to_remove: List[str] = [\"style\", \"svg\", \"script\"],\n        attributes_to_keep: List[str] = [\"id\", \"href\"],\n) -> str:\n    \"\"\"\n    Cleans HTML content by removing specified tags and their contents, and strips out all attributes except those specified.\n\n    Args:\n        html_to_clean (str): The HTML content to clean.\n        tags_to_remove (List[str]): List of HTML tags to remove from the content. Default tags are 'style', 'svg', and 'script'.\n        attributes_to_keep (List[str]): List of HTML attributes to retain in the remaining HTML tags. Default is ['id', 'href'].\n\n    Returns:\n        str: The cleaned HTML content, free of specified tags and with only desired attributes retained.\n\n    This function first removes all specified tags and their contents using regular expressions. It then eliminates all attributes\n    from the remaining tags except for those explicitly listed to be kept. This is useful for preparing HTML content for environments\n    where only basic styling or functionality is needed, or for security purposes to remove potentially malicious content.\n\n    Example:\n        >>> clean_html('<div id=\"main\" style=\"color:red\">Hello <script>alert(\"World\")</script></div>',\n                       tags_to_remove=['script'], attributes_to_keep=['id'])\n        '<div id=\"main\">Hello </div>'\n    \"\"\"\n",
  "solution": "    # Remove specified tags and their contents\n    for tag in tags_to_remove:\n        html_to_clean = re.sub(r'<{}[^>]*>.*?</{}>'.format(tag, tag), '', html_to_clean, flags=re.DOTALL)\n\n    # Remove all attributes from tags except those specified to keep\n    def clean_attributes(match):\n        tag = match.group(0)\n        # Keep only the specified attributes\n        cleaned_tag = re.sub(r'(\\s+)(\\w+)=(\"[^\"]*\"|\\'[^\\']*\\'|\\S+)', \n                             lambda attr: attr.group(0) if attr.group(2) in attributes_to_keep else '', \n                             tag)\n        return cleaned_tag\n\n    # Apply the attribute cleaning to all tags\n    cleaned_html = re.sub(r'<\\w+[^>]*>', clean_attributes, html_to_clean)\n\n    return cleaned_html\n\n# Example usage:\n# print(clean_html('<div id=\"main\" style=\"color:red\">Hello <script>alert(\"World\")</script></div>',\n#                  tags_to_remove=['script'], attributes_to_keep=['id']))",
  "canonical_solution": "\n    # Remove specified tags and their contents completely\n    for tag in tags_to_remove:\n        html_to_clean = re.sub(\n            rf\"<{tag}[^>]*>.*?</{tag}>\", \"\", html_to_clean, flags=re.DOTALL\n        )\n\n    # Compile a pattern that matches any attribute not in the list of attributes to keep\n    attributes_to_keep = \"|\".join(attributes_to_keep)\n    pattern = rf'\\b(?!({attributes_to_keep})\\b)\\w+(?:-\\w+)?\\s*=\\s*[\"\\'][^\"\\']*[\"\\']'\n\n    # Remove all attributes that are not explicitly listed to be kept\n    cleaned_html = re.sub(pattern, \"\", html_to_clean)\n\n    cleaned_html = re.sub(rf'\\s*>', \">\", cleaned_html)\n\n    return cleaned_html\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE('<div><style> .text { font-size: 12px; } </style><p id=\"info\">Visit our <a href=\"http://example.com\" style=\"color: red;\">website</a></p></div>') == clean_html('<div><style> .text { font-size: 12px; } </style><p id=\"info\">Visit our <a href=\"http://example.com\" style=\"color: red;\">website</a></p></div>'))\nassert(SOLUTION_SIGNATURE('<div id=\"container\"><script>alert(\\'Hello\\');</script><svg><circle cx=\"50\" cy=\"50\" r=\"40\" stroke=\"black\" fill=\"red\"></circle></svg><a href=\"#nothing\" onclick=\"javascript:void(0);\" style=\"color: black;\">Click here</a></div>') == clean_html('<div id=\"container\"><script>alert(\\'Hello\\');</script><svg><circle cx=\"50\" cy=\"50\" r=\"40\" stroke=\"black\" fill=\"red\"></circle></svg><a href=\"#nothing\" onclick=\"javascript:void(0);\" style=\"color: black;\">Click here</a></div>'))\nassert(SOLUTION_SIGNATURE('<header style=\"background: black;\"><nav id=\"top-nav\" class=\"navigation\" data-toggle=\"collapse\"><a href=\"/home\" title=\"Home\">Home</a></nav></header>') == clean_html('<header style=\"background: black;\"><nav id=\"top-nav\" class=\"navigation\" data-toggle=\"collapse\"><a href=\"/home\" title=\"Home\">Home</a></nav></header>'))\n"
 },
 {
  "task_id": "RWPB/68",
  "prompt": "import numpy as np\n\ndef last_before(val, arr):\n  \"\"\"\n  Find the index of the last element in a sorted array that is less than or equal to a given value.Requires sorted (monotonic) arr.\n  Args:\n      - val: int, the value to search for\n      - arr: numpy.ndarray, the sorted array\n  Returns:\n      idx: int, the index of the last element in the array that is less than or equal to val\n  \"\"\"\n",
  "solution": "Certainly! Here's the completion of the function `last_before` that finds the index of the last element in a sorted array that is less than or equal to a given value:\n\n```python\nimport numpy as np\n\ndef last_before(val, arr):\n    \"\"\"\n    Find the index of the last element in a sorted array that is less than or equal to a given value. Requires sorted (monotonic) arr.\n    Args:\n        - val: int, the value to search for\n        - arr: numpy.ndarray, the sorted array\n    Returns:\n        idx: int, the index of the last element in the array that is less than or equal to val\n    \"\"\"\n    # Use binary search to find the rightmost position where `val` can fit\n    low, high = 0, len(arr) - 1\n    best_idx = -1\n    while low <= high:\n        mid = (low + high) // 2\n        if arr[mid] <= val:\n            best_idx = mid  # This might be the answer, but continue to check to the right\n            low = mid + 1\n        else:\n            high = mid - 1\n    return best_idx\n```\n\nThis function uses a binary search algorithm to efficiently find the index of the last element in the array that is less than or equal to the specified value. The binary search is appropriate here because the array is assumed to be sorted. The function returns `-1` if no such element exists (i.e., all elements in the array are greater than the given value).",
  "canonical_solution": "  \n  assert np.all(np.diff(arr) >= 0)\n  return np.searchsorted(arr, val) - 1\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(val = 5, arr = np.array([1, 3, 5, 7])) == last_before(val = 5, arr = np.array([1, 3, 5, 7])))\nassert(SOLUTION_SIGNATURE(val = 4, arr = np.array([1, 3, 5, 7])) == last_before(val = 4, arr = np.array([1, 3, 5, 7])))\nassert(SOLUTION_SIGNATURE(val = 0, arr = np.array([1, 3, 5, 7])) == last_before(val = 0, arr = np.array([1, 3, 5, 7])))\n"
 },
 {
  "task_id": "RWPB/135",
  "prompt": "import torch\n\ndef mask_iou(mask1, mask2, eps=1e-7):\n    \"\"\"\n    Calculate masks IoU.\n\n    Args:\n        mask1 (torch.Tensor): A tensor of shape (N, n) where N is the number of ground truth objects and n is the\n                        product of image width and height.\n        mask2 (torch.Tensor): A tensor of shape (M, n) where M is the number of predicted objects and n is the\n                        product of image width and height.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n\n    Returns:\n        (torch.Tensor): A tensor of shape (N, M) representing masks IoU.\n    \"\"\"\n",
  "solution": "    # Ensure the masks are boolean or convert them to boolean (1 for mask, 0 for background)\n    mask1 = mask1.bool()\n    mask2 = mask2.bool()\n\n    # Compute the intersection between each pair of masks\n    intersection = torch.matmul(mask1, mask2.t())\n    \n    # Compute the area (sum of pixels) for each mask\n    area1 = mask1.sum(dim=1, keepdim=True)  # Shape (N, 1)\n    area2 = mask2.sum(dim=1, keepdim=True)  # Shape (M, 1)\n\n    # Compute the union for each pair of masks\n    union = area1 + area2.t() - intersection  # Broadcasting to shape (N, M)\n\n    # Compute the IoU for each pair of masks\n    iou = intersection / (union + eps)\n\n    return iou\n\n# Example usage:\n# mask1 = torch.tensor([[1, 0, 1], [0, 1, 1]], dtype=torch.float32)\n# mask2 = torch.tensor([[1, 1, 0], [0, 0, 1]], dtype=torch.float32)\n# print(mask_iou(mask1, mask2))",
  "canonical_solution": "\n    intersection = torch.matmul(mask1, mask2.T).clamp_(0)\n    union = (mask1.sum(1)[:, None] + mask2.sum(1)[None]) - intersection  # (area1 + area2) - intersection\n    return intersection / (union + eps)\n\n\n",
  "unprocess_testcases": "mask1 = torch.tensor([\n    [1, 0, 1, 0],  # Object 1\n    [0, 1, 0, 1]   # Object 2\n])\nmask2 = torch.tensor([\n    [1, 0, 0, 1],  # Prediction 1\n    [0, 1, 1, 0],  # Prediction 2\n    [1, 1, 1, 1]   # Prediction 3\n])\nassert((SOLUTION_SIGNATURE(mask1, mask2) == mask_iou(mask1, mask2)).all())\n\nmask1 = torch.tensor([\n    [1, 0, 0, 0],  # Object 1\n    [0, 1, 0, 0]   # Object 2\n])\nmask2 = torch.tensor([\n    [0, 0, 1, 0],  # Prediction 1\n    [0, 0, 0, 1]   # Prediction 2\n])\nassert((SOLUTION_SIGNATURE(mask1, mask2) == mask_iou(mask1, mask2)).all())\n\n\nmask1 = torch.tensor([\n    [1, 1, 0, 0, 1, 1],  # Object 1\n    [0, 1, 1, 1, 0, 0],  # Object 2\n    [1, 0, 1, 0, 1, 0]   # Object 3\n])\nmask2 = torch.tensor([\n    [1, 0, 0, 1, 1, 1],  # Prediction 1\n    [0, 1, 1, 0, 0, 1]   # Prediction 2\n])\nassert((SOLUTION_SIGNATURE(mask1, mask2) == mask_iou(mask1, mask2)).all())\n"
 },
 {
  "task_id": "RWPB/101",
  "prompt": "import torch\n\ndef generate_grids(lower_limit, upper_limit, grid_size):\n    \"\"\"\n    Generate a grid of points within the specified limits.\n\n    Args:\n        lower_limit (torch.Tensor): Lower boundary of the grid.\n        upper_limit (torch.Tensor): Upper boundary of the grid.\n        grid_size (torch.Tensor): Number of grid points per dimension.\n\n    Returns:\n        tuple: Two tensors, each containing the lower and upper bounds of each grid cell.\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `generate_grids` function that generates a grid of points within the specified limits and returns the lower and upper bounds of each grid cell:\n\n```python\nimport torch\n\ndef generate_grids(lower_limit, upper_limit, grid_size):\n    \"\"\"\n    Generate a grid of points within the specified limits.\n\n    Args:\n        lower_limit (torch.Tensor): Lower boundary of the grid.\n        upper_limit (torch.Tensor): Upper boundary of the grid.\n        grid_size (torch.Tensor): Number of grid points per dimension.\n\n    Returns:\n        tuple: Two tensors, each containing the lower and upper bounds of each grid cell.\n    \"\"\"\n    # Calculate the step size for each dimension\n    step_sizes = (upper_limit - lower_limit) / grid_size\n\n    # Generate tensors for each dimension's grid points\n    grids = [torch.linspace(lower_limit[i], upper_limit[i], int(grid_size[i] + 1)) for i in range(len(lower_limit))]\n\n    # Create mesh grids (Cartesian product of grids)\n    meshgrids = torch.meshgrid(*grids, indexing='ij')\n\n    # Calculate lower bounds grid by taking the lower end of each cell\n    lower_bounds = torch.stack([g[:-1] for g in meshgrids], dim=-1)\n\n    # Calculate upper bounds grid by taking the upper end of each cell\n    upper_bounds = torch.stack([g[1:] for g in meshgrids], dim=-1)\n\n    return lower_bounds, upper_bounds\n\n# Example usage:\n# lower_limit = torch.tensor([0.0, 0.0])\n# upper_limit = torch.tensor([1.0, 1.0])\n# grid_size = torch.tensor([10, 10])\n# lower_bounds, upper_bounds = generate_grids(lower_limit, upper_limit, grid_size)\n# print(\"Lower bounds of grid cells:\", lower_bounds)\n# print(\"Upper bounds of grid cells:\", upper_bounds)\n```\n\nThis function first calculates the step sizes for each dimension based on the range specified by `lower_limit` and `upper_limit` and the number of grid points `grid_size`. It then generates linearly spaced points for each dimension using `torch.linspace`. These points are used to create mesh grids representing the Cartesian product of the grid points in each dimension. Finally, it slices these grids to form the lower and upper bounds of each grid cell, which are returned as two separate tensors.",
  "canonical_solution": "    \n    ndim = lower_limit.size(0)\n    assert ndim == upper_limit.size(0)\n    assert lower_limit.ndim == upper_limit.ndim == 1\n    grids = [None] * ndim\n    steps = (upper_limit - lower_limit) / grid_size\n    for d in range(ndim):\n        grids[d] = torch.linspace(\n            lower_limit[d], upper_limit[d], grid_size[d] + 1, device=lower_limit.device\n        )[: grid_size[d]]\n    lower = torch.cartesian_prod(*grids)\n    upper = lower + steps\n    return lower, upper\n\n",
  "unprocess_testcases": "\nlower_limit = torch.tensor([0.0])\nupper_limit = torch.tensor([1.0])\ngrid_size = torch.tensor([2])\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[0] == generate_grids(lower_limit, upper_limit, grid_size)[0]).all())\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[1] == generate_grids(lower_limit, upper_limit, grid_size)[1]).all())\n\nlower_limit = torch.tensor([0.0, 0.0])\nupper_limit = torch.tensor([1.0, 1.0])\ngrid_size = torch.tensor([2, 2])\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[0] == generate_grids(lower_limit, upper_limit, grid_size)[0]).all())\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[1] == generate_grids(lower_limit, upper_limit, grid_size)[1]).all())\n\nlower_limit = torch.tensor([-1.0, 0.0, 2.0])\nupper_limit = torch.tensor([1.0, 1.0, 5.0])\ngrid_size = torch.tensor([2, 3, 1])\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[0] == generate_grids(lower_limit, upper_limit, grid_size)[0]).all())\nassert((SOLUTION_SIGNATURE(lower_limit, upper_limit, grid_size)[1] == generate_grids(lower_limit, upper_limit, grid_size)[1]).all())\n"
 },
 {
  "task_id": "RWPB/111",
  "prompt": "import torch\n\ndef mesh_grid(B, H, W):\n    \"\"\"Create a mesh grid.\n\n    Args:\n        B (int): Batch size.\n        H (int): Height of the grid.\n        W (int): Width of the grid.\n\n    Returns:\n        torch.Tensor: A tensor of shape (B, 2, H, W) containing the mesh grid coordinates.\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `mesh_grid` function using PyTorch. This function will generate a mesh grid for each batch, where the grid contains the normalized coordinates for each point in a 2D space defined by height (H) and width (W).\n\n```python\nimport torch\n\ndef mesh_grid(B, H, W):\n    \"\"\"Create a mesh grid.\n\n    Args:\n        B (int): Batch size.\n        H (int): Height of the grid.\n        W (int): Width of the grid.\n\n    Returns:\n        torch.Tensor: A tensor of shape (B, 2, H, W) containing the mesh grid coordinates.\n    \"\"\"\n    # Create a linear space for height and width\n    # torch.linspace(start, end, steps) creates a 1D tensor of steps evenly spaced points between start and end\n    ys = torch.linspace(-1, 1, H)\n    xs = torch.linspace(-1, 1, W)\n\n    # torch.meshgrid creates a mesh grid from the given 1D tensors\n    # ys is repeated along the width, xs is repeated along the height\n    ys, xs = torch.meshgrid(ys, xs, indexing=\"ij\")\n\n    # Stack the coordinates along a new dimension (at the front, index 0)\n    grid = torch.stack((ys, xs), dim=0)\n\n    # Expand the grid to include the batch size\n    # torch.Tensor.expand replicates the tensor along the specified dimensions\n    grid = grid.expand(B, 2, H, W)\n\n    return grid\n\n# Example usage:\nB, H, W = 4, 5, 5\ngrid = mesh_grid(B, H, W)\nprint(grid.shape)  # Should print: torch.Size([4, 2, 5, 5])\nprint(grid)\n```\n\nThis function first creates linear spaces for the height and width dimensions, which are then used to create a mesh grid using `torch.meshgrid`. The grid coordinates are stacked into a two-channel tensor (for y and x coordinates), and finally, the grid is expanded to match the batch size, ensuring that each element in the batch has its own copy of the grid. The coordinates are normalized between -1 and 1.",
  "canonical_solution": "    \n    x_base = torch.arange(0, W).repeat(B, H, 1)  # BHW\n    y_base = torch.arange(0, H).repeat(B, W, 1).transpose(1, 2)  # BHW\n\n    base_grid = torch.stack([x_base, y_base], 1)  # B2HW\n    return base_grid\n\n",
  "unprocess_testcases": "B=1\nH=1\nW=1\nassert((SOLUTION_SIGNATURE(B, H, W) == mesh_grid(B, H, W)).all())\n\nB=1\nH=5\nW=10\nassert((SOLUTION_SIGNATURE(B, H, W) == mesh_grid(B, H, W)).all())\n\nB=100\nH=10\nW=10\nassert((SOLUTION_SIGNATURE(B, H, W) == mesh_grid(B, H, W)).all())\n\nB=3\nH=10\nW=5\nassert((SOLUTION_SIGNATURE(B, H, W) == mesh_grid(B, H, W)).all())\n"
 },
 {
  "task_id": "RWPB/140",
  "prompt": "from typing import Callable, Dict, Tuple, Any\nimport inspect\nfrom functools import partial\n\ndef get_args_for_func(func: Callable, params: Dict) -> Tuple[Callable, Dict[str, Any]]:\n    \"\"\"\n    Filters keyword arguments from a dictionary based on a function's parameter list and creates a partially\n    applied function that already includes the matched keyword arguments.\n\n    Parameters:\n        func (Callable): The function for which the keyword arguments need to be matched.\n        params (Dict): A dictionary containing potential keyword arguments to pass to the function.\n\n    Returns:\n        Tuple[Callable, Dict[str, Any]]:\n            A tuple containing:\n            - A partial function with pre-applied keyword arguments that are valid for the specified function.\n            - A dictionary of the keyword arguments that were applicable to the function.\n\n    Example:\n        input_dict = {\"a\": \"foo\", \"b\": \"bar\"}\n        def example_func(a: str):\n            return a\n        func, kwargs = get_args_for_func(example_func, input_dict)\n        # func can now be called as func() and it would execute example_func(a=\"foo\")\n        # kwargs will be {'a': 'foo'}\n    \"\"\"\n",
  "solution": "    # Retrieve the parameter names of the function\n    func_params = inspect.signature(func).parameters\n\n    # Filter the params dictionary to only include keys that are valid function parameters\n    valid_kwargs = {key: value for key, value in params.items() if key in func_params}\n\n    # Create a partial function with the filtered keyword arguments\n    partial_func = partial(func, **valid_kwargs)\n\n    return partial_func, valid_kwargs\n\n# Example usage:\n# input_dict = {\"a\": \"foo\", \"b\": \"bar\"}\n# def example_func(a: str):\n#     return a\n# func, kwargs = get_args_for_func(example_func, input_dict)\n# result = func()  # Executes example_func(a=\"foo\")\n# print(result)  # Outputs: foo\n# print(kwargs)  # Outputs: {'a': 'foo'}",
  "canonical_solution": "    \n    # Extracts function argument names using inspect\n    _kwargs = {k: v for k, v in params.items() if k in inspect.getfullargspec(func).args}\n    # Creates a partial function that pre-applies these keyword arguments\n    return (\n        partial(func, **_kwargs),\n        _kwargs,\n    )\n\n\n",
  "unprocess_testcases": "def add(x, y):\n    return x + y\n\ndef multiply(x, y):\n    return x * y\n\ndef greet(name, msg=\"Hello\"):\n    return f\"{msg}, {name}\"\n\nfunc, kwargs = get_args_for_func(add, {'x': 10, 'y': 5})\n\nassert(SOLUTION_SIGNATURE(add, {'x': 10, 'y': 5})[0]() == 15)\nassert(SOLUTION_SIGNATURE(add, {'x': 10, 'y': 5})[1] == get_args_for_func(add, {'x': 10, 'y': 5})[1])\n\nassert(SOLUTION_SIGNATURE(multiply, {'x': 2, 'y': 8, 'z': 15})[0]() == 16)\nassert(SOLUTION_SIGNATURE(multiply, {'x': 2, 'y': 8, 'z': 15})[1] == get_args_for_func(multiply, {'x': 2, 'y': 8, 'z': 15})[1])\n\nassert(SOLUTION_SIGNATURE(greet, {'name': 'Alice'})[0]() == \"Hello, Alice\")\nassert(SOLUTION_SIGNATURE(greet, {'name': 'Alice'})[1] == get_args_for_func(greet, {'name': 'Alice'})[1])"
 },
 {
  "task_id": "RWPB/125",
  "prompt": "import numpy as np\n\ndef resample_segments(segments, n=1000):\n    \"\"\"\n    Resamples each segment in a list to have a specified number of equidistant points.\n\n    Args:\n        segments (list): A list of numpy arrays of shape (n, 2), where each array represents a segment with n points.\n        n (int): The number of points to resample each segment to, defaults to 1000.\n\n    Returns:\n        list: A list of numpy arrays, each reshaped to (n, 2), representing the resampled segments.\n    \"\"\"\n",
  "solution": "```python\nimport numpy as np\n\ndef resample_segments(segments, n=1000):\n    \"\"\"\n    Resamples each segment in a list to have a specified number of equidistant points.\n\n    Args:\n        segments (list): A list of numpy arrays of shape (n, 2), where each array represents a segment with n points.\n        n (int): The number of points to resample each segment to, defaults to 1000.\n\n    Returns:\n        list: A list of numpy arrays, each reshaped to (n, 2), representing the resampled segments.\n    \"\"\"\n    resampled_segments = []\n    for segment in segments:\n        # Calculate the cumulative distance along the segment\n        distances = np.sqrt(np.sum(np.diff(segment, axis=0)**2, axis=1))\n        cumulative_distances = np.insert(np.cumsum(distances), 0, 0)\n\n        # Create an interpolator for each dimension\n        x_interpolator = np.interp\n        y_interpolator = np.interp\n\n        # Generate the new equally spaced points along the cumulative distance\n        new_distances = np.linspace(0, cumulative_distances[-1], n)\n\n        # Interpolate x and y values at new distances\n        new_x = x_interpolator(new_distances, cumulative_distances, segment[:, 0])\n        new_y = y_interpolator(new_distances, cumulative_distances, segment[:, 1])\n\n        # Stack the new x and y arrays to form the new segment\n        new_segment = np.vstack((new_x, new_y)).T\n        resampled_segments.append(new_segment)\n\n    return resampled_segments\n```\n\nThis function first calculates the cumulative distances along each segment, then uses linear interpolation to generate new points that are equidistant along the path of the segment. The new points are then stacked to form the resampled segment, which is added to the list of resampled segments.",
  "canonical_solution": "    \n    for i, s in enumerate(segments):\n        # Ensure the segment is closed by appending the first point to the end\n        s = np.concatenate((s, s[0:1, :]), axis=0)\n        # Create an array of the desired number of points spaced between 0 and the last index of the segment\n        x = np.linspace(0, len(s) - 1, n)\n        # Create an array of existing indices\n        xp = np.arange(len(s))\n        # Interpolate new points for both x and y coordinates\n        segments[i] = np.stack(\n            [np.interp(x, xp, s[:, j]) for j in range(2)], axis=-1\n        )  # Stack interpolated x and y coordinates\n    return segments\n\n",
  "unprocess_testcases": "segment1 = np.array([[0, 0], [2.5, 2.5], [5, 5], [7.5, 7.5], [10, 10]])\nsegment2 = np.array([[0, 0], [1, 5], [2, 10], [3, 15], [4, 20], [5, 25]])\nsegments = [segment1, segment2]\nassert((SOLUTION_SIGNATURE(segments, n=100)[i] == resample_segments(segments, n=100)[i]).all() for i in range(len(resample_segments(segments, n=100))))\n\nsegment = np.array([[0, 0], [1, 1]])\nsegments = [segment]\nassert((SOLUTION_SIGNATURE(segments, n=3)[i] == resample_segments(segments, n=3)[i]).all for i in range(len(resample_segments(segments, n=3))))\n\nsegment = np.array([[0, 0], [1, 1]])\nsegments = [segment]\nassert((SOLUTION_SIGNATURE(segments, n=1)[i] == resample_segments(segments, n=1)[i]).all() for i in range(len(resample_segments(segments, n=1))))\n\nrepeated_point_segment = np.array([[1, 1]] * 5)\ncircle = np.array([[np.cos(theta), np.sin(theta)] for theta in np.linspace(0, 2 * np.pi, 100, endpoint=False)])\nsegments = [repeated_point_segment, circle]\nassert((SOLUTION_SIGNATURE(segments, n=50)[i] == resample_segments(segments, n=50)[i]).all() for i in range(len(resample_segments(segments, n=50))))\n"
 },
 {
  "task_id": "RWPB/78",
  "prompt": "import pandas\n\ndef check_all_columns_numeric(df):\n    \"\"\"\n    Check if all columns in a DataFrame are numeric. Return True if so, False otherwise.\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the function `check_all_columns_numeric` which checks if all columns in a given pandas DataFrame are numeric:\n\n```python\nimport pandas as pd\n\ndef check_all_columns_numeric(df):\n    \"\"\"\n    Check if all columns in a DataFrame are numeric. Return True if so, False otherwise.\n    \n    Args:\n    df (pandas.DataFrame): The DataFrame to check.\n\n    Returns:\n    bool: True if all columns are numeric, False otherwise.\n    \"\"\"\n    # Check if all columns in the DataFrame are of numeric data types\n    return df.select_dtypes(include=[np.number]).shape[1] == df.shape[1]\n```\n\nThis function uses `pandas.DataFrame.select_dtypes` to filter the columns of the DataFrame to only those that are numeric. It then compares the number of these numeric columns to the total number of columns in the DataFrame. If the numbers match, it means all columns are numeric, and the function returns `True`. Otherwise, it returns `False`.",
  "canonical_solution": "    \n    return df.select_dtypes(include=[int, float]).shape[1] == df.shape[1]\n\n\n",
  "unprocess_testcases": "\nimport pandas as pd\ndf_numeric = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4.1, 5.2, 6.3]\n})\nassert(SOLUTION_SIGNATURE(df_numeric) == check_all_columns_numeric(df_numeric))\n\ndf_mixed = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': ['x', 'y', 'z']\n})\nassert(SOLUTION_SIGNATURE(df_mixed) == check_all_columns_numeric(df_mixed))\n\ndf_empty = pd.DataFrame()\nassert(SOLUTION_SIGNATURE(df_empty) == check_all_columns_numeric(df_empty))\n\ndf_varied_types = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [1.5, 2.5, 3.5],\n    'C': [True, False, True]\n})\nassert(SOLUTION_SIGNATURE(df_varied_types) == check_all_columns_numeric(df_varied_types))\n"
 },
 {
  "task_id": "RWPB/29",
  "prompt": "import torch\n\ndef iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    \"\"\"\n    Computes the Intersection over Union (IoU) for the foreground class in binary segmentation.\n\n    Args:\n        preds (Tensor): Predictions from the model, where 1 indicates foreground and 0 indicates background.\n        labels (Tensor): Ground truth binary masks corresponding to the predictions.\n        EMPTY (float, optional): The value to return if there is no union between predicted and ground truth pixels.\n                                 Default is 1.0.\n        ignore (int, optional): Label value to ignore during IoU calculation. This is useful for ignoring certain\n                                pixels in evaluation, such as boundary or undefined areas.\n        per_image (bool, optional): If True, calculates IoU independently for each image and then averages them.\n                                    If False, calculates IoU across the entire batch as a whole. Default is True.\n\n    Returns:\n        float: The IoU score multiplied by 100 to convert it to a percentage.\n\n    This function calculates the IoU score for the foreground class, accounting for any ignore labels\n    and handling both individual image evaluations and batch-wide evaluations. The IoU is calculated\n    by determining the intersection and union of the predicted and actual foreground areas, and handling\n    cases where the union might be zero.\n    \"\"\"\n",
  "solution": "    # Convert predictions and labels to boolean tensors\n    preds = preds > 0.5\n    labels = labels > 0.5\n\n    if ignore is not None:\n        # Create a mask where ignore labels are False and others are True\n        ignore_mask = labels != ignore\n        # Apply the ignore mask\n        preds = preds & ignore_mask\n        labels = labels & ignore_mask\n\n    # Calculate intersection and union\n    intersection = (preds & labels).float().sum(dim=(1, 2, 3))\n    union = (preds | labels).float().sum(dim=(1, 2, 3))\n\n    # Calculate IoU, handle division by zero\n    iou = torch.where(union > 0, intersection / union, torch.tensor(EMPTY).to(intersection.device))\n\n    if per_image:\n        # Average IoU across all images in the batch\n        iou = iou.mean()\n    else:\n        # Calculate global IoU across the entire batch\n        iou = intersection.sum() / union.sum()\n\n    # Convert IoU to percentage\n    return iou.item() * 100\n\n# Example usage:\n# preds = torch.randn(10, 1, 256, 256)\n# labels = torch.randint(0, 2, (10, 1, 256, 256))\n# print(iou_binary(preds, labels))",
  "canonical_solution": "\n    # Helper function to calculate mean\n    def mean(l):\n        return sum(l) / len(l) if l else EMPTY\n\n    # Handle batch-wide calculation by wrapping predictions and labels in tuples if per_image is False\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n\n    ious = []\n    for pred, label in zip(preds, labels):\n        # Calculate intersection and union for each image\n        intersection = ((label == 1) & (pred == 1)).sum()\n        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n\n        # Calculate IoU, handling cases with no union\n        if not union:\n            iou = EMPTY\n        else:\n            iou = float(intersection) / float(union)\n\n        ious.append(iou)\n\n    # Calculate the mean IoU across images if per_image, or for the batch\n    iou = mean(ious)\n    return 100 * iou  # Return IoU as a percentage\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(torch.tensor([[1,0,1,0],[0,1,0,1]]), torch.tensor([[1,0,1,0],[0,1,0,1]]), 1.0, None, True) == iou_binary(torch.tensor([[1,0,1,0],[0,1,0,1]]), torch.tensor([[1,0,1,0],[0,1,0,1]]), 1.0, None, True))\nassert(SOLUTION_SIGNATURE(torch.tensor([[0,0,0,0],[1,1,1,1]]), torch.tensor([[0,0,0,0],[0,0,0,0]]), 0.5, 1, True) == iou_binary(torch.tensor([[0,0,0,0],[1,1,1,1]]), torch.tensor([[0,0,0,0],[0,0,0,0]]), 0.5, 1, True))\nassert(SOLUTION_SIGNATURE(torch.tensor([[1,1,0,0],[0,0,1,1]]), torch.tensor([[1,0,0,1],[1,1,0,0]]), 1.0, 0, False) == iou_binary(torch.tensor([[1,1,0,0],[0,0,1,1]]), torch.tensor([[1,0,0,1],[1,1,0,0]]), 1.0, 0, False))\n"
 },
 {
  "task_id": "RWPB/5",
  "prompt": "def seconds_to_hh_mm_ss(seconds):\n    \"\"\"\n    Converts a given number of seconds to a string in hh:mm:ss format.\n\n    Args:\n        seconds: An integer representing the number of seconds.\n\n    Returns:\n        A string in hh:mm:ss format, or \"00:00:00\" if seconds is negative.\n    \"\"\"\n",
  "solution": "Certainly! Here's the completed function that converts seconds into hours, minutes, and seconds in the format hh:mm:ss:\n\n```python\ndef seconds_to_hh_mm_ss(seconds):\n    \"\"\"\n    Converts a given number of seconds to a string in hh:mm:ss format.\n\n    Args:\n        seconds: An integer representing the number of seconds.\n\n    Returns:\n        A string in hh:mm:ss format, or \"00:00:00\" if seconds is negative.\n    \"\"\"\n    if seconds < 0:\n        return \"00:00:00\"\n    \n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    \n    return f\"{hours:02}:{minutes:02}:{seconds:02}\"\n```\n\nThis function first checks if the input `seconds` is negative, returning \"00:00:00\" if true. If not, it calculates the number of hours, minutes, and remaining seconds, and then formats them into a string with two digits each, padded with zeros if necessary.",
  "canonical_solution": "    if seconds < 0:\n        return \"00:00:00\"\n\n    hours = seconds // 3600  # Get hours by dividing by total seconds in an hour\n    seconds %= 3600  # Get remaining seconds after hours calculation\n\n    minutes = seconds // 60  # Get minutes by dividing by total seconds in a minute\n    seconds %= 60  # Get remaining seconds after minutes calculation\n\n    # Format hour, minute, and second values with leading zeros\n    hours_str = f\"{hours:02d}\"\n    minutes_str = f\"{minutes:02d}\"\n    seconds_str = f\"{seconds:02d}\"\n\n    return f\"{hours_str}:{minutes_str}:{seconds_str}\"\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(-10) == seconds_to_hh_mm_ss(-10))\nassert(SOLUTION_SIGNATURE(3661) == seconds_to_hh_mm_ss(3661))\nassert(SOLUTION_SIGNATURE(7200) == seconds_to_hh_mm_ss(7200))"
 },
 {
  "task_id": "RWPB/38",
  "prompt": "from typing import List\n\ndef extract_imports_from_lines(lines: List[str]) -> str:\n    \"\"\"\n    Extracts import statements from a list of Python code lines.\n\n    Args:\n        lines (List[str]): A list of strings, each representing a line of Python code.\n\n    Returns:\n        str: A single string containing all extracted import and from-import lines, separated by new lines.\n\n    This function filters through a list of Python code lines, identifying and extracting lines that are import statements.\n    It supports both 'import' and 'from ... import ...' formats. This is useful for analyzing or consolidating imports\n    from a set of Python code lines, such as when preparing a script or checking for consistency and redundancy in imports.\n    \"\"\"\n",
  "solution": "    # Initialize an empty list to store import lines\n    import_lines = []\n\n    # Loop through each line in the input list\n    for line in lines:\n        # Strip whitespace from the beginning and end of the line\n        stripped_line = line.strip()\n        \n        # Check if the line starts with 'import' or 'from' followed by a space\n        if stripped_line.startswith('import ') or stripped_line.startswith('from '):\n            # If it's an import line, add it to the list\n            import_lines.append(stripped_line)\n\n    # Join all collected import lines with a newline character and return the result\n    return '\\n'.join(import_lines)",
  "canonical_solution": "\n    # Use list comprehension to filter out lines that start with 'import' or 'from', indicating import statements\n    return \"\\n\".join(\n        [line for line in lines if line.startswith(\"from\") or line.startswith(\"import\")]\n    )\n\n\n\n",
  "unprocess_testcases": "a = [\n    \"import os\",\n    \"x = 10\",                 # Non-import line\n    \"from sys import argv\",\n    \"print('Hello World!')\",  # Non-import line\n    \"import math\"\n]\nassert(SOLUTION_SIGNATURE(a) == extract_imports_from_lines(a))\n\nb = [\n    \"Import os\",                  # Incorrect case\n    \"# from datetime import datetime\",  # Commented import\n    \"from collections import deque\",    # Correct import\n    \"IMPORT sys\",                 # Incorrect case\n    \"from math import pi\"         # Correct import\n]\nassert(SOLUTION_SIGNATURE(b) == extract_imports_from_lines(b))\n\nc = [\n    \"from numpy import array, random\",\n    \"import scipy.stats as stats\",  # Aliased import\n    \"from os.path import join, exists\",  # Multiple imports from same module\n    \"x = 'import should not be detected'\",  # Import within a string\n    \"import pandas as pd\"  # Aliased import\n]\nassert(SOLUTION_SIGNATURE(c) == extract_imports_from_lines(c))\n"
 },
 {
  "task_id": "RWPB/69",
  "prompt": "import numpy as np\nimport math\n\ndef fovx_to_fovy(fovx, aspect):\n    \"\"\"\n    Converts the horizontal field of view (FoVx) to the vertical field of view (FoVy) based on the aspect ratio.\n\n    Parameters:\n    fovx (float): The horizontal field of view in radians.\n    aspect (float): The aspect ratio of the image (width/height).\n\n    Returns:\n    float: The vertical field of view in radians.\n\n    Process:\n    1. Calculate the tangent of half the horizontal field of view (FoVx/2).\n    2. Divide this value by the aspect ratio to get the tangent of half the vertical field of view.\n    3. Use the arctangent function to find half the vertical field of view from its tangent.\n    4. Multiply by 2 to obtain the full vertical field of view (FoVy).\n    \"\"\"\n",
  "solution": "    # Step 1: Calculate the tangent of half the horizontal field of view\n    tan_half_fovx = math.tan(fovx / 2)\n\n    # Step 2: Divide by the aspect ratio to get the tangent of half the vertical field of view\n    tan_half_fovy = tan_half_fovx / aspect\n\n    # Step 3: Use the arctangent function to find half the vertical field of view from its tangent\n    half_fovy = math.atan(tan_half_fovy)\n\n    # Step 4: Multiply by 2 to obtain the full vertical field of view\n    fovy = 2 * half_fovy\n\n    return fovy\n\n# Example usage:\n# fovy = fovx_to_fovy(math.radians(90), 16/9)\n# print(\"FoVy in degrees:\", math.degrees(fovy))",
  "canonical_solution": "    \n    return np.arctan(np.tan(fovx / 2) / aspect) * 2.0\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(math.pi/4, 16/9) == fovx_to_fovy(math.pi/4, 16/9))\nassert(SOLUTION_SIGNATURE(math.pi/3, 0.5) == fovx_to_fovy(math.pi/3, 0.5))\nassert(SOLUTION_SIGNATURE(math.pi, 1) == fovx_to_fovy(math.pi, 1))\n"
 },
 {
  "task_id": "RWPB/134",
  "prompt": "def select_best_resolution(original_size, possible_resolutions):\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        original_size (tuple): The original size of the image in the format (width, height).\n        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2), ...].\n\n    Returns:\n        tuple: The best fit resolution in the format (width, height).\n    \"\"\"\n",
  "solution": "Certainly! Here's the completion of the function `select_best_resolution`:\n\n```python\ndef select_best_resolution(original_size, possible_resolutions):\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        original_size (tuple): The original size of the image in the format (width, height).\n        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2), ...].\n\n    Returns:\n        tuple: The best fit resolution in the format (width, height).\n    \"\"\"\n    original_width, original_height = original_size\n    best_resolution = None\n    min_diff = float('inf')\n\n    for width, height in possible_resolutions:\n        # Calculate the difference in area between the original and the possible resolution\n        diff = abs((original_width * original_height) - (width * height))\n        \n        # Update the best resolution if the current one has a smaller difference\n        if diff < min_diff:\n            min_diff = diff\n            best_resolution = (width, height)\n\n    return best_resolution\n```\n\nThis function works by iterating through each possible resolution and calculating the difference in the total pixel count (area) between the original size and each possible resolution. It then selects the resolution with the smallest difference in area, assuming that a smaller difference in area will provide a resolution closer to the original, thus maintaining more of the original image's characteristics.",
  "canonical_solution": "    \n    original_width, original_height = original_size\n    best_fit = None\n    max_effective_resolution = 0\n    min_wasted_resolution = float('inf')\n\n    for width, height in possible_resolutions:\n        scale = min(width / original_width, height / original_height)\n        downscaled_width, downscaled_height = int(original_width * scale), int(original_height * scale)\n        effective_resolution = min(downscaled_width * downscaled_height, original_width * original_height)\n        wasted_resolution = (width * height) - effective_resolution\n\n        if effective_resolution > max_effective_resolution or (effective_resolution == max_effective_resolution and wasted_resolution < min_wasted_resolution):\n            max_effective_resolution = effective_resolution\n            min_wasted_resolution = wasted_resolution\n            best_fit = (width, height)\n\n    return best_fit\n\n\n",
  "unprocess_testcases": "original_size = (1920, 1080)\npossible_resolutions = [(800, 600), (1920, 1080), (2560, 1440)]\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[0] == select_best_resolution(original_size, possible_resolutions)[0])\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[1] == select_best_resolution(original_size, possible_resolutions)[1])\n\noriginal_size = (1024, 768)\npossible_resolutions = [(800, 600), (1600, 1200), (1280, 960)]\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[0] == select_best_resolution(original_size, possible_resolutions)[0])\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[1] == select_best_resolution(original_size, possible_resolutions)[1])\n\n\noriginal_size = (1280, 720)\npossible_resolutions = [(1024, 768), (1920, 1080), (1600, 900)]\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[0] == select_best_resolution(original_size, possible_resolutions)[0])\nassert(SOLUTION_SIGNATURE(original_size, possible_resolutions)[1] == select_best_resolution(original_size, possible_resolutions)[1])\n\n\n"
 },
 {
  "task_id": "RWPB/100",
  "prompt": "import torch\n\ndef project_onto_planes(planes: torch.Tensor, coordinates: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Projects 3D coordinates onto a batch of 2D planes, returning 2D plane coordinates.\n\n    Args:\n        planes (torch.Tensor): Tensor defining plane axes with shape (n_planes, 3, 3).\n        coordinates (torch.Tensor): Tensor of 3D coordinates with shape (N, M, 3).\n\n    Returns:\n        torch.Tensor: Projected 2D coordinates with shape (N*n_planes, M, 2).\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the function `project_onto_planes` which projects 3D coordinates onto a batch of 2D planes. The function computes the dot product between the coordinates and the first two axes of each plane to obtain the 2D coordinates on those planes.\n\n```python\nimport torch\n\ndef project_onto_planes(planes: torch.Tensor, coordinates: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Projects 3D coordinates onto a batch of 2D planes, returning 2D plane coordinates.\n\n    Args:\n        planes (torch.Tensor): Tensor defining plane axes with shape (n_planes, 3, 3).\n        coordinates (torch.Tensor): Tensor of 3D coordinates with shape (N, M, 3).\n\n    Returns:\n        torch.Tensor: Projected 2D coordinates with shape (N*n_planes, M, 2).\n    \"\"\"\n    # Extract the number of planes and the dimensions of the coordinates\n    n_planes = planes.shape[0]\n    N, M, _ = coordinates.shape\n\n    # Reshape coordinates to broadcast across planes\n    # New shape will be (N, 1, M, 3) to align with (n_planes, 3, 3)\n    coordinates_expanded = coordinates.unsqueeze(1)\n\n    # Compute the dot product between coordinates and the first two axes of each plane\n    # planes[:, :2, :] has shape (n_planes, 2, 3)\n    # Resulting dot product will have shape (N, n_planes, M, 2)\n    projected = torch.einsum('nij,bnmj->bnmi', planes[:, :2, :], coordinates_expanded)\n\n    # Reshape to combine the batches of planes across all N\n    # Final shape will be (N*n_planes, M, 2)\n    projected = projected.view(N * n_planes, M, 2)\n\n    return projected\n```\n\nThis function uses `torch.einsum` to efficiently compute the dot products between the plane axes and the coordinates. The `einsum` operation specifies that for each plane and each batch of coordinates, the dot product is taken between the plane's first two axes (representing the 2D plane basis) and the coordinate vectors. The result is then reshaped to group all plane projections for each batch of coordinates together, matching the desired output shape.",
  "canonical_solution": "    \n    N, M, C = coordinates.shape\n    n_planes, _, _ = planes.shape\n    coordinates = coordinates.unsqueeze(1).expand(-1, n_planes, -1, -1).reshape(N*n_planes, M, 3)\n    inv_planes = torch.linalg.inv(planes).unsqueeze(0).expand(N, -1, -1, -1).reshape(N*n_planes, 3, 3)\n    projections = torch.bmm(coordinates, inv_planes)\n    return projections[..., :2]\n\n\n",
  "unprocess_testcases": "planes = torch.tensor([[[1, 0, 0], [0, 1, 0], [0, 0, 1]], \n                       [[0, 1, 0], [1, 0, 0], [0, 0, 1]]], dtype=torch.float32)\ncoordinates = torch.tensor([[[1, 2, 3], [4, 5, 6]]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(planes, coordinates) == project_onto_planes(planes, coordinates)).all())\n\nplanes = torch.tensor([[[1, 0, 0], [0, 1, 0], [0, 0, 1]], \n                       [[0, 1, 0], [1, 0, 0], [0, 0, 1]],\n                       [[1, 0, 0], [0, 0, 1], [0, 1, 0]]], dtype=torch.float32)\ncoordinates = torch.tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n                            [[9, 8, 7], [6, 5, 4], [3, 2, 1]]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(planes, coordinates) == project_onto_planes(planes, coordinates)).all())\n\nplanes = torch.tensor([[[1, 0, 0], [0, 1, 0], [0, 0, 1]]], dtype=torch.float32)\ncoordinates = torch.tensor([[[0, -1, -2]]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(planes, coordinates) == project_onto_planes(planes, coordinates)).all())\n"
 },
 {
  "task_id": "RWPB/110",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\n\ndef find_intersection(plane_normal,plane_point,ray_direction,ray_point):\n    \"\"\"\n    Finds the intersection point of a line and a plane.\n\n    Args:\n        - plane_normal: numpy array, representing a normal vector of the plane.\n        - plane_point: numpy array, representing a point on the plane.\n        - ray_direction: numpy array, representing a direction vector of the line.\n        - ray_point: numpy array, representing a point on the line.\n\n    Returns:\n        Psi: numpy array or None, the intersection point of the line and the plane.\n    \"\"\"\n",
  "solution": "    # Calculate the dot product of the plane normal and the ray direction\n    dot_product = np.dot(plane_normal, ray_direction)\n\n    # Check if the ray is parallel to the plane\n    if np.isclose(dot_product, 0):\n        return None  # No intersection, the line is parallel to the plane\n\n    # Calculate the vector from the ray point to the plane point\n    vector_diff = plane_point - ray_point\n\n    # Calculate the distance from the ray point to the intersection point on the plane\n    t = np.dot(plane_normal, vector_diff) / dot_product\n\n    # Calculate the intersection point\n    intersection_point = ray_point + t * ray_direction\n\n    return intersection_point\n\n# Example usage:\n# plane_normal = np.array([0, 0, 1])\n# plane_point = np.array([0, 0, 5])\n# ray_direction = np.array([0, 0, 1])\n# ray_point = np.array([0, 0, 0])\n# intersection = find_intersection(plane_normal, plane_point, ray_direction, ray_point)\n# print(intersection)  # Output should be [0, 0, 5] if the ray intersects the plane at z=5",
  "canonical_solution": "    \n    epsilon=1e-6\n    ndotu = plane_normal.dot(ray_direction)\n    if abs(ndotu) < epsilon:\n        # no intersection or line is within plane\n        return None\n    w = ray_point - plane_point\n    si = -plane_normal.dot(w) / ndotu\n    Psi = w + si * ray_direction + plane_point #intersections\n    return Psi\n\n\n",
  "unprocess_testcases": "plane_normal = np.array([0, 0, 1])\nplane_point = np.array([0, 0, 5])\nray_direction = np.array([1, 1, -1])\nray_point = np.array([1, 1, 10])\nassert((SOLUTION_SIGNATURE(plane_point, plane_point, ray_direction, ray_point) == find_intersection(plane_point, plane_point, ray_direction, ray_point)).all())\n\nplane_normal = np.array([0, 0, 1])\nplane_point = np.array([0, 3, 5])\nray_direction = np.array([1, 1, -1])\nray_point = np.array([1, 1, 9])\nassert((SOLUTION_SIGNATURE(plane_point, plane_point, ray_direction, ray_point) == find_intersection(plane_point, plane_point, ray_direction, ray_point)).all())\n\nplane_normal = np.array([0, 1, 0])\nplane_point = np.array([0, 2, 0])\nray_direction = np.array([1, 0, 0])\nray_point = np.array([0, 0, 0])\nassert(SOLUTION_SIGNATURE(plane_point, plane_point, ray_direction, ray_point) == find_intersection(plane_point, plane_point, ray_direction, ray_point))\n\nplane_normal = np.array([0, 0, 1])\nplane_point = np.array([0, 0, 0])\nray_direction = np.array([1, 1, 0])\nray_point = np.array([1, 1, 0])\nassert(SOLUTION_SIGNATURE(plane_point, plane_point, ray_direction, ray_point) == find_intersection(plane_point, plane_point, ray_direction, ray_point))\n"
 },
 {
  "task_id": "RWPB/124",
  "prompt": "import math\nimport torch\n\n\ndef make_divisible(x, divisor):\n    \"\"\"\n    Adjusts x to the nearest value that is divisible by divisor.\n\n    Arguments:\n    x : int or float\n        The value to be made divisible.\n    divisor : int or torch.Tensor\n        The divisor to be used. If a torch.Tensor is provided, uses its maximum value.\n\n    Returns:\n    int\n        The nearest value to x that is divisible by the specified divisor.\n    \"\"\"\n",
  "solution": "Certainly! Here's the completion of the `make_divisible` function:\n\n```python\nimport math\nimport torch\n\ndef make_divisible(x, divisor):\n    \"\"\"\n    Adjusts x to the nearest value that is divisible by divisor.\n\n    Arguments:\n    x : int or float\n        The value to be made divisible.\n    divisor : int or torch.Tensor\n        The divisor to be used. If a torch.Tensor is provided, uses its maximum value.\n\n    Returns:\n    int\n        The nearest value to x that is divisible by the specified divisor.\n    \"\"\"\n    if isinstance(divisor, torch.Tensor):\n        divisor = torch.max(divisor).item()\n    \n    x = float(x)  # Ensure x is a float for division\n    return int(math.ceil(x / divisor) * divisor)\n```\n\nThis function first checks if the `divisor` is a `torch.Tensor`. If it is, it extracts the maximum value from the tensor and uses it as the divisor. The function then ensures that `x` is treated as a float for the division operation. It calculates the nearest higher multiple of the divisor that is greater than or equal to `x` using `math.ceil`, and then multiplies it back by the divisor to get the adjusted value. Finally, it converts the result back to an integer before returning it.",
  "canonical_solution": "    \n    # Convert divisor to int if it is a torch.Tensor, taking the maximum value in the tensor\n    if isinstance(divisor, torch.Tensor):\n        divisor = int(divisor.max())  # Convert to int\n\n    # Compute the nearest value to x that is divisible by divisor\n    return math.ceil(x / divisor) * divisor  # Use ceil to ensure it rounds up to the nearest divisor\n\n",
  "unprocess_testcases": "x = 23\ndivisor = 5\nassert(SOLUTION_SIGNATURE(x, divisor) == make_divisible(x, divisor))\n\nx = 14.7\ndivisor = 3\nassert(SOLUTION_SIGNATURE(x, divisor) == make_divisible(x, divisor))\n\nx = 100\ndivisor = torch.Tensor([2,8,10])\nassert(SOLUTION_SIGNATURE(x, divisor) == make_divisible(x, divisor))\n"
 },
 {
  "task_id": "RWPB/79",
  "prompt": "import numpy as np\n\ndef pr_from_grav(grav):\n  \"\"\"\n  Compute pitch and roll angles from a matrix of gravity vectors in body frame.\n  Args:\n      - grav: numpy.ndarray, matrix of gravity vectors in body frame (shape: (n, 3))\n  Returns:\n      - pitches: numpy.ndarray, pitch angles (shape: (n,))\n      - rolls: numpy.ndarray, roll angles (shape: (n,))\n  \"\"\"\n",
  "solution": "Certainly! To compute the pitch and roll angles from a matrix of gravity vectors, we can use the following trigonometric relationships:\n\n1. **Pitch** (θ) can be calculated using the formula:\n   \\[\n   \\theta = \\arctan2(-g_x, \\sqrt{g_y^2 + g_z^2})\n   \\]\n   where \\( g_x \\), \\( g_y \\), and \\( g_z \\) are the x, y, and z components of the gravity vector respectively. This formula gives the angle of rotation around the y-axis.\n\n2. **Roll** (φ) can be calculated using the formula:\n   \\[\n   \\phi = \\arctan2(g_y, g_z)\n   \\]\n   where \\( g_y \\) and \\( g_z \\) are the y and z components of the gravity vector respectively. This formula gives the angle of rotation around the x-axis.\n\nHere's how you can implement this in the function:\n\n```python\nimport numpy as np\n\ndef pr_from_grav(grav):\n    \"\"\"\n    Compute pitch and roll angles from a matrix of gravity vectors in body frame.\n    Args:\n        - grav: numpy.ndarray, matrix of gravity vectors in body frame (shape: (n, 3))\n    Returns:\n        - pitches: numpy.ndarray, pitch angles (shape: (n,))\n        - rolls: numpy.ndarray, roll angles (shape: (n,))\n    \"\"\"\n    # Extract components of gravity vectors\n    gx = grav[:, 0]\n    gy = grav[:, 1]\n    gz = grav[:, 2]\n\n    # Compute pitch angles\n    pitches = np.arctan2(-gx, np.sqrt(gy**2 + gz**2))\n\n    # Compute roll angles\n    rolls = np.arctan2(gy, gz)\n\n    return pitches, rolls\n```\n\nThis function takes a numpy array `grav` where each row represents the gravity vector in the body frame of a device (e.g., a smartphone or an aircraft). It returns two arrays: `pitches` and `rolls`, which contain the pitch and roll angles for each gravity vector, respectively. The angles are in radians. If you need them in degrees, you can convert them using `np.degrees(pitches)` and `np.degrees(rolls)`.",
  "canonical_solution": "  \n  pitches = np.arcsin(-grav[:, 0])\n  rolls = np.arctan2(grav[:, 1], grav[:, 2])\n  return pitches, rolls\n\n\n",
  "unprocess_testcases": "a = np.array([[0, 0, -1],  # No tilt (pitch=roll=0)\n          [0, 0.7071, 0.7071],  # Tilted 45 degrees along Y-axis\n          [0.5, 0, 0.866],  # Tilted 30 degrees along X-axis\n          [0, -1, 0]])  # Upside down along Y-axis\n\nb = np.array([[-1, 0, 0],    # Pitch -90 degrees (pointing directly downward)\n          [1, 0, 0],     # Pitch 90 degrees (pointing directly upward)\n          [0, 1, 0],     # Roll 90 degrees\n          [0, 0, 1],     # No tilt, gravity perfectly aligned\n          [0, -1, 1e-6]]) # Near vertical Y-axis, small Z to avoid zero\n\nc = np.array([[0, 0, 0],  # Zero vector\n          [0, 0, 0],  # Repeated zero vector to test consistent handling\n          [0, 0, 0]])\n\nd = np.array([[1e-12, 0, -1],   # Extremely small x component\n          [0, 1e-12, -1],   # Extremely small y component\n          [1e-12, 1e-12, -1]])  # Small x and y components\n\ne = np.array([[1, 0, 0],   # Along X-axis\n          [0, 1, 0],   # Along Y-axis\n          [0, 0, 1],   # Along Z-axis\n          [0, 0, -1],  # Along -Z-axis\n          [-1, 0, 0],  # Along -X-axis\n          [0, -1, 0]]) # Along -Y-axis\n\nassert((SOLUTION_SIGNATURE(a)[0] == pr_from_grav(a)[0]).all())\nassert((SOLUTION_SIGNATURE(b)[0] == pr_from_grav(b)[0]).all())\nassert((SOLUTION_SIGNATURE(c)[0] == pr_from_grav(c)[0]).all())\nassert((SOLUTION_SIGNATURE(d)[0] == pr_from_grav(d)[0]).all())\nassert((SOLUTION_SIGNATURE(e)[0] == pr_from_grav(e)[0]).all())\nassert((SOLUTION_SIGNATURE(a)[1] == pr_from_grav(a)[1]).all())\nassert((SOLUTION_SIGNATURE(b)[1] == pr_from_grav(b)[1]).all())\nassert((SOLUTION_SIGNATURE(c)[1] == pr_from_grav(c)[1]).all())\nassert((SOLUTION_SIGNATURE(d)[1] == pr_from_grav(d)[1]).all())\nassert((SOLUTION_SIGNATURE(e)[1] == pr_from_grav(e)[1]).all())"
 },
 {
  "task_id": "RWPB/28",
  "prompt": "import numpy as np\nimport math\n\n\ndef mean(l, ignore_nan=False, empty=0):\n    \"\"\"\n    Calculates the mean of a collection of numbers, handling NaN values and empty collections.\n\n    Parameters\n    ----------\n    l : iterable\n        An iterable of numbers (e.g., list, tuple, generator).\n    ignore_nan : bool, optional\n        If True, NaN values are ignored in the calculation. Default is False.\n    empty : int, str, optional\n        Defines the behavior when the input iterable is empty:\n            - If 0, the function returns 0.\n            - If 'raise', the function return a ValueError.\n            - For other values, the function returns the value of `empty`.\n\n    Returns\n    -------\n    float\n        The mean of the input numbers, excluding NaN values if `ignore_nan` is True.\n\n    or\n    ------\n    ValueError\n        If the input iterable is empty and `empty` is set to 'raise'.\n\n    Notes\n    -----\n    This function is similar to the built-in `mean` function in numpy, but it is designed to work\n    with generators and other iterables that do not support indexing.\n    \"\"\"\n",
  "solution": "    # Convert the iterable to a numpy array for easier manipulation\n    arr = np.array(list(l))\n\n    # Check if the array is empty\n    if arr.size == 0:\n        if empty == 'raise':\n            raise ValueError(\"The input iterable is empty.\")\n        else:\n            return empty\n\n    # Handle NaN values if ignore_nan is True\n    if ignore_nan:\n        arr = arr[~np.isnan(arr)]\n\n    # Check again if the array is empty after removing NaN values\n    if arr.size == 0:\n        if empty == 'raise':\n            raise ValueError(\"The input iterable is empty after removing NaN values.\")\n        else:\n            return empty\n\n    # Calculate the mean of the array\n    return np.mean(arr)",
  "canonical_solution": "\n    # Create an iterator from the input iterable\n    l = iter(l)\n\n    # Initialize the accumulator and the count of elements\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        # If the iterable is empty, handle it based on the value of `empty`\n        if empty == 'raise':\n            return ValueError\n        return empty\n\n    # Sum the elements of the iterable and count them\n    for n, v in enumerate(l, 2):\n        acc += v\n\n    # Calculate the mean, taking care of the case when there's only one element\n    return acc / n if n > 1 else acc\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE([231, 2.5, 44, 5.0, 10.0], True, 0) == mean([231, 2.5, 44, 5.0, 10.0], True, 0))\nassert(SOLUTION_SIGNATURE([], False, 'raise') == mean([], False, 'raise'))\nassert(SOLUTION_SIGNATURE([1, 2.5, 3, 4.5], False, 0) == mean([1, 2.5, 3, 4.5], False, 0))\n"
 },
 {
  "task_id": "RWPB/1",
  "prompt": "from typing import Optional\n\nimport torch\nfrom matplotlib import cm\nfrom torchtyping import TensorType\n\ndef apply_colormap(\n    image: TensorType[\"bs\":..., 1],\n    cmap=\"viridis\",\n) -> TensorType[\"bs\":..., \"rgb\":3]:\n    \"\"\"Convert single channel to a color image. \n    Args:\n        image: Single channel image.\n        cmap: Colormap for image.\n    Returns:\n        TensorType: Colored image\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `apply_colormap` function which takes a single-channel image tensor and applies a colormap to it using PyTorch and matplotlib's colormap functionalities. This function will convert the single-channel image to a 3-channel RGB image based on the specified colormap.\n\n```python\nfrom typing import Optional\n\nimport torch\nfrom matplotlib import cm\nfrom torchtyping import TensorType\nfrom typeguard import typechecked\n\n@typechecked\ndef apply_colormap(\n    image: TensorType[\"bs\":..., 1],\n    cmap: str = \"viridis\",\n) -> TensorType[\"bs\":..., \"rgb\":3]:\n    \"\"\"Convert single channel to a color image. \n    Args:\n        image: Single channel image tensor with shape (bs, H, W, 1).\n        cmap: Colormap for image.\n    Returns:\n        TensorType: Colored image tensor with shape (bs, H, W, 3).\n    \"\"\"\n    # Ensure the input image tensor is on CPU and convert to numpy\n    image_np = image.squeeze(-1).cpu().numpy()  # Remove the channel dimension and convert to numpy\n\n    # Get the colormap\n    colormap = cm.get_cmap(cmap)\n\n    # Normalize the image to the range [0, 1]\n    image_min = image_np.min()\n    image_max = image_np.max()\n    if image_max > image_min:\n        image_normalized = (image_np - image_min) / (image_max - image_min)\n    else:\n        image_normalized = image_np\n\n    # Apply the colormap\n    image_colored = colormap(image_normalized)[:, :, :, :3]  # Ignore the alpha channel\n\n    # Convert back to tensor\n    image_colored_tensor = torch.from_numpy(image_colored).float()\n\n    return image_colored_tensor\n\n# Example usage:\n# Assuming `single_channel_image` is a PyTorch tensor of shape (bs, H, W, 1)\n# colored_image = apply_colormap(single_channel_image)\n```\n\nThis function first normalizes the input image to the range [0, 1], then applies the specified colormap, and finally converts the result back to a PyTorch tensor. The output tensor will have the same spatial dimensions as the input but will have three channels corresponding to RGB. Note that the function assumes the input tensor is in a format suitable for processing with matplotlib's colormaps, which typically operate on numpy arrays. The function also handles the case where the input image has zero variance by directly using the input values if the maximum and minimum values are the same.",
  "canonical_solution": "\n    colormap = cm.get_cmap(cmap)\n    colormap = torch.tensor(colormap.colors).to(image.device)  # type: ignore\n    image_long = (image * 255).long()\n    image_long_min = torch.min(image_long)\n    image_long_max = torch.max(image_long)\n    assert image_long_min >= 0, f\"the min value is {image_long_min}\"\n    assert image_long_max <= 255, f\"the max value is {image_long_max}\"\n    return colormap[image_long[..., 0]]\n\n\n",
  "unprocess_testcases": "a = torch.rand(2, 10, 10, 1)\nb = torch.rand(1, 100, 100, 1)\nc = torch.tensor([[[[0.0]], [[1.0]]]])\n\nassert((SOLUTION_SIGNATURE(a) == apply_colormap(a)).all())\nassert((SOLUTION_SIGNATURE(b, 'plasma') == apply_colormap(b, 'plasma')).all())\nassert((SOLUTION_SIGNATURE(c) == apply_colormap(c)).all())"
 },
 {
  "task_id": "RWPB/59",
  "prompt": "import torch\n\ndef modify_logit_for_repetition_penalty(logits, prev_output_tokens, repetition_penalty=1.0):\n    \"\"\"\n    Adjusts logits to penalize or favor the repetition of tokens that have already appeared,\n    based on the provided repetition penalty.\n\n    Args:\n        logits (torch.Tensor): The logits from a language model, of shape (batch_size, vocab_size).\n        prev_output_tokens (torch.Tensor): The indices of previously generated tokens for each sequence\n                                           in the batch, of shape (batch_size, seq_len).\n        repetition_penalty (float): The penalty factor for repetition. Values >1 penalize, values <1 favor\n                                    repetition. The default value of 1.0 means no penalty.\n\n    Returns:\n        torch.Tensor: The adjusted logits.\n\n    This function applies a repetition penalty as described in the paper:\n    \"CTRL: A Conditional Transformer Language Model for Controllable Generation\".\n    It decreases the probability of tokens that have appeared previously if the penalty is >1,\n    and increases if <1 but not equal to zero.\n    \"\"\"\n",
  "solution": "    # First, ensure that the repetition penalty is not 1, as no modification is needed in that case\n    if repetition_penalty == 1.0:\n        return logits\n\n    # Get the batch size and vocab size from logits\n    batch_size, vocab_size = logits.size()\n\n    # Iterate over each example in the batch\n    for i in range(batch_size):\n        # Get the set of tokens that have been used in this example\n        used_tokens = set(prev_output_tokens[i].tolist())\n\n        # Apply the repetition penalty to each token that has been used\n        for token in used_tokens:\n            if token < vocab_size:  # Ensure the token index is within the range of the vocabulary\n                # Apply penalty or favoring based on the repetition_penalty value\n                if repetition_penalty > 1.0:\n                    # Penalize the repetition by reducing the logit value\n                    logits[i, token] /= repetition_penalty\n                elif repetition_penalty < 1.0 and repetition_penalty > 0:\n                    # Favor the repetition by increasing the logit value\n                    logits[i, token] *= (1 / repetition_penalty)\n\n    return logits\n\n# Example usage:\n# logits = torch.randn(2, 10000)  # Example logits tensor for a batch of 2 and vocab size of 10000\n# prev_output_tokens = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Example of previously output tokens\n# modified_logits = modify_logit_for_repetition_penalty(logits, prev_output_tokens, repetition_penalty=1.2)",
  "canonical_solution": "\n    if repetition_penalty == 1.0:\n        return logits  # No modification needed if penalty is 1\n\n    # Gather scores of previously generated tokens from logits\n    score = torch.gather(logits, 1, prev_output_tokens)\n\n    # Apply repetition penalty:\n    # If score < 0 (unlikely tokens), multiplying by a penalty > 1 will make them even less likely\n    # If score > 0 (likely tokens), dividing by a penalty > 1 will make them less likely\n    adjusted_score = torch.where(score < 0, score * repetition_penalty, score / repetition_penalty)\n\n    # Scatter the adjusted scores back into the logits tensor at the positions of the previous tokens\n    logits.scatter_(1, prev_output_tokens, adjusted_score)\n\n    return logits\n\n\n",
  "unprocess_testcases": "a = torch.rand(1, 5)\n\nassert((SOLUTION_SIGNATURE(a, torch.tensor([[0, 1]])) == modify_logit_for_repetition_penalty(a, torch.tensor([[0, 1]]))).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[10.0, -5.0, 0.0, 3.0, -1.0]]), torch.tensor([[0, 3]]), 2.0) == modify_logit_for_repetition_penalty(torch.tensor([[10.0, -5.0, 0.0, 3.0, -1.0]]), torch.tensor([[0, 3]]), 2.0)).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[2.0, -2.0, 4.0, -1.0, 0.0]]), torch.tensor([[0, 1, 3]]), 0.5) == modify_logit_for_repetition_penalty(torch.tensor([[2.0, -2.0, 4.0, -1.0, 0.0]]), torch.tensor([[0, 1, 3]]), 0.5)).all())\n"
 },
 {
  "task_id": "RWPB/104",
  "prompt": "MAX_VAL = (1 << 256) - 1\n\n\n# does not overflow\ndef mulMod(x, y, z):\n    \"\"\"\n    Compute (x * y) % z without overflow.\n    Args:\n        - x: int, multiplicand\n        - y: int, multiplier\n        - z: int, modulus\n    Returns:\n        result: int, the result of (x * y) % z\n    \"\"\"\n",
  "solution": "Certainly! To compute \\((x \\times y) \\mod z\\) without causing overflow, especially with very large integers, you can use an approach that involves breaking down the multiplication into smaller parts. This can be done using properties of modular arithmetic. Here's the complete function:\n\n```python\ndef mulMod(x, y, z):\n    \"\"\"\n    Compute (x * y) % z without overflow.\n    Args:\n        - x: int, multiplicand\n        - y: int, multiplier\n        - z: int, modulus\n    Returns:\n        result: int, the result of (x * y) % z\n    \"\"\"\n    result = 0  # Initialize result\n    x = x % z  # Update x if it is more than or equal to z\n\n    while y > 0:\n        # If y is odd, add x to the result\n        if y % 2 == 1:\n            result = (result + x) % z\n        \n        # Double x and halve y\n        x = (x * 2) % z\n        y = y // 2\n\n    return result\n```\n\n### Explanation:\n1. **Initialization**: Start with `result` set to 0.\n2. **Modulo Operation**: Reduce `x` modulo `z` right at the start. This is useful if `x` is already larger than `z`.\n3. **Loop Until `y` is 0**: The loop continues until `y` becomes 0.\n   - **Odd Check for `y`**: If `y` is odd (i.e., `y % 2 == 1`), add `x` to `result` and take modulo `z`. This step ensures that we only add `x` to the result when necessary, reflecting the binary representation of `y`.\n   - **Doubling and Halving**: Double the value of `x` and halve the value of `y` in each iteration. Doubling `x` corresponds to shifting left in binary multiplication, and halving `y` corresponds to moving to the next lower bit.\n4. **Return the Result**: Once `y` is 0, the loop ends, and `result` contains \\((x \\times y) \\mod z\\).\n\nThis method is efficient and avoids overflow by never holding values larger than `z` during computation, making it suitable for operations involving very large numbers, such as those in cryptographic computations.",
  "canonical_solution": "    \n    return (x * y) % z\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(10, 20, 7) == mulMod(10, 20, 7))\nassert(SOLUTION_SIGNATURE(3214, 34, 423) == mulMod(3214, 34, 423))\n\nlarge_x = (1 << 255)\nlarge_y = (1 << 255)\nlarge_z = (1 << 256) - 1\nassert(SOLUTION_SIGNATURE(large_x, large_y, large_z) == mulMod(large_x, large_y, large_z))\n\nassert(SOLUTION_SIGNATURE(0, 10, 3) == mulMod(0, 10, 3))\nassert(SOLUTION_SIGNATURE(10, 0, 3) == mulMod(10, 0, 3))\nassert(SOLUTION_SIGNATURE(10, 20, 1) == mulMod(10, 20, 1))\n"
 },
 {
  "task_id": "RWPB/130",
  "prompt": "import numpy as np\n\ndef bbox_ioa(box1, box2, iou=False, eps=1e-7):\n    \"\"\"\n    Calculate the intersection over box2 area given box1 and box2. Boxes are in x1y1x2y2 format.\n\n    Args:\n        box1 (np.ndarray): A numpy array of shape (n, 4) representing n bounding boxes.\n        box2 (np.ndarray): A numpy array of shape (m, 4) representing m bounding boxes.\n        iou (bool): Calculate the standard IoU if True else return inter_area/box2_area.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n\n    Returns:\n        (np.ndarray): A numpy array of shape (n, m) representing the intersection over box2 area.\n    \"\"\"\n",
  "solution": "```python\n    # Calculate the coordinates of the intersection rectangles\n    inter_x1 = np.maximum(box1[:, None, 0], box2[:, 0])\n    inter_y1 = np.maximum(box1[:, None, 1], box2[:, 1])\n    inter_x2 = np.minimum(box1[:, None, 2], box2[:, 2])\n    inter_y2 = np.minimum(box1[:, None, 3], box2[:, 3])\n\n    # Calculate the area of the intersection rectangles\n    inter_area = np.maximum(inter_x2 - inter_x1, 0) * np.maximum(inter_y2 - inter_y1, 0)\n\n    # Calculate the area of each box2\n    box2_area = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n\n    # Calculate the intersection over box2 area (IoA)\n    ioa = inter_area / (box2_area + eps)\n\n    if iou:\n        # Calculate the area of each box1\n        box1_area = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n\n        # Calculate the union area\n        union_area = box1_area[:, None] + box2_area - inter_area\n\n        # Calculate the intersection over union (IoU)\n        return inter_area / (union_area + eps)\n\n    return ioa\n```\n\nThis function computes the Intersection over Area (IoA) between two sets of bounding boxes, `box1` and `box2`. If the `iou` flag is set to `True`, it calculates the Intersection over Union (IoU) instead. The function handles multiple bounding boxes in a vectorized manner using NumPy, which allows for efficient computation over large sets of bounding boxes.",
  "canonical_solution": "\n    # Get the coordinates of bounding boxes\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1.T\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2.T\n\n    # Intersection area\n    inter_area = (np.minimum(b1_x2[:, None], b2_x2) - np.maximum(b1_x1[:, None], b2_x1)).clip(0) * (\n        np.minimum(b1_y2[:, None], b2_y2) - np.maximum(b1_y1[:, None], b2_y1)\n    ).clip(0)\n\n    # Box2 area\n    area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n    if iou:\n        box1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n        area = area + box1_area[:, None] - inter_area\n\n    # Intersection over box2 area\n    return inter_area / (area + eps)\n\n\n",
  "unprocess_testcases": "box1 = np.array([[0, 0, 2, 2]])\nbox2 = np.array([[1, 1, 3, 3]])\niou = False\nassert((SOLUTION_SIGNATURE(box1, box2, iou) == bbox_ioa(box1, box2, iou)).all())\n\nbox1 = np.array([[0, 0, 1, 1]])\nbox2 = np.array([[2, 2, 3, 3]])\niou = False\nassert((SOLUTION_SIGNATURE(box1, box2, iou) == bbox_ioa(box1, box2, iou)).all())\n\nbox1 = np.array([[0, 0, 3, 3], [4, 4, 5, 5]])\nbox2 = np.array([[1, 1, 4, 4], [0, 0, 2, 2]])\niou = True\nassert((SOLUTION_SIGNATURE(box1, box2, iou) == bbox_ioa(box1, box2, iou)).all())\n\nbox1 = np.array([[0, 0, 3, 3], [4, 4, 5, 5]])\nbox2 = np.array([[1, 1, 4, 4], [0, 0, 2, 2]])\niou = True\nassert((SOLUTION_SIGNATURE(box1, box2, iou, 2e-5) == bbox_ioa(box1, box2, iou, 2e-5)).all())\n\nbox1 = np.array([[1, 3, 3, 6], [7, 7, 2, 6]])\nbox2 = np.array([[9, 3, 5, 4], [0, 1, 3, 8]])\niou = True\nassert((SOLUTION_SIGNATURE(box1, box2, iou) == bbox_ioa(box1, box2, iou)).all())\n"
 },
 {
  "task_id": "RWPB/120",
  "prompt": "def parity(cycles):\n    \"\"\"\n    Calculate the parity (even or odd) of a permutation given its disjoint cycles.\n\n    Args:\n        cycles (list): A list of disjoint cycles, where each cycle is represented as a list.\n\n    Returns:\n        int: 0 if the permutation is even, 1 if it is odd.\n    \"\"\"\n",
  "solution": "Certainly! To determine the parity of a permutation given its disjoint cycles, we need to consider the number of transpositions (pairwise swaps) required to produce each cycle. The parity of the permutation is even if the total number of transpositions is even, and odd if the total number of transpositions is odd.\n\nEach cycle of length \\( n \\) can be formed using \\( n-1 \\) transpositions. For example, the cycle \\([a, b, c]\\) can be formed by the transpositions \\((a, b)\\) and \\((b, c)\\), which are two transpositions. Therefore, the parity of a cycle is even if the length of the cycle minus one is even, and odd otherwise.\n\nTo find the parity of the entire permutation, sum the number of transpositions for each cycle and then determine if this sum is even or odd.\n\nHere's the completed function:\n\n```python\ndef parity(cycles):\n    \"\"\"\n    Calculate the parity (even or odd) of a permutation given its disjoint cycles.\n\n    Args:\n        cycles (list): A list of disjoint cycles, where each cycle is represented as a list.\n\n    Returns:\n        int: 0 if the permutation is even, 1 if it is odd.\n    \"\"\"\n    total_transpositions = 0\n\n    # Calculate the number of transpositions for each cycle\n    for cycle in cycles:\n        # Each cycle of length n requires n-1 transpositions\n        if len(cycle) > 1:\n            total_transpositions += len(cycle) - 1\n\n    # Determine the parity of the total number of transpositions\n    # Even number of transpositions means even permutation (return 0)\n    # Odd number of transpositions means odd permutation (return 1)\n    return total_transpositions % 2\n\n# Example usage:\n# print(parity([[1, 2, 3], [4, 5]]))  # Output: 1 (odd permutation)\n# print(parity([[1, 2], [3, 4, 5, 6]]))  # Output: 0 (even permutation)\n```\n\nThis function calculates the total number of transpositions by summing up \\( n-1 \\) for each cycle of length \\( n \\) and then checks if the total is even or odd by taking the modulo 2 of the sum.",
  "canonical_solution": "    \n    res = 0\n    for i in cycles:\n        if len(i) > 1:\n            res += len(i) - 1\n    return res % 2\n\n",
  "unprocess_testcases": "cycles = [[1,2,3,4]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1, 2], [3, 4, 5], [6, 7, 8, 9]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1], [2, 3], [4, 5, 6], [7]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = []\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1], [2], [3], [4]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1, 2, 3], [4, 5, 6], [7, 8, 9, 10, 11]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n\ncycles = [[1]]\nassert(SOLUTION_SIGNATURE(cycles) == parity(cycles))\n"
 },
 {
  "task_id": "RWPB/114",
  "prompt": "import torch\n\ndef reshape_weight_task_tensors(task_tensors, weights):\n    \"\"\"\n    Reshapes `weights` to match the shape of `task_tensors` by unsqeezing in the remaining dimensions.\n\n    Args:\n        task_tensors (torch.Tensor): The tensors that will be used to reshape `weights`.\n        weights (torch.Tensor): The tensor to be reshaped.\n\n    Returns:\n        torch.Tensor: The reshaped tensor.\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the provided function to reshape the `weights` tensor so that it matches the shape of `task_tensors`:\n\n```python\nimport torch\n\ndef reshape_weight_task_tensors(task_tensors, weights):\n    \"\"\"\n    Reshapes `weights` to match the shape of `task_tensors` by unsqueezing in the remaining dimensions.\n\n    Args:\n        task_tensors (torch.Tensor): The tensors that will be used to reshape `weights`.\n        weights (torch.Tensor): The tensor to be reshaped.\n\n    Returns:\n        torch.Tensor: The reshaped tensor.\n    \"\"\"\n    # Get the number of dimensions in task_tensors and weights\n    task_dim = task_tensors.dim()\n    weight_dim = weights.dim()\n\n    # Calculate the number of dimensions to unsqueeze\n    num_unsqueezes = task_dim - weight_dim\n\n    # Unsqueezing the weights tensor to match the dimensions of task_tensors\n    reshaped_weights = weights\n    for _ in range(num_unsqueezes):\n        reshaped_weights = reshaped_weights.unsqueeze(-1)\n\n    # Ensure the reshaped weights have the same number of dimensions as task_tensors\n    return reshaped_weights\n```\n\nThis function first calculates the difference in the number of dimensions between `task_tensors` and `weights`. It then unsqueezes `weights` in the trailing dimensions until it matches the number of dimensions of `task_tensors`. This ensures that the weights can be broadcasted or used in operations that require matching tensor shapes.",
  "canonical_solution": "    \n    new_shape = weights.shape + (1,) * (task_tensors.dim() - weights.dim())\n    weights = weights.view(new_shape)\n    return weights\n\n\n",
  "unprocess_testcases": "task_tensors = torch.rand((2, 3, 4))\nweights = torch.rand((2))\nassert((SOLUTION_SIGNATURE(task_tensors, weights) == reshape_weight_task_tensors(task_tensors, weights)).all())\n\ntask_tensors = torch.rand((5, 6, 7, 8))\nweights = torch.rand((5, 6))\nassert((SOLUTION_SIGNATURE(task_tensors, weights) == reshape_weight_task_tensors(task_tensors, weights)).all())\n\ntask_tensors = torch.rand((10, 20))\nweights = torch.rand((10))\nassert((SOLUTION_SIGNATURE(task_tensors, weights) == reshape_weight_task_tensors(task_tensors, weights)).all())\n"
 },
 {
  "task_id": "RWPB/49",
  "prompt": "import torch\n\ndef make_even_first_dim(tensor):\n    \"\"\"\n    Ensures that the first dimension of the tensor is even. If it's odd, reduces it by one.\n\n    Args:\n    tensor (torch.Tensor): Input tensor.\n\n    Returns:\n    torch.Tensor: Tensor with an even first dimension.\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the function `make_even_first_dim` which ensures that the first dimension of the input tensor is even. If the first dimension is odd, it slices the tensor to exclude the last element along that dimension.\n\n```python\nimport torch\n\ndef make_even_first_dim(tensor):\n    \"\"\"\n    Ensures that the first dimension of the tensor is even. If it's odd, reduces it by one.\n\n    Args:\n    tensor (torch.Tensor): Input tensor.\n\n    Returns:\n    torch.Tensor: Tensor with an even first dimension.\n    \"\"\"\n    # Check if the first dimension is odd\n    if tensor.size(0) % 2 != 0:\n        # If odd, slice the tensor to exclude the last element in the first dimension\n        tensor = tensor[:-1]\n    return tensor\n```\n\nThis function first checks if the size of the first dimension (`tensor.size(0)`) is odd by taking the modulo with 2. If the result is not zero (i.e., the size is odd), it slices the tensor to remove the last element along the first dimension (`tensor[:-1]`). This ensures that the returned tensor always has an even size along the first dimension.",
  "canonical_solution": "\n    size = list(tensor.size())\n    if size[0] % 2 == 1:\n        size[0] -= 1\n        return tensor[:size[0]]\n    return tensor\n\n",
  "unprocess_testcases": "a = torch.randn(4, 3, 2)\nb = torch.randn(5, 3, 2)\nc = torch.randn(1, 4, 4)\n\nassert((SOLUTION_SIGNATURE(a) == make_even_first_dim(a)).all())\nassert((SOLUTION_SIGNATURE(b) == make_even_first_dim(b)).all())\nassert((SOLUTION_SIGNATURE(c) == make_even_first_dim(c)).all())\n"
 },
 {
  "task_id": "RWPB/18",
  "prompt": "import numpy as np\n\ndef last_before(val, arr):\n  \"\"\"\n  Find the index of the last element in a sorted array that is less than or equal to a given value.Requires sorted (monotonic) arr.\n  Args:\n      - val: int, the value to search for\n      - arr: numpy.ndarray, the sorted array\n  Returns:\n      idx: int, the index of the last element in the array that is less than or equal to val\n  \"\"\"\n",
  "solution": "Certainly! Here's the completion of the function `last_before` using binary search to efficiently find the index of the last element in a sorted array that is less than or equal to a given value:\n\n```python\nimport numpy as np\n\ndef last_before(val, arr):\n    \"\"\"\n    Find the index of the last element in a sorted array that is less than or equal to a given value. Requires sorted (monotonic) arr.\n    Args:\n        - val: int, the value to search for\n        - arr: numpy.ndarray, the sorted array\n    Returns:\n        idx: int, the index of the last element in the array that is less than or equal to val\n    \"\"\"\n    low, high = 0, len(arr) - 1\n    result = -1\n\n    while low <= high:\n        mid = (low + high) // 2\n        if arr[mid] <= val:\n            result = mid\n            low = mid + 1\n        else:\n            high = mid - 1\n\n    return result\n```\n\nThis function uses a binary search algorithm to efficiently find the desired index. The search space is halved in each iteration by adjusting the `low` and `high` pointers based on the comparison of the middle element of the current search range (`arr[mid]`) with the target value `val`. If the middle element is less than or equal to `val`, it updates the `result` to the current middle index and moves the `low` pointer to `mid + 1` to continue searching in the right half of the array. If the middle element is greater than `val`, it moves the `high` pointer to `mid - 1` to search in the left half. The loop continues until the search space is exhausted. The function returns the index of the last element that is less than or equal to the value, or `-1` if no such element exists.",
  "canonical_solution": "\n  assert np.all(np.diff(arr) >= 0)\n  return np.searchsorted(arr, val) - 1\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(val = 5, arr = np.array([1, 2, 3, 4, 5])) == last_before(val = 5, arr = np.array([1, 2, 3, 4, 5])))\nassert(SOLUTION_SIGNATURE(val = 3.5, arr = np.array([1, 2, 3, 4, 5])) == last_before(val = 3.5, arr = np.array([1, 2, 3, 4, 5])))\nassert(SOLUTION_SIGNATURE(val = 0, arr = np.array([1, 2, 3, 4, 5])) == last_before(val = 0, arr = np.array([1, 2, 3, 4, 5])))\n"
 },
 {
  "task_id": "RWPB/66",
  "prompt": "import numpy as np\n\ndef focal_length_to_fovy(focal_length, sensor_height):\n    \"\"\"\n    Calculates the vertical field of view (FoVy) from the focal length of the lens and the sensor height.\n\n    Parameters:\n    focal_length (float): The focal length of the lens.\n    sensor_height (float): The height of the camera sensor.\n\n    Returns:\n    float: The vertical field of view in radians.\n\n    Process:\n    1. Compute half the sensor height.\n    2. Divide half the sensor height by the focal length to get the tangent of half the vertical field of view.\n    3. Use the arctangent function to determine half the vertical field of view.\n    4. Multiply by 2 to get the full vertical field of view (FoVy).\n    \"\"\"\n",
  "solution": "```python\nimport numpy as np\n\ndef focal_length_to_fovy(focal_length, sensor_height):\n    \"\"\"\n    Calculates the vertical field of view (FoVy) from the focal length of the lens and the sensor height.\n\n    Parameters:\n    focal_length (float): The focal length of the lens.\n    sensor_height (float): The height of the camera sensor.\n\n    Returns:\n    float: The vertical field of view in radians.\n\n    Process:\n    1. Compute half the sensor height.\n    2. Divide half the sensor height by the focal length to get the tangent of half the vertical field of view.\n    3. Use the arctangent function to determine half the vertical field of view.\n    4. Multiply by 2 to get the full vertical field of view (FoVy).\n    \"\"\"\n    half_sensor_height = sensor_height / 2.0\n    tan_half_fovy = half_sensor_height / focal_length\n    half_fovy = np.arctan(tan_half_fovy)\n    fovy = 2 * half_fovy\n    return fovy\n```",
  "canonical_solution": "    \n    return 2 * np.arctan(0.5 * sensor_height / focal_length)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(focal_length=50, sensor_height=24) == focal_length_to_fovy(focal_length=50, sensor_height=24))\nassert(SOLUTION_SIGNATURE(focal_length=1, sensor_height=24) == focal_length_to_fovy(focal_length=1, sensor_height=24))\nassert(SOLUTION_SIGNATURE(focal_length=1000, sensor_height=24) == focal_length_to_fovy(focal_length=1000, sensor_height=24))\n"
 },
 {
  "task_id": "RWPB/37",
  "prompt": "import math\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\n\ndef euler_and_translation_to_matrix(euler_angles, translation_vector):\n    \"\"\"\n    Converts Euler angles and a translation vector into a 4x4 transformation matrix.\n\n    Args:\n        euler_angles (list or np.array): Euler angles in degrees for rotation around the x, y, and z axes.\n        translation_vector (list or np.array): Translation vector for x, y, and z coordinates.\n\n    Returns:\n        np.array: A 4x4 transformation matrix combining both rotation and translation.\n\n    This function constructs a homogeneous transformation matrix that combines both rotation and translation.\n    The rotation is specified by Euler angles (roll, pitch, yaw), which are converted into a rotation matrix.\n    The translation is represented as a vector. This transformation matrix is useful in applications such as\n    robotics, 3D graphics, and computer vision, where transformations in 3D space are common.\n    \"\"\"\n",
  "solution": "    # Convert Euler angles from degrees to radians\n    euler_angles_rad = np.radians(euler_angles)\n\n    # Create a rotation matrix from the Euler angles\n    rotation_matrix = R.from_euler('xyz', euler_angles_rad).as_matrix()\n\n    # Create a 4x4 transformation matrix\n    transformation_matrix = np.eye(4)\n\n    # Insert the rotation matrix into the top-left 3x3 part of the transformation matrix\n    transformation_matrix[:3, :3] = rotation_matrix\n\n    # Insert the translation vector into the first three elements of the fourth column\n    transformation_matrix[:3, 3] = translation_vector\n\n    return transformation_matrix\n\n# Example usage:\n# euler_angles = [45, 30, 60]  # Roll, Pitch, Yaw in degrees\n# translation_vector = [1, 2, 3]  # Translation along x, y, z axes\n# transformation_matrix = euler_and_translation_to_matrix(euler_angles, translation_vector)\n# print(transformation_matrix)",
  "canonical_solution": "\n    # Create a rotation object from the Euler angles\n    rotation = R.from_euler('xyz', euler_angles, degrees=True)\n    # Convert the rotation object to a 3x3 rotation matrix\n    rotation_matrix = rotation.as_matrix()\n\n    # Initialize a 4x4 identity matrix\n    matrix = np.eye(4)\n    # Insert the rotation matrix into the top-left 3x3 part of the identity matrix\n    matrix[:3, :3] = rotation_matrix\n    # Insert the translation vector into the first three elements of the fourth column\n    matrix[:3, 3] = translation_vector\n\n    return matrix\n\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([45, 45, 45]), np.array([10, 0, 5])) == euler_and_translation_to_matrix(np.array([45, 45, 45]), np.array([10, 0, 5]))).all())\nassert((SOLUTION_SIGNATURE([0, 0, 0], [5, 5, 5]) == euler_and_translation_to_matrix([0, 0, 0], [5, 5, 5])).all())\nassert((SOLUTION_SIGNATURE([-90, 360, 270], [-1, -1, -1]) == euler_and_translation_to_matrix([-90, 360, 270], [-1, -1, -1])).all())\n"
 },
 {
  "task_id": "RWPB/95",
  "prompt": "import torch\n\ndef lines_focal(o, d):\n    \"\"\"\n    Estimates the focal point of a set of lines in 3D space given their origins and directions.\n\n    Parameters:\n    o (torch.Tensor): A tensor of shape (N, 3) representing the origins of N lines in 3D space.\n    d (torch.Tensor): A tensor of shape (N, 3) representing the directions of N lines in 3D space.\n\n    Returns:\n    torch.Tensor: A tensor of shape (3,) representing the estimated focal point in 3D space.\n\n    Process:\n    1. Normalize the direction vectors `d` to unit length along the last dimension.\n    2. Create a 3x3 identity matrix `I` with the same dtype and device as the input tensors.\n    3. Compute the sum `S` of the outer products of each direction vector with itself, subtracting the identity matrix `I` from each outer product. This results in a tensor of shape (3, 3).\n    4. Compute the sum `C` of the matrix products of each adjusted direction outer product with the corresponding origin vector. This results in a tensor of shape (3,).\n    5. Compute the pseudo-inverse of `S` using `torch.linalg.pinv`.\n    6. Multiply the pseudo-inverse of `S` with `C` to obtain the estimated focal point.\n\n    Returns the estimated focal point as a tensor of shape (3,).\n    \"\"\"\n",
  "solution": "```python\nimport torch\n\ndef lines_focal(o, d):\n    \"\"\"\n    Estimates the focal point of a set of lines in 3D space given their origins and directions.\n\n    Parameters:\n    o (torch.Tensor): A tensor of shape (N, 3) representing the origins of N lines in 3D space.\n    d (torch.Tensor): A tensor of shape (N, 3) representing the directions of N lines in 3D space.\n\n    Returns:\n    torch.Tensor: A tensor of shape (3,) representing the estimated focal point in 3D space.\n    \"\"\"\n    # Step 1: Normalize the direction vectors `d` to unit length along the last dimension.\n    d_normalized = d / d.norm(dim=1, keepdim=True)\n\n    # Step 2: Create a 3x3 identity matrix `I` with the same dtype and device as the input tensors.\n    I = torch.eye(3, dtype=d.dtype, device=d.device)\n\n    # Step 3: Compute the sum `S` of the outer products of each direction vector with itself,\n    # subtracting the identity matrix `I` from each outer product.\n    S = torch.zeros((3, 3), dtype=d.dtype, device=d.device)\n    for dn in d_normalized:\n        S += torch.outer(dn, dn) - I\n\n    # Step 4: Compute the sum `C` of the matrix products of each adjusted direction outer product with the corresponding origin vector.\n    C = torch.zeros(3, dtype=d.dtype, device=d.device)\n    for dn, origin in zip(d_normalized, o):\n        C += (torch.outer(dn, dn) - I) @ origin\n\n    # Step 5: Compute the pseudo-inverse of `S` using `torch.linalg.pinv`.\n    S_pinv = torch.linalg.pinv(S)\n\n    # Step 6: Multiply the pseudo-inverse of `S` with `C` to obtain the estimated focal point.\n    focal_point = S_pinv @ C\n\n    return focal_point\n```\n\nThis function follows the steps outlined in the comments to compute the estimated focal point of a set of lines in 3D space. Each step is implemented using PyTorch operations, ensuring compatibility with GPU acceleration if available.",
  "canonical_solution": "    \n    d = d / torch.norm(d, dim=-1, keepdim=True)\n    I = torch.eye(3, dtype=o.dtype, device=o.device)\n    S = torch.sum(d[..., None] @ torch.transpose(d[..., None], 1, 2) - I[None, ...], dim=0)\n    C = torch.sum((d[..., None] @ torch.transpose(d[..., None], 1, 2) - I[None, ...]) @ o[..., None], dim=0).squeeze(1)\n    return torch.linalg.pinv(S) @ C\n\n\n",
  "unprocess_testcases": "o = torch.tensor([[0, 0, 0], [1, 1, 1]], dtype=torch.float32)\nd = torch.tensor([[1, 0, 0], [0, 1, 0]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(o, d) == lines_focal(o, d)).all())\n\no = torch.tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=torch.float32)\nd = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(o, d) == lines_focal(o, d)).all())\n\no = torch.tensor([[0, 0, 0], [1, 1, 1], [2, 2, 2]], dtype=torch.float32)\nd = torch.tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=torch.float32)\nassert((SOLUTION_SIGNATURE(o, d) == lines_focal(o, d)).all())\n"
 },
 {
  "task_id": "RWPB/52",
  "prompt": "import torch\n\ndef reshape_tensor(x, heads):\n    \"\"\"\n    Reshape a tensor to fit a multi-head attention mechanism structure.\n    If the shape of x is invalid, please return ValueError.\n\n    Arguments:\n    x : torch.Tensor\n        The input tensor with shape (batch_size, length, width), where 'width'\n        is typically the embedding dimension or a multiple of 'heads'.\n    heads : int\n        The number of heads in the multi-head attention mechanism.\n\n    Returns:\n    torch.Tensor\n        A reshaped tensor with dimensions corresponding to multi-head structures,\n        specifically (batch_size * n_heads, length, dim_per_head).\n    \"\"\"\n",
  "solution": "```python\nimport torch\n\ndef reshape_tensor(x, heads):\n    \"\"\"\n    Reshape a tensor to fit a multi-head attention mechanism structure.\n    If the shape of x is invalid, please return ValueError.\n\n    Arguments:\n    x : torch.Tensor\n        The input tensor with shape (batch_size, length, width), where 'width'\n        is typically the embedding dimension or a multiple of 'heads'.\n    heads : int\n        The number of heads in the multi-head attention mechanism.\n\n    Returns:\n    torch.Tensor\n        A reshaped tensor with dimensions corresponding to multi-head structures,\n        specifically (batch_size * n_heads, length, dim_per_head).\n    \"\"\"\n    if not isinstance(x, torch.Tensor):\n        raise ValueError(\"Input must be a torch.Tensor.\")\n    \n    if not isinstance(heads, int) or heads <= 0:\n        raise ValueError(\"Heads must be a positive integer.\")\n    \n    batch_size, length, width = x.shape\n    \n    if width % heads != 0:\n        raise ValueError(\"Width of the tensor must be divisible by the number of heads.\")\n    \n    dim_per_head = width // heads\n    \n    # Reshape the tensor to (batch_size, heads, length, dim_per_head)\n    x = x.view(batch_size, length, heads, dim_per_head)\n    \n    # Transpose to (batch_size, heads, dim_per_head, length)\n    x = x.permute(0, 2, 1, 3)\n    \n    # Reshape to (batch_size * heads, length, dim_per_head)\n    reshaped_x = x.contiguous().view(batch_size * heads, length, dim_per_head)\n    \n    return reshaped_x\n```",
  "canonical_solution": "\n    bs, length, width = x.shape\n    # Reshape to introduce a new dimension for heads, assuming that width can be evenly divided by heads.\n    # width is divided into 'heads' and the dimension per head is calculated as width // heads\n    try:\n        x = x.view(bs, length, heads, -1)\n    except:\n        return ValueError\n    # (bs, length, heads, dim_per_head) --> (bs, heads, length, dim_per_head)\n    # Transpose to swap the 'length' and 'heads' dimensions to align heads for parallel processing\n    x = x.transpose(1, 2)\n    # (bs, heads, length, dim_per_head) --> (bs * heads, length, dim_per_head)\n    # Reshape to merge batch size and heads for processing each head independently in subsequent operations\n    x = x.reshape(bs * heads, length, -1)\n    return x\n\n\n",
  "unprocess_testcases": "a = torch.rand(2, 5, 10)\nb = torch.rand(3, 8, 15)\nc = torch.rand(1, 1, 4)\n\nassert((SOLUTION_SIGNATURE(a, heads=2) == reshape_tensor(a, heads=2)).all())\nassert(SOLUTION_SIGNATURE(b, heads=4) == reshape_tensor(b, heads=4))\nassert((SOLUTION_SIGNATURE(c, heads=2) == reshape_tensor(c, heads=2)).all())\n"
 },
 {
  "task_id": "RWPB/42",
  "prompt": "from PIL import Image\nimport numpy as np\n\ndef resize_and_center_crop(image, target_width, target_height):\n    \"\"\"\n    Resizes an input image and then crops it to the target dimensions, centered on the resized image.\n\n    Args:\n        image (np.array): The input image as a NumPy array.\n        target_width (int): The desired width of the output image.\n        target_height (int): The desired height of the output image.\n\n    Returns:\n        np.array: The processed image as a NumPy array, resized and cropped to the target dimensions.\n\n    This function first converts the input image to a PIL Image object, then calculates the scale factor needed\n    to ensure that the resized image will cover the target dimensions. It resizes the image using the LANCZOS filter\n    for high-quality downsampling. After resizing, it calculates the coordinates necessary to crop the image to the\n    target dimensions centered on the resized image, and finally converts the cropped PIL image back to a NumPy array.\n    \"\"\"\n",
  "solution": "    # Convert the input NumPy array to a PIL Image\n    image_pil = Image.fromarray(image)\n\n    # Calculate the scale needed to ensure the resized image covers the target dimensions\n    original_width, original_height = image_pil.size\n    scale = max(target_width / original_width, target_height / original_height)\n\n    # Calculate new dimensions based on the scale\n    new_width = int(original_width * scale)\n    new_height = int(original_height * scale)\n\n    # Resize the image using the LANCZOS filter\n    resized_image = image_pil.resize((new_width, new_height), Image.LANCZOS)\n\n    # Calculate the coordinates for cropping the image to the target size, centered\n    left = (new_width - target_width) // 2\n    top = (new_height - target_height) // 2\n    right = left + target_width\n    bottom = top + target_height\n\n    # Crop the image\n    cropped_image = resized_image.crop((left, top, right, bottom))\n\n    # Convert the cropped PIL image back to a NumPy array\n    final_image = np.array(cropped_image)\n\n    return final_image\n\n# Example usage:\n# image_array = np.array(Image.open('path_to_image.jpg'))\n# processed_image = resize_and_center_crop(image_array, 800, 600)\n# Image.fromarray(processed_image).show()",
  "canonical_solution": "\n    # Convert the input NumPy array to a PIL Image object\n    pil_image = Image.fromarray(image)\n    original_width, original_height = pil_image.size\n\n    # Calculate the scale factor to resize the image\n    scale_factor = max(target_width / original_width, target_height / original_height)\n\n    # Calculate the new dimensions\n    resized_width = int(round(original_width * scale_factor))\n    resized_height = int(round(original_height * scale_factor))\n\n    # Resize the image using the LANCZOS filter for high quality\n    resized_image = pil_image.resize((resized_width, resized_height), Image.LANCZOS)\n\n    # Calculate the coordinates for cropping the image to center it\n    left = (resized_width - target_width) / 2\n    top = (resized_height - target_height) / 2\n    right = (resized_width + target_width) / 2\n    bottom = (resized_height + target_height) / 2\n\n    # Crop the image using the calculated coordinates\n    cropped_image = resized_image.crop((left, top, right, bottom))\n\n    # Convert the cropped PIL image back to a NumPy array and return\n    return np.array(cropped_image)\n\n\n",
  "unprocess_testcases": "a = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\nb = np.random.randint(0, 256, (100, 200, 3), dtype=np.uint8)\nc = np.random.randint(0, 256, (1000, 800, 3), dtype=np.uint8)\n\nassert((SOLUTION_SIGNATURE(a, 100, 100) == resize_and_center_crop(a, 100, 100)).all())\nassert((SOLUTION_SIGNATURE(b, 150, 100) == resize_and_center_crop(b, 150, 100)).all())\nassert((SOLUTION_SIGNATURE(c, 200, 150) == resize_and_center_crop(c, 200, 150)).all())\n"
 },
 {
  "task_id": "RWPB/13",
  "prompt": "import numpy as np\n\n\ndef remove_scale_from_camera_pose(pose_matrix):\n    '''pose_matrix is a 4x4 numpy array\n\n    Args:\n        - pose_matrix: numpy.array\n    Return:\n        - pose_matrix: numpy.array\n    '''\n",
  "solution": "Certainly! To remove the scale from a camera pose matrix, we need to ensure that the rotation part of the matrix is orthogonal and has unit norm. The rotation part is typically the top-left 3x3 submatrix of the 4x4 pose matrix. The translation part (the top three elements of the fourth column) should remain unchanged. Here's how you can achieve this:\n\n```python\nimport numpy as np\n\ndef remove_scale_from_camera_pose(pose_matrix):\n    '''pose_matrix is a 4x4 numpy array\n\n    Args:\n        - pose_matrix: numpy.array\n    Return:\n        - pose_matrix: numpy.array\n    '''\n    # Extract the rotation part of the matrix\n    rotation_matrix = pose_matrix[:3, :3]\n    \n    # Normalize the rotation matrix to remove any scaling\n    # Use SVD to ensure the matrix is orthogonal\n    U, _, Vt = np.linalg.svd(rotation_matrix)\n    rotation_matrix_normalized = U @ Vt\n    \n    # Replace the rotation part with the normalized rotation matrix\n    pose_matrix[:3, :3] = rotation_matrix_normalized\n    \n    return pose_matrix\n```\n\nThis function first extracts the 3x3 rotation matrix from the top-left corner of the 4x4 pose matrix. It then uses Singular Value Decomposition (SVD) to decompose the rotation matrix. The product of `U` and `Vt` (where `U` and `Vt` are the orthogonal matrices from the SVD) gives the closest orthogonal matrix to the original rotation matrix, effectively removing any scaling and ensuring the matrix is purely rotational. Finally, this normalized rotation matrix is placed back into the original pose matrix, and the modified pose matrix is returned.",
  "canonical_solution": "\n    # Ensure the matrix is in floating point format to avoid type casting issues in division\n    pose_matrix = pose_matrix.astype(np.float64)\n\n    # For each of the first three columns, normalize to remove scale\n    for i in range(3):\n        column = pose_matrix[:, i]\n        scale_factor = np.linalg.norm(column[:3])  # Compute the norm of the column, excluding the bottom element\n        if scale_factor > 0:  # Avoid division by zero\n            pose_matrix[:, i] /= scale_factor  # Normalize column to remove scale\n\n    # The last column (translation) and the last row are not modified, as they do not contribute to scale\n    return pose_matrix\n\n",
  "unprocess_testcases": "a = np.array([[3, 0, 0, 0],\n          [0, 4, 0, 0],\n          [0, 0, 2, 0],\n          [0, 0, 0, 1]])\nb = np.array([[0, 0, 0, 0],\n          [0, 0, 0, 0],\n          [0, 0, 0, 0],\n          [0, 0, 0, 1]])\nc = np.array([[1, 2, 3, 0],\n          [4, 5, 6, 0],\n          [7, 8, 9, 0],\n          [0, 0, 0, 1]])\n\nassert((SOLUTION_SIGNATURE(a) == remove_scale_from_camera_pose(a)).all())\nassert((SOLUTION_SIGNATURE(b) == remove_scale_from_camera_pose(b)).all())\nassert((SOLUTION_SIGNATURE(c) == remove_scale_from_camera_pose(c)).all())"
 },
 {
  "task_id": "RWPB/85",
  "prompt": "from typing import Optional, Sequence\n\nimport torch\n\ndef window_partition(x: torch.Tensor, window_size: Sequence[int]):\n    \"\"\"\n    Partition an image feature map into smaller windows in an ANE (Apple Neural Engine) friendly manner,\n    avoiding the use of 6D tensors.\n\n    :param x: A 4D tensor representing the image feature map with shape (batch_size, H, W, C), where:\n              - batch_size (B) is the number of images in the batch,\n              - H is the height of the feature map,\n              - W is the width of the feature map,\n              - C is the number of channels.\n    :param window_size: A sequence of two integers (win_h, win_w) specifying the height and width of the windows.\n    :returns: A 4D tensor of partitioned feature map windows with shape (batch_size * num_windows, win_h, win_w, C), where:\n              - num_windows is the total number of windows per image (calculated as (H / win_h) * (W / win_w)),\n              - win_h is the window height,\n              - win_w is the window width,\n              - C is the number of channels.\n    :rtype: torch.Tensor\n\n    The process of partitioning is as follows:\n    1. Reshape the input tensor to split the height dimension (H) into smaller chunks of size win_h,\n       resulting in shape (B, H // win_h, win_h, W, C).\n    2. Flatten the batch and height dimensions together to shape (B * H // win_h, win_h, W, C).\n    3. Reshape again to split the width dimension (W) into smaller chunks of size win_w,\n       resulting in shape (B * H // win_h, win_h, W // win_w, win_w, C).\n    4. Permute the dimensions to arrange the tensor into shape (B * H // win_h, W // win_w, win_h, win_w, C).\n    5. Flatten the batch and width dimensions together to produce the final shape (batch_size * num_windows, win_h, win_w, C).\n    \"\"\"\n",
  "solution": "```python\n    # Step 1: Reshape the input tensor to split the height dimension (H) into smaller chunks of size win_h\n    B, H, W, C = x.shape\n    win_h, win_w = window_size\n    if H % win_h != 0 or W % win_w != 0:\n        raise ValueError(\"The window size must perfectly divide the height and width of the input tensor.\")\n\n    x = x.view(B, H // win_h, win_h, W, C)\n\n    # Step 2: Flatten the batch and height dimensions together\n    x = x.view(B * (H // win_h), win_h, W, C)\n\n    # Step 3: Reshape again to split the width dimension (W) into smaller chunks of size win_w\n    x = x.view(B * (H // win_h), win_h, W // win_w, win_w, C)\n\n    # Step 4: Permute the dimensions to arrange the tensor into shape (B * H // win_h, W // win_w, win_h, win_w, C)\n    x = x.permute(0, 2, 1, 3, 4)\n\n    # Step 5: Flatten the batch and width dimensions together\n    num_windows = (H // win_h) * (W // win_w)\n    x = x.view(B * num_windows, win_h, win_w, C)\n\n    return x\n```\n\nThis code completes the function `window_partition` by following the outlined steps to partition the input tensor into smaller windows without using a 6D tensor. Each step is implemented using PyTorch tensor operations to ensure the tensor is reshaped and permuted correctly to achieve the desired output shape.",
  "canonical_solution": "    \n    B, H, W, C = x.shape\n    # example partition process: 1, 12, 16, 160 -> 1, 2, 6, 16, 160 -> 2, 6, 16, 160 -> 2, 6, 2, 8, 160 -> ...\n    x = x.reshape(\n        (B, H // window_size[0], window_size[0], W, C)\n    )  # B, H//w_size, w_size, W, C\n    x = x.reshape(\n        (B * H // window_size[0], window_size[0], W, C)\n    )  # B * H // w_size, w_size, W, C\n    x = x.reshape(\n        (\n            B * H // window_size[0],\n            window_size[0],\n            W // window_size[1],\n            window_size[1],\n            -1,\n        )\n    )\n    x = x.permute((0, 2, 1, 3, 4))\n    windows = x.reshape((-1, window_size[0], window_size[1], C))\n    return windows\n\n\n",
  "unprocess_testcases": "a = torch.rand((2, 8, 8, 3))\nws = (4, 4)\nassert((SOLUTION_SIGNATURE(a, ws) == window_partition(a, ws)).all())\n\na = torch.rand((1, 10, 10, 5))\nws = (5, 2)\nassert((SOLUTION_SIGNATURE(a, ws) == window_partition(a, ws)).all())\n\na = torch.rand((1, 6, 8, 3))\nws = (3, 2)\nassert((SOLUTION_SIGNATURE(a, ws) == window_partition(a, ws)).all())\n"
 },
 {
  "task_id": "RWPB/27",
  "prompt": "import torch\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens the scores and labels from a batch of predictions, removing entries with a specified ignore label.\n\n    Parameters\n    ----------\n    scores : torch.Tensor\n        The tensor containing the prediction scores for each item in the batch.\n    labels : torch.Tensor\n        The tensor containing the true labels for each item in the batch.\n    ignore : int or float, optional\n        The label to ignore during the flattening process. If None, all labels are kept.\n\n    Returns\n    -------\n    vscores : torch.Tensor\n        A flattened tensor containing the prediction scores without the ignored labels.\n    vlabels : torch.Tensor\n        A flattened tensor containing the true labels without the ignored labels.\n\n    Notes\n    -----\n    This function is typically used in the binary classification case where 'scores' and 'labels'\n    are expected to be tensors of shape (batch_size, ...) and the function flattens them to a 1D\n    tensor (-1,). If the 'ignore' parameter is provided, it filters out the corresponding labels\n    and scores from the result.\n    \"\"\"\n",
  "solution": "    # Flatten the scores and labels tensors\n    scores_flat = scores.view(-1)\n    labels_flat = labels.view(-1)\n\n    if ignore is not None:\n        # Create a mask for elements to keep (where labels are not equal to the ignore value)\n        mask = labels_flat != ignore\n\n        # Apply the mask to scores and labels to filter out the ignored elements\n        vscores = scores_flat[mask]\n        vlabels = labels_flat[mask]\n    else:\n        # If no ignore label is specified, use all elements\n        vscores = scores_flat\n        vlabels = labels_flat\n\n    return vscores, vlabels\n\n# Example usage:\n# Assuming scores and labels are PyTorch tensors of appropriate shape\n# scores = torch.tensor([...])\n# labels = torch.tensor([...])\n# ignore_label = 0\n# vscores, vlabels = flatten_binary_scores(scores, labels, ignore=ignore_label)",
  "canonical_solution": "\n    # Flatten the scores and labels to 1D tensors\n    scores = scores.view(-1)\n    labels = labels.view(-1)\n\n    # If there's no ignore label, return the flattened scores and labels\n    if ignore is None:\n        return scores, labels\n\n    # Create a boolean mask for entries that do not have the ignore label\n    valid = (labels != ignore)\n\n    # Apply the mask to filter out the ignored labels and return the valid scores and labels\n    vscores = scores[valid]\n    vlabels = labels[valid]\n\n    return vscores, vlabels\n\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([[0.5, 0.2], [0.7, 0.4]]), torch.tensor([[1, 1], [0, 0]]), 2)[0] == flatten_binary_scores(torch.tensor([[0.5, 0.2], [0.7, 0.4]]), torch.tensor([[1, 1], [0, 0]]), 2)[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[0.5, 0.2], [0.7, 0.4]]), torch.tensor([[1, 1], [0, 0]]), 2)[1] == flatten_binary_scores(torch.tensor([[0.5, 0.2], [0.7, 0.4]]), torch.tensor([[1, 1], [0, 0]]), 2)[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[0.1, 0.3], [0.5, 0.7]]), torch.tensor([[1, 2], [1, 2]]), 2)[0] == flatten_binary_scores(torch.tensor([[0.1, 0.3], [0.5, 0.7]]), torch.tensor([[1, 2], [1, 2]]), 2)[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[0.1, 0.3], [0.5, 0.7]]), torch.tensor([[1, 2], [1, 2]]), 2)[1] == flatten_binary_scores(torch.tensor([[0.1, 0.3], [0.5, 0.7]]), torch.tensor([[1, 2], [1, 2]]), 2)[1]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[0.2, 0.4], [0.6, 0.8]]), torch.tensor([[0, 1], [0, 1]]), None)[0] == flatten_binary_scores(torch.tensor([[0.2, 0.4], [0.6, 0.8]]), torch.tensor([[0, 1], [0, 1]]), None)[0]).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[0.2, 0.4], [0.6, 0.8]]), torch.tensor([[0, 1], [0, 1]]), None)[1] == flatten_binary_scores(torch.tensor([[0.2, 0.4], [0.6, 0.8]]), torch.tensor([[0, 1], [0, 1]]), None)[1]).all())\n"
 },
 {
  "task_id": "RWPB/76",
  "prompt": "import torch\n\ndef quat_to_rotmat(quat: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Convert quaternion representation to rotation matrix.\n    Args:\n        quat (torch.Tensor) of shape (B, 4); 4 <===> (w, x, y, z).\n    Returns:\n        torch.Tensor: Corresponding rotation matrices with shape (B, 3, 3).\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `quat_to_rotmat` function that converts a quaternion to a rotation matrix. The function assumes that the input quaternion is in the format (w, x, y, z) and is normalized.\n\n```python\nimport torch\n\ndef quat_to_rotmat(quat: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Convert quaternion representation to rotation matrix.\n    Args:\n        quat (torch.Tensor) of shape (B, 4); 4 <===> (w, x, y, z).\n    Returns:\n        torch.Tensor: Corresponding rotation matrices with shape (B, 3, 3).\n    \"\"\"\n    # Ensure the quaternion is normalized\n    quat = quat / quat.norm(dim=-1, keepdim=True)\n\n    # Extract components of quaternion\n    w, x, y, z = quat[:, 0], quat[:, 1], quat[:, 2], quat[:, 3]\n\n    # Compute elements of the rotation matrix\n    xx, xy, xz = x * x, x * y, x * z\n    yy, yz, zz = y * y, y * z, z * z\n    wx, wy, wz = w * x, w * y, w * z\n\n    # Rotation matrix components\n    r00 = 1.0 - 2.0 * (yy + zz)\n    r01 = 2.0 * (xy - wz)\n    r02 = 2.0 * (xz + wy)\n    r10 = 2.0 * (xy + wz)\n    r11 = 1.0 - 2.0 * (xx + zz)\n    r12 = 2.0 * (yz - wx)\n    r20 = 2.0 * (xz - wy)\n    r21 = 2.0 * (yz + wx)\n    r22 = 1.0 - 2.0 * (xx + yy)\n\n    # Stack into rotation matrices\n    rotation_matrices = torch.stack([\n        torch.stack([r00, r01, r02], dim=-1),\n        torch.stack([r10, r11, r12], dim=-1),\n        torch.stack([r20, r21, r22], dim=-1)\n    ], dim=-2)\n\n    return rotation_matrices\n```\n\nThis function first normalizes the quaternion to ensure it represents a valid rotation. It then computes the elements of the corresponding rotation matrix using the relationships derived from quaternion algebra. Finally, it stacks these elements into a batch of 3x3 rotation matrices. Each matrix in the batch corresponds to a quaternion in the input tensor.",
  "canonical_solution": "    \n    norm_quat = quat\n    norm_quat = norm_quat / norm_quat.norm(p=2, dim=1, keepdim=True)\n    w, x, y, z = norm_quat[:, 0], norm_quat[:, 1], norm_quat[:, 2], norm_quat[:, 3]\n\n    B = quat.size(0)\n\n    w2, x2, y2, z2 = w.pow(2), x.pow(2), y.pow(2), z.pow(2)\n    wx, wy, wz = w * x, w * y, w * z\n    xy, xz, yz = x * y, x * z, y * z\n\n    rotMat = torch.stack(\n        [\n            w2 + x2 - y2 - z2,\n            2 * xy - 2 * wz,\n            2 * wy + 2 * xz,\n            2 * wz + 2 * xy,\n            w2 - x2 + y2 - z2,\n            2 * yz - 2 * wx,\n            2 * xz - 2 * wy,\n            2 * wx + 2 * yz,\n            w2 - x2 - y2 + z2,\n        ],\n        dim=1,\n    ).view(B, 3, 3)\n    return rotMat\n\n\n\n\n",
  "unprocess_testcases": "a = torch.tensor([[1.0, 0.0, 0.0, 0.0]])\nb = torch.tensor([\n    [0.7071, 0.7071, 0.0, 0.0],\n    [0.7071, 0.0, 0.7071, 0.0],\n    [0.7071, 0.0, 0.0, 0.7071]\n])\nc = torch.tensor([\n    [2.0, 0.0, 0.0, 0.0],\n    [1.4142, 1.4142, 0.0, 0.0],\n    [10.0, 0.0, 0.0, 10.0]\n])\n\nassert((SOLUTION_SIGNATURE(a) == quat_to_rotmat(a)).all())\nassert((SOLUTION_SIGNATURE(b) == quat_to_rotmat(b)).all())\nassert((SOLUTION_SIGNATURE(c) == quat_to_rotmat(c)).all())"
 },
 {
  "task_id": "RWPB/56",
  "prompt": "import torch\nimport numpy as np\nfrom torch.nn import functional as F\n\n\ndef grid_distortion(images, strength=0.5):\n    \"\"\"\n    Applies a grid-based distortion to a batch of images by perturbing the grid points.\n\n    Arguments:\n    images : torch.Tensor\n        A tensor of shape [B, C, H, W] representing the batch of images.\n    strength : float, optional\n        The strength of the distortion, must be a value between 0 and 1. Default is 0.5.\n\n    Returns:\n    torch.Tensor\n        A tensor of distorted images with the same shape as the input.\n    \"\"\"\n",
  "solution": "    # Ensure the strength is within the valid range\n    strength = max(0, min(strength, 1))\n\n    B, C, H, W = images.shape\n\n    # Create meshgrid for original coordinates\n    xx = torch.arange(W).view(1, -1).repeat(H, 1)\n    yy = torch.arange(H).view(-1, 1).repeat(1, W)\n    xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n    yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n\n    # Create grid for flow\n    grid_x = xx + (torch.rand(B, 1, H, W) * 2 - 1) * strength * W * 0.2\n    grid_y = yy + (torch.rand(B, 1, H, W) * 2 - 1) * strength * H * 0.2\n\n    # Normalize grid to [-1, 1]\n    grid_x_normalized = 2.0 * grid_x / (W - 1) - 1\n    grid_y_normalized = 2.0 * grid_y / (H - 1) - 1\n\n    # Stack to create the final grid\n    grid = torch.stack((grid_x_normalized, grid_y_normalized), dim=3)\n\n    # Sample images with grid\n    distorted_images = F.grid_sample(images, grid, mode='bilinear', padding_mode='border', align_corners=True)\n\n    return distorted_images\n\n# Example usage:\n# images = torch.randn(10, 3, 224, 224)  # Example batch of images\n# distorted_images = grid_distortion(images, strength=0.5)",
  "canonical_solution": "\n    B, C, H, W = images.shape\n\n    # Determine the resolution of the grid\n    num_steps = np.random.randint(8, 17)  # Random grid size between 8 and 16\n    grid_steps = torch.linspace(-1, 1, num_steps)  # Steps from -1 to 1 for normalized coordinates\n\n    grids = []  # To store grids for each image in the batch\n    for b in range(B):\n        # Generate distorted x-coordinates\n        x_steps = torch.linspace(0, 1, num_steps)  # Equally spaced steps between 0 and 1\n        # Add random perturbation scaled by the distortion strength\n        x_steps = (x_steps + strength * (torch.rand_like(x_steps) - 0.5) / (num_steps - 1)).clamp(0, 1)\n        x_steps = (x_steps * W).long()  # Scale steps to image width\n        x_steps[0] = 0\n        x_steps[-1] = W  # Ensure edges remain at the borders\n\n        xs = [torch.linspace(grid_steps[i], grid_steps[i + 1], x_steps[i + 1] - x_steps[i]) for i in\n              range(num_steps - 1)]\n        xs = torch.cat(xs, dim=0)  # Concatenate to form a single tensor\n\n        # Generate distorted y-coordinates using similar steps as x-coordinates\n        y_steps = torch.linspace(0, 1, num_steps)\n        y_steps = (y_steps + strength * (torch.rand_like(y_steps) - 0.5) / (num_steps - 1)).clamp(0, 1)\n        y_steps = (y_steps * H).long()\n        y_steps[0] = 0\n        y_steps[-1] = H\n\n        ys = [torch.linspace(grid_steps[i], grid_steps[i + 1], y_steps[i + 1] - y_steps[i]) for i in\n              range(num_steps - 1)]\n        ys = torch.cat(ys, dim=0)\n\n        # Construct the distortion grid using meshgrid\n        grid_x, grid_y = torch.meshgrid(xs, ys, indexing='xy')\n        grid = torch.stack([grid_x, grid_y], dim=-1)  # Stack to form [H, W, 2] grid\n\n        grids.append(grid)\n\n    grids = torch.stack(grids, dim=0).to(images.device)  # Convert list to tensor and move to the same device as images\n\n    # Apply grid sample to distort the images according to the generated grids\n    images = F.grid_sample(images, grids, align_corners=False)\n\n    return images\n\n",
  "unprocess_testcases": "a = torch.rand([2, 3, 32, 32])\nb = torch.rand([1, 1, 64, 64])\nc = torch.rand([1, 1, 64, 64])\nd = torch.rand([10, 3, 256, 256])\n\nassert(SOLUTION_SIGNATURE(a).size() == grid_distortion(a).size())\nassert(SOLUTION_SIGNATURE(b, 0).size()== grid_distortion(b, 0).size())\nassert(SOLUTION_SIGNATURE(c, 1).size() == grid_distortion(c, 1).size())\nassert(SOLUTION_SIGNATURE(d, 0.3).size() == grid_distortion(d, 0.3).size())\n"
 },
 {
  "task_id": "RWPB/33",
  "prompt": "import torch\n\n\ndef top_p_sample(prob_dist: torch.Tensor, top_p: float) -> torch.Tensor:\n    \"\"\"\n    Applies top-p filtering to a probability distribution to focus sampling on a subset of likely outputs.\n\n    Args:\n        prob_dist (torch.Tensor): The original probability distribution from which to sample, shape (batch_size, vocab_size).\n        top_p (float): The cumulative probability threshold used for filtering. Probabilities that add up to `top_p` are kept.\n\n    Returns:\n        torch.Tensor: The modified probability distribution after applying top-p filtering.\n\n    This method is used to reduce the sample space to the most probable tokens whose cumulative probability\n    exceeds the threshold `top_p`. It improves sampling outcomes by eliminating highly improbable options.\n    \"\"\"\n",
  "solution": "```python\n    # Sort the probabilities in descending order\n    sorted_probs, sorted_indices = torch.sort(prob_dist, descending=True, dim=1)\n\n    # Compute the cumulative probabilities\n    cumulative_probs = torch.cumsum(sorted_probs, dim=1)\n\n    # Create a mask where cumulative probabilities are less than or equal to top_p\n    cutoff_mask = cumulative_probs <= top_p\n\n    # Since cutoff_mask will have 'True' values up to the first 'False', we need to include at least one 'True' beyond the top_p\n    # This is to ensure that at least one element is sampled even if top_p is very small\n    cutoff_mask[:, 1:] = cutoff_mask[:, :-1].clone()\n    cutoff_mask[:, 0] = True\n\n    # Use the mask to filter the probabilities\n    filtered_probs = torch.zeros_like(prob_dist)\n    filtered_probs.scatter_(1, sorted_indices, sorted_probs * cutoff_mask.float())\n\n    # Re-normalize the probabilities\n    sum_filtered_probs = filtered_probs.sum(dim=1, keepdim=True)\n    normalized_probs = filtered_probs / sum_filtered_probs\n\n    return normalized_probs\n```\n\nThis function first sorts the probabilities and their indices in descending order. It then computes the cumulative probabilities and creates a mask to filter out probabilities that contribute to the cumulative sum beyond the specified `top_p` threshold. The mask is adjusted to ensure that at least one value is always selected. The probabilities are then filtered and normalized to ensure they sum to 1, maintaining the properties of a probability distribution.",
  "canonical_solution": "\n    # Sort the probabilities and their corresponding indices in descending order\n    sorted_probs, sorted_indices = torch.sort(prob_dist, descending=True, dim=-1)\n\n    # Compute the cumulative sum of the sorted probabilities\n    cum_sum_probs = torch.cumsum(sorted_probs, dim=-1)  # Shape: (batch_size, vocab_size)\n\n    # Create a boolean mask where probabilities that exceed the threshold are set to True\n    sorted_indices_to_remove = cum_sum_probs > top_p\n\n    # Shift the indices to the right to include the first token above the threshold\n    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n    sorted_indices_to_remove[:, 0] = 0  # Ensure the first token is always included\n\n    # Convert the mask to boolean and apply it to sorted_probs by setting removed probs to 0\n    sorted_probs[sorted_indices_to_remove] = 0\n\n    # Reverse the sorting process to reconstruct the original order of probabilities\n    reversed_indices = torch.argsort(sorted_indices)\n    prob_dist = torch.gather(sorted_probs, -1, reversed_indices)\n\n    # Normalize the probabilities to ensure they sum to 1\n    prob_dist = prob_dist / prob_dist.sum(dim=-1, keepdim=True)\n\n    return prob_dist\n\n\n",
  "unprocess_testcases": "a = torch.tensor([[0.5, 0.2, 0.1, 0.1, 0.05, 0.05]])\nb = torch.tensor([[0.05, 0.15, 0.2, 0.3, 0.25, 0.05],\n                          [0.4, 0.1, 0.1, 0.1, 0.1, 0.2],\n                          [0.25, 0.25, 0.25, 0.15, 0.05, 0.05]])\nc = torch.tensor([[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]])\n\nassert((SOLUTION_SIGNATURE(a, 0.3) == top_p_sample(a, 0.3)).all())\nassert((SOLUTION_SIGNATURE(b, 0.3) == top_p_sample(b, 0.3)).all())\nassert((SOLUTION_SIGNATURE(c, 0.5) == top_p_sample(c, 0.5)).all())\n"
 },
 {
  "task_id": "RWPB/62",
  "prompt": "import torch\n\n\ndef broadcat(tensors, dim=-1):\n    \"\"\"\n    Broadcasts a list of tensors to a common shape and concatenates them along a specified dimension.\n    The function should return ValueError when the first dimensions of the tensors are incompatible for broadcasting.\n    \n    Arguments:\n    tensors : list of torch.Tensor\n        List of tensors to broadcast and concatenate.\n    dim : int, optional\n        Dimension along which to concatenate the tensors.\n\n    Returns:\n    torch.Tensor\n        A tensor resulting from broadcasting and concatenating the input tensors along the specified dimension.\n    \"\"\"\n",
  "solution": "```python\nimport torch\n\ndef broadcast_and_concat(tensors, dim=-1):\n    \"\"\"\n    Broadcasts a list of tensors to a common shape and concatenates them along a specified dimension.\n    The function should return ValueError when the first dimensions of the tensors are incompatible for broadcasting.\n    \n    Arguments:\n    tensors : list of torch.Tensor\n        List of tensors to broadcast and concatenate.\n    dim : int, optional\n        Dimension along which to concatenate the tensors.\n\n    Returns:\n    torch.Tensor\n        A tensor resulting from broadcasting and concatenating the input tensors along the specified dimension.\n    \"\"\"\n    if not tensors:\n        raise ValueError(\"The input list of tensors is empty.\")\n\n    # Determine the target shape after broadcasting\n    try:\n        target_shape = torch.broadcast_shapes(*[t.shape for t in tensors])\n    except RuntimeError as e:\n        raise ValueError(\"The tensors are not broadcastable.\") from e\n\n    # Broadcast each tensor to the target shape\n    broadcasted_tensors = [torch.broadcast_to(t, target_shape) for t in tensors]\n\n    # Concatenate the broadcasted tensors along the specified dimension\n    concatenated_tensor = torch.cat(broadcasted_tensors, dim=dim)\n\n    return concatenated_tensor\n```\n\nThis function first checks if the input list is empty and raises a `ValueError` if it is. It then computes the target shape that all tensors can be broadcasted to using `torch.broadcast_shapes`. If the tensors are not broadcastable, it raises a `ValueError`. Each tensor is then broadcasted to the target shape using `torch.broadcast_to`. Finally, all broadcasted tensors are concatenated along the specified dimension using `torch.cat`.",
  "canonical_solution": "    \n\n    num_tensors = len(tensors)\n    if num_tensors == 0:\n        return ValueError\n\n    # Check that all tensors have the same number of dimensions\n    shape_lens = set(map(lambda t: len(t.shape), tensors))\n    if len(shape_lens) != 1:\n        return ValueError\n    shape_len = list(shape_lens)[0]\n\n    # Handle negative dimension indices\n    dim = (dim + shape_len) if dim < 0 else dim\n    if dim >= shape_len:\n        return ValueError\n\n    # Collect dimensions of all tensors and find maximum dimension sizes for broadcasting\n    dimensions = list(zip(*[t.shape for t in tensors]))\n    max_dims = [max(dim_sizes) for dim_sizes in dimensions]\n\n    # Ensure all dimensions are compatible for broadcasting\n    for i, dim_sizes in enumerate(dimensions):\n        if i != dim and any(size != max_dims[i] and size != 1 for size in dim_sizes):\n            return ValueError\n\n    # Broadcast each tensor to the maximum dimension size\n    broadcasted_tensors = [t.expand(*max_dims) for t in tensors]\n\n    # Concatenate the broadcasted tensors along the specified dimension\n    return torch.cat(broadcasted_tensors, dim=dim)\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE([torch.tensor([[1], [2]]), torch.tensor([[3, 3, 3], [4, 4, 4]])]) == broadcat([torch.tensor([[1], [2]]), torch.tensor([[3, 3, 3], [4, 4, 4]])])).all())\nassert((SOLUTION_SIGNATURE([torch.tensor([[1], [2], [3]]), torch.tensor([[4, 4], [5, 5], [6, 6]])], dim=-1) == broadcat([torch.tensor([[1], [2], [3]]), torch.tensor([[4, 4], [5, 5], [6, 6]])], dim=-1)).all())\nassert(SOLUTION_SIGNATURE([torch.tensor([[1, 1, 1], [2, 2, 2]]), torch.tensor([[3, 3, 3], [4, 4, 4], [5, 5, 5]])]) == broadcat([torch.tensor([[1, 1, 1], [2, 2, 2]]), torch.tensor([[3, 3, 3], [4, 4, 4], [5, 5, 5]])]))\n"
 },
 {
  "task_id": "RWPB/91",
  "prompt": "import math\nimport numpy as np\n\n\ndef r_sqrd(x, y):\n    \"\"\"\n    Calculate R^2 of best fit simple least squares regression line.\n    If denominator is zero, the code should return KeyError\n    Args:\n        - x: numpy array, shape (n,), x coordinates of (x,y) points.\n        - y: numpy array, shape (n,), y coordinates of (x,y) points.\n\n    Return:\n        r_squared: float, R^2 of the best fit simple least squares regression line.\n\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the function `r_sqrd` which calculates the R-squared value for a simple linear regression model:\n\n```python\nimport math\nimport numpy as np\n\ndef r_sqrd(x, y):\n    \"\"\"\n    Calculate R^2 of best fit simple least squares regression line.\n    If denominator is zero, the code should return KeyError\n    Args:\n        - x: numpy array, shape (n,), x coordinates of (x,y) points.\n        - y: numpy array, shape (n,), y coordinates of (x,y) points.\n\n    Return:\n        r_squared: float, R^2 of the best fit simple least squares regression line.\n\n    \"\"\"\n    # Mean of x and y\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n\n    # Total sum of squares\n    ss_tot = np.sum((y - y_mean) ** 2)\n\n    # Check if the denominator is zero\n    if ss_tot == 0:\n        raise KeyError(\"Denominator in R-squared calculation is zero, which leads to division by zero.\")\n\n    # Regression sum of squares\n    # Calculate slope (b1) and intercept (b0) for y = b0 + b1*x\n    b1 = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n    b0 = y_mean - b1 * x_mean\n\n    # Calculate predicted y values\n    y_pred = b0 + b1 * x\n\n    # Sum of squares of residuals\n    ss_res = np.sum((y - y_pred) ** 2)\n\n    # R-squared calculation\n    r_squared = 1 - (ss_res / ss_tot)\n\n    return r_squared\n```\n\nThis function first calculates the mean of the x and y arrays. It then computes the total sum of squares (`ss_tot`) and checks if it is zero, which would cause a division by zero error when calculating R-squared. If `ss_tot` is zero, a `KeyError` is raised. The function then proceeds to calculate the slope and intercept of the best fit line, predicts y values based on this line, and calculates the sum of squares of residuals (`ss_res`). Finally, it computes the R-squared value and returns it.",
  "canonical_solution": "    \n    x_dot_x = x.dot(x)\n    x_dot_y = x.dot(y)\n    x_mean = x.mean()\n    x_sum = x.sum()\n    y_mean = y.mean()\n    denominator = x_dot_x - x_mean * x_sum\n    if denominator == 0:\n        return KeyError\n    \n    m = (x_dot_y - y_mean * x_sum) / denominator\n    b = (y_mean * x_dot_x - x_mean * x_dot_y) / denominator\n    y_pred = m * x + b\n    residuals = y - y_pred\n    tot = y - y_mean\n\n    if int(tot.dot(tot)) == 0:\n        return KeyError\n\n    return 1.0 - residuals.dot(residuals) / tot.dot(tot)\n\n    \n\n",
  "unprocess_testcases": "x = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 4, 6, 8, 10])\nassert(SOLUTION_SIGNATURE(x, y) == r_sqrd(x, y))\n\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 2, 2, 2, 2])\nassert(SOLUTION_SIGNATURE(x, y) == r_sqrd(x, y))\n\nx = np.array([1, 1, 1, 1, 1])\ny = np.array([1, 2, 3, 4, 5])\nassert(SOLUTION_SIGNATURE(x, y) == r_sqrd(x, y))\n\nx = np.array([1, 1, 3, 1, 5])\ny = np.array([1, 0, 3, 1, 5])\nassert(SOLUTION_SIGNATURE(x, y) == r_sqrd(x, y))\n\nx = np.array([6, 1, 5, 7, 5])\ny = np.array([0, -1, 3, 2, 3])\nassert(SOLUTION_SIGNATURE(x, y) == r_sqrd(x, y))\n"
 },
 {
  "task_id": "RWPB/81",
  "prompt": "import math\nimport numpy as np\n# from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_DOWN\n\n\ndef remove_outliers(arr, num_std_devs=2):\n    \"\"\"\n    Remove outliers from an array based on a specified number of standard deviations from the mean.\n    Args:\n        - arr: numpy array, the input array\n        - num_std_devs: int, number of standard deviations from the mean to consider as outliers (default is 2)\n    Returns:\n        filtered_arr: numpy array, the array with outliers removed\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `remove_outliers` function which removes elements from the array that are more than a specified number of standard deviations away from the mean.\n\n```python\nimport math\nimport numpy as np\n\ndef remove_outliers(arr, num_std_devs=2):\n    \"\"\"\n    Remove outliers from an array based on a specified number of standard deviations from the mean.\n    Args:\n        - arr: numpy array, the input array\n        - num_std_devs: int, number of standard deviations from the mean to consider as outliers (default is 2)\n    Returns:\n        filtered_arr: numpy array, the array with outliers removed\n    \"\"\"\n    if len(arr) == 0:\n        return arr\n\n    mean = np.mean(arr)\n    std_dev = np.std(arr)\n    \n    lower_bound = mean - num_std_devs * std_dev\n    upper_bound = mean + num_std_devs * std_dev\n    \n    # Create a boolean mask that is True for values within the bounds\n    mask = (arr >= lower_bound) & (arr <= upper_bound)\n    \n    # Apply the mask to get the filtered array\n    filtered_arr = arr[mask]\n    \n    return filtered_arr\n```\n\nThis function first calculates the mean and standard deviation of the input array. It then determines the lower and upper bounds for what is considered \"normal\" based on the number of standard deviations specified. Using these bounds, it creates a boolean mask that identifies which elements of the array fall within the acceptable range. Finally, it applies this mask to the original array to produce an array with outliers removed.",
  "canonical_solution": "    \n    if len(arr) < 2:\n        return arr\n    mean = np.mean(arr)\n    std_dev = np.std(arr)\n    return arr[np.abs(arr - mean) <= num_std_devs * std_dev]\n\n\n",
  "unprocess_testcases": "a = np.random.normal(50, 10, size=100)\nb = np.linspace(0, 100, 100)\nc = np.append(np.random.normal(50, 10, size=95), [150, 200, -50, -100])\n\nassert((SOLUTION_SIGNATURE(a) == remove_outliers(a)).all())\nassert((SOLUTION_SIGNATURE(b) == remove_outliers(b)).all())\nassert((SOLUTION_SIGNATURE(c) == remove_outliers(c)).all())\n"
 },
 {
  "task_id": "RWPB/72",
  "prompt": "import numpy as np\n\ndef deriv_fitting_matrix(degree, t_end=1.0):\n  \"\"\"\n    Returns A s.t. that the vector x that satisfies\n    If degree % 2 != 0, return AssertionError\n\n    Ax = b\n\n    contains polynomial coefficients\n\n      p(t) = x_1 + x_2 t + x_3 t^2 ...\n\n    so that\n\n    p(0) = b_1\n    p^(i)(0) = b_{i+1}\n    p(t_end) = b_{degree / 2}\n    p^(i)(t_end) = b_{degree / 2 + i + 1}\n\n    i.e. the first degree / 2 derivatives of p at 0 match the first degree / 2\n         entries of b and the first degree / 2 derivatives of p at 1, match the last\n         degree / 2 entries of b\n    Args:\n        - degree: int, degree of the polynomial\n        - t_end: float, end point for the polynomial (default is 1.0)\n\n    Returns:\n        - A: numpy.ndarray, fitting matrix A\n  \"\"\"\n",
  "solution": "```python\nimport numpy as np\n\ndef deriv_fitting_matrix(degree, t_end=1.0):\n    \"\"\"\n    Returns a matrix A such that the vector x that satisfies Ax = b contains polynomial coefficients\n    for a polynomial p(t) that matches specified derivatives at t=0 and t=t_end.\n\n    Args:\n        - degree: int, degree of the polynomial\n        - t_end: float, end point for the polynomial (default is 1.0)\n\n    Returns:\n        - A: numpy.ndarray, fitting matrix A\n    \"\"\"\n    if degree % 2 != 0:\n        raise AssertionError(\"Degree must be even.\")\n\n    n = degree + 1\n    half_deg = degree // 2\n\n    # Initialize the matrix A with zeros\n    A = np.zeros((n, n))\n\n    # Fill the matrix for derivatives at t=0\n    for i in range(half_deg + 1):\n        for j in range(n):\n            if j >= i:\n                # Derivative of t^j at t=0 is j!/(j-i)! * t^(j-i) evaluated at t=0\n                A[i, j] = np.math.factorial(j) / np.math.factorial(j - i) if j == i else 0\n\n    # Fill the matrix for derivatives at t=t_end\n    for i in range(half_deg + 1):\n        for j in range(n):\n            if j >= i:\n                # Derivative of t^j at t=t_end is j!/(j-i)! * t^(j-i) evaluated at t=t_end\n                A[half_deg + 1 + i, j] = np.math.factorial(j) / np.math.factorial(j - i) * t_end**(j - i)\n\n    return A\n```\n\nThis function constructs a matrix `A` such that when multiplied by a vector `x` of polynomial coefficients, the resulting vector `b` will have its entries corresponding to the values and derivatives of the polynomial at `t=0` and `t=t_end`. The matrix is constructed by setting up equations for each derivative condition at the specified points. The function ensures that the degree of the polynomial is even, as required by the problem statement.",
  "canonical_solution": "\n  if degree % 2 == 0:\n    return AssertionError\n\n  A = np.zeros((degree, degree))\n\n  ts = t_end ** np.array(range(degree))\n\n  constant_term = 1\n  poly = np.ones(degree)\n  for i in range(degree // 2):\n    A[i, i] = constant_term\n    A[degree // 2 + i, :] = np.hstack((np.zeros(i), poly * ts[:degree - i]))\n    poly = np.polynomial.polynomial.polyder(poly)\n    constant_term *= (i + 1)\n\n  return A\n\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(2, 1.0) == deriv_fitting_matrix(2, 1.0))\nassert(SOLUTION_SIGNATURE(6, 1.0) == deriv_fitting_matrix(6, 1.0))\nassert((SOLUTION_SIGNATURE(5, 0.5) == deriv_fitting_matrix(5, 0.5)).all())\nassert((SOLUTION_SIGNATURE(31, 0.9) == deriv_fitting_matrix(31, 0.9)).all())\nassert((SOLUTION_SIGNATURE(73, 0.1) == deriv_fitting_matrix(73, 0.1)).all())\nassert((SOLUTION_SIGNATURE(9, 3) == deriv_fitting_matrix(9, 3)).all())\n"
 },
 {
  "task_id": "RWPB/23",
  "prompt": "def merge(ids, pair, idx):\n    \"\"\"\n    Merges consecutive occurrences of a pair of integers in a list with a new integer.\n\n    Parameters\n    ----------\n    ids : list[int]\n        The list of integers where the merging will occur.\n    pair : tuple[int, int]\n        The pair of integers to be merged.\n    idx : int\n        The new integer that will replace the consecutive occurrences of 'pair'.\n\n    Returns\n    -------\n    list[int]\n        A new list of integers with all occurrences of 'pair' merged into 'idx'.\n\n    Examples\n    --------\n    >>> merge([1, 2, 3, 1, 2], (1, 2), 4)\n    [4, 3, 4]\n\n    The function iterates through the list 'ids' and checks for consecutive occurrences\n    of the integers in 'pair'. When such a pair is found, it is replaced by 'idx' in the\n    new list 'newids'. If the pair is not found, the current integer is simply appended to\n    'newids'. The function returns the modified list with the pairs merged.\n    \"\"\"\n",
  "solution": "Here's the complete implementation of the `merge` function:\n\n```python\ndef merge(ids, pair, idx):\n    newids = []\n    i = 0\n    while i < len(ids):\n        # Check if we are at a position where the next two elements form the pair\n        if i + 1 < len(ids) and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n            # Append the new integer and skip the next element of the pair\n            newids.append(idx)\n            i += 2\n        else:\n            # Append the current element\n            newids.append(ids[i])\n            i += 1\n    return newids\n```\n\nThis function iterates through the list `ids` and checks for consecutive occurrences of the integers in `pair`. When such a pair is found, it is replaced by `idx` in the new list `newids`. If the pair is not found, the current integer is simply appended to `newids`. The function returns the modified list with the pairs merged.",
  "canonical_solution": "\n    newids = []\n    i = 0\n    while i < len(ids):\n        # Check if the current and next item form the pair and are not the last element\n        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n            # Append the idx to the new list and skip the next item\n            newids.append(idx)\n            i += 2\n        else:\n            # Append the current item to the new list\n            newids.append(ids[i])\n            i += 1\n    return newids\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(ids = [1, 2, 1, 2, 1, 2, 3], pair = (1, 2), idx = 4) == merge(ids = [1, 2, 1, 2, 1, 2, 3], pair = (1, 2), idx = 4))\nassert(SOLUTION_SIGNATURE(ids = [1, 3, 4, 5, 6], pair = (1, 2), idx = 4) == merge(ids = [1, 3, 4, 5, 6], pair = (1, 2), idx = 4))\nassert(SOLUTION_SIGNATURE(ids = [1, 2, 3, 4, 1, 2], pair = (1, 2), idx = 4) == merge(ids = [1, 2, 3, 4, 1, 2], pair = (1, 2), idx = 4))\n"
 },
 {
  "task_id": "RWPB/17",
  "prompt": "import numpy as np\n\ndef generate_linear_parameters(start, stop, num):\n    \"\"\"\n    Generates a list of linearly spaced parameters between start and stop.\n\n    Parameters\n    ----------\n    start : float\n        The starting value of the sequence.\n    stop : float\n        The ending value of the sequence.\n    num : int\n        The number of values to generate.\n\n    Returns\n    -------\n    List[float]\n        A list containing 'num' values, linearly spaced between 'start' and 'stop'.\n\n    Notes\n    -----\n    This function uses numpy's linspace function to create an array of 'num' values evenly\n    distributed over the interval [start, stop]. The resulting array is then converted to a list.\n    \"\"\"\n",
  "solution": "```python\n    # Use numpy's linspace function to generate the linearly spaced values\n    linear_values = np.linspace(start, stop, num)\n    \n    # Convert the numpy array to a list of floats\n    return list(linear_values)\n```\n\nThis completes the function `generate_linear_parameters`, which now uses `np.linspace` to generate a specified number of linearly spaced values between `start` and `stop`, and then converts the resulting numpy array into a list of floats before returning it.",
  "canonical_solution": "\n    # Generate linearly spaced values between 'start' and 'stop'\n    parames = list(\n        np.linspace(\n            start=start,\n            stop=stop,\n            num=num,\n        )\n    )\n    return parames\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(start=0, stop=10, num=5) == generate_linear_parameters(start=0, stop=10, num=5))\nassert(SOLUTION_SIGNATURE(start=0, stop=10, num=1) == generate_linear_parameters(start=0, stop=10, num=1))\nassert(SOLUTION_SIGNATURE(start=0, stop=10, num=0) == generate_linear_parameters(start=0, stop=10, num=0))\n"
 },
 {
  "task_id": "RWPB/46",
  "prompt": "from typing import List, Union\n\nimport numpy as np\ndef cosine_similarity(X: Union[List[List[float]], List[np.ndarray], np.ndarray], Y: Union[List[List[float]], List[np.ndarray], np.ndarray]) -> np.ndarray:\n    \"\"\"\n    Compute the row-wise cosine similarity between two matrices of equal width.\n    If the length of x is not equal the length of Y, return ValueError\n    \n    Parameters:\n    X (Union[List[List[float]], List[np.ndarray], np.ndarray]): A matrix (or a list of lists) where each row represents a vector.\n    Y (Union[List[List[float]], List[np.ndarray], np.ndarray]): Another matrix (or a list of lists) where each row represents a vector. Must have the same number of columns as X.\n\n    Returns:\n    np.ndarray: A matrix of cosine similarities. Each element at position (i, j) represents the cosine similarity between the i-th row of X and the j-th row of Y.\n\n    The process involves:\n    1. Converting the inputs X and Y to numpy arrays if they are not already in that format.\n    2. Checking that X and Y have the same number of columns to ensure they are comparable.\n    3. Calculating the L2 norm (Euclidean norm) of each row in X and Y.\n    4. Computing the cosine similarity using the dot product of X and Y, normalized by the outer product of the row norms.\n    5. Handling cases where division by zero or invalid operations might occur by setting such results to zero.\n    \"\"\"\n",
  "solution": "```python\n    # Step 1: Convert X and Y to numpy arrays if they are not already\n    X = np.array(X)\n    Y = np.array(Y)\n\n    # Step 2: Check if X and Y have the same number of columns\n    if X.shape[1] != Y.shape[1]:\n        raise ValueError(\"X and Y must have the same number of columns\")\n\n    # Step 3: Calculate the L2 norm of each row in X and Y\n    X_norms = np.linalg.norm(X, axis=1)\n    Y_norms = np.linalg.norm(Y, axis=1)\n\n    # Avoid division by zero by replacing zero norms with very small positive numbers\n    X_norms[X_norms == 0] = 1e-10\n    Y_norms[Y_norms == 0] = 1e-10\n\n    # Step 4: Compute the dot product of X and Y\n    dot_product = np.dot(X, Y.T)\n\n    # Step 5: Compute the outer product of the row norms\n    norm_product = np.outer(X_norms, Y_norms)\n\n    # Calculate the cosine similarity\n    cosine_similarity = dot_product / norm_product\n\n    # Handle any numerical issues such as division by zero by setting invalid values to zero\n    cosine_similarity[np.isnan(cosine_similarity)] = 0\n\n    return cosine_similarity\n```\n\nThis function first converts the input lists or arrays to numpy arrays. It then checks that the two arrays have the same number of columns, which is necessary for computing the cosine similarity. The L2 norms of each row in both matrices are computed, and these norms are used to normalize the dot product of the matrices. The result is a matrix where each element (i, j) is the cosine similarity between the i-th row of X and the j-th row of Y. The function also includes error handling for division by zero and other numerical issues by setting problematic values to zero.",
  "canonical_solution": "\n    if len(X) == 0 or len(Y) == 0:\n        return np.array([])\n\n    X = np.array(X)\n    Y = np.array(Y)\n    if X.shape[1] != Y.shape[1]:\n        return ValueError\n    \n    X_norm = np.linalg.norm(X, axis=1)\n    Y_norm = np.linalg.norm(Y, axis=1)\n    # Ignore divide by zero errors run time warnings as those are handled below.\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        similarity = np.dot(X, Y.T) / np.outer(X_norm, Y_norm)\n    similarity[np.isnan(similarity) | np.isinf(similarity)] = 0.0\n    return similarity\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE([[1, 2], [3, 4]], [[5, 6], [7, 8]]) == cosine_similarity([[1, 2], [3, 4]], [[5, 6], [7, 8]])).all())\nassert((SOLUTION_SIGNATURE([[0, 0], [1, 1]], [[1, 1], [0, 0]]) == cosine_similarity([[0, 0], [1, 1]], [[1, 1], [0, 0]])).all())\nassert(SOLUTION_SIGNATURE([[1, 2, 3], [4, 5, 6]], [[1, 2], [3, 4]]) == cosine_similarity([[1, 2, 3], [4, 5, 6]], [[1, 2], [3, 4]]))\nassert((SOLUTION_SIGNATURE(np.array([[1, 2], [3, 4]]), [[5, 6], [7, 8]]) == cosine_similarity(np.array([[1, 2], [3, 4]]), [[5, 6], [7, 8]])).all())\nassert((SOLUTION_SIGNATURE(np.array([[0, 0], [1, 1]]), np.array([[1, 1], [0, 0]])) == cosine_similarity(np.array([[0, 0], [1, 1]]), np.array([[1, 1], [0, 0]]))).all())\n"
 },
 {
  "task_id": "RWPB/57",
  "prompt": "def clean_folder_name(folder_name: str) -> str:\n    \"\"\"\n    Sanitizes a folder name by replacing characters that are invalid in file paths with underscores.\n\n    Arguments:\n    folder_name : str\n        The original folder name that may contain invalid characters.\n\n    Returns:\n    str\n        A sanitized version of the folder name where all invalid characters have been replaced with underscores.\n    \"\"\"\n",
  "solution": "Certainly! Here's the completed function that sanitizes the folder name by replacing invalid characters with underscores:\n\n```python\nimport re\n\ndef clean_folder_name(folder_name: str) -> str:\n    \"\"\"\n    Sanitizes a folder name by replacing characters that are invalid in file paths with underscores.\n\n    Arguments:\n    folder_name : str\n        The original folder name that may contain invalid characters.\n\n    Returns:\n    str\n        A sanitized version of the folder name where all invalid characters have been replaced with underscores.\n    \"\"\"\n    # Define a regular expression pattern for invalid file name characters\n    invalid_chars_pattern = r'[<>:\"/\\\\|?*]'\n\n    # Replace all occurrences of invalid characters with underscores\n    sanitized_folder_name = re.sub(invalid_chars_pattern, '_', folder_name)\n\n    return sanitized_folder_name\n```\n\nThis function uses the `re` module to replace all characters that are invalid in file paths (such as `<`, `>`, `:`, `\"`, `/`, `\\`, `|`, `?`, `*`) with underscores. This ensures that the resulting string can be safely used as a folder name on most file systems.",
  "canonical_solution": "\n    cleaned_name = folder_name  # Start with the original folder name\n    invalid_chars = '<>:\"/\\\\|?*.'  # Define a string of characters that are invalid in file names\n\n    # Replace each invalid character with an underscore\n    for char in invalid_chars:\n        cleaned_name = cleaned_name.replace(char, \"_\")\n\n    return cleaned_name\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(\"Project<Name>:Version/1.2\\3|4*5?6\") == clean_folder_name(\"Project<Name>:Version/1.2\\3|4*5?6\"))\nassert(SOLUTION_SIGNATURE(\"Regular_Folder_Name\") == clean_folder_name(\"Regular_Folder_Name\"))\nassert(SOLUTION_SIGNATURE(\"<>>:|?**....\") == clean_folder_name(\"<>>:|?**....\"))\n"
 },
 {
  "task_id": "RWPB/32",
  "prompt": "import math\nimport torch\n\ndef repeat_to_batch_size(tensor, batch_size):\n    \"\"\"\n    Repeats the elements of the tensor to match the specified batch size or truncates it if larger.\n\n    Parameters\n    ----------\n    tensor : torch.Tensor\n        The input tensor which may need to be repeated or truncated.\n    batch_size : int\n        The target batch size to match the tensor's size.\n\n    Returns\n    -------\n    torch.Tensor\n        The tensor with its first dimension adjusted to match the batch size, either by\n        repeating elements or truncating the tensor.\n\n    Notes\n    -----\n    This function checks the size of the input tensor along its first dimension and performs\n    one of the following operations:\n    - If the tensor's size is greater than the batch size, it truncates the tensor to the batch size.\n    - If the tensor's size is smaller than the batch size, it repeats the elements of the tensor\n      to match the batch size, rounding up the number of repetitions if necessary.\n    - If the tensor's size is equal to the batch size, it returns the tensor unchanged.\n\n    The function uses PyTorch's repeat method to duplicate the tensor elements along the first\n    dimension and math.ceil to ensure that the tensor is repeated enough times to reach at least\n    the batch size.\n    \"\"\"\n",
  "solution": "    # Get the current size of the tensor along the first dimension\n    current_size = tensor.size(0)\n\n    if current_size == batch_size:\n        # If the current size is already equal to the batch size, return the tensor as is\n        return tensor\n    elif current_size > batch_size:\n        # If the current size is greater than the batch size, truncate the tensor\n        return tensor[:batch_size]\n    else:\n        # If the current size is less than the batch size, calculate how many times to repeat\n        num_repeats = math.ceil(batch_size / current_size)\n        # Repeat the tensor along the first dimension\n        repeated_tensor = tensor.repeat(num_repeats, *[1]*len(tensor.shape[1:]))\n        # Truncate the repeated tensor to match the exact batch size\n        return repeated_tensor[:batch_size]",
  "canonical_solution": "\n    if tensor.shape[0] > batch_size:\n        # If the tensor is larger than the batch size, truncate it\n        return tensor[:batch_size]\n    elif tensor.shape[0] < batch_size:\n        # If the tensor is smaller than the batch size, calculate the number of repetitions\n        repeat_times = math.ceil(batch_size / tensor.shape[0])\n        # Repeat the tensor along the first dimension and then truncate to batch size\n        return tensor.repeat([repeat_times] + [1] * (len(tensor.shape) - 1))[:batch_size]\n    # If the tensor's size is equal to the batch size, return it as is\n    return tensor\n\n\n",
  "unprocess_testcases": "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\nb = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15], [16, 17, 18]])\nc = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n\nassert((SOLUTION_SIGNATURE(a, 4) == repeat_to_batch_size(a, 4)).all())\nassert((SOLUTION_SIGNATURE(b, 4) == repeat_to_batch_size(b, 4)).all())\nassert((SOLUTION_SIGNATURE(c, 4) == repeat_to_batch_size(c, 4)).all())\n"
 },
 {
  "task_id": "RWPB/63",
  "prompt": "import torch\nfrom einops import rearrange\n\n\n\ndef rotate_half(x):\n    \"\"\"\n    Rotates the last dimension of a tensor by 90 degrees counterclockwise,\n    assuming the last dimension represents 2D coordinates or complex numbers.\n\n    Arguments:\n    x : torch.Tensor\n        Input tensor with its last dimension size as a multiple of 2, representing 2D vectors.\n\n    Returns:\n    torch.Tensor\n        A tensor with the same shape as the input, where each 2D vector has been rotated.\n    \"\"\"\n",
  "solution": "Certainly! To complete the function `rotate_half(x)` that rotates each 2D vector in the last dimension of the tensor by 90 degrees counterclockwise, we can use simple matrix multiplication. For a 2D vector represented as (x, y), a 90-degree counterclockwise rotation results in the vector (-y, x). Here's how you can implement this:\n\n```python\nimport torch\n\ndef rotate_half(x):\n    \"\"\"\n    Rotates the last dimension of a tensor by 90 degrees counterclockwise,\n    assuming the last dimension represents 2D coordinates or complex numbers.\n\n    Arguments:\n    x : torch.Tensor\n        Input tensor with its last dimension size as a multiple of 2, representing 2D vectors.\n\n    Returns:\n    torch.Tensor\n        A tensor with the same shape as the input, where each 2D vector has been rotated.\n    \"\"\"\n    # Check if the last dimension is a multiple of 2\n    if x.shape[-1] % 2 != 0:\n        raise ValueError(\"The last dimension of the tensor must be a multiple of 2.\")\n\n    # Split the last dimension into two separate dimensions\n    # Assuming the last dimension represents pairs (x, y)\n    x_reshaped = x.view(*x.shape[:-1], -1, 2)\n    \n    # Extract x and y components\n    x_component = x_reshaped[..., 0]\n    y_component = x_reshaped[..., 1]\n    \n    # Perform the rotation (-y, x)\n    rotated_x = -y_component\n    rotated_y = x_component\n    \n    # Combine the rotated components back into the original shape\n    rotated = torch.stack((rotated_x, rotated_y), dim=-1)\n    \n    # Reshape to match the original tensor's shape\n    return rotated.view(*x.shape)\n\n# Example usage:\n# x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n# rotated_x = rotate_half(x)\n# print(rotated_x)\n```\n\nThis function first checks if the last dimension is a multiple of 2, reshapes the tensor to separate the x and y components, applies the rotation transformation, and then reshapes the tensor back to its original shape. The rotation transformation itself is achieved by swapping the x and y components and negating the new x component to achieve a 90-degree counterclockwise rotation.",
  "canonical_solution": "    \n    # Reshape x assuming the last dimension can be split into 2 parts\n    x = rearrange(x, '... (d r) -> ... d r', r=2)\n\n    # Unbind the last dimension into two separate tensors\n    x1, x2 = x.unbind(dim=-1)\n\n    # Rotate each vector by 90 degrees counterclockwise\n    x = torch.stack((-x2, x1), dim=-1)\n\n    # Rearrange back to the original dimension structure\n    return rearrange(x, '... d r -> ... (d r)')\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([[1, 0], [0, 1]])) == rotate_half(torch.tensor([[1, 0], [0, 1]]))).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[[1, 0, 0, 1], [0, 1, -1, 0]]])) == rotate_half(torch.tensor([[[1, 0, 0, 1], [0, 1, -1, 0]]]))).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[1, 0]])) == rotate_half(torch.tensor([[1, 0]]))).all())\n"
 },
 {
  "task_id": "RWPB/90",
  "prompt": "import torch\n\ndef scale(s, device=None):\n    \"\"\"\n    Creates a 4x4 homogeneous transformation matrix for uniform scaling in 3D space.\n\n    Parameters:\n    s (float): Scaling factor applied equally to the x, y, and z axes.\n    device (torch.device, optional): The device on which to create the tensor (e.g., 'cpu' or 'cuda'). Default is None, which means the tensor will be created on the default device.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the scaling transformation matrix.\n\n    Process:\n    1. Construct a 4x4 identity matrix.\n    2. Replace the diagonal elements with the scaling factor for the x, y, and z coordinates.\n    \"\"\"\n",
  "solution": "    # Step 1: Create a 4x4 identity matrix\n    scale_matrix = torch.eye(4, device=device)\n\n    # Step 2: Replace the diagonal elements with the scaling factor for x, y, and z coordinates\n    scale_matrix[0, 0] = s  # Scale x\n    scale_matrix[1, 1] = s  # Scale y\n    scale_matrix[2, 2] = s  # Scale z\n\n    # The last element of the diagonal (scale_matrix[3, 3]) remains 1 as it is for homogeneous coordinates\n\n    return scale_matrix\n\n# Example usage:\n# scale_matrix = scale(2.0, device=torch.device('cpu'))\n# print(scale_matrix)",
  "canonical_solution": "    \n    return torch.tensor([[ s, 0, 0, 0], \n                         [ 0, s, 0, 0], \n                         [ 0, 0, s, 0], \n                         [ 0, 0, 0, 1]], dtype=torch.float32, device=device)\n\n\n",
  "unprocess_testcases": "a = torch.pi / 4\nassert((SOLUTION_SIGNATURE(a) == scale(a)).all())\n\na = 0\nassert((SOLUTION_SIGNATURE(a) == scale(a)).all())\n\na = 2 * torch.pi\nassert((SOLUTION_SIGNATURE(a) == scale(a)).all())\n"
 },
 {
  "task_id": "RWPB/80",
  "prompt": "import pandas as pd\n\ndef non_numeric_columns_and_head(df, num_rows=20):\n    \"\"\"\n    Identify non-numeric columns in a DataFrame and return their names and head.\n\n    :param df: Pandas DataFrame to be examined.\n    :param num_rows: Number of rows to include in the head (default is 20).\n    :return: A tuple with two elements:\n             1. List of column names that are not numeric (integer or float).\n             2. DataFrame containing the head of the non-numeric columns.\n    \"\"\"\n",
  "solution": "```python\n    # Identify non-numeric columns\n    non_numeric_cols = [col for col in df.columns if df[col].dtype not in ['int64', 'float64']]\n    \n    # Extract the head of the non-numeric columns\n    non_numeric_head = df[non_numeric_cols].head(num_rows)\n    \n    return non_numeric_cols, non_numeric_head\n```",
  "canonical_solution": "    \n    # Identify columns that are not of numeric data type\n    non_numeric_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]\n    \n    # Get the head of the non-numeric columns\n    non_numeric_head = df[non_numeric_cols].head(num_rows).to_csv()\n    \n    return non_numeric_cols, non_numeric_head\n\n",
  "unprocess_testcases": "data = {\n  'id': [1, 2, 3, 4, 5],\n  'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n  'date_of_birth': pd.to_datetime(['1990-01-01', '1992-05-15', '1988-08-21', '1990-07-30', '1985-01-01']),\n  'salary': [50000.00, 60000.00, 55000.00, 45000.00, 70000.00],\n  'is_active': [True, False, True, False, True]\n}\ndf = pd.DataFrame(data)\nassert(SOLUTION_SIGNATURE(df)[0] == non_numeric_columns_and_head(df)[0])\nassert(SOLUTION_SIGNATURE(df)[1] == non_numeric_columns_and_head(df)[1])\n\ndata = {\n  'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n  'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'],\n  'email': ['alice@example.com', 'bob@example.com', 'charlie@example.com', 'david@example.com', 'eva@example.com']\n}\ndf = pd.DataFrame(data)\nassert(SOLUTION_SIGNATURE(df)[0] == non_numeric_columns_and_head(df)[0])\nassert(SOLUTION_SIGNATURE(df)[1] == non_numeric_columns_and_head(df)[1])\n\ndata = {\n  'id': [1, 2, 3],\n  'name': ['Alice', 'Bob', 'Charlie'],\n  'temperature': [98.6, 99.1, 98.7]\n}\ndf = pd.DataFrame(data)\nnum_rows = 10\nassert(SOLUTION_SIGNATURE(df)[0] == non_numeric_columns_and_head(df)[0])\nassert(SOLUTION_SIGNATURE(df)[1] == non_numeric_columns_and_head(df)[1])\n"
 },
 {
  "task_id": "RWPB/73",
  "prompt": "import numpy as np\n\n\ndef mse_to_psnr(mse):\n  \"\"\"\n  Compute the Peak Signal-to-Noise Ratio (PSNR) from the Mean Squared Error (MSE).\n  If mse is equal zero, the code need to return ValueError\n\n  Parameters:\n  mse (float): Mean Squared Error value. This value should be a non-negative float, \n               representing the average squared difference between the reference \n               and the distorted image.\n\n  Returns:\n  float: The PSNR value in decibels (dB).\n\n  Process:\n  - The function assumes that the maximum possible pixel value (I_max) in the image \n    is 1.\n  - The formula to convert MSE to PSNR is given by:\n      PSNR = -10 * log10(MSE)\n  - Use np.log to calculate.\n  \"\"\"\n",
  "solution": "```python\nimport numpy as np\n\ndef mse_to_psnr(mse):\n    \"\"\"\n    Compute the Peak Signal-to-Noise Ratio (PSNR) from the Mean Squared Error (MSE).\n    If mse is equal zero, the code need to return ValueError\n\n    Parameters:\n    mse (float): Mean Squared Error value. This value should be a non-negative float, \n                 representing the average squared difference between the reference \n                 and the distorted image.\n\n    Returns:\n    float: The PSNR value in decibels (dB).\n\n    Process:\n    - The function assumes that the maximum possible pixel value (I_max) in the image \n      is 1.\n    - The formula to convert MSE to PSNR is given by:\n        PSNR = -10 * log10(MSE)\n    - Use np.log to calculate.\n    \"\"\"\n    if mse <= 0:\n        raise ValueError(\"MSE must be greater than zero to calculate PSNR.\")\n    \n    psnr = -10 * np.log10(mse)\n    return psnr\n```",
  "canonical_solution": "  \n  if mse == 0:\n    return ValueError\n  \n  return -10. / np.log(10.) * np.log(mse)\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(0.01) == mse_to_psnr(0.01))\nassert(SOLUTION_SIGNATURE(1e-10) == mse_to_psnr(1e-10))\nassert(SOLUTION_SIGNATURE(0) == mse_to_psnr(0))\nassert(SOLUTION_SIGNATURE(1) == mse_to_psnr(1))\nassert(SOLUTION_SIGNATURE(0.2312) == mse_to_psnr(0.2312))\n"
 },
 {
  "task_id": "RWPB/22",
  "prompt": "def get_stats(ids, counts=None):\n    \"\"\"\n    Computes the frequency of consecutive integer pairs in a list.\n\n    Args:\n        ids (list of int): List of integers whose consecutive pairs are to be counted.\n        counts (dict, optional): Existing dictionary to update with new counts. If None, a new dictionary is created.\n\n    Returns:\n        dict: A dictionary where each key is a tuple representing a pair of consecutive integers,\n              and the value is the count of how often each pair appears.\n\n    Example:\n        >>> get_stats([1, 2, 3, 1, 2])\n        {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n\n    This function iterates over consecutive elements in the provided list, creating pairs,\n    and either updates an existing dictionary or creates a new one to record the frequency of each pair.\n    \"\"\"\n",
  "solution": "Here's the completed function `get_stats` that computes the frequency of consecutive integer pairs in a list:\n\n```python\ndef get_stats(ids, counts=None):\n    if counts is None:\n        counts = {}\n\n    for i in range(len(ids) - 1):\n        pair = (ids[i], ids[i + 1])\n        if pair in counts:\n            counts[pair] += 1\n        else:\n            counts[pair] = 1\n\n    return counts\n```\n\nThis function initializes a dictionary if none is provided, iterates through the list of integers, forms pairs of consecutive integers, and counts their occurrences in the dictionary. It then returns the dictionary containing these counts.",
  "canonical_solution": "\n    counts = {} if counts is None else counts  # Initialize the dictionary if not provided\n    for pair in zip(ids, ids[1:]):  # Create pairs from consecutive elements\n        counts[pair] = counts.get(pair, 0) + 1  # Increment the count for each pair in the dictionary\n    return counts\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE([]) == get_stats([]))\nassert(SOLUTION_SIGNATURE([1, 2, 1, 2, 1, 2, 3, 4, 3, 4]) == get_stats([1, 2, 1, 2, 1, 2, 3, 4, 3, 4]))\nassert(SOLUTION_SIGNATURE([5, 6, 7, 5, 6], counts={(5, 6): 1, (6, 7): 2}) == get_stats([5, 6, 7, 5, 6], counts={(5, 6): 1, (6, 7): 2}))\n"
 },
 {
  "task_id": "RWPB/16",
  "prompt": "import numpy as np\n\ndef his_match(src, dst):\n    \"\"\"\n    Performs histogram matching between two color images src and dst.\n\n    Parameters\n    ----------\n    src : numpy.ndarray\n        Source image array of shape (H, W, 3) and type float64.\n    dst : numpy.ndarray\n        Destination image array of shape (H, W, 3) and type float64.\n\n    Returns\n    -------\n    numpy.ndarray\n        The matched image array of the same shape and type as src and dst.\n\n    Notes\n    -----\n    This function converts the images to uint8 type, calculates the cumulative\n    distribution functions (CDF) for each channel of the source and destination\n    images, and then finds the best match for each pixel in the destination image\n    in the source image's CDF. The matched values are then used to create a new\n    image that has a similar color distribution to the source image.\n    \"\"\"\n",
  "solution": "    # Convert images from float64 to uint8\n    src_uint8 = (src * 255).astype(np.uint8)\n    dst_uint8 = (dst * 255).astype(np.uint8)\n\n    # Initialize the output image\n    matched_img = np.zeros_like(src_uint8)\n\n    # Process each channel separately (assuming RGB channels)\n    for channel in range(3):\n        # Flatten the channel data for easier histogram and CDF calculation\n        src_hist, _ = np.histogram(src_uint8[:, :, channel].flatten(), bins=256, range=[0, 256])\n        dst_hist, _ = np.histogram(dst_uint8[:, :, channel].flatten(), bins=256, range=[0, 256])\n\n        # Calculate the cumulative distribution function (CDF) for source and destination\n        src_cdf = np.cumsum(src_hist).astype(float)\n        src_cdf /= src_cdf[-1]  # Normalize to range [0, 1]\n        dst_cdf = np.cumsum(dst_hist).astype(float)\n        dst_cdf /= dst_cdf[-1]  # Normalize to range [0, 1]\n\n        # Create a mapping from destination pixels to source pixels\n        mapping = np.zeros(256, dtype=np.uint8)\n        src_idx = 0\n        for dst_idx in range(256):\n            while src_cdf[src_idx] < dst_cdf[dst_idx] and src_idx < 255:\n                src_idx += 1\n            mapping[dst_idx] = src_idx\n\n        # Apply the mapping to get the matched image\n        matched_img[:, :, channel] = mapping[dst_uint8[:, :, channel]]\n\n    # Convert matched image back to float64\n    matched_img = matched_img.astype(np.float64) / 255\n\n    return matched_img\n\n# Note: This function assumes that the input images are properly scaled to [0, 1] as float64\n# and that they have the same dimensions. The function does not handle cases where the input\n# images have different dimensions or are not in the expected range. Adjustments or validations\n# might be needed depending on the actual use case.",
  "canonical_solution": "\n    # Scale images to [0, 255] range and convert to uint8 type\n    src = src * 255.0\n    dst = dst * 255.0\n    src = src.astype(np.uint8)\n    dst = dst.astype(np.uint8)\n    res = np.zeros_like(dst)\n\n    # Initialize CDF arrays for each channel\n    cdf_src = np.zeros((3, 256))\n    cdf_dst = np.zeros((3, 256))\n    cdf_res = np.zeros((3, 256))\n\n    # Keyword arguments for histogram calculation\n    kw = dict(bins=256, range=(0, 256), density=True)\n\n    # Calculate histograms and CDFs for each channel\n    for ch in range(3):\n        his_src, _ = np.histogram(src[:, :, ch], **kw)\n        hist_dst, _ = np.histogram(dst[:, :, ch], **kw)\n        cdf_src[ch] = np.cumsum(his_src)\n        cdf_dst[ch] = np.cumsum(hist_dst)\n\n        # Find the best match index for each pixel in the destination image\n        index = np.searchsorted(cdf_src[ch], cdf_dst[ch], side=\"left\")\n        np.clip(index, 0, 255, out=index)\n\n        # Assign the matched values to the result image\n        res[:, :, ch] = index[dst[:, :, ch]]\n\n        # Update histogram and CDF for the result image\n        his_res, _ = np.histogram(res[:, :, ch], **kw)\n        cdf_res[ch] = np.cumsum(his_res)\n\n    # Scale the result back to [0, 1] range and return\n    return res / 255.0\n\n\n\n",
  "unprocess_testcases": "a = np.random.rand(100, 100, 3)\nb = np.full((50, 50, 3), 0.5)\nc = np.full((50, 50, 3), 0.7)\nd = np.random.rand(64, 64, 3)\ne = np.random.rand(128, 128, 3)\n\nassert((SOLUTION_SIGNATURE(src = a, dst = a) == his_match(src = a, dst = a)).all())\nassert((SOLUTION_SIGNATURE(src = b, dst = c) == his_match(src = b, dst = c)).all())\nassert((SOLUTION_SIGNATURE(src = d, dst = e) == his_match(src = d, dst = e)).all())\n\n"
 },
 {
  "task_id": "RWPB/47",
  "prompt": "def disjoint_cycles(perm):\n    \"\"\"\n    Find the disjoint cycles of a permutation.\n\n    Args:\n        perm (list): The permutation as a list.\n\n    Returns:\n        list: A list of lists, where each inner list represents a disjoint cycle.\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `disjoint_cycles` function which finds the disjoint cycles of a given permutation:\n\n```python\ndef disjoint_cycles(perm):\n    \"\"\"\n    Find the disjoint cycles of a permutation.\n\n    Args:\n        perm (list): The permutation as a list.\n\n    Returns:\n        list: A list of lists, where each inner list represents a disjoint cycle.\n    \"\"\"\n    seen = set()\n    cycles = []\n\n    for i in range(len(perm)):\n        if i not in seen:\n            cycle = []\n            x = i\n            while x not in seen:\n                seen.add(x)\n                cycle.append(x)\n                x = perm[x]\n            if len(cycle) > 1:\n                cycles.append(cycle)\n\n    return cycles\n```\n\nThis function works by iterating over each element in the permutation. If an element has not been visited (`seen`), it starts tracing a cycle from that element. It continues to follow the permutation until it returns to the starting point, collecting elements into `cycle`. Each completed cycle is added to the `cycles` list if it contains more than one element. This ensures that trivial cycles (cycles of length 1) are not included, which is a common convention in cycle notation for permutations.",
  "canonical_solution": "\n    used = set()\n    cycles = []\n    minimum = 1\n    while len(used) < len(perm):\n        cycle = [minimum]\n        used.add(minimum)\n        elt = perm[minimum - 1]\n        while elt not in cycle:\n            cycle.append(elt)\n            used.add(elt)\n            elt = perm[elt - 1]\n        if len(set(perm).symmetric_difference(used)) != 0:\n            minimum = min(set(perm).symmetric_difference(used))\n        cycles.append(cycle)\n    return cycles\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE([2, 3, 4, 5, 1]) == disjoint_cycles([2, 3, 4, 5, 1]))\nassert(SOLUTION_SIGNATURE([2, 1, 4, 3, 6, 5]) == disjoint_cycles([2, 1, 4, 3, 6, 5]))\nassert(SOLUTION_SIGNATURE([1, 3, 2, 5, 4]) == disjoint_cycles([1, 3, 2, 5, 4]))\n"
 },
 {
  "task_id": "RWPB/67",
  "prompt": "MAX_VAL = (1 << 256) - 1\n\n# does not revert on overflow\ndef unsafeMul(x, y):\n    \"\"\"\n    Multiply two integers, ignoring overflow.\n    Args:\n        - x: int, first operand\n        - y: int, second operand\n    Returns:\n        result: int, the product of x and y, with overflow ignored\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `unsafeMul` function which multiplies two integers and ignores overflow by using Python's inherent ability to handle arbitrarily large integers:\n\n```python\ndef unsafeMul(x, y):\n    \"\"\"\n    Multiply two integers, ignoring overflow.\n    Args:\n        - x: int, first operand\n        - y: int, second operand\n    Returns:\n        result: int, the product of x and y, with overflow ignored\n    \"\"\"\n    result = x * y\n    # Since Python automatically handles large integers, we simulate ignoring overflow\n    # by using modulo operation with a very large number (like 2^256 for Ethereum-like overflow)\n    return result % (MAX_VAL + 1)\n\n# Example usage:\n# result = unsafeMul(10, 20)\n# print(result)  # Output will be 200\n```\n\nThis function multiplies `x` and `y`, and then uses modulo operation with `MAX_VAL + 1` to simulate the behavior of ignoring overflow in environments where integer overflow is a concern (like in many blockchain platforms where integers are typically 256-bit). In Python, this step is not strictly necessary for handling large numbers but is included to reflect the overflow behavior as described.",
  "canonical_solution": "    \n    return (x * y) & MAX_VAL\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(15, 10) == unsafeMul(15, 10))\nassert(SOLUTION_SIGNATURE((1 << 256) - 1, 1) == unsafeMul((1 << 256) - 1, 1))\nassert(SOLUTION_SIGNATURE(pow(2, 128), pow(2, 129)) == unsafeMul(pow(2, 128), pow(2, 129)))\n"
 },
 {
  "task_id": "RWPB/36",
  "prompt": "import numpy as np\n\ndef smooth_pose_seq(pose_seq, window_size=5):\n    \"\"\"\n    Smooths a sequence of poses using a moving average filter.\n\n    Args:\n        pose_seq (np.array): Array of pose data, where each pose is expected to be an array of coordinates.\n        window_size (int, optional): The number of frames to include in the moving average window. Default is 5.\n\n    Returns:\n        np.array: An array of smoothed poses with the same shape as the input.\n\n    This function applies a simple moving average smoothing technique to a sequence of poses, which is often useful\n    in motion analysis or animation to reduce noise or fluctuations between frames. The function handles edge cases by\n    adjusting the window size at the start and end of the sequence, ensuring all poses are smoothed.\n    \"\"\"\n",
  "solution": "    # Check if the input pose sequence is empty or if the window size is larger than the sequence length\n    if len(pose_seq) == 0 or window_size <= 0:\n        return pose_seq\n\n    # Initialize the smoothed pose sequence with zeros having the same shape as the input\n    smoothed_pose_seq = np.zeros_like(pose_seq)\n\n    # Iterate over each pose in the sequence\n    for i in range(len(pose_seq)):\n        # Determine the start and end of the window for the moving average\n        start = max(0, i - window_size // 2)\n        end = min(len(pose_seq), i + window_size // 2 + 1)\n\n        # Compute the moving average for the current window\n        window = pose_seq[start:end]\n        smoothed_pose_seq[i] = np.mean(window, axis=0)\n\n    return smoothed_pose_seq\n\n# Example usage:\n# pose_seq = np.random.rand(10, 3)  # Random sequence of 10 poses, each with 3 coordinates\n# smoothed_poses = smooth_pose_seq(pose_seq)\n# print(smoothed_poses)",
  "canonical_solution": "\n    # Initialize an array of zeros with the same shape as the input sequence to store the smoothed poses\n    smoothed_pose_seq = np.zeros_like(pose_seq)\n\n    # Iterate over each pose in the sequence\n    for i in range(len(pose_seq)):\n        # Calculate the start and end indices of the window, ensuring they are within the bounds of the sequence\n        start = max(0, i - window_size // 2)\n        end = min(len(pose_seq), i + window_size // 2 + 1)\n\n        # Compute the mean of the poses within the window and assign it to the smoothed sequence\n        smoothed_pose_seq[i] = np.mean(pose_seq[start:end], axis=0)\n\n    return smoothed_pose_seq\n\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(np.array([[1, 2], [2, 3], [3, 4]])) == smooth_pose_seq(np.array([[1, 2], [2, 3], [3, 4]]))).all())\nassert((SOLUTION_SIGNATURE(np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6], [7, 7], [8, 8]])) == smooth_pose_seq(np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6], [7, 7], [8, 8]]))).all())\nassert((SOLUTION_SIGNATURE(np.array([[1e10, 1e10], [1e-10, 1e-10], [1.5, 2.5], [0.001, 0.002], [5000, 5000]])) == smooth_pose_seq(np.array([[1e10, 1e10], [1e-10, 1e-10], [1.5, 2.5], [0.001, 0.002], [5000, 5000]]))).all())\n"
 },
 {
  "task_id": "RWPB/94",
  "prompt": "import torch\n\ndef lookAt(eye, at, up):\n    \"\"\"\n    Creates a view transformation matrix using the \"look-at\" method, which is commonly used in 3D graphics to create a camera view matrix.\n\n    Parameters:\n    eye (torch.Tensor): A 1D tensor with 3 elements representing the position of the camera in world coordinates.\n    at (torch.Tensor): A 1D tensor with 3 elements representing the point in world coordinates that the camera is looking at.\n    up (torch.Tensor): A 1D tensor with 3 elements representing the up direction of the camera.\n\n    Returns:\n    torch.Tensor: A 4x4 tensor representing the view transformation matrix.\n\n    Process:\n    1. Compute the forward vector `w` as the normalized vector pointing from `at` to `eye`.\n    2. Compute the right vector `u` as the normalized cross product of `up` and `w`.\n    3. Compute the true up vector `v` as the cross product of `w` and `u`.\n    4. Construct the translation matrix to move the scene so that the camera is at the origin.\n    5. Construct the rotation matrix to align the camera's orientation with the computed `u`, `v`, and `w` vectors.\n    6. Combine the translation and rotation matrices by matrix multiplication to get the final view transformation matrix.\n    \"\"\"\n",
  "solution": "    # Step 1: Compute the forward vector `w`\n    w = eye - at\n    w = torch.nn.functional.normalize(w, dim=0)\n\n    # Step 2: Compute the right vector `u`\n    u = torch.cross(up, w)\n    u = torch.nn.functional.normalize(u, dim=0)\n\n    # Step 3: Compute the true up vector `v`\n    v = torch.cross(w, u)\n\n    # Step 4: Construct the translation matrix\n    translation = torch.eye(4)\n    translation[0, 3] = -eye[0]\n    translation[1, 3] = -eye[1]\n    translation[2, 3] = -eye[2]\n\n    # Step 5: Construct the rotation matrix\n    rotation = torch.eye(4)\n    rotation[0, :3] = u\n    rotation[1, :3] = v\n    rotation[2, :3] = -w\n\n    # Step 6: Combine the translation and rotation matrices\n    view_matrix = torch.mm(rotation, translation)\n\n    return view_matrix\n\n# Example usage:\n# eye = torch.tensor([1.0, 1.0, 1.0])\n# at = torch.tensor([0.0, 0.0, 0.0])\n# up = torch.tensor([0.0, 1.0, 0.0])\n# view_matrix = lookAt(eye, at, up)\n# print(view_matrix)",
  "canonical_solution": "    \n    a = eye - at\n    w = a / torch.linalg.norm(a)\n    u = torch.cross(up, w)\n    u = u / torch.linalg.norm(u)\n    v = torch.cross(w, u)\n    translate = torch.tensor([[1, 0, 0, -eye[0]], \n                              [0, 1, 0, -eye[1]], \n                              [0, 0, 1, -eye[2]], \n                              [0, 0, 0, 1]], dtype=eye.dtype, device=eye.device)\n    rotate = torch.tensor([[u[0], u[1], u[2], 0], \n                           [v[0], v[1], v[2], 0], \n                           [w[0], w[1], w[2], 0], \n                           [0, 0, 0, 1]], dtype=eye.dtype, device=eye.device)\n    return rotate @ translate\n\n\n",
  "unprocess_testcases": "\neye = torch.tensor([1.0, 1, 1])\nat = torch.tensor([0.0, 0, 0])\nup = torch.tensor([0.0, 1, 0])\nassert((SOLUTION_SIGNATURE(eye, at, up) == lookAt(eye, at, up)).all())\n\neye = torch.tensor([0.0, 0, 1])\nat = torch.tensor([0.0, 0, 0])\nup = torch.tensor([0.0, 1, 0])\nassert((SOLUTION_SIGNATURE(eye, at, up) == lookAt(eye, at, up)).all())\n\neye = torch.tensor([-3.0, 2, 5])\nat = torch.tensor([4.0, -1, 0])\nup = torch.tensor([1.0, 1, 1])\nassert((SOLUTION_SIGNATURE(eye, at, up) == lookAt(eye, at, up)).all())\n"
 },
 {
  "task_id": "RWPB/53",
  "prompt": "import re\n\ndef general_detokenize(string):\n    \"\"\"\n    Detokenizes a string by fixing spacing around punctuation and contractions.\n\n    Arguments:\n    string : str\n        The input string that needs to be detokenized.\n\n    Returns:\n    str\n        The detokenized string with corrected spacing.\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `general_detokenize` function which aims to correct the spacing around punctuation and contractions in a given string:\n\n```python\nimport re\n\ndef general_detokenize(string):\n    \"\"\"\n    Detokenizes a string by fixing spacing around punctuation and contractions.\n\n    Arguments:\n    string : str\n        The input string that needs to be detokenized.\n\n    Returns:\n    str\n        The detokenized string with corrected spacing.\n    \"\"\"\n    # Fix spaces before punctuations\n    string = re.sub(r'\\s+([?,.!\"])', r'\\1', string)\n\n    # Fix spaces after punctuations if they are followed by a word character\n    string = re.sub(r'([?,.!\"])(\\w)', r'\\1 \\2', string)\n\n    # Fix contractions\n    string = re.sub(r\"\\s+'(s|m|d|ll|re|ve|t)\\b\", r\"'\\1\", string)\n\n    # Fix multiple spaces to a single space\n    string = re.sub(r'\\s+', ' ', string)\n\n    # Strip leading and trailing spaces\n    string = string.strip()\n\n    return string\n```\n\nThis function uses regular expressions to:\n1. Remove unnecessary spaces before common punctuation marks.\n2. Ensure there is a space after punctuation if it is followed by a word character.\n3. Correct the spacing around common English contractions.\n4. Replace multiple spaces with a single space.\n5. Trim leading and trailing spaces from the string.\n\nThis should handle a variety of common cases in text detokenization.",
  "canonical_solution": "\n    # Fix the space before contractions like \"n't\" (e.g., \" ca n't\" -> \"can't\")\n    string = string.replace(\" n't\", \"n't\")\n\n    # Remove space before closing parentheses and after opening parentheses\n    string = string.replace(\" )\", \")\")\n    string = string.replace(\"( \", \"(\")\n\n    # Correct spacing issues with quotation marks\n    string = string.replace('\" ', '\"')\n    string = string.replace(' \"', '\"')\n\n    # Use regular expressions to remove space before commas and periods, and other single character punctuations\n    string = re.sub(r\" (['.,])\", r\"\\1\", string)\n\n    return string\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE('She said , \" Hello world ! \" I can not believe it ; can you ? Yes , I ca n\\'t .') == general_detokenize('She said , \" Hello world ! \" I can not believe it ; can you ? Yes , I ca n\\'t .'))\nassert(SOLUTION_SIGNATURE('We need ( in an ideal world ) a kind of setup that works ( efficiently and effectively ).') == general_detokenize('We need ( in an ideal world ) a kind of setup that works ( efficiently and effectively ).'))\nassert(SOLUTION_SIGNATURE('This is a test . What do you think ? Isn\\'t it interesting ?') == general_detokenize('This is a test . What do you think ? Isn\\'t it interesting ?'))\n"
 },
 {
  "task_id": "RWPB/43",
  "prompt": "import torch\nimport numpy as np\n\n\ndef inv(mat):\n    \"\"\"\n    Inverts a matrix that can be either a torch.Tensor or a numpy.ndarray.\n\n    Parameters\n    ----------\n    mat : torch.Tensor or np.ndarray\n        A square matrix to be inverted.\n\n    Returns\n    -------\n    torch.Tensor or np.ndarray\n        The inverse of the input matrix, with the same type as the input.\n\n    Raises\n    ------\n    ValueError\n        If the input is not a torch.Tensor or np.ndarray.\n\n    Notes\n    -----\n    This function checks the type of the input matrix and applies the appropriate inverse\n    operation using either PyTorch's torch.linalg.inv for tensor objects or NumPy's\n    np.linalg.inv for ndarray objects. It is important that the input matrix is square\n    and non-singular (i.e., it has an inverse), otherwise the inversion will fail.\n    \"\"\"\n",
  "solution": "    # Check the type of the input matrix\n    if isinstance(mat, torch.Tensor):\n        # Invert the matrix using PyTorch\n        try:\n            inverse_mat = torch.linalg.inv(mat)\n        except RuntimeError as e:\n            raise ValueError(f\"Failed to invert the matrix: {e}\")\n    elif isinstance(mat, np.ndarray):\n        # Invert the matrix using NumPy\n        try:\n            inverse_mat = np.linalg.inv(mat)\n        except np.linalg.LinAlgError as e:\n            raise ValueError(f\"Failed to invert the matrix: {e}\")\n    else:\n        # Raise an error if the input type is neither torch.Tensor nor np.ndarray\n        raise ValueError(\"Input must be a torch.Tensor or a numpy.ndarray\")\n\n    return inverse_mat\n\n# Example usage:\n# For a torch tensor\n# tensor_mat = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32)\n# inv_tensor_mat = inv(tensor_mat)\n# print(inv_tensor_mat)\n\n# For a numpy array\n# array_mat = np.array([[1, 2], [3, 4]], dtype=float)\n# inv_array_mat = inv(array_mat)\n# print(inv_array_mat)",
  "canonical_solution": "    \n    # Check if the input matrix is a torch.Tensor and invert it using torch.linalg.inv\n    if isinstance(mat, torch.Tensor):\n        return torch.linalg.inv(mat)\n\n    # Check if the input matrix is a np.ndarray and invert it using np.linalg.inv\n    if isinstance(mat, np.ndarray):\n        return np.linalg.inv(mat)\n\n    # If the input is neither a torch.Tensor nor a np.ndarray, raise a ValueError\n    return ValueError\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([[4.0, 7.0], [2.0, 6.0]])) == inv(torch.tensor([[4.0, 7.0], [2.0, 6.0]]))).all())\nassert((SOLUTION_SIGNATURE(np.array([[1, 2, 3], [0, 1, 4], [5, 6, 0]])) == inv(np.array([[1, 2, 3], [0, 1, 4], [5, 6, 0]]))).all())\nassert(SOLUTION_SIGNATURE([[1, 2], [3, 4]]) == inv([[1, 2], [3, 4]]))\n"
 },
 {
  "task_id": "RWPB/12",
  "prompt": "import torch\n\n\ndef normalize_activation(x, eps=1e-10):\n    \"\"\"\n    Normalize activation values along the channel dimension.\n    Args:\n        - x: Tensor, the input activation tensor.\n        - eps: float, optional, small value to avoid division by zero.\n    Return:\n        normalized_activation: Tensor, the normalized activation tensor.\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `normalize_activation` function which normalizes the input tensor `x` along the channel dimension. This function assumes that the channel dimension is the first dimension (dimension 0) after the batch dimension.\n\n```python\nimport torch\n\ndef normalize_activation(x, eps=1e-10):\n    \"\"\"\n    Normalize activation values along the channel dimension.\n    Args:\n        - x: Tensor, the input activation tensor.\n        - eps: float, optional, small value to avoid division by zero.\n    Return:\n        normalized_activation: Tensor, the normalized activation tensor.\n    \"\"\"\n    # Calculate the sum along the channel dimension\n    sum_x = torch.sum(x, dim=1, keepdim=True)\n    \n    # Avoid division by zero by adding a small epsilon\n    sum_x = sum_x + eps\n    \n    # Normalize the activations\n    normalized_activation = x / sum_x\n    \n    return normalized_activation\n```\n\nThis function computes the sum of the activations along the channel dimension (using `torch.sum` with `dim=1`), adds a small epsilon to this sum to avoid division by zero, and then divides the original tensor `x` by this sum to normalize the activations. The `keepdim=True` argument in `torch.sum` ensures that the output tensor has the same number of dimensions as the input tensor, which is necessary for the division operation to broadcast correctly.",
  "canonical_solution": "    \n    norm_factor = torch.sqrt(torch.sum(x ** 2, dim=1, keepdim=True))\n    return x / (norm_factor + eps)\n\n\n\n",
  "unprocess_testcases": "a = torch.rand([1, 3, 5, 5])\nb = torch.rand([2, 3, 4, 4])\nc = torch.rand([1, 1, 1, 1])\n\nassert((SOLUTION_SIGNATURE(a) == normalize_activation(a)).all())\nassert((SOLUTION_SIGNATURE(b) == normalize_activation(b)).all())\nassert((SOLUTION_SIGNATURE(c) == normalize_activation(c)).all())\n"
 },
 {
  "task_id": "RWPB/84",
  "prompt": "import torch\n\ndef build_homog_matrix(R, t=None):\n    \"\"\"\n    Create a batch of homogeneous transformation matrices from given rotation matrices and translation vectors.\n    If the shape of R do not match the shape of t, please return ValueError\n\n    Args:\n    R (torch.Tensor): Rotation matrices of shape (B, Nj, 3, 3), where B is the batch size, and Nj is the number of joints.\n    t (torch.Tensor, optional): Translation vectors of shape (B, Nj, 3, 1). If None, a zero translation vector is used.\n    \n    Returns:\n    torch.Tensor: Homogeneous transformation matrices of shape (B, Nj, 4, 4).\n    \n    Process:\n    1. If the translation vector `t` is not provided, initialize it to zero vectors of the appropriate shape.\n    2. If the rotation matrix `R` is not provided, initialize it to identity matrices of the appropriate shape.\n    3. Ensure that the shapes of `R` and `t` match the expected dimensions.\n    4. Concatenate the rotation matrix `R` and translation vector `t` along the last dimension to form a (B, Nj, 3, 4) local transformation matrix.\n    5. Create a padding row [0, 0, 0, 1] and expand it to match the batch and joint dimensions, forming a (B, Nj, 1, 4) tensor.\n    6. Concatenate the padded row to the local transformation matrix to create a final homogeneous matrix of shape (B, Nj, 4, 4).\n    7. Return the resulting homogeneous transformation matrix.\n    \"\"\"\n",
  "solution": "```python\nimport torch\n\ndef build_homog_matrix(R, t=None):\n    \"\"\"\n    Create a batch of homogeneous transformation matrices from given rotation matrices and translation vectors.\n    If the shape of R do not match the shape of t, please return ValueError\n\n    Args:\n    R (torch.Tensor): Rotation matrices of shape (B, Nj, 3, 3), where B is the batch size, and Nj is the number of joints.\n    t (torch.Tensor, optional): Translation vectors of shape (B, Nj, 3, 1). If None, a zero translation vector is used.\n    \n    Returns:\n    torch.Tensor: Homogeneous transformation matrices of shape (B, Nj, 4, 4).\n    \"\"\"\n    B, Nj, _, _ = R.shape\n\n    # Step 1: Initialize translation vector if not provided\n    if t is None:\n        t = torch.zeros(B, Nj, 3, 1, dtype=R.dtype, device=R.device)\n    \n    # Step 2: Check if R and t have compatible shapes\n    if R.shape[0] != t.shape[0] or R.shape[1] != t.shape[1]:\n        raise ValueError(\"The shapes of R and t do not match.\")\n    \n    # Step 3: Concatenate R and t to form the upper part of the homogeneous matrix\n    upper_part = torch.cat([R, t], dim=-1)  # Shape: (B, Nj, 3, 4)\n    \n    # Step 4: Create the lower part [0, 0, 0, 1] of the homogeneous matrix\n    lower_part = torch.tensor([0, 0, 0, 1], dtype=R.dtype, device=R.device)\n    lower_part = lower_part.view(1, 1, 1, 4).repeat(B, Nj, 1, 1)  # Shape: (B, Nj, 1, 4)\n    \n    # Step 5: Concatenate the upper and lower parts to form the full homogeneous matrix\n    homog_matrix = torch.cat([upper_part, lower_part], dim=2)  # Shape: (B, Nj, 4, 4)\n    \n    return homog_matrix\n```\n\nThis function constructs a batch of homogeneous transformation matrices from rotation matrices and optional translation vectors. It handles cases where the translation vector is not provided by defaulting to zero vectors. It also ensures that the input tensors have compatible shapes and raises a `ValueError` if they do not match. The function concatenates the rotation and translation parts to form the upper part of the homogeneous matrix and appends a bottom row to complete the transformation matrix.",
  "canonical_solution": "    \n    \n    if t is None:\n        B = R.shape[0]\n        Nj = R.shape[1]\n        t = torch.zeros(B, Nj, 3, 1).to(R.device)\n    \n    if R is None:\n        B = t.shape[0]\n        Nj = t.shape[1]\n        R = torch.eye(3).unsqueeze(0).unsqueeze(0).repeat(B, Nj, 1, 1).to(t.device)\n    \n    B = t.shape[0]\n    Nj = t.shape[1]\n        \n    # import ipdb; ipdb.set_trace()\n    if R.shape != (B, Nj, 3, 3) or t.shape != (B, Nj, 3, 1):\n        return KeyError\n    \n    G = torch.cat([R, t], dim=-1) # BxJx3x4 local transformation matrix\n    pad_row = torch.FloatTensor([0, 0, 0, 1]).to(R.device).view(1, 1, 1, 4).expand(B, Nj, -1, -1) # BxJx1x4\n    G = torch.cat([G, pad_row], dim=2) # BxJx4x4 padded to be 4x4 matrix an enable multiplication for the kinematic chain\n\n    return G\n\n\n",
  "unprocess_testcases": "R = torch.eye(3).unsqueeze(0).unsqueeze(0).repeat(2, 3, 1, 1)\nt = torch.tensor([[[[1.0], [2.0], [3.0]], [[4.0], [5.0], [6.0]], [[7.0], [8.0], [9.0]]],\n                  [[[1.0], [0.0], [1.0]], [[2.0], [2.0], [2.0]], [[3.0], [3.0], [3.0]]]])\nassert(((SOLUTION_SIGNATURE(R=R, t=t) == build_homog_matrix(R=R, t=t))).all())\n\nR = torch.tensor([[[[0.0, -1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],\n                   [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]]])\nassert(SOLUTION_SIGNATURE(R=R, t=t) == build_homog_matrix(R=R, t=t))\n\nR = torch.eye(3).unsqueeze(0).unsqueeze(0).repeat(3, 2, 1, 1)\nt = torch.zeros(2, 3, 3, 1)  # Intentional shape mismatch to trigger an assertion error\nassert(SOLUTION_SIGNATURE(R=R, t=t) == build_homog_matrix(R=R, t=t))\n"
 },
 {
  "task_id": "RWPB/26",
  "prompt": "import torch\n\ndef to_one_hot(tensor, num_cls, dim=1, ignore_index=255):\n    \"\"\"\n    Converts a given tensor to a one-hot encoded tensor.\n\n    Args:\n        tensor (torch.Tensor): A tensor containing class indices.\n        num_cls (int): The total number of classes.\n        dim (int, optional): The dimension along which to apply one-hot encoding. Default is 1.\n        ignore_index (int, optional): A specific index that should be ignored during the conversion. Default is 255.\n\n    Returns:\n        torch.Tensor: A one-hot encoded tensor.\n\n    This function takes a tensor with class indices (usually integers) and converts it into a one-hot\n    encoded format. Entries with the 'ignore_index' are treated as background and set to zero before encoding.\n    The function uses the scatter_ method to expand the tensor along the specified 'dim' dimension\n    and sets the corresponding index to 1, indicating class presence.\n    \"\"\"\n",
  "solution": "Certainly! Below is the completion of the `to_one_hot` function which converts a given tensor containing class indices into a one-hot encoded tensor. The function handles an `ignore_index` by setting those positions to zero in the resulting tensor.\n\n```python\nimport torch\n\ndef to_one_hot(tensor, num_cls, dim=1, ignore_index=255):\n    \"\"\"\n    Converts a given tensor to a one-hot encoded tensor.\n\n    Args:\n        tensor (torch.Tensor): A tensor containing class indices.\n        num_cls (int): The total number of classes.\n        dim (int, optional): The dimension along which to apply one-hot encoding. Default is 1.\n        ignore_index (int, optional): A specific index that should be ignored during the conversion. Default is 255.\n\n    Returns:\n        torch.Tensor: A one-hot encoded tensor.\n\n    This function takes a tensor with class indices (usually integers) and converts it into a one-hot\n    encoded format. Entries with the 'ignore_index' are treated as background and set to zero before encoding.\n    The function uses the scatter_ method to expand the tensor along the specified 'dim' dimension\n    and sets the corresponding index to 1, indicating class presence.\n    \"\"\"\n    # Create a mask where ignore_index is present\n    mask = tensor == ignore_index\n\n    # Replace ignore_index with 0 temporarily for one-hot encoding\n    tensor = tensor.clone()\n    tensor[mask] = 0\n\n    # Prepare the tensor shape for one-hot encoding\n    shape = list(tensor.shape)\n    shape.insert(dim, num_cls)\n\n    # Create a one-hot encoded tensor with all zeros\n    one_hot = torch.zeros(shape, dtype=tensor.dtype, device=tensor.device)\n\n    # Use scatter_ to set the appropriate indices to 1\n    one_hot.scatter_(dim, tensor.unsqueeze(dim), 1)\n\n    # Reset the ignored indices to zero\n    one_hot.scatter_(dim, mask.unsqueeze(dim), 0)\n\n    return one_hot\n```\n\nThis function first creates a mask for the `ignore_index` in the input tensor. It then clones the tensor and temporarily replaces the `ignore_index` values with zero to facilitate one-hot encoding. After preparing the tensor shape, it initializes a zero tensor for the one-hot encoding. The `scatter_` method is used to set the indices corresponding to class labels to 1. Finally, it uses the mask to reset the positions of the `ignore_index` back to zero in the one-hot encoded tensor. This ensures that the `ignore_index` is properly ignored in the output tensor.",
  "canonical_solution": "\n    # Get dimensions of the input tensor\n    b, h, w = tensor.shape\n\n    # Set ignore_index to 0 to treat it as background\n    tensor[tensor == ignore_index] = 0\n\n    # Initialize a new tensor of zeros with dimensions for batch, classes, height, and width\n    onehot_tensor = torch.zeros(b, num_cls, h, w)\n\n    # Perform the one-hot encoding using scatter_\n    onehot_tensor.scatter_(dim, tensor.unsqueeze(dim), 1)\n\n    return onehot_tensor\n\n\n\n",
  "unprocess_testcases": "assert((SOLUTION_SIGNATURE(torch.tensor([[[1, 2], [0, 1]]]), num_cls = 3) == to_one_hot(torch.tensor([[[1, 2], [0, 1]]]), num_cls = 3)).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[[255, 1], [2, 255]]]), num_cls = 4, ignore_index = 255) == to_one_hot(torch.tensor([[[255, 1], [2, 255]]]), num_cls = 4, ignore_index = 255)).all())\nassert((SOLUTION_SIGNATURE(torch.tensor([[[0, 1, 2, 3, 4, 5]]]), num_cls = 1000) == to_one_hot(torch.tensor([[[0, 1, 2, 3, 4, 5]]]), num_cls = 1000)).all())\n"
 },
 {
  "task_id": "RWPB/77",
  "prompt": "def is_power_of_two(n):\n    \"\"\"\n    chat.openai.com/chat\n    Return True if n is a power of 2, otherwise return False.\n\n    The function is_power_of_two takes an integer n as input and returns True if n is a power of 2, otherwise it returns False.\n    The function works by first checking if n is less than or equal to 0. If n is less than or equal to 0, it can't be a power of 2, so the function returns False.\n    If n is greater than 0, the function checks whether n is a power of 2 by using a bitwise AND operation between n and n-1. If n is a power of 2, then it will have only one bit set to 1 in its binary representation. When we subtract 1 from a power of 2, all the bits to the right of that bit become 1, and the bit itself becomes 0. So, when we perform a bitwise AND between n and n-1, we get 0 if n is a power of 2, and a non-zero value otherwise.\n    Thus, if the result of the bitwise AND operation is 0, then n is a power of 2 and the function returns True. Otherwise, the function returns False.\n\n    \"\"\"\n",
  "solution": "    if n <= 0:\n        return False\n    return (n & (n - 1)) == 0\n\n# This function checks if the given integer n is a power of two. It first checks if n is less than or equal to zero, in which case it returns False since no non-positive number can be a power of two. If n is positive, it uses a bitwise AND operation between n and n-1. If the result is zero, it indicates that n is a power of two and returns True. Otherwise, it returns False.",
  "canonical_solution": "    \n    if n <= 0:\n        return False\n    return (n & (n - 1)) == 0\n\n\n",
  "unprocess_testcases": "assert(SOLUTION_SIGNATURE(0) == is_power_of_two(0))\nassert(SOLUTION_SIGNATURE(-16) == is_power_of_two(-16))\nassert(SOLUTION_SIGNATURE(16) == is_power_of_two(16))\nassert(SOLUTION_SIGNATURE(1024) == is_power_of_two(1024))\nassert(SOLUTION_SIGNATURE(18) == is_power_of_two(18))\nassert(SOLUTION_SIGNATURE(999) == is_power_of_two(999))\n"
 }
]